Computer science is the study of computation, automation, and information.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).[4][5][6] Computer science is generally considered an academic discipline and distinct from computer programming.[7]
The fundamental concern of computer science is determining what can and cannot be automated.[2][9][3][10][11] The Turing Award is generally recognized as the highest distinction in computer science.[12][13]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[17]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[21] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[24] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[25]
Although first proposed in 1956,[32] the term "computer science" appears in a 1959 article in Communications of the ACM,[33]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[34] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[33]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[35] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[36] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[37] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[43] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[44]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science,[45] mathematics,[46] or engineering.[47] Allen Newell and Herbert A. Simon argued in 1975, 
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[47]
 It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[47] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[47] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[47]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[47] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[47]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[48] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[49] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[50] identifiable in some branches of artificial intelligence).[51]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[52]
Computer science is no more about computers than astronomy is about telescopes.
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[29] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[56] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[57]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[58]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[59] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[60] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[61]
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[62] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[63] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[64] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[65]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[66] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[67]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[73]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[74][75] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[76]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[79] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[80] According to a 2021 report, only 51% of high schools in the US offer computer science.[81]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[82][83] and several others are following.[84]
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
To protect the wiki against automated account creation, we kindly ask you to enter the words that appear below in the box (more info):
To protect the wiki against automated account creation, we kindly ask you to enter the words that appear below in the box (more info):
Full help contents page
Training for students
A single-page guide to contributing
A training adventure game
Resources for new editors
People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using.
Many IP addresses change periodically, and are often shared by several people. You may create an account or log in to avoid future confusion with other logged out users. Creating an account also hides your IP address.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
Wikipedia is a compendium of the world's knowledge. If you know what you are looking for, type it into Wikipedia's search box. If, however, you need a bird's eye view of what Wikipedia has to offer, see its main contents pages below, which in turn list more specific pages.
Wikipedia's main contents systems are arranged into these subject classifications. Each subject is further divided into subtopics. 
Timelines list events chronologically, sometimes including links to articles with more detail.  There are several ways to find timelines:
You can help us keep Wikipedia up to date! The list below is for encyclopedia entries that describe and pertain to events happening on a current basis. 
Speaking of reference works, various third-party classification systems have been mapped to Wikipedia articles, which can be accessed from these pages:
Bibliographies list sources on a given topic, for verification or further reading outside Wikipedia:
Overview articles summarize in prose a broad topic like biology, and also have illustrations and links to subtopics like cell biology, biographies like Carl Linnaeus, and other related articles like Human Genome Project.
Outline pages have trees of topics in an outline format, which in turn are linked to further outlines and articles providing more detail.  Outlines show how important subtopics relate to each other based on how they are arranged in the tree, and they are useful as a more condensed, non-prose alternative to overview articles.
List pages enumerate items of a particular type, such as the List of sovereign states or List of South Africans.  Wikipedia has "lists of lists" when there are too many items to fit on a single page, when the items can be sorted in different ways, or as a way of navigating lists on a topic (for example Lists of countries and territories or Lists of people).  There are several ways to find lists:
Portals contain featured articles and images, news, categories, excerpts of key articles, links to related portals, and to-do lists for editors.  There are two ways to find portals:
Glossaries are lists of terms with definitions. Wikipedia includes hundreds of alphabetical glossaries; they can be found in two ways:
Wikipedia's collection of category pages is a classified index system.  It is automatically generated from category tags at the bottoms of articles and most other pages. Nearly all of the articles available so far on the website can be found through these subject indexes.
If you are simply looking to browse articles by topic, there are two top-level pages to choose from:
Category:Contents is technically at the top of the category hierarchy, but contains many categories useful to editors but not readers. Special:Categories lists every category alphabetically.
Vital articles are lists of subjects for which the English Wikipedia should have corresponding high-quality articles. They serve as centralized watchlists to track the quality status of Wikipedia's most important articles and to give editors guidance on which articles to prioritize for improvement.
Featured content represents the best of Wikipedia, and has undergone a thorough review process to ensure that it meets the highest encyclopedic standards. Presented by type:
Good articles are articles that meet a core set of editorial standards, the good article criteria, and successfully pass through the good article nomination process. They are well written, contain factually accurate and verifiable information, are broad in coverage, neutral in point of view, stable, and illustrated, where possible, by relevant images with suitable copyright licenses.
Growing collections of Wikipedia articles are starting to become available as spoken word recordings as well.
Wikipedia is a dynamic free online encyclopedia that anyone can edit in good faith, and tens of millions already have!
Wikipedia's purpose is to benefit readers by containing information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, whose articles also have numerous links to guide readers to more information.
Written collaboratively by largely anonymous volunteers, anyone with Internet access and not blocked, can write and make changes to Wikipedia articles (except in limited cases where editing is restricted to prevent disruption or vandalism). Since its creation on January 15, 2001, Wikipedia has grown into the world's largest reference website, attracting over a billion visitors monthly. It currently has more than sixty million articles in more than 300 languages, including 6,623,677 articles in English with 129,357 active contributors in the past month.
The fundamental principles of Wikipedia are summarized in its five pillars. The Wikipedia community has developed many policies and guidelines, but you do not need to be familiar with every one of them before contributing.
Anyone can edit Wikipedia's text, references, and images. What is written is more important than who writes it. The content must conform with Wikipedia's policies, including being verifiable by published sources. Editors' opinions, beliefs, personal experiences, unreviewed research, libelous material, and copyright violations will not remain. Wikipedia's software allows easy reversal of errors, and experienced editors watch and patrol bad edits.
Wikipedia differs from printed references in important ways. It is continually created and updated, and encyclopedic articles on new events appear within minutes rather than months or years. Because anyone can improve Wikipedia, it has become more comprehensive, clear, and balanced than any other encyclopedia. Its contributors improve the quality and quantity of the articles as well as remove misinformation, errors, and vandalism. Any reader can fix a mistake or add more information to articles (see Researching with Wikipedia). 
Wikipedia has tested the wisdom of the crowd since 2001 and found that it succeeds.
We could not find the above page on our servers.
Alternatively, you can visit the Main Page or read more information about this type of error.
This page provides help with the most common questions about Wikipedia.
You can also search Wikipedia's help pages using the search box below, or browse the Help menu or the Help directory.
The Readers' FAQ and our about page contain the most commonly sought information about Wikipedia.
There are other ways to browse and explore Wikipedia articles; many can be found at Wikipedia:Contents. See our disclaimer for cautions about Wikipedia's limitations.
For mobile access, press the mobile view link at the very bottom of every desktop view page.
Contributing is easy: see how to edit a page. For a quick summary on participating, see contributing to Wikipedia, and for a friendly tutorial, see our introduction. For a listing of introductions and tutorials by topic, see getting started. The Simplified Manual of Style and Cheatsheet can remind you of basic wiki markup.
The simple guide to vandalism cleanup can help you undo malicious edits.
If you're looking for places you can help out, the Task Center is the place to go, or check out what else is happening at the community portal. You can practice editing and experiment in a sandboxyour sandbox.
If there is a problem with an article about yourself, a family member, a friend or a colleague, please read Biographies of living persons/Help.
If you spot a problem with an article, you can fix it directly, by clicking on the "Edit" link at the top of that page. See the "edit an article" section of this page for more information.
If you don't feel ready to fix the article yourself, post a message on the article's talk page. This will bring the matter to the attention of others who work on that article. There is a "Talk" link at the beginning of every article page.
Check Your first article to see if your topic is appropriate, then the Article wizard will walk you through creating the article.
Once you have created an article, see Writing better articles for guidance on how to improve it and what to include (like reference citations).
For contributing images, audio or video files, see the Introduction to uploading images. Then the Upload wizard will guide you through that process.
Answers to common problems can be found at frequently asked questions.
Or check out where to ask questions or make comments.
New users having problems editing Wikipedia should ask at the Teahouse. More complex questions can be posed at the Help desk. Volunteers will respond as soon as they're able.
Or ask for help on your talk page and a volunteer will visit you there!
You can get live help with editing in the help chatroom.
For help with technical issues, ask at the Village pump.
If searching Wikipedia has not answered your question (for example, questions like "Which country has the world's largest fishing fleet?"), try the Reference Desk. Volunteers there will attempt to answer your questions on any topic, or point you toward the information you need.
Screen readers are a form of assistive technology for people with disabilities. A list of screen readers is available including a section, Software aids for people with reading difficulties.
Reader software examples include Spoken Web, JAWS, and NonVisual Desktop Access (NVDA). In addition, Fangs screen reader emulator is an open-source extension for the Pale Moon browser that simulates how a web page would look in JAWS.
Full help contents page
Training for students
A single-page guide to contributing
A training adventure game
Resources for new editors
This page provides a listing of current collaborations, tasks, and news about English Wikipedia. New to Wikipedia? See the contributing to Wikipedia page or our tutorial for everything you need to know to get started. For a listing of internal project pages of interest, see the department directory.
For a listing of ongoing discussions and current requests, see the Dashboard.
You can help improve the articles listed below! This list updates frequently, so check back here for more tasks to try. (See Wikipedia:Maintenance or the  Task Center for further information.)
Help counter systemic bias by creating new articles on important women.
Welcome to the community bulletin board, which is a page used for announcements from WikiProjects and other groups. Included here are coordinated efforts, events, projects, and other general announcements.
Also consider posting WikiProject, Task Force, and Collaboration news at The Signpost's WikiProject Report page.
Latest tech news from the Wikimedia technical community. Please tell other users about these changes. Not all changes will affect you. Translations are available.
Discussions in the following areas have requested wider attention via Requests for comment:
The School and university projects page collects information about Wikipedia editing projects for school and university classes, including an archive of many past class projects.
A list of current classes using Wikipedia can be found at current projects.
Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.
Uploads locally to Wikipedia; must comply with the non-free content criteria
Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript.
Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again.
Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. 
In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed.
The filename you chose seems to be very short, or overly generic. Please don't use:
If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
This should not be done, except in very rare exceptional cases.
Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.
Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the Wikimedia Commons.
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
Please note that by "entirely self-made" we really mean just that. 
Do not use this section for any of the following:
Editors who falsely declare such items as their "own work" will be blocked from editing.
Use this only if there is an explicit licensing statement in the source. 
The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
If the source website doesn't say so explicitly, please do not upload the file.
Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it.
 Please remember that you will need to demonstrate that:
Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually.
Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
If this is an article you are only planning to write, please write it first and upload the file afterwards.
The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
Please upload this file only if it is going to be used in an actual article.
If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
The page Example is not a real article, but a disambiguation page pointing to a number of other pages.
Please check and enter the exact title of the actual target article you meant.
If neither of these two statements applies, then please do not upload this image.
This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
In view of this, please explain how the use of this file will be minimal.
Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you.
This is the data that will be submitted to upload:
This might take a minute or two, depending on the size of the file and the speed of your internet connection.
Once uploading is completed, you will find your new file at this link:
Your file has been uploaded successfully and can now be found here:
Please follow the link and check that the image description page has all the information you meant to include.
If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
To insert this file into an article, you may want to use code similar to the following:
If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages.
Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page.
Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes.
Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.
Uploads locally to Wikipedia; must comply with the non-free content criteria
Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript.
Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again.
Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. 
In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed.
The filename you chose seems to be very short, or overly generic. Please don't use:
If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
This should not be done, except in very rare exceptional cases.
Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.
Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the Wikimedia Commons.
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
Please note that by "entirely self-made" we really mean just that. 
Do not use this section for any of the following:
Editors who falsely declare such items as their "own work" will be blocked from editing.
Use this only if there is an explicit licensing statement in the source. 
The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
If the source website doesn't say so explicitly, please do not upload the file.
Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it.
 Please remember that you will need to demonstrate that:
Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually.
Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
If this is an article you are only planning to write, please write it first and upload the file afterwards.
The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
Please upload this file only if it is going to be used in an actual article.
If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
The page Example is not a real article, but a disambiguation page pointing to a number of other pages.
Please check and enter the exact title of the actual target article you meant.
If neither of these two statements applies, then please do not upload this image.
This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
In view of this, please explain how the use of this file will be minimal.
Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you.
This is the data that will be submitted to upload:
This might take a minute or two, depending on the size of the file and the speed of your internet connection.
Once uploading is completed, you will find your new file at this link:
Your file has been uploaded successfully and can now be found here:
Please follow the link and check that the image description page has all the information you meant to include.
If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
To insert this file into an article, you may want to use code similar to the following:
If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages.
Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page.
This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators).
Computer science is the study of computation, automation, and information.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).[4][5][6] Computer science is generally considered an academic discipline and distinct from computer programming.[7]
The fundamental concern of computer science is determining what can and cannot be automated.[2][9][3][10][11] The Turing Award is generally recognized as the highest distinction in computer science.[12][13]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[17]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[21] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[24] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[25]
Although first proposed in 1956,[32] the term "computer science" appears in a 1959 article in Communications of the ACM,[33]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[34] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[33]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[35] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[36] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[37] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[43] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[44]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science,[45] mathematics,[46] or engineering.[47] Allen Newell and Herbert A. Simon argued in 1975, 
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[47]
 It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[47] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[47] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[47]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[47] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[47]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[48] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[49] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[50] identifiable in some branches of artificial intelligence).[51]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[52]
Computer science is no more about computers than astronomy is about telescopes.
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[29] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[56] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[57]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[58]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[59] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[60] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[61]
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[62] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[63] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[64] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[65]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[66] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[67]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[73]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[74][75] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[76]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[79] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[80] According to a 2021 report, only 51% of high schools in the US offer computer science.[81]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[82][83] and several others are following.[84]
This page is a member of 22 hidden categories (help):
Pages transcluded onto the current version of this page (help):
Please remember to check your manual of style, standards guide or instructor's guidelines for the exact syntax to suit your needs.  For more detailed advice, see Citing Wikipedia.
Wikipedia contributors. (2023, February 24). Computer science. In Wikipedia, The Free Encyclopedia. Retrieved 16:08, February 26, 2023, from https://en.wikipedia.org/w/index.php?title=Computer_science&oldid=1141311918
Wikipedia contributors. "Computer science." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 24 Feb. 2023. Web. 26 Feb. 2023.
Wikipedia contributors, 'Computer science',  Wikipedia, The Free Encyclopedia, 24 February 2023, 12:03 UTC, <https://en.wikipedia.org/w/index.php?title=Computer_science&oldid=1141311918> [accessed 26 February 2023]
Wikipedia contributors, "Computer science,"  Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Computer_science&oldid=1141311918 (accessed February 26, 2023).
Wikipedia contributors. Computer science [Internet].  Wikipedia, The Free Encyclopedia;  2023 Feb 24, 12:03 UTC [cited 2023 Feb 26].  Available from: 
https://en.wikipedia.org/w/index.php?title=Computer_science&oldid=1141311918.
Wikipedia contributors. Computer science. Wikipedia, The Free Encyclopedia. February 24, 2023, 12:03 UTC. Available at: https://en.wikipedia.org/w/index.php?title=Computer_science&oldid=1141311918. Accessed February 26, 2023.
When using the LaTeX package url (\usepackage{url} somewhere in the preamble), which tends to give much more nicely formatted web addresses, the following may be preferred:
Computer science is the study of computation, automation, and information.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).[4][5][6] Computer science is generally considered an academic discipline and distinct from computer programming.[7]
The fundamental concern of computer science is determining what can and cannot be automated.[2][9][3][10][11] The Turing Award is generally recognized as the highest distinction in computer science.[12][13]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[17]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[21] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[24] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[25]
Although first proposed in 1956,[32] the term "computer science" appears in a 1959 article in Communications of the ACM,[33]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[34] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[33]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[35] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[36] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[37] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[43] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[44]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science,[45] mathematics,[46] or engineering.[47] Allen Newell and Herbert A. Simon argued in 1975, 
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[47]
 It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[47] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[47] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[47]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[47] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[47]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[48] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[49] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[50] identifiable in some branches of artificial intelligence).[51]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[52]
Computer science is no more about computers than astronomy is about telescopes.
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[29] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[56] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[57]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[58]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[59] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[60] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[61]
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[62] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[63] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[64] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[65]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[66] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[67]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[73]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[74][75] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[76]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[79] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[80] According to a 2021 report, only 51% of high schools in the US offer computer science.[81]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[82][83] and several others are following.[84]
Computer science is the study of computation, automation, and information.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).[4][5][6] Computer science is generally considered an academic discipline and distinct from computer programming.[7]
The fundamental concern of computer science is determining what can and cannot be automated.[2][9][3][10][11] The Turing Award is generally recognized as the highest distinction in computer science.[12][13]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[17]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[21] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[24] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[25]
Although first proposed in 1956,[32] the term "computer science" appears in a 1959 article in Communications of the ACM,[33]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[34] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[33]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[35] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[36] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[37] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[43] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[44]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science,[45] mathematics,[46] or engineering.[47] Allen Newell and Herbert A. Simon argued in 1975, 
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[47]
 It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[47] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[47] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[47]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[47] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[47]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[48] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[49] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[50] identifiable in some branches of artificial intelligence).[51]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[52]
Computer science is no more about computers than astronomy is about telescopes.
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[29] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[56] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[57]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[58]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[59] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[60] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[61]
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[62] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[63] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[64] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[65]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[66] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[67]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[73]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[74][75] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[76]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[79] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[80] According to a 2021 report, only 51% of high schools in the US offer computer science.[81]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[82][83] and several others are following.[84]
To fill out this checklist, please add the following code to the template call:
 This article was the subject of a Wiki Education Foundation-supported course assignment, between 26 January 2021 and 29 April 2021. Further details are available on the course page. Student editor(s): Foxccon.
 This article was the subject of a Wiki Education Foundation-supported course assignment, between 19 January 2022 and 13 May 2022. Further details are available on the course page. Student editor(s): Sh3nl0ng16.
how its work and many function are we can do.
Split the category of Theoretical Programming in the Theoretical Computer Science section, and Programming as a discipline in Applied Computer Science Ejenriquez (talk) 04:48, 31 December 2022 (UTC)Reply[reply]
In the field sections, Artificial Intelligence is on 4.3, above Applied Computer Science (4.4). Should we swap their positions?
The numerical category might be just arbitrary, but I feel like Applied Computer Science should be right next to Theoretical Computer Science down, and Computer Systems could be dropped down too. So there's like a sense or prioritization.
Artificial Intelligence seems to be a more recently development in computer science, so it could be the last on the list. Ejenriquez (talk) 01:32, 1 January 2023 (UTC)Reply[reply]
Computer science is the study of computation, automation, and information.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).[4][5][6] Computer science is generally considered an academic discipline and distinct from computer programming.[7]
The fundamental concern of computer science is determining what can and cannot be automated.[2][9][3][10][11] The Turing Award is generally recognized as the highest distinction in computer science.[12][13]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[17]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[21] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[24] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[25]
Although first proposed in 1956,[32] the term "computer science" appears in a 1959 article in Communications of the ACM,[33]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[34] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[33]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[35] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[36] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[37] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[43] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[44]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science,[45] mathematics,[46] or engineering.[47] Allen Newell and Herbert A. Simon argued in 1975, 
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[47]
 It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[47] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[47] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[47]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[47] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[47]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[48] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[49] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[50] identifiable in some branches of artificial intelligence).[51]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[52]
Computer science is no more about computers than astronomy is about telescopes.
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[29] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[56] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[57]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[58]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[59] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[60] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[61]
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[62] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[63] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[64] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[65]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[66] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[67]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[73]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[74][75] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[76]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[79] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[80] According to a 2021 report, only 51% of high schools in the US offer computer science.[81]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[82][83] and several others are following.[84]
You do not have permission to edit this page, for the following reason:
Pages transcluded onto the current version of this page (help):
Computer science is the study of computation, automation, and information.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).[4][5][6] Computer science is generally considered an academic discipline and distinct from computer programming.[7]
The fundamental concern of computer science is determining what can and cannot be automated.[2][9][3][10][11] The Turing Award is generally recognized as the highest distinction in computer science.[12][13]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[17]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[21] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[24] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[25]
Although first proposed in 1956,[32] the term "computer science" appears in a 1959 article in Communications of the ACM,[33]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[34] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[33]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[35] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[36] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[37] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[43] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[44]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science,[45] mathematics,[46] or engineering.[47] Allen Newell and Herbert A. Simon argued in 1975, 
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[47]
 It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[47] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[47] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[47]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[47] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[47]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[48] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[49] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[50] identifiable in some branches of artificial intelligence).[51]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[52]
Computer science is no more about computers than astronomy is about telescopes.
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[29] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[56] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[57]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[58]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[59] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[60] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[61]
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[62] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[63] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[64] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[65]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[66] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[67]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[73]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[74][75] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[76]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[79] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[80] According to a 2021 report, only 51% of high schools in the US offer computer science.[81]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[82][83] and several others are following.[84]
You do not have permission to edit this page, for the following reason:
Pages transcluded onto the current version of this page (help):
In some circumstances, pages may need to be protected from modification by certain groups of editors. Pages are protected when a specific damaging event has been identified that can not be prevented through other means such as a block. Otherwise, Wikipedia is built on the principle that anyone can edit it, and it therefore aims to have as many of its pages as possible open for public editing so that anyone can add material and correct errors. This policy states in detail the protection types and procedures for page protection and unprotection and when each protection should and should not be applied.
Protection is a technical restriction applied only by administrators, although any user may request protection. Protection can be indefinite or expire after a specified time. The various levels of protection are detailed below, and they can be applied to the page edit, page move, page create, and file upload actions. Even when a page is protected from editing, the source code (wikitext) of the page can still be viewed and copied by anyone.
A protected page is marked at its top right by a padlock icon, usually added by the {{pp-protected}} template.
Applying page protection as a preemptive measure is contrary to the open nature of Wikipedia and is generally not allowed if applied solely for these reasons. However, brief periods of an appropriate and reasonable protection level are allowed in situations where blatant vandalism, disruption, or abuse is occurring by multiple users and at a level of frequency that requires its use in order to stop it. The duration of the protection should be set as short as possible, and the protection level should be set to the lowest restriction needed in order to stop the disruption while still allowing productive editors to make changes.
The following technical options are available to administrators for protecting different actions to pages:
The following technical options are available to administrators for adding protection levels to the different actions to pages:
Any type of protection (with the exception of cascading protection) may be requested at Wikipedia:Requests for page protection. Changes to a protected page should be proposed on the corresponding talk page, and then (if necessary) requested by adding an edit request. From there, if the requested changes are uncontroversial or if there is consensus for them, the changes can be carried out by a user who can edit the page.
Except in the case of office actions (see below), Arbitration Committee remedies, or pages in the MediaWiki namespace (see below), administrators may unprotect a page if the reason for its protection no longer applies, a reasonable period has elapsed, and there is no consensus that continued protection is necessary. Editors desiring the unprotection of a page should, in the first instance, ask the administrator who applied the protection unless the administrator is inactive or no longer an administrator; thereafter, requests may be made at Requests for unprotection. Note that such requests will normally be declined if the protecting administrator is active and was not consulted first. A log of protections and unprotections is available at Special:Log/protect.
Semi-protected pages like this page cannot be edited by unregistered users (IP addresses), as well as accounts that are not confirmed or autoconfirmed (accounts that are at least four days old and have made at least ten edits to Wikipedia). Semi-protection is useful when there is a significant amount of disruption or vandalism from new or unregistered users, or to prevent sockpuppets of blocked or banned users from editing, especially when it occurs on biographies of living persons who have had a recent high level of media interest. An alternative to semi-protection is pending changes, which is sometimes favored when an article is being vandalized regularly, but otherwise receives a low amount of editing.
Such users can request edits to a semi-protected page by proposing them on its talk page, using the {{Edit semi-protected}} template if necessary to gain attention. If the page in question and its talk page are both protected, the edit request should be made at Wikipedia:Requests for page protection instead. New users may also request the confirmed user right at Wikipedia:Requests for permissions/Confirmed.
Administrators may apply indefinite semi-protection to pages that are subject to heavy and persistent vandalism or violations of content policy (such as biographies of living persons, neutral point of view). Semi-protection should not be used as a preemptive measure against vandalism that has not yet occurred or to privilege registered users over unregistered users in (valid) content disputes.
In addition, administrators may apply temporary semi-protection on pages that are:
Today's featured article may be semi-protected just like any other article. However since the article is subject to sudden spurts of vandalism during certain times of day, administrators should semi-protect it for brief periods of time in most instances. For the former guideline, see Wikipedia:Main Page featured article protection.
When a page under pending changes protection is edited by an unregistered (IP addresses) editor or a new user, the edit is not directly visible to the majority of Wikipedia readers, until it is reviewed and accepted by an editor with the pending changes reviewer right. When a page under pending changes protection is edited by an autoconfirmed user, the edit will be immediately visible to Wikipedia readers, unless there are pending edits waiting to be reviewed.
Pending changes are visible in the page history, where they are marked as pending review. Readers that are not logged in (the vast majority of readers) are shown the latest accepted version of the page; logged-in users see the latest version of the page, with all changes (reviewed or not) applied. When editors who are not reviewers make changes to an article with unreviewed pending changes, their edits are also marked as pending and are not visible to most readers.
A user who clicks "edit this page" is always, at that point, shown the latest version of the page for editing regardless of whether the user is logged in or not.
Reviewing of pending changes should be resolved within reasonable time limits.
Pending changes protection should not be used as a preemptive measure against violations that have not yet occurred. Like semi-protection, PC protection should never be used in genuine content disputes, where there is a risk of placing a particular group of editors (unregistered users) at a disadvantage. Pending changes protection should not be used on articles with a very high edit rate, even if they meet the aforementioned criteria. Instead, semi-protection should be considered.
In addition, administrators may apply temporary pending changes protection on pages that are subject to significant but temporary vandalism or disruption (for example, due to media attention) when blocking individual users is not a feasible option. As with other forms of protection, the time frame of the protection should be proportional to the problem. Indefinite PC protection should be used only in cases of severe long-term disruption.
Removal of pending changes protection can be requested of any administrator, or at requests for unprotection.
The reviewing process is described in detail at Wikipedia:Reviewing pending changes.
Administrators can prevent the creation of pages. This type of protection is useful for pages that have been deleted but repeatedly recreated. Such protection is case-sensitive. There are several levels of creation protection that can be applied to pages, identical to the levels for edit protection. A list of protected titles may be found at Special:ProtectedTitles (see also historical lists).
Pre-emptive restrictions on new article titles are instituted through the title blacklist system, which allows for more flexible protection with support for substrings and regular expressions.
Pages that have been creation-protected are sometimes referred to as "salted". Editors wishing to re-create a salted title with appropriate content should either contact an administrator (preferably the protecting administrator), file a request at Wikipedia:Requests for page protection#Current requests for reduction in protection level, or use the deletion review process. To make a convincing case for re-creation, it is helpful to show a draft version of the intended article when filing a request.
While creation-protection is usually permanent, temporary creation protection may be applied if a page is repeatedly recreated by a single user (or sockpuppets of that user, if applicable).
Move-protected pages, or more technically, fully move-protected pages, cannot be moved to a new title except by an administrator. Move protection is commonly applied to:
As with full edit protection, protection because of edit warring should not be considered an endorsement of the current name. When move protection is applied during a requested move discussion, the page should be protected at the location it was at when the move request was started.
All files are implicitly move-protected; only file movers and administrators can rename files.
Upload-protected files, or more technically, fully upload-protected files, cannot be replaced with new versions except by an administrator. Upload protection does not protect file pages from editing. It may be applied by an administrator to:
As with full edit protection, administrators should avoid favoring one version over another, and protection should not be considered an endorsement of the current version. An exception to this rule is when they are protected due to upload vandalism.
Extended confirmed protection, also known as 30/500 protection, allows edits only by editors with the extended confirmed user access level, granted automatically to registered users with at least 30 days tenure and at least 500 edits.
Where semi-protection has proven to be ineffective, administrators may use extended confirmed protection to combat disruption (such as vandalism, abusive sockpuppetry, edit wars, etc.) on any topic.[2] Extended confirmed protection should not be used as a preemptive measure against disruption that has not yet occurred, nor should it be used to privilege extended confirmed users over unregistered/new users in valid content disputes (except as general sanction enforcement; see below).[1]
Two topic areas are under Arbitration Committee "extended confirmed restrictions" as a general sanction, in which only extended confirmed users may edit affected content; one is under a similar community general sanction. The extended confirmed restriction slightly differs from the earlier "30/500 restriction", which was independent of extended confirmed status. Administrators are authorized to enforce this restriction through extended confirmed protection or any other means.[3] It applies to:
When necessary to prevent disruption in designated contentious topic areas, administrators are authorized to make protections at any level. (This is distinct from the topic-wide restrictions discussed above.) Some community sanctions grant similar discretionary authorizations.
High-risk templates may be extended-confirmed protected at administrator discretion when template protection would be too restrictive and semi-protection would be ineffective to stop widespread disruption.[8]
Extended confirmed protection may be applied at the discretion of an administrator when creation-protecting a page.[1]
As of September 23, 2016, a bot posts a notification in a subsection of AN when this protection level is used.[9] Any protection made as arbitration enforcement must be logged at Wikipedia:Arbitration enforcement log. A full list of the 4188 pages under 30/500 protection can be found here.
Users can request edits to an extended confirmed-protected page by proposing them on its talk page, using the {{Edit extended-protected}} template if necessary to gain attention.
A template-protected page can be edited only by administrators or users in the Template editors group. This protection level should be used almost exclusively on high-risk templates and modules. In cases where pages in other namespaces become transcluded to a very high degree, this protection level is also valid.
Editors may request edits to a template-protected page by proposing them on its talk page, using the {{Edit template-protected}} template if necessary to gain attention.
A fully protected page cannot be edited or moved by anyone except administrators. The protection may be for a specified time or may be indefinite.
Modifications to a fully protected page can be proposed on its talk page (or at another appropriate forum) for discussion. Administrators can make changes to the protected article reflecting consensus. Placing the {{Edit fully-protected}} template on the talk page will draw the attention of administrators for implementing uncontroversial changes.
While content disputes and edit warring may be addressed with user blocks issued by uninvolved administrators, allowing normal page editing by other editors at the same time, the protection policy provides an alternative approach as administrators have the discretion to temporarily fully protect an article to end an ongoing edit war. This approach may better suit multi-party disputes and contentious content, as it makes talk page consensus a requirement for implementation of requested edits.
When protecting a page because of a content dispute, administrators have a duty to avoid protecting a version that contains policy-violating content, such as vandalism, copyright violations, defamation, or poor-quality coverage of living people. Administrators are deemed to remain uninvolved when exercising discretion on whether to apply protection to the current version of an article, or to an older, stable, or pre-edit-war version.
Fully protected pages may not be edited except to make changes that are uncontroversial or for which there is clear consensus. Editors convinced that the protected version of an article contains policy-violating content, or that protection has rewarded edit warring or disruption by establishing a contentious revision, may identify a stable version prior to the edit war and request reversion to that version. Before making such a request, editors should consider how independent editors might view the suggestion and recognize that continuing an edit war is grounds for being blocked.
Administrators who have made substantive content changes to an article are considered involved and must not use their advanced permissions to further their own positions. When involved in a dispute, it is almost always wisest to respect the editing policies that bind all editors and call for input from an uninvolved administrator, rather than to invite controversy by acting unilaterally.
If a deleted page is going through deletion review, only administrators are normally capable of viewing the former content of the page. If they feel it would benefit the discussion to allow other users to view the page content, administrators may restore the page, blank it or replace the contents with {{Temporarily undeleted}} template or a similar notice, and fully protect the page to prevent further editing. The previous contents of the page are then accessible to everyone via the page history.
Generic file names such as File:Photo.jpg, File:Example.jpg, File:Map.jpg, and File:Sound.wav are fully protected to prevent new versions from being uploaded. Furthermore, File:Map.jpg and File:Sound.wav are salted.
Cascading protection fully protects a page, and extends that full protection automatically to any page that is transcluded onto the protected page, whether directly or indirectly. This includes templates, images and other media that are hosted on the English Wikipedia. Files stored on Commons are not protected by any other wiki's cascading protection and, if they are to be protected, must be either temporarily uploaded to the English Wikipedia or explicitly protected at Commons (whether manually or through cascading protection there). When operational, KrinkleBot cascade-protects Commons files transcluded at Wikipedia:Main Page/Tomorrow, Wikipedia:Main Page/Commons media protection and Main Page. As the bot's response time varies, media should not be transcluded on the main page (or its constituent templates) until after it has been protected. (This is particularly relevant to Template:In the news, for which upcoming images are not queued at Wikipedia:Main Page/Tomorrow.) Cascading protection:
The list of cascading-protected pages can be found at Wikipedia:Cascade-protected items. Requests to add or remove cascading protection on a page should be made at Wikipedia talk:Cascade-protected items as an edit request.
Administrators cannot change or remove the protection for some areas on Wikipedia, which are permanently protected by the MediaWiki software:
Such protection is called permanent or indefinite protection, and interface protection in the case of CSS and JavaScript pages.
In addition to hard-coded protection, the following are usually fully protected for an indefinite period of time (though not necessarily with interface protection):
Superprotect was a level of protection, allowing editing only by Wikimedia Foundation employees who are in the Staff global group. It was implemented on August 10, 2014, and used the same day to override community consensus regarding the use of the Media Viewer on the German Wikipedia's primary site JavaScript, common.js. It was never used on the English Wikipedia. On November 5, 2015, the WMF decided to remove superprotect from all Wikimedia wikis.
The Gadget and Gadget definition namespaces have namespace-wide protection, and the permissions to edit them are only available to WMF Staff.  There is one page on the English Wikipedia in these namespaces. A request for local access to this namespace has been pending since 2019.
Cascading semi-protection was formerly possible, but it was disabled in 2007 after users noticed that non-administrators could fully protect any page by transcluding it onto the page to which cascading semi-protection had been applied by an administrator.
Modifications to a protected page can be proposed on its talk page (or at another appropriate forum) for discussion. Administrators can make changes to the protected article reflecting consensus. Placing the {{Edit protected}} template on the talk page will draw the attention of administrators for implementing uncontroversial changes.
Talk pages are not usually protected, and are semi-protected only for a limited duration in the most severe cases of vandalism.
User talk pages are rarely protected. However, protection may be applied if there is severe vandalism or abuse. Users whose talk pages are protected may wish to have an unprotected user talk subpage linked conspicuously from their main talk page to allow good-faith comments from users that the protection restricts editing from.
A user's request to have their own talk page protected is not a sufficient rationale by itself to protect the page, although requests may be considered if a reason is provided.
Blocked users' user talk pages should not ordinarily be protected, as this interferes with the user's ability to contest their block through the normal process. It also prevents others from being able to use the talk page to communicate with the blocked editor.
In extreme cases of abuse by the blocked user, such as abuse of the {{unblock}} template, re-blocking the user with talk page access removed should be preferred over applying protection to the page. If the user has been blocked and with the ability to edit their user talk page disabled, they should be informed of this in a block notice, subsequent notice, or message, and it should include information and instructions for appealing their block off-wiki, such as through the UTRS tool interface or, as a last recourse, the Arbitration Committee.
When required, protection should be implemented for only a brief period, not exceeding the duration of the block.
Confirmed socks of registered users should be dealt with in accordance with Wikipedia:Sockpuppetry; their pages are not normally protected.
Base user pages (for example, the page User:Example, and not User:Example/subpage or User talk:Example) are automatically protected from creation or editing by unconfirmed accounts and anonymous IP users. An exception to this includes an unconfirmed registered account attempting to create or edit their own user page. IP editors and unconfirmed accounts are also unable to create or edit user pages that do not belong to a currently-registered account. This protection is enforced by an edit filter.[13] Users may opt-out of this protection by placing {{unlocked userpage}} anywhere on their own user page.
User pages and subpages within their own user space may be protected upon a request from the user, as long as a need exists. Pages within the user space should not be automatically or pre-emptively protected without good reason or cause.[14][15] Requests for protection specifically at uncommon levels (such as template protection) may be granted if the user has expressed a genuine and realistic need.
When a filter is insufficient to stop user page vandalism, a user may choose to create a ".css" subpage (ex. User:Example/Userpage.css), copy all the contents of their user page onto the subpage, transclude the subpage by putting {{User:Example/Userpage.css}} on their user page, and then ask an administrator to fully protect their user page. Because user space pages that end in ".css", ".js", and ".json" are editable only by the user to which that user space belongs (and interface administrators), this will protect one's user page from further vandalism.
In the event of the confirmed death of a user, the user's user page (but not the user talk page) should be fully protected.
Protected templates should normally have the {{documentation}} template. It loads the unprotected /doc page, so that non-admins and IP-users can edit the documentation, categories and interwiki links. It also automatically adds {{pp-template}} to protected templates, which displays a small padlock in the top right corner and categorizes the template as protected. Only manually add {{pp-template}} to protected templates that don't use {{documentation}} (mostly the flag templates).
Cascading protection should generally not be applied directly to templates, as it will not protect transclusions inside <includeonly> tags or transclusions that depend on template parameters, but will protect the template's documentation subpage. Instead, consider any of the following:
Note: All editnotice templates (except those in userspace) are already protected via MediaWiki:Titleblacklist. They can be edited by admins, template editors and page movers only.
Sandboxes should not ordinarily be protected since their purpose is to let new users test and experiment with wiki syntax. Most sandboxes are automatically cleaned every 12 hours, although they are frequently overwritten by other testing users. The Wikipedia:Sandbox is cleaned every hour. Those who use sandboxes for malicious purposes, or to violate policies such as no personal attacks, civility, or copyrights, should instead be warned and/or blocked.
The following templates may be added at the very top of a page to indicate that it is protected:
On redirect pages, use the {{Redirect category shell}} template, which automatically categorizes by protection level, below the redirect line. A protection template may also be added below the redirect line, but it will serve only to categorize the page, as it will not be visible on the page, and it will have to be manually removed when protection is removed.
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages. Programming language theory is closely related to other fields including mathematics, software engineering, and linguistics. There are a number of academic conferences and journals in the area.
In some ways, the history of programming language theory predates even the development of programming languages themselves. The lambda calculus, developed by Alonzo Church and Stephen Cole Kleene in the 1930s, is considered by some to be the world's first programming language, even though it was intended to model computation rather than being a means for programmers to describe algorithms to a computer system. Many modern functional programming languages have been described as providing a "thin veneer" over the lambda calculus,[2] and many are easily described in terms of it.
Some other key events in the history of programming language theory since then:
There are several fields of study which either lie within programming language theory, or which have a profound influence on it; many of these have considerable overlap. In addition, PLT makes use of many other branches of mathematics, including computability theory, category theory, and set theory.
Formal semantics is the formal specification of the behaviour of computer programs and programming languages. Three common approaches to describe the semantics or "meaning" of a computer program are denotational semantics, operational semantics and axiomatic semantics.
Type theory is the study of type systems; which are "a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute".[4] Many programming languages are distinguished by the characteristics of their type systems.
Program analysis is the general problem of examining a program and determining key characteristics (such as the absence of classes of program errors). Program transformation is the process of transforming a program in one form (language) to another form.
Comparative programming language analysis seeks to classify programming languages into different types based on their characteristics; broad categories of programming languages are often known as programming paradigms.
Metaprogramming is the generation of higher-order programs which, when executed, produce programs (possibly in a different language, or in a subset of the original language) as a result.
Domain-specific languages are languages constructed to efficiently solve problems of a particular part of domain.
Compiler theory is the theory of writing compilers (or more generally, translators); programs which translate a program written in one language into another form. The actions of a compiler are traditionally broken up into syntax analysis (scanning and parsing), semantic analysis (determining what a program should do), optimization (improving the performance of a program as indicated by some metric; typically execution speed) and code generation (generation and output of an equivalent program in some target language; often the instruction set of a CPU).
Run-time systems refer to the development of programming language runtime environments and their components, including virtual machines, garbage collection, and foreign function interfaces.
Conferences are the primary venue for presenting research in programming languages. The most well known conferences include the Symposium on Principles of Programming Languages (POPL), Programming Language Design and Implementation (PLDI), the International Conference on Functional Programming (ICFP), the International Conference on Object Oriented Programming, Systems, Languages and Applications (OOPSLA) and the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS).
Notable journals that publish PLT research include the ACM Transactions on Programming Languages and Systems (TOPLAS), Journal of Functional Programming (JFP), Journal of Functional and Logic Programming, and Higher-Order and Symbolic Computation.
If you have an image of similar quality that can be published under a suitable copyright license, be sure to upload it, tag it, and nominate it.
This image has been assessed under the valued image criteria and is considered the most valued image on Commons within the scope: Quicksort, animation. You can see its nomination here.
Click on a date/time to view the file as it appeared at that time.
In theoretical computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.
A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.[1]
Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.
A computational problem can be viewed as an infinite collection of instances together with a set (possibly empty) of solutions for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g., 15) and the solution is "yes" if the number is prime and "no" otherwise (in this case, 15 is not prime and the answer is "no"). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.
When considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.
Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.
Decision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either yes or no, or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer yes, the algorithm is said to accept the input string, otherwise it is said to reject the input.
To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2n vertices compared to the time taken for a graph with n vertices?
If the input size is n, the time taken can be expressed as a function of n. Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n. If T(n) is a polynomial in n, then the algorithm is said to be a polynomial time algorithm. Cobham's thesis argues that a problem can be solved with a feasible amount of resources if it admits a polynomial-time algorithm.
Many types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.
A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.
Many machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random-access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary.[2] What all these models have in common is that the machines operate deterministically.
However, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.
For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer ("yes" or "no"). A Turing machine M is said to operate within time f(n) if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).
Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.
The complexity of an algorithm is often expressed using big O notation.
The best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size n may be faster to solve than others, we define the following complexities:
The order from cheap to costly is: Best, average (of discrete uniform distribution), amortized, worst.
For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the pivot is always the largest or smallest value in the list (so the list is never divided). In this case the algorithm takes time O(n2). If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.
To classify the computation time (or similar resources, such as space consumption), it is helpful to demonstrate upper and lower bounds on the maximum amount of time required by the most efficient algorithm to solve a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most T(n). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase "all possible algorithms" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of T(n) for a problem requires showing that no algorithm can have time complexity lower than T(n).
A complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:
Some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:
But bounding the computation time above by some concrete function f(n) often yields complexity classes that depend on the chosen machine model. For instance, the language {xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that "the time complexities in any two reasonable and general models of computation are polynomially related" (Goldreich 2008, Chapter 1.2). This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.
Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:
The logarithmic-space classes (necessarily) do not take into account the space needed to represent the problem.
It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch's theorem.
Other important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.
For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(n) is contained in DTIME(n2), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.
The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.
Many complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at most as difficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is no more difficult than Y, and we say that X reduces to Y. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.
The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.
This motivates the concept of a problem being hard for a complexity class. A problem X is hard for a class of problems C if every problem in C can be reduced to X. Thus no problem in C is harder than X, since an algorithm for X allows us to solve any problem in C. The notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.
The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution.[3] If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology,[5] and the ability to find formal proofs of pure mathematics theorems.[6] The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.[7]
Along the same lines, co-NP is the class containing the complement problems (i.e. problems with the yes/no answers reversed) of NP problems. It is believed[13] that NP is not equal to co-NP; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then P is not equal to NP, since P=co-P.  Thus if P=NP we would have co-P=co-NP whence NP=P=co-P=co-NP.
Similarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.
It is suspected that P and BPP are equal. However, it is currently open if BPP = NEXP.
A problem that can be solved in theory (e.g. given large but finite resources, especially time), but for which in practice any solution takes too many resources to be useful, is known as an intractable problem.[14] Conversely, a problem that can be solved in practice is called a tractable problem, literally "a problem that can be handled". The term infeasible (literally "cannot be done") is sometimes used interchangeably with intractable,[15] though this risks confusion with a feasible solution in mathematical optimization.[16]
However, this identification is inexact: a polynomial-time solution with large degree or large leading coefficient grows quickly, and may be impractical for practical size problems; conversely, an exponential-time solution that grows slowly may be practical on realistic input, or a solution that takes a long time in the worst case may take a short time in most cases or the average case, and thus still be practical. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.
Similarly, a polynomial time algorithm is not always practical. If its running time is, say, n15, it is unreasonable to consider it efficient and it is still useless except on small instances. Indeed, in practice even n3 or n2 algorithms are often impractical on realistic sizes of problems.
Continuous complexity theory can refer to complexity theory of problems that involve continuous functions that are approximated by discretizations, as studied in numerical analysis. One approach to complexity theory of numerical analysis[17] is information based complexity.
Continuous complexity theory can also refer to complexity theory of the use of analog computation, which uses continuous dynamical systems and differential equations.[18] Control theory can be considered a form of computation and differential equations are used in the modelling of continuous-time and hybrid discrete-continuous-time systems.[19]
Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.
The beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper "On the Computational Complexity of Algorithms" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems.[20] In addition, in 1965 Edmonds suggested to consider a "good" algorithm to be one with running time bounded by a polynomial of the input size.[21]
Earlier papers studying problems solvable by Turing machines with specific bounded resources include[20] John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper[22] on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure.[23] As he remembers:
However, [my] initial interest [in automata theory] was increasingly set aside in favor of computational complexity, an exciting fusion of combinatorial methods, inherited from switching theory, with the conceptual arsenal of the theory of algorithms. These ideas had occurred to me earlier in 1955 when I coined the term "signalizing function", which is nowadays commonly known as "complexity measure".[24]
In 1967, Manuel Blum formulated a set of axioms (now known as Blum axioms) specifying desirable properties of complexity measures on the set of computable functions and proved an important result, the so-called speed-up theorem. The field began to flourish in 1971 when Stephen Cook and Leonid Levin proved the existence of practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, "Reducibility Among Combinatorial Problems", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.[25]
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
AI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).[1]
As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of AI, a phenomenon known as the AI effect.[2] For instance, optical character recognition is frequently excluded from things considered to be AI,[3] having become a routine technology.[4]
Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,[5][6] followed by disappointment and the loss of funding (known as an "AI winter"),[7][8] followed by new approaches, success and renewed funding.[6][9] AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.[9][10]
The field was founded on the assumption that human intelligence "can be so precisely described that a machine can be made to simulate it".[b] This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[13] Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals.[c]
By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known as Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the "heuristic search" approach, which likened intelligence to a problem of exploring a space of possibilities for answers.
The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.[20] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.[21]
The field of AI research was born at a workshop at Dartmouth College in 1956.[d][24] The attendees became the founders and leaders of AI research.[e] They and their students produced programs that the press described as "astonishing":[f] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[g][26]
By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[27] and laboratories had been established around the world.[28]
They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[32] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an "AI winter", a period when obtaining funding for AI projects was difficult.[7]
In the early 1980s, AI research was revived by the commercial success of expert systems,[33] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[6] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[8]
Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into "sub-symbolic" approaches to specific AI problems.[34] Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.[h]
Interest in neural networks and "connectionism" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s.[39] Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.
AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).[40] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence".[10]
Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.[41] According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a "sporadic usage" in 2012 to more than 2,700 projects.[i] He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.[9]
Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s.[11]
The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[a]
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[44] By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[45]
Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a "combinatorial explosion": they became exponentially slower as the problems grew larger.[46] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[47]
Knowledge representation and knowledge engineering[48] allow AI programs to answer questions intelligently and make deductions about real-world facts.
A representation of "what exists" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.[49] The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.[50]
AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects;[50] situations, events, states and time;[51] causes and effects;[52] knowledge about knowledge (what we know about what other people know);.[53] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[54] as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous);[55] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express verbally).[47]
Formal knowledge representations are used in content-based indexing and retrieval,[56] scene interpretation,[57] clinical decision support,[58] knowledge discovery (mining "interesting" and actionable inferences from large databases),[59] and other areas.[60]
Machine learning (ML), a fundamental concept of AI research since the field's inception,[j] is the study of computer algorithms that improve automatically through experience.[k]
In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.[65]
Transfer learning is when the knowledge gained from one problem is applied to a new problem.[66]
Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[67]
Natural language processing (NLP)[68]
allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.[69]
Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic[46] and the breadth of commonsense knowledge.[55] Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), "Keyword spotting" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others.[70] They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.[71]
Machine perception[72]
is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[73]
facial recognition, and object recognition.[74]
Computer vision is the ability to analyze visual input.[75]
A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.[80]
Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, "master algorithm" that could lead to AGI.[81]
Others believe that anthropomorphic features like an artificial brain[82]
or simulated child development[l]
will someday reach a critical point where general intelligence emerges.
AI can solve many problems by intelligently searching through many possible solutions.[83] Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.[84] Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[85] Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.[86]
Simple exhaustive searches[87]
are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use "heuristics" or "rules of thumb" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called "pruning the search tree"). Heuristics supply the program with a "best guess" for the path on which the solution lies.[88]
Heuristics limit the search for solutions into a smaller sample size.[89]
A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing.[90] Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming.[91] Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[92]
Logic[93]
is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning[94]
and inductive logic programming is a method for learning.[95]
Several different forms of logic are used in AI research. Propositional logic[96] involves truth functions such as "or" and "not". First-order logic[97]
adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a "degree of truth" (between 0 and 1) to vague statements such as "Alice is old" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.[98]
Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.[54]
Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;[50]
situation calculus, event calculus and fluent calculus (for representing events and time);[51]
causal calculus;[52]
belief calculus (belief revision); and modal logics.[53]
Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.[99]
Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[100]
Bayesian networks[101]
are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),[m][103]
learning (using the expectation-maximization algorithm),[n][105]
planning (using decision networks)[106] and perception (using dynamic Bayesian networks).[107]
Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[107]
A key concept from the science of economics is "utility", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[108]
and information value theory.[109] These tools include models such as Markov decision processes,[110] dynamic decision networks,[107] game theory and mechanism design.[111]
The simplest AI applications can be divided into two types: classifiers ("if shiny then diamond") and controllers ("if diamond then pick up"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[112]
A classifier can be trained in various ways; there are many statistical and machine learning approaches.
The decision tree is the simplest and most widely used symbolic machine learning algorithm.[113]
K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.[114]
Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[115]
The naive Bayes classifier is reportedly the "most widely used learner"[116] at Google, due in part to its scalability.[117]
Neural networks are also used for classification.[118]
Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as "naive Bayes" on most practical data sets.[119]
Neural networks[118]
were inspired by the architecture of neurons in the human brain. A simple "neuron" N accepts input from other neurons, each of which, when activated (or "fired"), casts a weighted "vote" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed "fire together, wire together") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.
The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.[122]
Deep learning[124]
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.[125] Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification[126] and others.
Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons,[127] and creates a hierarchy similar to the organization of the animal visual cortex.[128]
In a recurrent neural network (RNN) the signal will propagate through a layer more than once;[129]
thus, an RNN is an example of deep learning.[130]
RNNs can be trained by gradient descent,[131]
however long-term gradients which are back-propagated can "vanish" (that is, they can tend to zero) or "explode" (that is, they can tend to infinity), known as the vanishing gradient problem.[132]
The long short term memory (LSTM) technique can prevent this in most cases.[133]
Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing.
AI is relevant to any intellectual task.[134]
Modern artificial intelligence techniques are pervasive and are too numerous to list here.[135]
Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[136]
In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),
targeting online advertisements,[137] recommendation systems (offered by Netflix, YouTube or Amazon),
driving internet traffic,[138][139] targeted advertising (AdSense, Facebook),
virtual assistants (such as Siri or Alexa),[140] autonomous vehicles (including drones, ADAS and self-driving cars),
automatic language translation (Microsoft Translator, Google Translate),
facial recognition (Apple's Face ID or Microsoft's DeepFace),
image labeling (used by Facebook, Apple's iPhoto and TikTok)
, spam filtering and chatbots (such as Chat GPT).
There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage,[141] deepfakes,[142] medical diagnosis, military logistics, or supply chain management.
Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[143] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[144]
In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[145] Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus[o] and Cepheus.[147] DeepMind in the 2010s developed a "generalized artificial intelligence" that could learn many diverse Atari games on its own.[148]
By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.[149]
DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[150]
Other applications predict the result of judicial decisions,[151] create art (such as poetry or painting) and prove mathematical theorems.
Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[152]
Alan Turing wrote in 1950 "I propose to consider the question 'can machines think'?"[155]
He advised changing the question from whether a machine "thinks", to "whether or not it is possible for machinery to show intelligent behaviour".[155]
He devised the Turing test, which measures the ability of a machine to simulate human conversation.[156] Since we can only observe the behavior of the machine, it does not matter if it is "actually" thinking or literally has a "mind". Turing notes that we can not determine these things about other people[p] but "it is usual to have a polite convention that everyone thinks"[157]
Russell and Norvig agree with Turing that AI must be defined in terms of "acting" and not "thinking".[158] However, they are critical that the test compares machines to people. "Aeronautical engineering texts," they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"[159] AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence".[160]
No established unifying theory or paradigm has guided AI research for most of its history.[q] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.
Symbolic AI (or "GOFAI")[165] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at "intelligent" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."[166]
However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level "intelligent" tasks were easy for AI, but low level "instinctive" tasks were extremely difficult.[167]
Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a "feel" for the situation, rather than explicit symbolic knowledge.[168]
Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.[r][47]
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[170][171] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.
"Neats" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). "Scruffies" expect that it necessarily requires solving a large number of unrelated problems (especially in areas like common sense reasoning). This issue was actively discussed in the 70s and 80s,[172]
but in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed "the victory of the neats".[173]
Finding a provably correct or optimal solution is intractable for many important problems.[46] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.
AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[174][175]
General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.
David Chalmers identified two problems in understanding the mind, which he named the "hard" and "easy" problems of consciousness.[177] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[178]
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[179]
Philosopher John Searle characterized this position as "strong AI": "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."[s]
Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.[182]
If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so, then it could also suffer, and thus it would be entitled to certain rights.[183]
Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.[184]
This issue has been considered in fiction for centuries,[185]
and is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.[186]
A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.[175]
If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.[187]
Its intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the "singularity".[188]
Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.[189]
Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.[190]
Edward Fredkin argues that "artificial intelligence is the next stage in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[191]
In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI.[192]
A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[193]
Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classifies only 9% of U.S. jobs as "high risk".[t][195]
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously".[196]
Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[197]
AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.[198]
Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.[199]
Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.[200]
AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.[201]
Bias can be inadvertently introduced by the way training data is selected.[202]
It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair.[203] An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.[204]
Health equity issues may also be exacerbated when many-to-many mapping are done without taking steps to ensure equity for populations at risk for bias. At this time equity-focused tools and regulations are not in place to ensure equity application representation and usage.[205] Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.
At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[206]
Superintelligent AI may be able to improve itself to the point that humans could not control it. This could, as physicist Stephen Hawking puts it, "spell the end of the human race".[207] Philosopher Nick Bostrom argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's, it might need to harm humanity to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or "friendly" its stated goals might be.[208]
Political scientist Charles T. Rubin argues that "any sufficiently advanced benevolence may be indistinguishable from malevolence." Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would share our system of morality.[209]
AI's decisions making abilities raises the questions of legal responsibility and copyright status of created works. This issues are being refined in various jurisdictions.[216]
Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[217]
Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[218]
Machine ethics is also called machine morality, computational ethics or computational morality,[218]
and was founded at an AAAI symposium in 2005.[219]
Other approaches include Wendell Wallach's "artificial moral agents"[220]
and Stuart J. Russell's three principles for developing provably beneficial machines.[221]
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.[222]
The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[223]
Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[43]
Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[43]
The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[43] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[224]
Thought-capable artificial beings have appeared as storytelling devices since antiquity,[14]
and have been a persistent theme in science fiction.[16]
A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[225]
Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the "Multivac" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;[226]
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[227]
Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune.
These were the four the most widely used AI textbooks in 2008:
The two most widely used textbooks in 2021.Open Syllabus: Explorer
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
In computer engineering, computer architecture is a description of the structure of a computer system made from component parts.[1] It can sometimes be a high-level description that ignores details of the implementation.[2] At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation.[3]
The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. When building the computer Z1 in 1936, Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept.[4][5] Two other early and important examples are:
The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959. Johnson had the opportunity to write a proprietary research communication about the Stretch, an IBM-developed supercomputer for Los Alamos National Laboratory (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of "system architecture", a term that seemed more useful than "machine organization".[8]
Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, "Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints."[9]
Brooks went on to help develop the IBM System/360 (now called the IBM zSeries) line of computers, in which "architecture" became a noun defining "what the user needs to know".[10] Later, computer users came to use the term in many less explicit ways.[11]
There are other technologies in computer architecture. The following technologies are used in bigger companies like Intel, and were estimated in 2002[14] to count for 1% of all of computer architecture:
Computer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. The case of instruction set architecture can be used to illustrate the balance of these competing factors. More complex instruction sets enable programmers to write more space efficient programs, since a single instruction can encode some higher-level abstraction (such as the x86 Loop instruction).[17] However, longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways.
The implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with compilers, operating systems to logic design, and packaging.[18]
An instruction set architecture (ISA) is the interface between the computer's software and hardware and also can be viewed as the programmer's view of the machine. Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand.
The ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler.  An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form.  Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs.
ISAs vary in quality and completeness.  A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time).  Memory organization defines how instructions interact with the memory, and how memory interacts with itself.
During design emulation, emulators can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine whether a particular ISA is meeting its goals.
Computer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite a detailed analysis of the computer's organization.  For example, in an SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.
Computer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well.  For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.
Once an instruction set and micro-architecture have been designed, a practical machine must be developed. This design process is called the implementation. Implementation is usually not considered architectural design, but rather hardware design engineering. Implementation can be further broken down into several steps:
For CPUs, the entire implementation process is organized differently and is often referred to as CPU design.
The exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.
The most common scheme does an in-depth power analysis and figures out how to keep power consumption low while maintaining adequate performance.
Modern computer performance is often described in instructions per cycle (IPC), which measures the efficiency of the architecture at any clock frequency; a faster IPC rate means the computer is faster. Older computers had IPC counts as low as 0.1 while modern processors easily reach nearly 1. Superscalar processors may reach three to five IPC by executing several instructions per clock cycle.[citation needed]
Counting machine-language instructions would be misleading because they can do varying amounts of work in different ISAs. The "instruction" in the standard measurements is not a count of the ISA's machine-language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture.
Many people used to measure a computer's speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance.
Other factors influence speed, such as the mix of functional units, bus speeds, available memory, and the type and order of instructions in the programs.
There are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time.  Interrupt latency is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data).
Benchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn't be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don't offer similar advantages to general tasks.
Power efficiency is another important measurement in modern computers. Higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt).
Modern circuits have less power required per transistor as the number of transistors per chip grows.[19] This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However, the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible.[20] In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency.
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
The history of computer science began long before the modern discipline of computer science, usually appearing in forms like mathematics or physics. Developments in previous centuries alluded to the discipline that we now know as computer science.[1] This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.[2]
The Antikythera mechanism is believed to be an early mechanical analog computer.[7]  It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.[7]
When John Napier discovered logarithms for computational purposes in the early 17th century,[15] there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624.[16] Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria.[17] Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.[18]
In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.[19]
Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.
In 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. Leibniz simplified the binary system and articulated logical properties such as conjunction, disjunction, negation, identity, inclusion, and the empty set.[21] He anticipated Lagrangian interpolation and algorithmic information theory. His calculus ratiocinator anticipated aspects of the universal Turing machine. In 1961, Norbert Wiener suggested that Leibniz should be considered the patron saint of cybernetics.[22] Wiener is quoted with "Indeed, the general idea of a computing machine is nothing but a mechanization of Leibniz's Calculus Ratiocinator."[23] But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled.[24]
By this time, the first mechanical devices driven by a binary pattern had been invented. The industrial revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems.[24]
Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his "Analytical Engine", the first mechanical computer.[26] During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers,[27] although this is arguable as Charles was the first to design the difference engine and consequently its corresponding difference based algorithms, making him the first computer algorithm designer.  Moreover, Lovelace's work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not.[28] While she was never able to see the results of her work, as the "Analytical Engine" was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.[29]
Following Babbage, although at first unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.[30][31] Two other inventors, Leonardo Torres y Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1913) Torres y Quevedo designed a Babbage type of calculating machine that used electromechanical parts which included floating point number representations and built a prototype in 1920. Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.[32]
Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as a logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935 to 1938).
Up to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor.  This changed with switching circuit theory in the 1930s. From 1934 to 1936, Akira Nakashima, Claude Shannon, and Viktor Shetakov published a series of papers showing that the two-valued Boolean algebra, can describe the operation of switching circuits.[37][38][39][40] This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.[40]
While taking an undergraduate philosophy class, Shannon had been exposed to Boole's work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems. His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.[41]
Before the 1920s, computers (sometimes computors) were human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Many of these clerks who served as human computers were women.[42][43][44][45] Some performed astronomical calculations for calendars, others ballistic tables for the military.[46]
After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.
Machines that computed with continuous values became known as the analog kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.
Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.
The phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.
Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described "purely mechanical." The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.
In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine. This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use.[49] These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability. If a Turing machine can complete the task, it is considered Turing computable.[50]
The Los Alamos physicist Stanley Frankel, has described John von Neumann's view of the fundamental importance of Turing's 1936 paper, in a letter:[49]
In 1948, the Manchester Baby was completed; it was the world's first electronic digital computer that ran programs stored in its memory, like almost all modern computers.[49] The influence on Max Newman of Turing's seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Baby.[49]
Claude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit.  This work is one of the theoretical foundations for many areas of study, including data compression and cryptography.[56]
From experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for "steersman." He published "Cybernetics" in 1948, which influenced artificial intelligence. Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.[57]
In 1946, a model for computer architecture was introduced and became known as Von Neumann architecture. Since 1950, the von Neumann model provided uniformity in subsequent computer designs. The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space.[58]  The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU). In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.[59]
The term artificial intelligence was credited by John McCarthy to explain the research that they were doing for a proposal for the Dartmouth Summer Research. The naming of artificial intelligence also led to the birth of a new field in computer science.[61] On August 31, 1955, a research project was proposed consisting of John McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon. The official project began in 1956 that consisted of several significant parts they felt would help them better understand artificial intelligence's makeup.[62]
McCarthy and his colleagues' ideas behind automatic computers was while a machine is capable of completing a task, then the same should be confirmed with a computer by compiling a program to perform the desired results. They also discovered that the human brain was too complex to replicate, not by the machine itself but by the program. The knowledge to produce a program that sophisticated was not there yet.[62]
The concept behind this was looking at how humans understand our own language and structure of how we form sentences, giving different meaning and rule sets and comparing them to a machine process.[62] The way computers can understand is at a hardware level. This language is written in binary (1s and 0's). This has to be written in a specific format that gives the computer the ruleset to run a particular hardware piece.[63]
Minsky's process determined how these artificial neural networks could be arranged to have similar qualities to the human brain. However, he could only produce partial results and needed to further the research into this idea.[62]
McCarthy and Shannon's idea behind this theory was to develop a way to use complex problems to determine and measure the machine's efficiency through mathematical theory and computations.[64] However, they were only to receive partial test results.[62]
The idea behind self-improvement is how a machine would use self-modifying code to make itself smarter. This would allow for a machine to grow in intelligence and increase calculation speeds.[65] The group believed they could study this if a machine could improve upon the process of completing a task in the abstractions part of their research.[62]
The group thought that research in this category could be broken down into smaller groups. This would consist of sensory and other forms of information about artificial intelligence.[62] Abstractions in computer science can refer to mathematics and programming language.[66]
Their idea of computational creativity is how the program or a machine can be seen in having similar ways of human thinking.[67] They wanted to see if a machine could take a piece of incomplete information and improve upon it to fill in the missing details as the human mind can do. If this machine could do this; they needed to think of how did the machine determine the outcome.[62]
Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.
Computer science can be described as all of the following:
This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.
Also simply binary search, half-interval search,[24] logarithmic search,[25] or binary chop.[26]
Also bitmap image file, device independent bitmap (DIB) file format, or simply bitmap.
Also executable code, executable file, executable program, or simply executable.
This category has the following 26 subcategories, out of 26 total.
The following 27 pages are in this category, out of  27 total. This list may not reflect recent changes.
Talk pages are where people discuss how to make content on Wikipedia the best that it can be. You can use this page to start a discussion with others about how to improve the "Template:TopicTOC-Computer science" page.
Computation is any type of arithmetic or non-arithmetic calculation that follows a well-defined model (e.g., an algorithm).[1][2]
Mechanical or electronic devices (or, historically, people) that perform computations are known as computers. An especially well-known discipline of the study of computation is computer science.
Computation can be seen as a purely physical process occurring inside a closed physical system called a computer. Examples of such physical systems are digital computers, mechanical computers, quantum computers, DNA computers, molecular computers, microfluidics-based computers, analog computers, and wetware computers.
This point of view has been adopted by the physics of computation, a branch of theoretical physics, as well as the field of natural computing.
An even more radical point of view, pancomputationalism (inaudible word), is the postulate of digital physics that argues that the evolution of the universe is itself a computation.
The classic account of computation is found throughout the works of Hilary Putnam and others. Peter Godfrey-Smith has dubbed this the "simple mapping account."[3] Gualtiero Piccinini's summary of this account states that a physical system can be said to perform a specific computation when there is a mapping between the state of that system and the computation such that the "microphysical states [of the system] mirror the state transitions between the computational states."[4]
Philosophers such as Jerry Fodor[5] have suggested various accounts of computation with the restriction that semantic content be a necessary condition for computation (that is, what differentiates an arbitrary physical system from a computing system is that the operands of the computation represent something).  This notion attempts to prevent the logical abstraction of the mapping account of pancomputationalism, the idea that everything can be said to be computing everything.
Gualtiero Piccinini proposes an account of computation based on mechanical philosophy. It states that physical computing systems are types of mechanisms that, by design, perform physical computation, or the manipulation (by a functional mechanism) of a "medium-independent" vehicle according to a rule. "Medium-independence" requires that the property can be instantiated[clarification needed] by multiple realizers[clarification needed] and multiple mechanisms, and that the inputs and outputs of the mechanism also be multiply realizable. In short, medium-independence allows for the use of physical variables with properties other than voltage (as in typical digital computers); this is imperative in considering other types of computation, such as that which occurs in the brain or in a quantum computer. A rule, in this sense, provides a mapping among inputs, outputs, and internal states of the physical computing system.[6]
In the theory of computation, a diversity of mathematical models of computation has been developed.
Typical mathematical models of computers are the following:
Automation describes a wide range of technologies that reduce human intervention in processes, namely by predetermining decision criteria, subprocess relationships, and related actions, as well as embodying those predeterminations in machines.[1] Automation has been achieved by various means including mechanical, hydraulic, pneumatic, electrical, electronic devices, and computers, usually in combination. Complicated systems, such as modern factories, airplanes, and ships typically use combinations of all of these techniques. The benefit of automation includes labor savings, reducing waste, savings in electricity costs, savings in material costs, and improvements to quality, accuracy, and precision.
Automation includes the use of various equipment and control systems such as machinery, processes in factories, boilers,[2] and heat-treating ovens, switching on telephone networks, steering, and stabilization of ships, aircraft, and other applications and vehicles with reduced human intervention.[3] Examples range from a household thermostat controlling a boiler to a large industrial control system with tens of thousands of input measurements and output control signals. Automation has also found a home in the banking industry. It can range from simple on-off control to multi-variable high-level algorithms in terms of control complexity.
In the simplest type of an automatic control loop, a controller compares a measured value of a process with a desired set value and processes the resulting error signal to change some input to the process, in such a way that the process stays at its set point despite disturbances. This closed-loop control is an application of negative feedback to a system. The mathematical basis of control theory was begun in the 18th century and advanced rapidly in the 20th. The term automation, inspired by the earlier word automatic (coming from automaton), was not widely used before 1947, when Ford established an automation department.[4] It was during this time that industry was rapidly adopting feedback controllers, which were introduced in the 1930s.[5]
The World Bank's World Development Report of 2019 shows evidence that the new industries and jobs in the technology sector outweigh the economic effects of workers being displaced by automation.[6] Job losses and downward mobility blamed on automation have been cited as one of many factors in the resurgence of nationalist, protectionist and populist politics in the US, UK and France, among other countries since the 2010s.[7][8][9][10][11]
Fundamentally, there are two types of control loops: open-loop control, and closed-loop control.
In open-loop control, the control action from the controller is independent of the "process output" (or "controlled process variable"). A good example of this is a central heating boiler controlled only by a timer, so that heat is applied for a constant time, regardless of the temperature of the building. (The control action is switching the boiler off and on. The process output is building temperature.)
In closed-loop control, the control action from the controller is dependent on the process output. In the case of the boiler analogy, this would include a temperature sensor to monitor the building temperature, and thereby feed a signal back to the controller to ensure it maintains the building at the temperature set on the thermostat. A closed-loop controller, therefore, has a feedback loop that ensures the controller exerts a control action to give a process output equal to the "reference input" or "set point". For this reason, closed-loop control is also called feedback control.[12]
The definition of a closed-loop control system according to the British Standard Institution is 'a control system possessing monitoring feedback, the deviation signal formed as a result of this feedback being used to control the action of a final control element in such a way as to tend to reduce the deviation to zero.'[13]
Likewise, a feedback control system is a system that tends to maintain a prescribed relationship of one system variable to another by comparing functions of these variables and using the difference as a means of control.[13]
The advanced type of automation that revolutionized manufacturing, aircraft, communications, and other industries, is feedback control, which is usually continuous and involves taking measurements using a sensor and making calculated adjustments to keep the measured variable within a set range.[5][14] The theoretical basis of closed-loop automation is control theory.
One of the simplest types of control is on-off control. An example is a thermostat used on household appliances which either open or close an electrical contact. (Thermostats were originally developed as true feedback-control mechanisms rather than the on-off common household appliance thermostat.)
Sequence control, in which a programmed sequence of discrete operations is performed, often based on system logic that involves system states. An elevator control system is an example of sequence control.
In a PID loop, the controller continuously calculates an error value 



e
(
t
)


{\displaystyle e(t)}

 as the difference between a desired setpoint and a measured process variable and applies a correction based on proportional, integral, and derivative terms, respectively (sometimes denoted P, I, and D) which give their name to the controller type.
The theoretical understanding and application date from the 1920s, and they are implemented in nearly all analog control systems; originally in mechanical controllers, and then using discrete electronics and latterly in industrial process computers.
Sequential control may be either to a fixed sequence or to a logical one that will perform different actions depending on various system states. An example of an adjustable but otherwise fixed sequence is a timer on a lawn sprinkler.
States refer to the various conditions that can occur in a use or sequence scenario of the system. An example is an elevator, which uses logic based on the system state to perform certain actions in response to its state and operator input. For example, if the operator presses the floor n button, the system will respond depending on whether the elevator is stopped or moving, going up or down, or if the door is open or closed, and other conditions.[15]
Early development of sequential control was relay logic, by which electrical relays engage electrical contacts which either start or interrupt power to a device. Relays were first used in telegraph networks before being developed for controlling other devices, such as when starting and stopping industrial-sized electric motors or opening and closing solenoid valves. Using relays for control purposes allowed event-driven control, where actions could be triggered out of sequence, in response to external events. These were more flexible in their response than the rigid single-sequence cam timers. More complicated examples involved maintaining safe sequences for devices such as swing bridge controls, where a lock bolt needed to be disengaged before the bridge could be moved, and the lock bolt could not be released until the safety gates had already been closed.
The total number of relays and cam timers can number into the hundreds or even thousands in some factories. Early programming techniques and languages were needed to make such systems manageable, one of the first being ladder logic, where diagrams of the interconnected relays resembled the rungs of a ladder. Special computers called programmable logic controllers were later designed to replace these collections of hardware with a single, more easily re-programmed unit.
In a typical hard wired motor start and stop circuit (called a control circuit) a motor is started by pushing a "Start" or "Run" button that activates a pair of electrical relays. The "lock-in" relay locks in contacts that keep the control circuit energized when the push-button is released. (The start button is a normally open contact and the stop button is normally closed contact.) Another relay energizes a switch that powers the device that throws the motor starter switch (three sets of contacts for three-phase industrial power) in the main power circuit. Large motors use high voltage and experience high in-rush current, making speed important in making and breaking contact. This can be dangerous for personnel and property with manual switches. The "lock-in" contacts in the start circuit and the main power contacts for the motor are held engaged by their respective electromagnets until a "stop" or "off" button is pressed, which de-energizes the lock in relay.[16]
Commonly interlocks are added to a control circuit. Suppose that the motor in the example is powering machinery that has a critical need for lubrication. In this case, an interlock could be added to ensure that the oil pump is running before the motor starts. Timers, limit switches, and electric eyes are other common elements in control circuits.
Solenoid valves are widely used on compressed air or hydraulic fluid for powering actuators on mechanical components. While motors are used to supply continuous rotary motion, actuators are typically a better choice for intermittently creating a limited range of movement for a mechanical component, such as moving various mechanical arms, opening or closing valves, raising heavy press-rolls, applying pressure to presses.
It was a preoccupation of the Greeks and Arabs (in the period between about 300 BC and about 1200 AD) to keep accurate track of time. In Ptolemaic Egypt, about 270 BC, Ctesibius described a float regulator for a water clock, a device not unlike the ball and cock in a modern flush toilet. This was the earliest feedback-controlled mechanism.[17] The appearance of the mechanical clock in the 14th century made the water clock and its feedback control system obsolete.
The centrifugal governor was invented by Christiaan Huygens in the seventeenth century, and used to adjust the gap between millstones.[22][23][24]
The introduction of prime movers, or self-driven machines advanced grain mills, furnaces, boilers, and the steam engine created a new requirement for automatic control systems including temperature regulators (invented in 1624; see Cornelius Drebbel), pressure regulators (1681), float regulators (1700) and speed control devices. Another control mechanism was used to tent the sails of windmills. It was patented by Edmund Lee in 1745.[25] Also in 1745, Jacques de Vaucanson invented the first automated loom. Around 1800, Joseph Marie Jacquard created a punch-card system to program looms.[26]
In 1771 Richard Arkwright invented the first fully automated spinning mill driven by water power, known at the time as the water frame.[27] An automatic flour mill was developed by Oliver Evans in 1785, making it the first completely automated industrial process.[28][29]
A centrifugal governor was used by Mr. Bunce of England in 1784 as part of a model steam crane.[30][31] The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt's partner Boulton saw one at a flour mill Boulton & Watt were building.[25] The governor could not actually hold a set speed; the engine would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped with this governor were not suitable for operations requiring constant speed, such as cotton spinning.[25]
Several improvements to the governor, plus improvements to valve cut-off timing on the steam engine, made the engine suitable for most industrial uses before the end of the 19th century. Advances in the steam engine stayed well ahead of science, both thermodynamics and control theory.[25] The governor received relatively little scientific attention until James Clerk Maxwell published a paper that established the beginning of a theoretical basis for understanding control theory.
Relay logic was introduced with factory electrification, which underwent rapid adaption from 1900 through the 1920s. Central electric power stations were also undergoing rapid growth and the operation of new high-pressure boilers, steam turbines and electrical substations created a large demand for instruments and controls. Central control rooms became common in the 1920s, but as late as the early 1930s, most process controls were on-off. Operators typically monitored charts drawn by recorders that plotted data from instruments. To make corrections, operators manually opened or closed valves or turned switches on or off. Control rooms also used color-coded lights to send signals to workers in the plant to manually make certain changes.[32]
Controllers, which were able to make calculated changes in response to deviations from a set point rather than on-off control, began being introduced in the 1930s. Controllers allowed manufacturing to continue showing productivity gains to offset the declining influence of factory electrification.[33]
The First and Second World Wars saw major advancements in the field of mass communication and signal processing. Other key advances in automatic controls include differential equations, stability theory and system theory (1938), frequency domain analysis (1940), ship control (1950), and stochastic analysis (1941).
In 1959 Texaco's Port Arthur Refinery became the first chemical plant to use digital control.[41]
Conversion of factories to digital control began to spread rapidly in the 1970s as the price of computer hardware fell.
The logic performed by telephone switching relays was the inspiration for the digital computer.
The first commercially successful glass bottle-blowing machine was an automatic model introduced in 1905.[44] The machine, operated by a two-man crew working 12-hour shifts, could produce 17,280 bottles in 24 hours, compared to 2,880 bottles made by a crew of six men and boys working in a shop for a day. The cost of making bottles by machine was 10 to 12 cents per gross compared to $1.80 per gross by the manual glassblowers and helpers.
Sectional electric drives were developed using control theory. Sectional electric drives are used on different sections of a machine where a precise differential must be maintained between the sections. In steel rolling, the metal elongates as it passes through pairs of rollers, which must run at successively faster speeds. In paper making paper, the sheet shrinks as it passes around steam-heated drying arranged in groups, which must run at successively slower speeds. The first application of a sectional electric drive was on a paper machine in 1919.[45] One of the most important developments in the steel industry during the 20th century was continuous wide strip rolling, developed by Armco in 1928.[46]
Before automation, many chemicals were made in batches. In 1930, with the widespread use of instruments and the emerging use of controllers, the founder of Dow Chemical Co. was advocating continuous production.[47]
Self-acting machine tools that displaced hand dexterity so they could be operated by boys and unskilled laborers were developed by James Nasmyth in the 1840s.[48] Machine tools were automated with Numerical control (NC) using punched paper tape in the 1950s. This soon evolved into computerized numerical control (CNC).
Today extensive automation is practiced in practically every type of manufacturing and assembly process. Some of the larger processes include electrical power generation, oil refining, chemicals, steel mills, plastics, cement plants, fertilizer plants, pulp and paper mills, automobile and truck assembly, aircraft production, glass manufacturing, natural gas separation plants, food and beverage processing, canning and bottling and manufacture of various kinds of parts. Robots are especially useful in hazardous applications like automobile spray painting. Robots are also used to assemble electronic circuit boards. Automotive welding is done with robots and automatic welders are used in applications like pipelines.
With the advent of the space age in 1957, controls design, particularly in the United States, turned away from the frequency-domain techniques of classical control theory and backed into the differential equation techniques of the late 19th century, which were couched in the time domain. During the 1940s and 1950s, German mathematician Irmgard Flugge-Lotz developed the theory of discontinuous automatic control, which became widely used in hysteresis control systems such as navigation systems, fire-control systems, and electronics. Through Flugge-Lotz and others, the modern era saw time-domain design for nonlinear systems (1961), navigation (1960), optimal control and estimation theory (1962), nonlinear control theory (1969), digital control and filtering theory (1974), and the personal computer (1983).
Perhaps the most cited advantage of automation in industry is that it is associated with faster production and cheaper labor costs. Another benefit could be that it replaces hard, physical, or monotonous work.[49] Additionally, tasks that take place in hazardous environments or that are otherwise beyond human capabilities can be done by machines, as machines can operate even under extreme temperatures or in atmospheres that are radioactive or toxic. They can also be maintained with simple quality checks. However, at the time being, not all tasks can be automated, and some tasks are more expensive to automate than others. Initial costs of installing the machinery in factory settings are high, and failure to maintain a system could result in the loss of the product itself.
Moreover, some studies seem to indicate that industrial automation could impose ill effects beyond operational concerns, including worker displacement due to systemic loss of employment and compounded environmental damage; however, these findings are both convoluted and controversial in nature, and could potentially be circumvented.[50]
Automation primarily describes machines replacing human action, but it is also loosely associated with mechanization, machines replacing human labor. Coupled with mechanization, extending human capabilities in terms of size, strength, speed, endurance, visual range & acuity, hearing frequency & precision, electromagnetic sensing & effecting, etc., advantages include:[52]
The paradox of automation says that the more efficient the automated system, the more crucial the human contribution of the operators. Humans are less involved, but their involvement becomes more critical. Lisanne Bainbridge, a cognitive psychologist, identified these issues notably in her widely cited paper "Ironies of Automation."[53] If an automated system has an error, it will multiply that error until it is fixed or shut down. This is where human operators come in.[54] A fatal example of this was Air France Flight 447, where a failure of automation put the pilots into a manual situation they were not prepared for.[55]
Many roles for humans in industrial processes presently lie beyond the scope of automation. Human-level pattern recognition, language comprehension, and language production ability are well beyond the capabilities of modern mechanical and computer systems (but see Watson computer). Tasks requiring subjective assessment or synthesis of complex sensory data, such as scents and sounds, as well as high-level tasks such as strategic planning, currently require human expertise. In many cases, the use of humans is more cost-effective than mechanical approaches even where the automation of industrial tasks is possible. Overcoming these obstacles is a theorized path to post-scarcity economics.
Increased automation often causes workers to feel anxious about losing their jobs as technology renders their skills or experience unnecessary. Early in the Industrial Revolution, when inventions like the steam engine were making some job categories expendable, workers forcefully resisted these changes. Luddites, for instance, were English textile workers who protested the introduction of weaving machines by destroying them.[57] More recently, some residents of Chandler, Arizona, have slashed tires and pelted rocks at driver-less cars, in protest over the cars' perceived threat to human safety and job prospects.[58]
The relative anxiety about automation reflected in opinion polls seems to correlate closely with the strength of organized labor in that region or nation. For example, while a study by the Pew Research Center indicated that 72% of Americans are worried about increasing automation in the workplace, 80% of Swedes see automation and artificial intelligence (AI) as a good thing, due to the country's still-powerful unions and a more robust national safety net.[59]
In the U.S., 47% of all current jobs have the potential to be fully automated by 2033, according to the research of experts Carl Benedikt Frey and Michael Osborne. Furthermore, wages and educational attainment appear to be strongly negatively correlated with an occupation's risk of being automated.[60] Even highly skilled professional jobs like a lawyer, doctor, engineer, journalist are at risk of automation.[61]
Prospects are particularly bleak for occupations that do not presently require a university degree, such as truck driving.[62] Even in high-tech corridors like Silicon Valley, concern is spreading about a future in which a sizable percentage of adults have little chance of sustaining gainful employment.[63] "In The Second Machine Age, Erik Brynjolfsson and Andrew McAfee argue that "...there's never been a better time to be a worker with special skills or the right education, because these people can use technology to create and capture value. However, there's never been a worse time to be a worker with only 'ordinary' skills and abilities to offer, because computers, robots, and other digital technologies are acquiring these skills and abilities at an extraordinary rate."[64] As the example of Sweden suggests, however, the transition to a more automated future need not inspire panic, if there is sufficient political will to promote the retraining of workers whose positions are being rendered obsolete.
According to a 2020 study in the Journal of Political Economy, automation has robust negative effects on employment and wages: "One more robot per thousand workers reduces the employment-to-population ratio by 0.2 percentage points and wages by 0.42%."[65]
Research by Carl Benedikt Frey and Michael Osborne of the Oxford Martin School argued that employees engaged in "tasks following well-defined procedures that can easily be performed by sophisticated algorithms" are at risk of displacement, and 47% of jobs in the US were at risk. The study, released as a working paper in 2013 and published in 2017, predicted that automation would put low-paid physical occupations most at risk, by surveying a group of colleagues on their opinions.[66] However, according to a study published in McKinsey Quarterly[67] in 2015 the impact of computerization in most cases is not the replacement of employees but automation of portions of the tasks they perform.[68] The methodology of the McKinsey study has been heavily criticized for being intransparent and relying on subjective assessments.[69] The methodology of Frey and Osborne has been subjected to criticism, as lacking evidence, historical awareness, or credible methodology.[70][71] Additionally, the Organisation for Economic Co-operation and Development (OECD) found that across the 21 OECD countries, 9% of jobs are automatable.[72]
Based on a formula by Gilles Saint-Paul, an economist at Toulouse 1 University, the demand for unskilled human capital declines at a slower rate than the demand for skilled human capital increases.[75] In the long run and for society as a whole it has led to cheaper products, lower average work hours, and new industries forming (i.e., robotics industries, computer industries, design industries). These new industries provide many high salary skill-based jobs to the economy. By 2030, between 3 and 14 percent of the global workforce will be forced to switch job categories due to automation eliminating jobs in an entire sector. While the number of jobs lost to automation is often offset by jobs gained from technological advances, the same type of job loss is not the same one replaced and that leading to increasing unemployment in the lower-middle class. This occurs largely in the US and developed countries where technological advances contribute to higher demand for highly skilled labor but demand for middle-wage labor continues to fall. Economists call this trend "income polarization" where unskilled labor wages are driven down and skilled labor is driven up and it is predicted to continue in developed economies.[76]
Lights-out manufacturing is a production system with no human workers, to eliminate labor costs.
Lights out manufacturing grew in popularity in the U.S. when General Motors in 1982 implemented humans "hands-off" manufacturing to "replace risk-averse bureaucracy with automation and robots". However, the factory never reached full "lights out" status.[77]
The automation of vehicles could prove to have a substantial impact on the environment, although the nature of this impact could be beneficial or harmful depending on several factors. Because automated vehicles are much less likely to get into accidents compared to human-driven vehicles, some precautions built into current models (such as anti-lock brakes or laminated glass) would not be required for self-driving versions. Removing these safety features would also significantly reduce the weight of the vehicle, thus increasing fuel economy and reducing emissions per mile. Self-driving vehicles are also more precise concerning acceleration and breaking, and this could contribute to reduced emissions. Self-driving cars could also potentially utilize fuel-efficient features such as route mapping that can calculate and take the most efficient routes. Despite this potential to reduce emissions, some researchers theorize that an increase in the production of self-driving cars could lead to a boom of vehicle ownership and use. This boom could potentially negate any environmental benefits of self-driving cars if a large enough number of people begin driving personal vehicles more frequently.[79]
Automation of homes and home appliances is also thought to impact the environment, but the benefits of these features are also questioned. A study of energy consumption of automated homes in Finland showed that smart homes could reduce energy consumption by monitoring levels of consumption in different areas of the home and adjusting consumption to reduce energy leaks (e.g. automatically reducing consumption during the nighttime when activity is low). This study, along with others, indicated that the smart home's ability to monitor and adjust consumption levels would reduce unnecessary energy usage. However, new research suggests that smart homes might not be as efficient as non-automated homes. A more recent study has indicated that, while monitoring and adjusting consumption levels do decrease unnecessary energy use, this process requires monitoring systems that also consume a significant amount of energy. This study suggested that the energy required to run these systems is so much so that it negates any benefits of the systems themselves, resulting in little to no ecological benefit.[80]
Another major shift in automation is the increased demand for flexibility and convertibility in manufacturing processes. Manufacturers are increasingly demanding the ability to easily switch from manufacturing Product A to manufacturing Product B without having to completely rebuild the production lines. Flexibility and distributed processes have led to the introduction of Automated Guided Vehicles with Natural Features Navigation.
Digital electronics helped too. Former analog-based instrumentation was replaced by digital equivalents which can be more accurate and flexible, and offer greater scope for more sophisticated configuration, parametrization, and operation. This was accompanied by the fieldbus revolution which provided a networked (i.e. a single cable) means of communicating between control systems and field-level instrumentation, eliminating hard-wiring.
Discrete manufacturing plants adopted these technologies fast. The more conservative process industries with their longer plant life cycles have been slower to adopt and analog-based measurement and control still dominate. The growing use of Industrial Ethernet on the factory floor is pushing these trends still further, enabling manufacturing plants to be integrated more tightly within the enterprise, via the internet if necessary. Global competition has also increased demand for Reconfigurable Manufacturing Systems.
Engineers can now have numerical control over automated devices. The result has been a rapidly expanding range of applications and human activities. Computer-aided technologies (or CAx) now serve as the basis for mathematical and organizational tools used to create complex systems. Notable examples of CAx include computer-aided design (CAD software) and computer-aided manufacturing (CAM software). The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry.[81]
Information technology, together with industrial machinery and processes, can assist in the design, implementation, and monitoring of control systems. One example of an industrial control system is a programmable logic controller (PLC). PLCs are specialized hardened computers which are frequently used to synchronize the flow of inputs from (physical) sensors and events with the flow of outputs to actuators and events.[82]
Human-machine interfaces (HMI) or computer human interfaces (CHI), formerly known as man-machine interfaces, are usually employed to communicate with PLCs and other computers. Service personnel who monitor and control through HMIs can be called by different names. In the industrial process and manufacturing environments, they are called operators or something similar. In boiler houses and central utility departments, they are called stationary engineers.[83]
Host simulation software (HSS) is a commonly used testing tool that is used to test the equipment software. HSS is used to test equipment performance concerning factory automation standards (timeouts, response time, processing time).[84]
Cognitive automation, as a subset of AI, is an emerging genus of automation enabled by cognitive computing. Its primary concern is the automation of clerical tasks and workflows that consist of structuring unstructured data.[85] Cognitive automation relies on multiple disciplines: natural language processing, real-time computing, machine learning algorithms, big data analytics, and evidence-based learning.[86]
According to Deloitte, cognitive automation enables the replication of human tasks and judgment "at rapid speeds and considerable scale."[87] Such tasks include:
Many agricultural operations are automated with machinery and equipment to improve their diagnosis, decision-making and/or performing. Agricultural automation can relieve the drudgery of agricultural work, improve the timeliness and precision of agricultural operations, raise productivity and resource-use efficiency, build resilience, and improve food quality and safety.[88] Increased productivity can free up labour, allowing agricultural households to spend more time elsewhere.[89]
The technological evolution in agriculture has resulted in progressive shifts to digital equipment and robotics.[88] Motorized mechanization using engine power automates the performance of agricultural operations such as ploughing and milking.[90] With digital automation technologies, it also becomes possible to automate diagnosis and decision-making of agricultural operations.[88] For example, autonomous crop robots can harvest and seed crops, while drones can gather information to help automate input application.[89] Precision agriculture often employs such automation technologies[89]
Motorized mechanization has generally increased in recent years.[91] Sub-Saharan Africa is the only region where the adoption of motorized mechanization has stalled over the past decades.[92][89]
Automation technologies are increasingly used for managing livestock, though evidence on adoption is lacking. Global automatic milking system sales have increased over recent years,[93] but adoption is likely mostly in Northern Europe,[94] and likely almost absent in low- and middle-income countries.[95][89] Automated feeding machines for both cows and poultry also exist, but data and evidence regarding their adoption trends and drivers is likewise scarce.[89][91]
Many supermarkets and even smaller stores are rapidly introducing self-checkout systems reducing the need for employing checkout workers. In the U.S., the retail industry employs 15.9 million people as of 2017 (around 1 in 9 Americans in the workforce). Globally, an estimated 192 million workers could be affected by automation according to research by Eurasia Group.[96]
The food retail industry has started to apply automation to the ordering process; McDonald's has introduced touch screen ordering and payment systems in many of its restaurants, reducing the need for as many cashier employees.[97] The University of Texas at Austin has introduced fully automated cafe retail locations.[98] Some Cafes and restaurants have utilized mobile and tablet "apps" to make the ordering process more efficient by customers ordering and paying on their device.[99] Some restaurants have automated food delivery to tables of customers using a Conveyor belt system. The use of robots is sometimes employed to replace waiting staff.[100]
Automation in construction is the combination of methods, processes, and systems that allow for greater machine autonomy in construction activities. Construction automation may have multiple goals, including but not limited to, reducing jobsite injuries, decreasing activity completion times, and assisting with quality control and quality assurance. [101]
 Automated mining involves the removal of human labor from the mining process.[102] The mining industry is currently in the transition towards automation. Currently, it can still require a large amount of human capital, particularly in the third world where labor costs are low so there is less incentive for increasing efficiency through automation.
The Defense Advanced Research Projects Agency (DARPA) started the research and development of automated visual surveillance and monitoring (VSAM) program, between 1997 and 1999, and airborne video surveillance (AVS) programs, from 1998 to 2002. Currently, there is a major effort underway in the vision community to develop a fully-automated tracking surveillance system. Automated video surveillance monitors people and vehicles in real-time within a busy environment. Existing automated surveillance systems are based on the environment they are primarily designed to observe, i.e., indoor, outdoor or airborne, the number of sensors that the automated system can handle and the mobility of sensors, i.e., stationary camera vs. mobile camera. The purpose of a surveillance system is to record properties and trajectories of objects in a given area, generate warnings or notify the designated authorities in case of occurrence of particular events.[103]
As demands for safety and mobility have grown and technological possibilities have multiplied, interest in automation has grown. Seeking to accelerate the development and introduction of fully automated vehicles and highways, the U.S. Congress authorized more than $650 million over six years for intelligent transport systems (ITS) and demonstration projects in the 1991 Intermodal Surface Transportation Efficiency Act (ISTEA). Congress legislated in ISTEA that:[104]
[T]he Secretary of Transportation shall develop an automated highway and vehicle prototype from which future fully automated intelligent vehicle-highway systems can be developed. Such development shall include research in human factors to ensure the success of the man-machine relationship. The goal of this program is to have the first fully automated highway roadway or an automated test track in operation by 1997. This system shall accommodate the installation of equipment in new and existing motor vehicles.
Full automation commonly defined as requiring no control or very limited control by the driver; such automation would be accomplished through a combination of sensor, computer, and communications systems in vehicles and along the roadway. Fully automated driving would, in theory, allow closer vehicle spacing and higher speeds, which could enhance traffic capacity in places where additional road building is physically impossible, politically unacceptable, or prohibitively expensive. Automated controls also might enhance road safety by reducing the opportunity for driver error, which causes a large share of motor vehicle crashes. Other potential benefits include improved air quality (as a result of more-efficient traffic flows), increased fuel economy, and spin-off technologies generated during research and development related to automated highway systems.[105]
Automated waste collection trucks prevent the need for as many workers as well as easing the level of labor required to provide the service.[106]
Business process automation (BPA) is the technology-enabled automation of complex business processes.[107] It can help to streamline a business for simplicity, achieve digital transformation, increase service quality, improve service delivery or contain costs. BPA consists of integrating applications, restructuring labor resources and using software applications throughout the organization. Robotic process automation (RPA; or RPAAI for self-guided RPA 2.0) is an emerging field within BPA and uses AI. BPAs can be implemented in a number of business areas including marketing, sales and workflow.
Home automation (also called domotics) designates an emerging practice of increased automation of household appliances and features in residential dwellings, particularly through electronic means that allow for things impracticable, overly expensive or simply not possible in recent past decades. The rise in the usage of home automation solutions has taken a turn reflecting the increased dependency of people on such automation solutions. However, the increased comfort that gets added through these automation solutions is remarkable.[108]
Automation is essential for many scientific and clinical applications.[109] Therefore, automation has been extensively employed in laboratories. From as early as 1980 fully automated laboratories have already been working.[110] However, automation has not become widespread in laboratories due to its high cost. This may change with the ability of integrating low-cost devices with standard laboratory equipment.[111][112] Autosamplers are common devices used in laboratory automation.
Logistics automation is the application of computer software or automated machinery to improve the efficiency of logistics operations. Typically this refers to operations within a warehouse or distribution center, with broader tasks undertaken by supply chain engineering systems and enterprise resource planning systems.
Industrial automation deals primarily with the automation of manufacturing, quality control, and material handling processes. General-purpose controllers for industrial processes include programmable logic controllers, stand-alone I/O modules, and computers. Industrial automation is to replace the human action and manual command-response activities with the use of mechanized equipment and logical programming commands. One trend is increased use of machine vision[113] to provide automatic inspection and robot guidance functions, another is a continuing increase in the use of robots. Industrial automation is simply required in industries.
Energy efficiency in industrial processes has become a higher priority. Semiconductor companies like Infineon Technologies are offering 8-bit micro-controller applications for example found in motor controls, general purpose pumps, fans, and ebikes to reduce energy consumption and thus increase efficiency.
Industrial robotics is a sub-branch in industrial automation that aids in various manufacturing processes. Such manufacturing processes include machining, welding, painting, assembling and material handling to name a few.[117] Industrial robots use various mechanical, electrical as well as software systems to allow for high precision, accuracy and speed that far exceed any human performance. The birth of industrial robots came shortly after World War II as the U.S. saw the need for a quicker way to produce industrial and consumer goods.[118] Servos, digital logic and solid-state electronics allowed engineers to build better and faster systems and over time these systems were improved and revised to the point where a single robot is capable of running 24 hours a day with little or no maintenance. In 1997, there were 700,000 industrial robots in use, the number has risen to 1.8M in 2017[119] In recent years, AI with robotics is also used in creating an automatic labeling solution, using robotic arms as the automatic label applicator, and AI for learning and detecting the products to be labelled.[120]
Industrial automation incorporates programmable logic controllers in the manufacturing process. Programmable logic controllers (PLCs) use a processing system which allows for variation of controls of inputs and outputs using simple programming. PLCs make use of programmable memory, storing instructions and functions like logic, sequencing, timing, counting, etc. Using a logic-based language, a PLC can receive a variety of inputs and return a variety of logical outputs, the input devices being sensors and output devices being motors, valves, etc. PLCs are similar to computers, however, while computers are optimized for calculations, PLCs are optimized for control tasks and use in industrial environments. They are built so that only basic logic-based programming knowledge is needed and to handle vibrations, high temperatures, humidity, and noise. The greatest advantage PLCs offer is their flexibility. With the same basic controllers, a PLC can operate a range of different control systems. PLCs make it unnecessary to rewire a system to change the control system. This flexibility leads to a cost-effective system for complex and varied control systems.[121]
PLCs can range from small "building brick" devices with tens of I/O in a housing integral with the processor, to large rack-mounted modular devices with a count of thousands of I/O, and which are often networked to other PLC and SCADA systems.
They can be designed for multiple arrangements of digital and analog inputs and outputs (I/O), extended temperature ranges, immunity to electrical noise, and resistance to vibration and impact. Programs to control machine operation are typically stored in battery-backed-up or non-volatile memory.
It was from the automotive industry in the USA that the PLC was born. Before the PLC, control, sequencing, and safety interlock logic for manufacturing automobiles was mainly composed of relays, cam timers, drum sequencers, and dedicated closed-loop controllers. Since these could number in the hundreds or even thousands, the process for updating such facilities for the yearly model change-over was very time-consuming and expensive, as electricians needed to individually rewire the relays to change their operational characteristics.
When digital computers became available, being general-purpose programmable devices, they were soon applied to control sequential and combinatorial logic in industrial processes. However, these early computers required specialist programmers and stringent operating environmental control for temperature, cleanliness, and power quality. To meet these challenges, the PLC was developed with several key attributes. It would tolerate the shop-floor environment, it would support discrete (bit-form) input and output in an easily extensible manner, it would not require years of training to use, and it would permit its operation to be monitored. Since many industrial processes have timescales easily addressed by millisecond response times, modern (fast, small, reliable) electronics greatly facilitate building reliable controllers, and performance could be traded off for reliability.[122]
Agent-assisted automation refers to automation used by call center agents to handle customer inquiries. The key benefit of agent-assisted automation is compliance and error-proofing. Agents are sometimes not fully trained or they forget or ignore key steps in the process. The use of automation ensures that what is supposed to happen on the call actually does, every time. There are two basic types: desktop automation and automated voice solutions.
Desktop automation refers to software programming that makes it easier for the call center agent to work across multiple desktop tools. The automation would take the information entered into one tool and populate it across the others so it did not have to be entered more than once, for example.
Automated voice solutions allow the agents to remain on the line while disclosures and other important information is provided to customers in the form of pre-recorded audio files. Specialized applications of these automated voice solutions enable the agents to process credit cards without ever seeing or hearing the credit card numbers or CVV codes.[123]
Information is an abstract concept that refers to that which has the power to inform.  At the most fundamental level information pertains to the interpretation of that which may be sensed.  Any natural process that is not completely random, and any observable pattern in any medium can be said to convey some amount of information.  Whereas digital signals and other data use discrete signs to convey information, other phenomena and artifacts such as analog signals, poems, pictures, music or other sounds, and currents convey information in a more continuous form.[1]  Information is not knowledge itself, but the meaning that may be derived from a representation through interpretation.[2]
Information is often processed iteratively: Data available at one step are processed into information to be interpreted and processed at the next step.  For example, in written text each symbol or letter conveys information relevant to the word it is part of, each word conveys information relevant to the phrase it is part of, each phrase conveys information relevant to the sentence it is part of, and so on until at the final step information is interpreted and becomes knowledge in a given domain.  In a digital signal, bits may be interpreted into the symbols, letters, numbers, or structures that convey the information available at the next level up.  The key characteristic of information is that it is subject to interpretation and processing.
The concept of information is relevant in various contexts,[3] including those of constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, proposition, representation, and entropy.
The derivation of information from a signal or message may be thought of as the resolution of ambiguity or uncertainty that arises during the interpretation of patterns within the signal or message.[4]
Information may be structured as data. Redundant data can be compressed up to an optimal size, which is the theoretical limit of compression.
The information available through a collection of data may be derived by analysis.  For example, data may be collected from a single customer's order at a restaurant.  The information available from many orders may be analyzed, and then becomes knowledge that is put to use when the business subsequently is able to identify the most popular or least popular dish.[5]
Information can be transmitted in time, via data storage, and space, via communication and telecommunication.[6] Information is expressed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, all information is always conveyed as the content of a message.
Information can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication.
The uncertainty of an event is measured by its probability of occurrence. Uncertainty is inversely proportional to the probability of occurrence. Information theory takes advantage of this by concluding that more uncertain events require more information to resolve their uncertainty. The bit is a typical unit of information. It is 'that which reduces uncertainty by half'.[7] Other units such as the nat may be used. For example, the information encoded in one "fair" coin flip is log2(2/1) = 1 bit, and in two fair coin flips is log2(4/1) = 2 bits. A 2011 Science article estimated that 97% of technologically stored information was already in digital bits in 2007, and that the year 2002 was the beginning of the digital age for information storage (with digital storage capacity bypassing analog for the first time).[8]
Information theory is the scientific study of the quantification, storage, and communication of information. The field was fundamentally established by the works of Harry Nyquist and Ralph Hartley in the 1920s, and Claude Shannon in the 1940s. The field is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.
A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory, and information-theoretic security.
There is another opinion regarding the universal definition of information. It lies in the fact that the concept itself has changed along with the change of various historical epochs, and in order to find such a definition, it is necessary to find common features and patterns of this transformation. For example, researchers in the field of information Petrichenko E. A. and Semenova V. G., based on a retrospective analysis of changes in the concept of information, give the following universal definition: "Information is a form of transmission of human experience (knowledge)." In their opinion, the change in the essence of the concept of information occurs after various breakthrough technologies for the transfer of experience (knowledge), i.e. the appearance of writing, the printing press, the first encyclopedias, the telegraph, the development of cybernetics, the creation of a microprocessor, the Internet, smartphones, etc. Each new form of experience transfer is a synthesis of the previous ones. That is why we see such a variety of definitions of information, because, according to the law of dialectics "negation-negation", all previous ideas about information are contained in a "filmed" form and in its modern representation.[10]
Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference,[11] cryptography, neurobiology,[12] perception,[13] linguistics, the evolution[14] and function[15] of molecular codes (bioinformatics), thermal physics,[16] quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection,[17] pattern recognition, anomaly detection[18] and even art creation.
Often information can be viewed as a type of input to an organism or system. Inputs are of two kinds; some inputs are important to the function of the organism (for example, food) or system (energy) by themselves. In his book Sensory Ecology[19] biophysicist David B. Dusenbery called these causal inputs. Other inputs (information) are important only because they are associated with causal inputs and can be used to predict the occurrence of a causal input at a later time (and perhaps another place). Some information is important because of association with other information but eventually there must be a connection to a causal input.
In practice, information is usually carried by weak stimuli that must be detected by specialized sensory systems and amplified by energy inputs before they can be functional to the organism or system. For example, light is mainly (but not only, e.g. plants can grow in the direction of the lightsource) a causal input to plants but for animals it only provides information. The colored light reflected from a flower is too weak for photosynthesis but the visual system of the bee detects it and the bee's nervous system uses the information to guide the bee to the flower, where the bee often finds nectar or pollen, which are causal inputs, serving a nutritional function.
Michael Grieves has proposed that the focus on information should be what it does as opposed to defining what it is. Grieves has proposed [23] that information can be substituted for wasted physical resources, time, energy, and material, for goal oriented tasks. Goal oriented tasks can be divided into two components: the most cost efficient use of physical resources: time, energy and material, and the additional use of physical resources used by the task.This second category is by definition wasted physical resources. Information does not substitute or replace the most cost efficient use of physical resources, but can be used to replace the wasted physical resources. The condition that this occurs under is that the cost of information is less than the cost of the wasted physical resources. Since information is a non-rival good, this can be especially beneficial for repeatable tasks. In manufacturing, the task category of the most cost efficient use of physical resources is called lean manufacturing.
Information is any type of pattern that influences the formation or transformation of other patterns.[24][25] In this sense, there is no need for a conscious mind to perceive, much less appreciate, the pattern. Consider, for example, DNA. The sequence of nucleotides is a pattern that influences the formation and development of an organism without any need for a conscious mind. One might argue though that for a human to consciously define a pattern, for example a nucleotide, naturally involves conscious information processing.
Systems theory at times seems to refer to information in this sense, assuming information does not necessarily involve any conscious mind, and patterns circulating (due to feedback) in the system can be called information. In other words, it can be said that information in this sense is something potentially perceived as representation, though not created or presented for that purpose. For example, Gregory Bateson defines "information" as a "difference that makes a difference".[26]
If, however, the premise of "influence" implies that information has been perceived by a conscious mind and also interpreted by it, the specific context associated with this interpretation may cause the transformation of the information into knowledge. Complex definitions of both "information" and "knowledge" make such semantic and logical analysis difficult, but the condition of "transformation" is an important point in the study of information as it relates to knowledge, especially in the business discipline of knowledge management. In this practice, tools and processes are used to assist a knowledge worker in performing research and making decisions, including steps such as:
Stewart (2001) argues that transformation of information into knowledge is critical, lying at the core of value creation and competitive advantage for the modern enterprise.
The Danish Dictionary of Information Terms[27] argues that information only provides an answer to a posed question. Whether the answer provides knowledge depends on the informed person. So a generalized definition of the concept should be: "Information" = An answer to a specific question".
When Marshall McLuhan speaks of media and their effects on human cultures, he refers to the structure of artifacts that in turn shape our behaviors and mindsets. Also, pheromones are often said to be "information" in this sense.
These sections are using measurements of data rather than information, as information cannot be directly measured.
The world's combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.[8]
The world's combined effective capacity to exchange information through two-way telecommunication networks was the informational equivalent of 6 newspapers per person per day in 2007.[6]
As of 2007, an estimated 90% of all new information is digital, mostly stored on hard drives.[28]
The total amount of data created, captured, copied, and consumed globally is forecast to increase rapidly, reaching 64.2 zettabytes in 2020. Over the next five years up to 2025, global data creation is projected to grow to more than 180 zettabytes.[29]
Records are specialized forms of information. Essentially, records are information produced consciously or as by-products of business activities or transactions and retained because of their value. Primarily, their value is as evidence of the activities of the organization but they may also be retained for their informational value. Sound records management[30] ensures that the integrity of records is preserved for as long as they are required.
The international standard on records management, ISO 15489, defines records as "information created, received, and maintained as evidence and information by an organization or person, in pursuance of legal obligations or in the transaction of business".[31] The International Committee on Archives (ICA) Committee on electronic records defined a record as, "recorded information produced or received in the initiation, conduct or completion of an institutional or individual activity and that comprises content, context and structure sufficient to provide evidence of the activity".[32]
Records may be maintained to retain corporate memory of the organization or to meet legal, fiscal or accountability requirements imposed on the organization. Willis expressed the view that sound management of business records and information delivered "...six key requirements for good corporate governance...transparency; accountability; due process; compliance; meeting statutory and common law requirements; and security of personal and corporate information."[33]
Michael Buckland has classified "information" in terms of its uses: "information as process", "information as knowledge", and "information as thing".[34]
Beynon-Davies[35][36] explains the multi-faceted concept of information in terms of signs and signal-sign systems. Signs themselves can be considered in terms of four inter-dependent levels, layers or branches of semiotics: pragmatics, semantics, syntax, and empirics. These four layers serve to connect the social world on the one hand with the physical or technical world on the other.
Pragmatics is concerned with the purpose of communication. Pragmatics links the issue of signs with the context within which signs are used. The focus of pragmatics is on the intentions of living agents underlying communicative behaviour. In other words, pragmatics link language to action.
Syntax is concerned with the formalism used to represent a message. Syntax as an area studies the form of communication in terms of the logic and grammar of sign systems. Syntax is devoted to the study of the form rather than the content of signs and sign-systems.
Nielsen (2008) discusses the relationship between semiotics and information in relation to dictionaries. He introduces the concept of lexicographic information costs and refers to the effort a user of a dictionary must make to first find, and then understand data so that they can generate information.
Communication normally exists within the context of some social situation. The social situation sets the context for the intentions conveyed (pragmatics) and the form of communication. In a communicative situation intentions are expressed through messages that comprise collections of inter-related signs taken from a language mutually understood by the agents involved in the communication. Mutual understanding implies that agents involved understand the chosen language in terms of its agreed syntax (syntactics) and semantics. The sender codes the message in the language and sends the message as signals along some communication channel (empirics). The chosen communication channel has inherent properties that determine outcomes such as the speed at which communication can take place, and over what distance.
The information cycle (addressed as a whole or in its distinct components) is of great concern to information technology, information systems, as well as information science. These fields deal with those processes and techniques pertaining to information capture (through sensors) and generation (through computation, formulation or composition), processing (including encoding, encryption, compression, packaging), transmission (including all telecommunication methods), presentation (including visualization / display methods), storage (such as magnetic or optical, including holographic methods), etc.
Information visualization (shortened as InfoVis) depends on the computation and digital representation of data, and assists users in pattern recognition and anomaly detection.
Partial map of the Internet, with nodes representing IP addresses
Galactic (including dark) matter distribution in a cubic section of the Universe
Information embedded in an abstract mathematical object with symmetry breaking nucleus
Visual representation of a strange attractor, with converted data of its fractal structure
Information security (shortened as InfoSec) is the ongoing process of exercising due diligence to protect information, and information systems, from unauthorized access, use, disclosure, destruction, modification, disruption or distribution, through algorithms and procedures focused on monitoring and detection, as well as incident response and repair.
Information analysis is the process of inspecting, transforming, and modelling information, by converting raw data into actionable knowledge, in support of the decision-making process.
Information quality (shortened as InfoQ) is the potential of a dataset to achieve a specific (scientific or practical) goal using a given empirical analysis method.
Information communication represents the convergence of informatics, telecommunication and audio-visual media & content.
In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.[3]
As an effective method, an algorithm can be expressed within a finite amount of space and time,[4] and in a well-defined formal language[5] for calculating a function.[6] Starting from an initial state and initial input (perhaps empty),[7] the instructions describe a computation that, when executed, proceeds through a finite[8] number of well-defined successive states, eventually producing "output"[9] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[10]
The concept of algorithms has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC.[11] Greek mathematicians later used algorithms in 240 BC in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers.[12] Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.[13]
In English, the word algorithm was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it was not until the late 19th century that "algorithm" took on the meaning that it has in modern English.[26]
Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:
Haec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.
Algorism is the art by which at present we use those Indian figures, which number two times five.
The poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.[27]
An informal definition could be "a set of rules that precisely defines a sequence of operations",[30][need quotation to verify] which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure[31]
or cook-book recipe.[32]
A prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.
Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word "algorithm" in the following quotation:
An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary "input variables" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.
Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.
For some of these computational processes, the algorithm must be rigorously defined: and specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).
For some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.
There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see Turing machine for more).
Representations of algorithms can be classed into three accepted levels of Turing machine description, as follows:[42]
For an example of the simple algorithm "Add m+n" described in all three levels, see Examples.
Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns,[43] with examples including the template method pattern and the decorator pattern.
One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.
"Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:
Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".[46]
Computers (and computors), models of computation: A computer (or human "computer"[47]) is a restricted type of machine, a "discrete deterministic mechanical device"[48] that blindly follows its instructions.[49] Melzak's and Lambek's primitive models[50] reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters[51] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[52]
Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example".[56] But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computer must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.[57]
This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).
But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, the arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters".[58] When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").
One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:
(Quasi-)formal description:
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:
For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be "proper"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).
Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest.[67] While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.
E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.
E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.
E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.
The following version can be used with programming languages from the C-family:
Does an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source[68] uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.
But "exceptional cases"[69] must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).
Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[70] Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.[71]
Elegance (compactness) versus goodness (speed): With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[72] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.
The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm;[73] rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.
The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.
The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.[74]
To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[75] In general, speed improvements depend on special properties of the problem, which are very common in practical applications.[76] Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.
There are various ways to classify algorithms, each with its own merits.
Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:
Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.
Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.
Algorithms can be classified by the amount of time they need to complete compared to their input size:
Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.
The adjective "continuous" when applied to the word "algorithm" can mean:
Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.
Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).
The earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm.[11] During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas.[82] Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[83]
A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.[86]
The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.[13]
This machine he displayed in 1870 before the Fellows of the Royal Society.[91] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".[92]
Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".[94]
The mathematician Martin Davis observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):
Emil Post (1936) described the actions of a "computer" (human being) as follows:
Alan Turing's work[109] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'".[110] Given the prevalence at the time of Morse code, telegraphy, ticker tape machines, and teletypewriters, it is quite possible that all were influences on Turing during his youth.
"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:
A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:
J. Barkley Rosser defined an "effective [mathematical] method" in the following manner (italicization added):
In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: "What are the fundamental capabilities and limitations of computers?".[1]
The theory of computation can be considered the creation of models of all kinds in the field of computer science. Therefore, mathematics and logic are used. In the last century it became an independent academic discipline and was separated from mathematics.
Language theory is a branch of mathematics concerned with describing languages as a set of operations over an alphabet. It is closely linked with automata theory, as automata are used to generate and recognize formal languages. There are several classes of formal languages, each allowing more complex language specification than the one before it, i.e. Chomsky hierarchy,[6] and each corresponding to a class of automata which recognizes it. Because automata are used as models for computation, formal languages are the preferred mode of specification for any problem that must be computed.
Computability theory deals primarily with the question of the extent to which a problem is solvable on a computer. The statement that the halting problem cannot be solved by a Turing machine[7] is one of the most important results in computability theory, as it is an example of a concrete problem that is both easy to formulate and impossible to solve using a Turing machine.  Much of computability theory builds on the halting problem result.
Another important step in computability theory was Rice's theorem, which states that for all non-trivial properties of partial functions, it is undecidable whether a Turing machine computes a partial function with that property.[8]
Computability theory is closely related to the branch of mathematical logic called recursion theory, which removes the restriction of studying only models of computation which are reducible to the Turing model.[9]  Many mathematicians and computational theorists who study recursion theory will refer to it as computability theory.
Complexity theory considers not only whether a problem can be solved at all on a computer, but also how efficiently the problem can be solved.  Two major aspects are considered: time complexity and space complexity, which are respectively how many steps does it take to perform a computation, and how much memory is required to perform that computation.
In order to analyze how much time and space a given algorithm requires, computer scientists express the time or space required to solve the problem as a function of the size of the input problem.  For example, finding a particular number in a long list of numbers becomes harder as the list of numbers grows larger.  If we say there are n numbers in the list, then if the list is not sorted or indexed in any way we may have to look at every number in order to find the number we're seeking.  We thus say that  in order to solve this problem, the computer needs to perform a number of steps that grows linearly in the size of the problem.
To simplify this problem, computer scientists have adopted Big O notation, which allows functions to be compared in a way that ensures that particular aspects of a machine's construction do not need to be considered, but rather only the asymptotic behavior as problems become large.  So in our previous example, we might say that the problem requires 



O
(
n
)


{\displaystyle O(n)}

 steps to solve.
Perhaps the most important open problem in all of computer science is the question of whether a certain broad class of problems denoted NP can be solved efficiently. This is discussed further at Complexity classes P and NP, and P versus NP problem is one of the seven Millennium Prize Problems stated by the Clay Mathematics Institute in 2000. The Official Problem Description was given by Turing Award winner Stephen Cook.
In addition to the general computational models, some simpler computational models are useful for special, restricted applications.  Regular expressions, for example,  specify string patterns in many contexts, from office productivity software to programming languages. Another formalism mathematically equivalent to regular expressions, Finite automata are used in circuit design and in some kinds of problem-solving. Context-free grammars  specify programming language syntax.  Non-deterministic pushdown automata are another formalism equivalent to context-free grammars. Primitive recursive functions are a defined subclass of the recursive functions.
Different models of computation have the ability to do different tasks. One way to measure the power of a computational model is to study the class of formal languages that the model can generate; in such a way to the Chomsky hierarchy of languages is obtained.
(There are many textbooks in this area; this list is by necessity incomplete.)
A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security.
Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference,[3] cryptography, neurobiology,[4] perception,[5] linguistics, the evolution[6] and function[7] of molecular codes (bioinformatics), thermal physics,[8] molecular dynamics,[9] quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection,[10] pattern recognition, anomaly detection[11] and even art creation.
Information theory studies the transmission, processing, extraction, and utilization of information. Abstractly, information can be thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was formalized in 1948 by Claude Shannon in a paper entitled A Mathematical Theory of Communication, in which information is thought of as a set of possible messages, and the goal is to send these messages over a noisy channel, and to have the receiver reconstruct the message with low probability of error, in spite of the channel noise. Shannon's main result, the noisy-channel coding theorem showed that, in the limit of many channel uses, the rate of information that is asymptotically achievable is equal to the channel capacity, a quantity dependent merely on the statistics of the channel over which the messages are sent.[4]
Coding theory is concerned with finding explicit methods, called codes, for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity. These codes can be roughly subdivided into data compression (source coding) and error-correction (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible.
A third class of information theory codes are cryptographic algorithms (both codes and ciphers). Concepts, methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis. See the article ban (unit) for a historical application.
The landmark event establishing the discipline of information theory and bringing it to immediate worldwide attention was the publication of Claude E. Shannon's classic paper "A Mathematical Theory of Communication" in the Bell System Technical Journal in July and October 1948.
Prior to this paper, limited information-theoretic ideas had been developed at Bell Labs, all implicitly assuming events of equal probability. Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying "intelligence" and the "line speed" at which it can be transmitted by a communication system, giving the relation W = K log m (recalling the Boltzmann constant), where W is the speed of transmission of intelligence, m is the number of different voltage levels to choose from at each time step, and K is a constant. Ralph Hartley's 1928 paper, Transmission of Information, uses the word information as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as H = log Sn = n log S, where S was the number of possible symbols, and n the number of symbols in a transmission. The unit of information was therefore the decimal digit, which since has sometimes been called the hartley in his honor as a unit or scale or measure of information. Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.
Much of the mathematics behind information theory with events of different probabilities were developed for the field of thermodynamics by Ludwig Boltzmann and J. Willard Gibbs.  Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by Rolf Landauer in the 1960s, are explored in Entropy in thermodynamics and information theory.
In Shannon's revolutionary and groundbreaking paper, the work for which had been substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion: 
Information theory is based on probability theory and statistics, where quantified information is usually described in terms of bits.  Information theory often concerns itself with measures of information of the distributions associated with random variables. One of the most important measures is called entropy, which forms the building block of many other measures. Entropy allows quantification of measure of information in a single random variable. Another useful concept is mutual information defined on two random variables, which describes the measure of information in common between those variables, which can be used to describe their correlation.  The former quantity is a property of the probability distribution of a random variable and gives a limit on the rate at which data generated by independent samples with the given distribution can be reliably compressed. The latter is a property of the joint distribution of two random variables, and is the maximum rate of reliable communication across a noisy channel in the limit of long block lengths, when the channel statistics are determined by the joint distribution.
The choice of logarithmic base in the following formulae determines the unit of information entropy that is used.  A common unit of information is the bit, based on the binary logarithm. Other units include the nat, which is based on the natural logarithm, and the decimal digit, which is based on the common logarithm.
Based on the probability mass function of each source symbol to be communicated, the Shannon entropy H, in units of bits (per symbol), is given by
where pi is the probability of occurrence of the i-th possible value of the source symbol. This equation gives the entropy in the units of "bits" (per symbol) because it uses a logarithm of base 2, and this base-2 measure of entropy has sometimes been called the shannon in his honor. Entropy is also commonly computed using the natural logarithm (base e, where e is Euler's number), which produces a measurement of entropy in nats per symbol and sometimes simplifies the analysis by avoiding the need to include extra constants in the formulas. Other bases are also possible, but less commonly used. For example, a logarithm of base 28 = 256 will produce a measurement in bytes per symbol, and a logarithm of base 10 will produce a measurement in decimal digits (or hartleys) per symbol.
Intuitively, the entropy HX of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known.
(Here, I(x) is the self-information, which is the entropy contribution of an individual message, and 





E


X




{\displaystyle \mathbb {E} _{X}}

 is the expected value.) A property of entropy is that it is maximized when all the messages in the message space are equiprobable p(x) = 1/n; i.e., most unpredictable, in which case H(X) = log n.
The special case of information entropy for a random variable with two outcomes is the binary entropy function, usually taken to the logarithmic base 2, thus having the shannon (Sh) as unit:
The joint entropy of two discrete random variables X and Y is merely the entropy of their pairing: (X, Y).  This implies that if X and Y are independent, then their joint entropy is the sum of their individual entropies.
Despite similar notation, joint entropy should not be confused with cross entropy.
The conditional entropy or conditional uncertainty of X given random variable Y (also called the equivocation of X about Y) is the average conditional entropy over Y:[13]
Because entropy can be conditioned on a random variable or on that random variable being a certain value, care should be taken not to confuse these two definitions of conditional entropy, the former of which is in more common use.  A basic property of this form of conditional entropy is that:
Mutual information measures the amount of information that can be obtained about one random variable by observing another.  It is important in communication where it can be used to maximize the amount of information shared between sent and received signals.  The mutual information of X relative to Y is given by:
where SI (Specific mutual Information) is the pointwise mutual information.
That is, knowing Y, we can save an average of I(X; Y) bits in encoding X compared to not knowing Y.
In other words, this is a measure of how much, on the average, the probability distribution on X will change if we are given the value of Y.  This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:
Although it is sometimes used as a 'distance metric', KL divergence is not a true metric since it is not symmetric and does not satisfy the triangle inequality (making it a semi-quasimetric).
Another interpretation of the KL divergence is the "unnecessary surprise" introduced by a prior from the truth: suppose a number X is about to be drawn randomly from a discrete set with probability distribution 



p
(
x
)


{\displaystyle p(x)}

.  If Alice knows the true distribution 



p
(
x
)


{\displaystyle p(x)}

, while Bob believes (has a prior) that the distribution is 



q
(
x
)


{\displaystyle q(x)}

, then Bob will be more surprised than Alice, on average, upon seeing the value of X.  The KL divergence is the (objective) expected value of Bob's (subjective) surprisal minus Alice's surprisal, measured in bits if the log is in base 2.  In this way, the extent to which Bob's prior is "wrong" can be quantified in terms of how "unnecessarily surprised" it is expected to make him.
Coding theory is one of the most important and direct applications of information theory. It can be subdivided into source coding theory and channel coding theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.
Any process that generates successive messages can be considered a source of information.  A memoryless source is one in which each message is an independent identically distributed random variable, whereas the properties of ergodicity and stationarity impose less restrictive constraints.  All such sources are stochastic.  These terms are well studied in their own right outside information theory.
Information rate is the average entropy per symbol.  For memoryless sources, this is merely the entropy of each symbol, while, in the case of a stationary stochastic process, it is
that is, the conditional entropy of a symbol given all the previous symbols generated.  For the more general case of a process that is not necessarily stationary, the average rate is
that is, the limit of the joint entropy per symbol.  For stationary sources, these two expressions give the same result.[22]
It is common in information theory to speak of the "rate" or "entropy" of a language.  This is appropriate, for example, when the source of information is English prose.  The rate of a source of information is related to its redundancy and how well it can be compressed, the subject of source coding.
Communications over a channel is the primary motivation of information theory.  However, channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.
Consider the communications process over a discrete channel. A simple model of the process is shown below:
Here X represents the space of messages transmitted, and Y the space of messages received during a unit time over our channel. Let p(y|x) be the conditional probability distribution function of Y given X. We will consider p(y|x) to be an inherent fixed property of our communications channel (representing the nature of the noise of our channel). Then the joint distribution of X and Y is completely determined by our channel and by our choice of f(x), the marginal distribution of messages we choose to send over the channel. Under these constraints, we would like to maximize the rate of information, or the signal, we can communicate over the channel. The appropriate measure for this is the mutual information, and this maximum mutual information is called the channel capacity and is given by:
Channel coding is concerned with finding such nearly optimal codes that can be used to transmit data over a noisy channel with a small coding error at a rate near the channel capacity.
Information theoretic concepts apply to cryptography and cryptanalysis. Turing's information unit, the ban, was used in the Ultra project, breaking the German Enigma machine code and hastening the end of World War II in Europe.  Shannon himself defined an important concept now called the unicity distance. Based on the redundancy of the plaintext, it attempts to give a minimum amount of ciphertext necessary to ensure unique decipherability.
Information theory leads us to believe it is much more difficult to keep secrets than it might first appear.  A brute force attack can break systems based on asymmetric key algorithms or on most commonly used methods of symmetric key algorithms (sometimes called secret key algorithms), such as block ciphers.  The security of all such methods currently comes from the assumption that no known attack can break them in a practical amount of time.
Information theoretic security refers to methods such as the one-time pad that are not vulnerable to such brute force attacks.  In such cases, the positive conditional mutual information between the plaintext and ciphertext (conditioned on the key) can ensure proper transmission, while the unconditional mutual information between the plaintext and ciphertext remains zero, resulting in absolutely secure communications.  In other words, an eavesdropper would not be able to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key. However, as in any other cryptographic system, care must be used to correctly apply even information-theoretically secure methods; the Venona project was able to crack the one-time pads of the Soviet Union due to their improper reuse of key material.
One early commercial application of information theory was in the field of seismic oil exploration. Work in this field made it possible to strip off and separate the unwanted noise from the desired seismic signal. Information theory and digital signal processing offer a major improvement of resolution and image clarity over previous analog methods.[25]
Concepts from information theory such as redundancy and code control have been used by semioticians such as Umberto Eco and Ferruccio Rossi-Landi to explain ideology as a form of message transmission whereby a dominant social class emits its message by using signs that exhibit a high degree of redundancy such that only one message is decoded among a selection of competing ones.[28]
Quantitative information theoretic methods have been applied in cognitive science to analyze the integrated process organization of neural information in the context of the binding problem in cognitive neuroscience.[29] In this context, either an information-theoretical measure, such as functional clusters (Gerald Edelman and Giulio Tononi's functional clustering model and dynamic core hypothesis (DCH)[30]) or effective information (Tononi's integrated information theory (IIT) of consciousness[31][32][33]), is defined (on the basis of a reentrant process organization, i.e. the synchronization of neurophysiological activity between groups of neuronal populations), or the measure of the minimization of free energy on the basis of statistical methods (Karl J. Friston's free energy principle (FEP), an information-theoretical measure which states that every adaptive change in a self-organized system leads to a minimization of free energy, and the Bayesian brain hypothesis[34][35][36][37][38]).
Information theory also has applications in gambling, black holes, and bioinformatics.
Applied science is the use of the scientific method and knowledge obtained via conclusions from the method to attain practical goals. It includes a broad range of disciplines such as engineering and medicine. Applied science is often contrasted with basic science, which is focused on advancing scientific theories and laws that explain and predict natural or other phenomena.[1]
Applied science can also apply formal science, such as statistics and probability theory, as in epidemiology. Genetic epidemiology is an applied science applying both biological and statistical methods. Applied science can also apply social science, such as application of psychology in applied psychology, criminology and law.
Applied research is the practical application of science. It accesses and uses accumulated theories, knowledge, methods, and techniques, for a specific state-, business-, or client-driven purpose. In contrast to engineering, applied research does not include analyses or optimization of business, economics, and costs. Applied research can be better understood in any area when contrasting it with, basic, or pure, research. Basic geography research strives to create new theories and methods that aid in the explanation of the processes that shape the spatial structure of physical or human environments. Rather, applied research utilizes the already existing geographical theories and methods to comprehend and address particular empirical issues.
[2]Applied research usually has specific commercial objectives related to products, procedures, or services.[3] The comparison of pure research and applied research provides a basic framework and direction for businesses to follow.
Applied research deals with solving practical problems[4] and generally employs empirical methodologies. Because applied research resides in the messy real world, strict research protocols may need to be relaxed. For example, it may be impossible to use a random sample. Thus, transparency in the methodology is crucial. Implications for interpretation of results brought about by relaxing an otherwise strict canon of methodology should also be considered.[5]
Moreover, this type of research method applies natural sciences to human conditions:[6]
Since applied research has a provisional close-to-the-problem and close-to-the-data orientation, it may also use a more provisional conceptual framework such as working hypotheses or pillar questions.[7][8] The OECD's Frascati Manual[9] describes applied research as one of the three forms of research, along with basic research & experimental development.[10]
Due to its practical focus, applied research information will be found in the literature associated with individual disciplines.[11]
Applied science works as a system that branches into other fields of work that go more in depth of the system. Applied research is a method of problem solving and also practical in areas of science such as its presence in applied psychology. Applied psychology uses human behavior to grab information to be able locate a main focus in an area that can contribute to finding a resolution.[12] More specific, this study is applied in the area of criminal psychology. With the knowledge obtained of applied research, studies are conducted on criminals alongside their behavior to apprehend them.[13] Moreover, the research extends to criminal investigations. Under this category, research methods demonstrate an understanding of the scientific method and social research designs used in criminological research. These reach more branches along the procedure towards the investigations, alongside laws, policy, and criminological theory.
Engineering fields include thermodynamics, heat transfer, fluid mechanics, statics, dynamics, mechanics of materials, kinematics, electromagnetism, materials science, earth sciences, engineering physics. These fields are also within the scope of basic science.
Medical sciences, for instance medical microbiology, pharmaceutical research and clinical virology, are applied sciences that apply biology and chemistry toward medicine. Pharmaceutical development would fall within the scope of engineering.
In Canada, the Netherlands and other places the Bachelor of Applied Science (BASc) is sometimes equivalent to the Bachelor of Engineering, and is classified as a professional degree. This is based on the age of the school where applied science used to include boiler making, surveying and engineering. There are also Bachelor of Applied Science degrees in Child Studies. The BASc tends to focus more on the application of the engineering sciences. In Australia and New Zealand, this degree is awarded in various fields of study and is considered a highly specialized professional degree.
In the United Kingdom's educational system, Applied Science refers to a suite of "vocational" science qualifications that run alongside "traditional" General Certificate of Secondary Education or A-Level Sciences.[14] Applied Science courses generally contain more coursework (also known as portfolio or internally assessed work) compared to their traditional counterparts. These are an evolution of the GNVQ qualifications that were offered up to 2005. These courses regularly come under scrutiny and are due for review following the Wolf Report 2011;[15] however, their merits are argued elsewhere.[16]
In computer engineering, computer architecture is a description of the structure of a computer system made from component parts.[1] It can sometimes be a high-level description that ignores details of the implementation.[2] At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation.[3]
The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. When building the computer Z1 in 1936, Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept.[4][5] Two other early and important examples are:
The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959. Johnson had the opportunity to write a proprietary research communication about the Stretch, an IBM-developed supercomputer for Los Alamos National Laboratory (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of "system architecture", a term that seemed more useful than "machine organization".[8]
Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, "Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints."[9]
Brooks went on to help develop the IBM System/360 (now called the IBM zSeries) line of computers, in which "architecture" became a noun defining "what the user needs to know".[10] Later, computer users came to use the term in many less explicit ways.[11]
There are other technologies in computer architecture. The following technologies are used in bigger companies like Intel, and were estimated in 2002[14] to count for 1% of all of computer architecture:
Computer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. The case of instruction set architecture can be used to illustrate the balance of these competing factors. More complex instruction sets enable programmers to write more space efficient programs, since a single instruction can encode some higher-level abstraction (such as the x86 Loop instruction).[17] However, longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways.
The implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with compilers, operating systems to logic design, and packaging.[18]
An instruction set architecture (ISA) is the interface between the computer's software and hardware and also can be viewed as the programmer's view of the machine. Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand.
The ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler.  An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form.  Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs.
ISAs vary in quality and completeness.  A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time).  Memory organization defines how instructions interact with the memory, and how memory interacts with itself.
During design emulation, emulators can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine whether a particular ISA is meeting its goals.
Computer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite a detailed analysis of the computer's organization.  For example, in an SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.
Computer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well.  For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.
Once an instruction set and micro-architecture have been designed, a practical machine must be developed. This design process is called the implementation. Implementation is usually not considered architectural design, but rather hardware design engineering. Implementation can be further broken down into several steps:
For CPUs, the entire implementation process is organized differently and is often referred to as CPU design.
The exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.
The most common scheme does an in-depth power analysis and figures out how to keep power consumption low while maintaining adequate performance.
Modern computer performance is often described in instructions per cycle (IPC), which measures the efficiency of the architecture at any clock frequency; a faster IPC rate means the computer is faster. Older computers had IPC counts as low as 0.1 while modern processors easily reach nearly 1. Superscalar processors may reach three to five IPC by executing several instructions per clock cycle.[citation needed]
Counting machine-language instructions would be misleading because they can do varying amounts of work in different ISAs. The "instruction" in the standard measurements is not a count of the ISA's machine-language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture.
Many people used to measure a computer's speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance.
Other factors influence speed, such as the mix of functional units, bus speeds, available memory, and the type and order of instructions in the programs.
There are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time.  Interrupt latency is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data).
Benchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn't be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don't offer similar advantages to general tasks.
Power efficiency is another important measurement in modern computers. Higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt).
Modern circuits have less power required per transistor as the number of transistors per chip grows.[19] This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However, the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible.[20] In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency.
Computer programming is the process of performing a particular computation (or more generally, accomplishing a specific computing result), usually by designing and building an executable computer program. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms (usually in a chosen programming language, commonly referred to as coding).[1][2] The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus usually requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
Tasks accompanying and related to programming include testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices. Reverse engineering is a related process used by designers, analysts, and programmers to understand an existing program and re-implement its function.[3]
Code-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.[8]
The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.[9]
In the 1880s Herman Hollerith invented the concept of storing data in machine-readable form.[10] Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers. However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.[11]
Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.
These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target for varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier[15] by allowing programmers to specify calculations by entering a formula using infix notation.
Programs were mostly entered using punched cards or paper tape. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.
Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:[16]
[17]
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.[19]
Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[20] Some of these factors include:
The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.
The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.
Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
It is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language,[21] the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers[22] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.
After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash. Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear. Scripting and breakpointing is also part of this process.
Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.
Programming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.
Allen Downey, in his book How To Think Like A Computer Scientist, writes:
Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Computer programmers are those who write computer software. Their jobs usually involve:
An academic discipline or academic field is a subdivision of knowledge that is taught and researched at the college or university level. Disciplines are defined (in part) and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties within colleges and universities to which their practitioners belong. Academic disciplines are conventionally divided into the humanities, including language, art and cultural studies, and the scientific disciplines, such as physics, chemistry, and biology; the social sciences are sometimes considered a third category.
Individuals associated with academic disciplines are commonly referred to as experts or specialists. Others, who may have studied liberal arts or systems theory rather than concentrating in a specific academic discipline, are classified as generalists.
While academic disciplines in and of themselves are more or less focused practices, scholarly approaches such as multidisciplinarity/interdisciplinarity, transdisciplinarity, and cross-disciplinarity integrate aspects from multiple academic disciplines, therefore addressing any problems that may arise from narrow concentration within specialized fields of study. For example, professionals may encounter trouble communicating across academic disciplines because of differences in language, specified concepts, or methodology.
Some researchers believe that academic disciplines may, in the future, be replaced by what is known as Mode 2[1] or "post-academic science",[2] which involves the acquisition of cross-disciplinary knowledge through the collaboration of specialists from various academic disciplines.
It is also known as a field of study, field of inquiry, research field and branch of knowledge. The different terms are used in different countries and fields.
The University of Paris in 1231 consisted of four faculties: Theology, Medicine, Canon Law and Arts.[3] Educational institutions originally used the term "discipline" to catalog and archive the new and expanding body of information produced by the scholarly community. Disciplinary designations originated in German universities during the beginning of the nineteenth century.
Most academic disciplines have their roots in the mid-to-late-nineteenth century secularization of universities, when the traditional curricula were supplemented with non-classical languages and literatures, social sciences such as political science, economics, sociology and public administration, and natural science and technology disciplines such as physics, chemistry, biology, and engineering.
In the early twentieth century, new academic disciplines such as education and psychology were added. In the 1970s and 1980s, there was an explosion of new academic disciplines focusing on specific themes, such as media studies, women's studies, and Africana studies. Many academic disciplines designed as preparation for careers and professions, such as nursing, hospitality management, and corrections, also emerged in the universities. Finally, interdisciplinary scientific fields of study such as biochemistry and geophysics gained prominence as their contribution to knowledge became widely recognized. Some new disciplines, such as public administration, can be found in more than one disciplinary setting; some public administration programs are associated with business schools (thus emphasizing the public management aspect), while others are linked to the political science field (emphasizing the policy analysis aspect).
As the twentieth century approached, these designations were gradually adopted by other countries and became the accepted conventional subjects. However, these designations differed between various countries.[4] In the twentieth century, the natural science disciplines included: physics, chemistry, biology, geology, and astronomy. The social science disciplines included: economics, politics, sociology, and psychology.
Prior to the twentieth century, few opportunities existed for science as an occupation outside the educational system. Higher education provided the institutional structure for scientific investigation, as well as economic support for research and teaching. Soon, the volume of scientific information rapidly increased and researchers realized the importance of concentrating on smaller, narrower fields of scientific activity. Because of this narrowing, scientific specializations emerged. As these specializations developed, modern scientific disciplines in universities also improved their sophistication. Eventually, academia's identified disciplines became the foundations for scholars of specific specialized interests and expertise.[6]
Communities of academic disciplines can be found outside academia within corporations, government agencies, and independent organizations, where they take the form of associations of professionals with common interests and specific knowledge. Such communities include corporate think tanks, NASA, and IUPAC. Communities such as these exist to benefit the organizations affiliated with them by providing specialized new ideas, research, and findings.
Nations at various developmental stages will find the need for different academic disciplines during different times of growth. A newly developing nation will likely prioritize government, political matters and engineering over those of the humanities, arts and social sciences. On the other hand, a well-developed nation may be capable of investing more in the arts and social sciences. Communities of academic disciplines would contribute at varying levels of importance during different stages of development.
These categories explain how the different academic disciplines interact with one another.
Multidisciplinary knowledge is associated with more than one existing academic discipline or profession.
A multidisciplinary community or project is made up of people from different academic disciplines and professions. These people are engaged in working together as equal stakeholders in addressing a common challenge. A multidisciplinary person is one with degrees from two or more academic disciplines. This one person can take the place of two or more people in a multidisciplinary community. Over time, multidisciplinary work does not typically lead to an increase or a decrease in the number of academic disciplines. One key question is how well the challenge can be decomposed into subparts, and then addressed via the distributed knowledge in the community. The lack of shared vocabulary between people and communication overhead can sometimes be an issue in these communities and projects. If challenges of a particular type need to be repeatedly addressed so that each one can be properly decomposed, a multidisciplinary community can be exceptionally efficient and effective.[citation needed]
There are many examples of a particular idea appearing in different academic disciplines, all of which came about around the same time. One example of this scenario is the shift from the approach of focusing on sensory awareness of the whole, "an attention to the 'total field'", a "sense of the whole pattern, of form and function as a unity", an "integral idea of structure and configuration". This has happened in art (in the form of cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from the era of mechanization, which brought sequentiality, to the era of the instant speed of electricity, which brought simultaneity.[8]
Multidisciplinary approaches also encourage people to help shape the innovation of the future. The political dimensions of forming new multidisciplinary partnerships to solve the so-called societal Grand Challenges were presented in the Innovation Union and in the European Framework Programme, the Horizon 2020 operational overlay. Innovation across academic disciplines is considered the pivotal foresight of the creation of new products, systems, and processes for the benefit of all societies' growth and wellbeing. Regional examples such as Biopeople and industry-academia initiatives in translational medicine such as SHARE.ku.dk in Denmark provide evidence of the successful endeavour of multidisciplinary innovation and facilitation of the paradigm shift.[citation needed]
In practice, transdisciplinary can be thought of as the union of all interdisciplinary efforts. While interdisciplinary teams may be creating new knowledge that lies between several existing disciplines, a transdisciplinary team is more holistic and seeks to relate all disciplines into a coherent whole.
Cross-disciplinary knowledge is that which explains aspects of one discipline in terms of another. Common examples of cross-disciplinary approaches are studies of the physics of music or the politics of literature.
Bibliometrics can be used to map several issues in relation to disciplines, for example, the flow of ideas within and among disciplines (Lindholm-Romantschuk, 1998)[9] or the existence of specific national traditions within disciplines.[10] Scholarly impact and influence of one discipline on another may be understood by analyzing the flow of citations.[11]
The Bibliometrics approach is described as straightforward because it is based on simple counting. The method is also objective but the quantitative method may not be compatible with a qualitative assessment and therefore manipulated. The number of citations is dependent on the number of persons working in the same domain instead of inherent quality or published result's originality.[12]
Computer programming is the process of performing a particular computation (or more generally, accomplishing a specific computing result), usually by designing and building an executable computer program. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms (usually in a chosen programming language, commonly referred to as coding).[1][2] The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus usually requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.
Tasks accompanying and related to programming include testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices. Reverse engineering is a related process used by designers, analysts, and programmers to understand an existing program and re-implement its function.[3]
Code-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.[8]
The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine.[9]
In the 1880s Herman Hollerith invented the concept of storing data in machine-readable form.[10] Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers. However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.[11]
Machine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language,  two machines with different instruction sets also have different assembly languages.
These compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target for varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier[15] by allowing programmers to specify calculations by entering a formula using infix notation.
Programs were mostly entered using punched cards or paper tape. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.
Whatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:[16]
[17]
In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.
Readability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.[19]
Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[20] Some of these factors include:
The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.
Various visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.
The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
The first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.
Popular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.
A similar technique used for database design is Entity-Relationship Modeling (ER Modeling).
Implementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.
It is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language,[21] the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).
Some languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers[22] often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).
Debugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.
After the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash. Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear. Scripting and breakpointing is also part of this process.
Debugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.
Different programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from "low-level" to "high-level"; "low-level" languages are typically more machine-oriented and faster to execute, whereas "high-level" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in "high-level" languages than in "low-level" ones.
Programming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.
Allen Downey, in his book How To Think Like A Computer Scientist, writes:
Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
Computer programmers are those who write computer software. Their jobs usually involve:
In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.[3]
As an effective method, an algorithm can be expressed within a finite amount of space and time,[4] and in a well-defined formal language[5] for calculating a function.[6] Starting from an initial state and initial input (perhaps empty),[7] the instructions describe a computation that, when executed, proceeds through a finite[8] number of well-defined successive states, eventually producing "output"[9] and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.[10]
The concept of algorithms has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC.[11] Greek mathematicians later used algorithms in 240 BC in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers.[12] Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.[13]
In English, the word algorithm was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it was not until the late 19th century that "algorithm" took on the meaning that it has in modern English.[26]
Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:
Haec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.
Algorism is the art by which at present we use those Indian figures, which number two times five.
The poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.[27]
An informal definition could be "a set of rules that precisely defines a sequence of operations",[30][need quotation to verify] which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure[31]
or cook-book recipe.[32]
A prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.
Boolos, Jeffrey & 1974, 1999 offer an informal meaning of the word "algorithm" in the following quotation:
An "enumerably infinite set" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that "creates" output integers from an arbitrary "input" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary "input variables" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.
Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.
For some of these computational processes, the algorithm must be rigorously defined: and specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case by case; the criteria for each case must be clear (and computable).
For some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.
There is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called "sets of quadruples" (see Turing machine for more).
Representations of algorithms can be classed into three accepted levels of Turing machine description, as follows:[42]
For an example of the simple algorithm "Add m+n" described in all three levels, see Examples.
Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns,[43] with examples including the template method pattern and the decorator pattern.
One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.
"Elegant" (compact) programs, "good" (fast) programs : The notion of "simplicity and elegance" appears informally in Knuth and precisely in Chaitin:
Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that "It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms".[46]
Computers (and computors), models of computation: A computer (or human "computer"[47]) is a restricted type of machine, a "discrete deterministic mechanical device"[48] that blindly follows its instructions.[49] Melzak's and Lambek's primitive models[50] reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters[51] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[52]
Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example".[56] But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computer must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.[57]
This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).
But what model should be used for the simulation? Van Emde Boas observes "even if we base complexity theory on abstract instead of concrete machines, the arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters".[58] When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a "modulus" instruction available rather than just subtraction (or worse: just Minsky's "decrement").
One of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:
(Quasi-)formal description:
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:
For Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be "proper"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).
Euclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the greatest.[67] While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number "1" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.
E1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.
E2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.
E3: [Interchange s and r]: The nut of Euclid's algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.
The following version can be used with programming languages from the C-family:
Does an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source[68] uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.
But "exceptional cases"[69] must be identified and tested. Will "Inelegant" perform properly when R > S, S > R, R = S? Ditto for "Elegant": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? ("Inelegant" computes forever in all cases; "Elegant" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).
Proof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an "extended" version of Euclid's algorithm, and he proposes "a general method applicable to proving the validity of any algorithm".[70] Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.[71]
Elegance (compactness) versus goodness (speed): With only six core instructions, "Elegant" is the clear winner, compared to "Inelegant" at thirteen instructions. However, "Inelegant" is faster (it arrives at HALT in fewer steps). Algorithm analysis[72] indicates why this is the case: "Elegant" does two conditional tests in every subtraction loop, whereas "Inelegant" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a "B = 0?" test that is needed only after the remainder is computed.
The compactness of "Inelegant" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm;[73] rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with "Elegant" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it "more elegant" than "Elegant", at nine steps.
The speed of "Elegant" can be improved by moving the "B=0?" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now "Elegant" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.
It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.
The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
Empirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.[74]
To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging.[75] In general, speed improvements depend on special properties of the problem, which are very common in practical applications.[76] Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.
There are various ways to classify algorithms, each with its own merits.
Another way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:
Every field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.
Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.
Algorithms can be classified by the amount of time they need to complete compared to their input size:
Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.
The adjective "continuous" when applied to the word "algorithm" can mean:
Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.
Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).
The earliest evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm.[11] During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas.[82] Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.[83]
A good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.[86]
The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.[13]
This machine he displayed in 1870 before the Fellows of the Royal Society.[91] Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: "I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines"; see more at Algorithm characterizations. But not to be outdone he too presented "a plan somewhat analogous, I apprehend, to Prof. Jevon's abacus ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine".[92]
Telephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the "burdensome' use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".[94]
The mathematician Martin Davis observes the particular importance of the electromechanical relay (with its two "binary states" open and closed):
Emil Post (1936) described the actions of a "computer" (human being) as follows:
Alan Turing's work[109] preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: "Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'".[110] Given the prevalence at the time of Morse code, telegraphy, ticker tape machines, and teletypewriters, it is quite possible that all were influences on Turing during his youth.
"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:
A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:
J. Barkley Rosser defined an "effective [mathematical] method" in the following manner (italicization added):
In computer science, a data structure is a data organization, management, and storage format that is usually chosen for efficient access to data.[1][2][3] More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data,[4] i.e., it is an algebraic structure about data.
Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.[5]
Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval,[6] while compiler implementations usually use hash tables to look up identifiers.[7]
Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.[8]
The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).[9]
There are numerous types of data structures, generally built upon simpler primitive data types. Well known examples are:[10]
Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.[12][13]
Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework.
Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose.
Many known data structures have concurrent versions which allow multiple computing threads to access a single concrete instance of a data structure simultaneously.[14]
In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: "What are the fundamental capabilities and limitations of computers?".[1]
The theory of computation can be considered the creation of models of all kinds in the field of computer science. Therefore, mathematics and logic are used. In the last century it became an independent academic discipline and was separated from mathematics.
Language theory is a branch of mathematics concerned with describing languages as a set of operations over an alphabet. It is closely linked with automata theory, as automata are used to generate and recognize formal languages. There are several classes of formal languages, each allowing more complex language specification than the one before it, i.e. Chomsky hierarchy,[6] and each corresponding to a class of automata which recognizes it. Because automata are used as models for computation, formal languages are the preferred mode of specification for any problem that must be computed.
Computability theory deals primarily with the question of the extent to which a problem is solvable on a computer. The statement that the halting problem cannot be solved by a Turing machine[7] is one of the most important results in computability theory, as it is an example of a concrete problem that is both easy to formulate and impossible to solve using a Turing machine.  Much of computability theory builds on the halting problem result.
Another important step in computability theory was Rice's theorem, which states that for all non-trivial properties of partial functions, it is undecidable whether a Turing machine computes a partial function with that property.[8]
Computability theory is closely related to the branch of mathematical logic called recursion theory, which removes the restriction of studying only models of computation which are reducible to the Turing model.[9]  Many mathematicians and computational theorists who study recursion theory will refer to it as computability theory.
Complexity theory considers not only whether a problem can be solved at all on a computer, but also how efficiently the problem can be solved.  Two major aspects are considered: time complexity and space complexity, which are respectively how many steps does it take to perform a computation, and how much memory is required to perform that computation.
In order to analyze how much time and space a given algorithm requires, computer scientists express the time or space required to solve the problem as a function of the size of the input problem.  For example, finding a particular number in a long list of numbers becomes harder as the list of numbers grows larger.  If we say there are n numbers in the list, then if the list is not sorted or indexed in any way we may have to look at every number in order to find the number we're seeking.  We thus say that  in order to solve this problem, the computer needs to perform a number of steps that grows linearly in the size of the problem.
To simplify this problem, computer scientists have adopted Big O notation, which allows functions to be compared in a way that ensures that particular aspects of a machine's construction do not need to be considered, but rather only the asymptotic behavior as problems become large.  So in our previous example, we might say that the problem requires 



O
(
n
)


{\displaystyle O(n)}

 steps to solve.
Perhaps the most important open problem in all of computer science is the question of whether a certain broad class of problems denoted NP can be solved efficiently. This is discussed further at Complexity classes P and NP, and P versus NP problem is one of the seven Millennium Prize Problems stated by the Clay Mathematics Institute in 2000. The Official Problem Description was given by Turing Award winner Stephen Cook.
In addition to the general computational models, some simpler computational models are useful for special, restricted applications.  Regular expressions, for example,  specify string patterns in many contexts, from office productivity software to programming languages. Another formalism mathematically equivalent to regular expressions, Finite automata are used in circuit design and in some kinds of problem-solving. Context-free grammars  specify programming language syntax.  Non-deterministic pushdown automata are another formalism equivalent to context-free grammars. Primitive recursive functions are a defined subclass of the recursive functions.
Different models of computation have the ability to do different tasks. One way to measure the power of a computational model is to study the class of formal languages that the model can generate; in such a way to the Chomsky hierarchy of languages is obtained.
(There are many textbooks in this area; this list is by necessity incomplete.)
In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized.[1] The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.
Models of computation can be classified into three categories: sequential models, functional models, and concurrent models.
Some of these models have both deterministic and  nondeterministic variants. Nondeterministic models are not useful for practical computation;[citation needed] they are used in the study of computational complexity of algorithms.
Models differ in their expressive power; for example, each function that can be computed by a Finite state machine can also be computed by a Turing machine, but not vice versa.
In the field of runtime analysis of algorithms, it is common to specify a  computational model in terms of primitive operations allowed which have unit cost, or simply unit-cost operations. A commonly used example is the random-access machine, which has unit cost for read and write access to all of its memory cells. In this respect, it differs from the above-mentioned Turing machine model.
In theoretical computer science, a computational problem is a problem that may be solved by an algorithm. For example, the problem of factoring
is a computational problem. A computational problem can be viewed as a set of instances or cases together with a, possibly empty, set of solutions for every instance/case. For example, in the factoring problem, the instances are the integers n, and solutions are prime numbers p that are the nontrivial prime factors of n.
Computational problems are one of the main objects of study in theoretical computer science. The field of computational complexity theory attempts to determine the amount of resources (computational complexity) solving a given problem will require and explain why some problems are intractable or undecidable. Computational problems belong to complexity classes that define broadly the resources (e.g. time, space/memory, energy, circuit depth) it takes to compute (solve) them with various abstract machines. For example, the complexity class P for classical machines, and BQP for quantum machines.
Both instances and solutions are represented by binary strings, namely elements of {0, 1}*.[a] For example, natural numbers are usually represented as binary strings using binary encoding. This is important since the complexity is expressed as a function of the length of the input representation.
A decision problem is a computational problem where the answer for every instance is either yes or no. An example of a decision problem is primality testing:
A decision problem is typically represented as the set of all instances for which the answer is yes. For example, primality testing can be represented as the infinite set
In a search problem, the answers can be arbitrary strings. For example, factoring is a search problem where the instances are (string representations of) positive integers and the solutions are (string representations of) collections of primes.
A search problem is represented as a relation consisting of all the instance-solution pairs, called a search relation. For example, factoring can be represented as the relation
which consist of all pairs of numbers (n, p), where p is a nontrivial prime factor of n.
A counting problem asks for the number of solutions to a given search problem. For example, a counting problem associated with factoring is
A counting problem can be represented by a function f from {0, 1}* to the nonnegative integers. For a search relation R, the counting problem associated to R is the function
An optimization problem asks for finding a "best possible" solution among the set of all possible solutions to a search problem. One example is the maximum independent set problem:
In a function problem a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem, that is, it isn't just "yes" or "no". One of the most famous examples is the  traveling salesman problem:
It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.
In computational complexity theory, it is usually implicitly assumed that any string in {0, 1}* represents an instance of the computational problem in question. However, sometimes not all strings {0, 1}* represent valid instances, and one specifies a proper subset of {0, 1}* as the set of "valid instances".  Computational problems of this type are called promise problems.
The following is an example of a (decision) promise problem:
Here, the valid instances are those graphs whose maximum independent set size is either at most 5 or at least 10.
Promise problems play an important role in several areas of computational complexity, including hardness of approximation, property testing, and interactive proof systems.
Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed "computationally secure"; theoretical advances (e.g., improvements in integer factorization algorithms) and faster computing technology require these designs to be continually reevaluated, and if necessary, adapted.  Information-theoretically secure schemes that provably cannot be broken even with unlimited computing power, such as the one-time pad, are much more difficult to use in practice than the best theoretically breakable, but computationally secure, schemes.
The growth of cryptographic technology has raised a number of legal issues in the Information Age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export.[7] In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation.[8][9] Cryptography also plays a major role in digital rights management and copyright infringement disputes in regard to digital media.[10]
Until modern times, cryptography referred almost exclusively to "encryption", which is the process of converting ordinary information (called plaintext) into an unintelligible form (called ciphertext).[13] Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that carry out the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and, in each instance, by a "key". The key is a secret (ideally known only to the communicants), usually a string of characters (ideally short so it can be remembered by the user), which is needed to decrypt the ciphertext.  In formal mathematical terms, a "cryptosystem" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms that correspond to each key.  Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes. Historically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks.
In colloquial use, the term "code" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning: the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, "wallaby" replaces "attack at dawn").  A cypher, in contrast, is a scheme for changing or substituting an element below such a level (a letter, a syllable, or a pair of letters, etc.) in order to produce a cyphertext.
Cryptanalysis is the term used for the study of methods for obtaining the meaning of encrypted information without access to the key normally required to do so; i.e., it is the study of how to "crack" encryption algorithms or their implementations.
The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics.
The Greeks of Classical times are said to have known of ciphers (e.g., the scytale transposition cipher claimed to have been used by the Spartan military).[21] Steganography (i.e., hiding even the existence of a message so as to keep it confidential) was also first developed in ancient times. An early example, from Herodotus, was a message tattooed on a slave's shaved head and concealed under the regrown hair.[13] More modern examples of steganography include the use of invisible ink, microdots, and digital watermarks to conceal information.
Ciphertexts produced by a classical cipher (and some modern ciphers) will reveal statistical information about the plaintext, and that information can often be used to break the cipher. After the discovery of frequency analysis, perhaps by the Arab mathematician and polymath Al-Kindi (also known as Alkindus) in the 9th century,[25] nearly all such ciphers could be broken by an informed attacker. Such classical ciphers still enjoy popularity today, though mostly as puzzles (see cryptogram). Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu'amma (Manuscript for the Deciphering Cryptographic Messages), which described the first known use of frequency analysis cryptanalysis techniques.[25][26]
Language letter frequencies may offer little help for some extended historical encryption techniques such as homophonic cipher that tend to flatten the frequency distribution. For those ciphers, language letter group (or n-gram) frequencies may provide an attack.
Cryptanalysis of the new mechanical ciphering devices proved to be both difficult and laborious. In the United Kingdom, cryptanalytic efforts at Bletchley Park during WWII spurred the development of more efficient means for carrying out repetitious tasks, such as military code breaking (decryption). This culminated in the development of the Colossus, the world's first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army's Lorenz SZ40/42 machine.
Some modern cryptographic techniques can only keep their keys secret if certain mathematical problems are intractable, such as the integer factorization or the discrete logarithm problems, so there are deep connections with abstract mathematics. There are very few cryptosystems that are proven to be unconditionally secure. The one-time pad is one, and was proven to be so by Claude Shannon. There are a few important algorithms that have been proven secure under certain assumptions. For example, the infeasibility of factoring extremely large integers is the basis for believing that RSA is secure, and some other systems, but even so, proof of unbreakability is unavailable since the underlying mathematical problem remains open. In practice, these are widely used, and are believed unbreakable in practice by most competent observers.  There are systems similar to RSA, such as one by Michael O. Rabin that are provably secure provided factoring  n = pq is impossible;  it is quite unusable in practice. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure relative to the solvability or insolvability discrete log problem.[33]
As well as being aware of cryptographic history, cryptographic algorithm and system designers must also sensibly consider probable future developments while working on their designs. For instance, continuous improvements in computer processing power have increased the scope of brute-force attacks, so when specifying key lengths, the required key lengths are similarly advancing.[34] The potential impact of quantum computing are already being considered by some cryptographic system designers developing post-quantum cryptography.[when?] The announced imminence of small implementations of these machines may be making the need for preemptive caution rather more than merely speculative.[5]
Prior to the early 20th century, cryptography was mainly concerned with linguistic and lexicographic patterns. Since then cryptography has broadened in scope, and now makes extensive use of mathematical subdisciplines, including information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics.[35] Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition; other kinds of engineering (e.g., civil or chemical engineering) need deal only with neutral natural forces. There is also active research examining the relationship between cryptographic problems and quantum physics.
Just as the development of digital computers and electronics helped in cryptanalysis, it made possible much more complex ciphers. Furthermore, computers allowed for the encryption of any kind of data representable in any binary format, unlike classical ciphers which only encrypted written language texts; this was new and significant. Computer use has thus supplanted linguistic cryptography, both for cipher design and cryptanalysis. Many computer ciphers can be characterized by their operation on binary bit sequences (sometimes in groups or blocks), unlike classical and mechanical schemes, which generally manipulate traditional characters (i.e., letters and digits) directly. However, computers have also assisted cryptanalysis, which has compensated to some extent for increased cipher complexity. Nonetheless, good modern ciphers have stayed ahead of cryptanalysis; it is typically the case that use of a quality cipher is very efficient (i.e., fast and requiring few resources, such as memory or CPU capability), while breaking it requires an effort many orders of magnitude larger, and vastly larger than that required for any classical cipher, making cryptanalysis so inefficient and impractical as to be effectively impossible.
Symmetric-key cryptography refers to encryption methods in which both the sender and receiver share the same key (or, less commonly, in which their keys are different, but related in an easily computable way). This was the only kind of encryption publicly known until June 1976.[31]
Symmetric key ciphers are implemented as either block ciphers or stream ciphers. A block cipher enciphers input in blocks of plaintext as opposed to individual characters, the input form used by a stream cipher.
The Data Encryption Standard (DES) and the Advanced Encryption Standard (AES) are block cipher designs that have been designated cryptography standards by the US government (though DES's designation was finally withdrawn after the AES was adopted).[36] Despite its deprecation as an official standard, DES (especially its still-approved and much more secure triple-DES variant) remains quite popular; it is used across a wide range of applications, from ATM encryption[37] to e-mail privacy[38] and secure remote access.[39] Many other block ciphers have been designed and released, with considerable variation in quality. Many, even some designed by capable practitioners, have been thoroughly broken, such as FEAL.[5][40]
Stream ciphers, in contrast to the 'block' type, create an arbitrarily long stream of key material, which is combined with the plaintext bit-by-bit or character-by-character, somewhat like the one-time pad. In a stream cipher, the output stream is created based on a hidden internal state that changes as the cipher operates. That internal state is initially set up using the secret key material. RC4 is a widely used stream cipher.[5] Block ciphers can be used as stream ciphers by generating blocks of a keystream (in place of a Pseudorandom number generator) and applying an XOR operation to each bit of the plaintext with each bit of the keystream.[41]
Message authentication codes (MACs) are much like cryptographic hash functions, except that a secret key can be used to authenticate the hash value upon receipt;[5][42] this additional complication blocks an attack scheme against bare digest algorithms, and so has been thought worth the effort. Cryptographic hash functions are a third type of cryptographic algorithm. They take a message of any length as input, and output a short, fixed-length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it "prudent" from a security perspective to develop a new standard to "significantly improve the robustness of NIST's overall hash algorithm toolkit."[43] Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm.[44] Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.
Symmetric-key cryptosystems use the same key for encryption and decryption of a message, although a message or group of messages can have a different key than others. A significant disadvantage of symmetric ciphers is the key management necessary to use them securely. Each distinct pair of communicating parties must, ideally, share a different key, and perhaps for each ciphertext exchanged as well. The number of keys required increases as the square of the number of network members, which very quickly requires complex key management schemes to keep them all consistent and secret.
Diffie and Hellman's publication sparked widespread academic efforts in finding a practical public-key encryption system. This race was finally won in 1978 by Ronald Rivest, Adi Shamir, and Len Adleman, whose solution has since become known as the RSA algorithm.[49]
Public-key cryptography is also used for implementing digital signature schemes. A digital signature is reminiscent of an ordinary signature; they both have the characteristic of being easy for a user to produce, but difficult for anyone else to forge. Digital signatures can also be permanently tied to the content of the message being signed; they cannot then be 'moved' from one document to another, for any attempt will be detectable. In digital signature schemes, there are two algorithms: one for signing, in which a secret key is used to process the message (or a hash of the message, or both), and one for verification, in which the matching public key is used with the message to check the validity of the signature. RSA and DSA are two of the most popular digital signature schemes. Digital signatures are central to the operation of public key infrastructures and many network security schemes (e.g., SSL/TLS, many VPNs, etc.).[40]
Cryptographic hash functions are cryptographic algorithms that generate and use keys to encrypt data, and such functions may be viewed as keys themselves. They take a message of any length as input, and output a short, fixed-length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it "prudent" from a security perspective to develop a new standard to "significantly improve the robustness of NIST's overall hash algorithm toolkit."[43] Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm.[44] Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.
The goal of cryptanalysis is to find some weakness or insecurity in a cryptographic scheme, thus permitting its subversion or evasion.
It is a common misconception that every encryption method can be broken. In connection with his WWII work at Bell Labs, Claude Shannon proved that the one-time pad cipher is unbreakable, provided the key material is truly random, never reused, kept secret from all possible attackers, and of equal or greater length than the message.[53] Most ciphers, apart from the one-time pad, can be broken with enough computational effort by brute force attack, but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., "work factor", in Shannon's terms) is beyond the ability of any adversary. This means it must be shown that no efficient method (as opposed to the time-consuming brute force method) can be found to break the cipher. Since no such proof has been found to date, the one-time-pad remains the only theoretically unbreakable cipher. Although well-implemented one-time-pad encryption cannot be broken, traffic analysis is still possible.
There are a wide variety of cryptanalytic attacks, and they can be classified in any of several ways. A common distinction turns on what Eve (an attacker) knows and what capabilities are available. In a ciphertext-only attack, Eve has access only to the ciphertext (good modern cryptosystems are usually effectively immune to ciphertext-only attacks). In a known-plaintext attack, Eve has access to a ciphertext and its corresponding plaintext (or to many such pairs). In a chosen-plaintext attack, Eve may choose a plaintext and learn its corresponding ciphertext (perhaps many times); an example is gardening, used by the British during WWII. In a chosen-ciphertext attack, Eve may be able to choose ciphertexts and learn their corresponding plaintexts.[5] Finally in a man-in-the-middle attack Eve gets in between Alice (the sender) and Bob (the recipient), accesses and modifies the traffic and then forwards it to the recipient.[54] Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved).
Cryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a perfect cipher. For example, a simple brute force attack against DES requires one known plaintext and 255 decryptions, trying approximately half of the possible keys, to reach a point at which chances are better than even that the key sought will have been found. But this may not be enough assurance; a linear cryptanalysis attack against DES requires 243 known plaintexts (with their corresponding ciphertexts) and approximately 243 DES operations.[55] This is a considerable improvement over brute force attacks.
Public-key algorithms are based on the computational difficulty of various problems. The most famous of these are the difficulty of integer factorization of semiprimes and the difficulty of calculating discrete logarithms, both of which are not yet proven to be solvable in polynomial time (P) using only a classical Turing-complete computer. Much public-key cryptanalysis concerns designing algorithms in P that can solve these problems, or using other technologies, such as quantum computers. For instance, the best-known algorithms for solving the elliptic curve-based version of discrete logarithm are much more time-consuming than the best-known algorithms for factoring, at least for problems of more or less equivalent size. Thus, to achieve an equivalent strength of encryption, techniques that depend upon the difficulty of factoring large composite numbers, such as the RSA cryptosystem, require larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s.
While pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on actual use of the algorithms in real devices, and are called side-channel attacks. If a cryptanalyst has access to, for example, the amount of time the device took to encrypt a number of plaintexts or report an error in a password or PIN character, he may be able to use a timing attack to break a cipher that is otherwise resistant to analysis. An attacker might also study the pattern and length of messages to derive valuable information; this is known as traffic analysis[56] and can be quite useful to an alert adversary. Poor administration of a cryptosystem, such as permitting too short keys, will make any system vulnerable, regardless of other virtues. Social engineering and other attacks against humans (e.g., bribery, extortion, blackmail, espionage, torture, ...) are usually employed due to being more cost-effective and feasible to perform in a reasonable amount of time compared to pure cryptanalysis by a high margin.
One or more cryptographic primitives are often used to develop a more complex algorithm, called a cryptographic system, or cryptosystem. Cryptosystems (e.g., El-Gamal encryption) are designed to provide particular functionality (e.g., public key encryption) while guaranteeing certain security properties (e.g., chosen-plaintext attack (CPA) security in the random oracle model). Cryptosystems use the properties of the underlying cryptographic primitives to support the system's security properties. As the distinction between primitives and cryptosystems is somewhat arbitrary, a sophisticated cryptosystem can be derived from a combination of several more primitive cryptosystems. In many cases, the cryptosystem's structure involves back and forth communication among two or more parties in space (e.g., between the sender of a secure message and its receiver) or across time (e.g., cryptographically protected backup data). Such cryptosystems are sometimes called cryptographic protocols.
Some widely known cryptosystems include RSA, Schnorr signature, ElGamal encryption, and Pretty Good Privacy (PGP). More complex cryptosystems include electronic cash[57] systems, signcryption systems, etc. Some more 'theoretical'[clarification needed] cryptosystems include interactive proof systems,[58] (like zero-knowledge proofs),[59] systems for secret sharing,[60][61] etc.
Lightweight cryptography (LWC) concerns cryptographic algorithms developed for a strictly constrained environment. The growth of Internet of Things (IoT) has spiked research into the development of lightweight algorithms that are better suited for the environment. An IoT environment requires strict constraints on power consumption, processing power, and security.[62] Algorithms such as PRESENT, AES, and SPECK are examples of the many LWC algorithms that have been developed to achieve the standard set by the National Institute of Standards and Technology.[63]
Cryptography is widely used on the internet to help protect user-data and prevent eavesdropping. To ensure secrecy during transmission, many systems use private key cryptography to protect transmitted information. With public-key systems, one can maintain secrecy without a master key or a large number of keys.[64] But, some algorithms like Bitlocker and Veracrypt are generally not private-public key cryptography. For example, Veracrypt uses a password hash to generate the single private key. However, it can be configured to run in public-private key systems. The C++ opensource encryption library OpenSSL provides free and opensource encryption software and tools. The most commonly used encryption cipher suit is AES,[65] as it has hardware acceleration for all x86 based processors that has AES-NI. A close contender is ChaCha20-Poly1305, which is a stream cipher, however it is commonly used for mobile devices as they are ARM based which does not feature AES-NI instruction set extension.
Cryptography can be used to secure communications by encrypting them. Websites use encryption via HTTPS.[66] "End-to-end" encryption, where only sender and receiver can read messages, is implemented for email in Pretty Good Privacy and for secure messaging in general in WhatsApp, Signal and Telegram.[66]
Operating systems use encryption to keep passwords secret, conceal parts of the system, and ensure that software updates are truly from the system maker.[66] Instead of storing plaintext passwords, computer systems store hashes thereof; then, when a user logs in, the system passes the given password through a cryptographic hash function and compares it to the hashed value on file. In this manner, neither the system nor an attacker has at any point access to the password in plaintext.[66]
Encryption is sometimes used to encrypt one's entire drive. For example, University College London has implemented BitLocker (a program by Microsoft) to render drive data opaque without users logging in.[66]
Cryptographic techniques enable cryptocurrency technologies, such as distributed ledger technologies (e.g., blockchains), which finance cryptoeconomics applications such as decentralized finance (DeFi). Key cryptographic techniques that enable cryptocurrencies and cryptoeconomics include, but are not limited to: cryptographic keys, cryptographic hash functions, asymmetric (public key) encryption, Multi-Factor Authentication (MFA), End-to-End Encryption (E2EE), and Zero Knowledge Proofs (ZKP).
Cryptography has long been of interest to intelligence gathering and law enforcement agencies.[9] Secret communications may be criminal or even treasonous[citation needed]. Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest to civil rights supporters. Accordingly, there has been a history of controversial legal issues surrounding cryptography, especially since the advent of inexpensive computers has made widespread access to high-quality cryptography possible.
In some countries, even the domestic use of cryptography is, or has been, restricted. Until 1999, France significantly restricted the use of cryptography domestically, though it has since relaxed many of these rules. In China and Iran, a license is still required to use cryptography.[7] Many countries have tight restrictions on the use of cryptography. Among the more restrictive are laws in Belarus, Kazakhstan, Mongolia, Pakistan, Singapore, Tunisia, and Vietnam.[67]
In the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography.[9] One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List.[68] Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.
In the 1990s, there were several challenges to US export regulation of cryptography. After the source code for Philip Zimmermann's Pretty Good Privacy (PGP) encryption program found its way onto the Internet in June 1991, a complaint by RSA Security (then called RSA Data Security, Inc.) resulted in a lengthy criminal investigation of Zimmermann by the US Customs Service and the FBI, though no charges were ever filed.[69][70] Daniel J. Bernstein, then a graduate student at UC Berkeley, brought a lawsuit against the US government challenging some aspects of the restrictions based on free speech grounds. The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution.[71]
In 1996, thirty-nine countries signed the Wassenaar Arrangement, an arms control treaty that deals with the export of arms and "dual-use" technologies such as cryptography. The treaty stipulated that the use of cryptography with short key-lengths (56-bit for symmetric encryption, 512-bit for RSA) would no longer be export-controlled.[72] Cryptography exports from the US became less strictly regulated as a consequence of a major relaxation in 2000;[73] there are no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via their browsers (e.g., via Transport Layer Security). The Mozilla Thunderbird and Microsoft Outlook E-mail client programs similarly can transmit and receive emails via TLS, and can send and receive email encrypted with S/MIME. Many Internet users don't realize that their basic application software contains such extensive cryptosystems. These browsers and email programs are so ubiquitous that even governments whose intent is to regulate civilian use of cryptography generally don't find it practical to do much to control distribution or use of cryptography of this quality, so even when such laws are in force, actual enforcement is often effectively impossible.[citation needed]
Another contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy.[9] The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography.[74] DES was designed to be resistant to differential cryptanalysis,[75] a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s.[76] According to Steven Levy, IBM discovered differential cryptanalysis,[70] but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have.
Another instance of the NSA's involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak in order to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs's Principle, as the scheme included a special escrow key held by the government for use by law enforcement (i.e. wiretapping).[70]
Cryptography is central to digital rights management (DRM), a group of techniques for technologically controlling use of copyrighted material, being widely implemented and deployed at the behest of some copyright holders. In 1998, U.S. President Bill Clinton signed the Digital Millennium Copyright Act (DMCA), which criminalized all production, dissemination, and use of certain cryptanalytic techniques and technology (now known or later discovered); specifically, those that could be used to circumvent DRM technological schemes.[77] This had a noticeable impact on the cryptography research community since an argument can be made that any cryptanalytic research violated the DMCA. Similar statutes have since been enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states.
The United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but the law, nonetheless, remains a controversial one. Niels Ferguson, a well-respected cryptography researcher, has publicly stated that he will not release some of his research into an Intel security design for fear of prosecution under the DMCA.[78] Cryptologist Bruce Schneier has argued that the DMCA encourages vendor lock-in, while inhibiting actual measures toward cyber-security.[79] Both Alan Cox (longtime Linux kernel developer) and Edward Felten (and some of his students at Princeton) have encountered problems related to the Act. Dmitry Sklyarov was arrested during a visit to the US from Russia, and jailed for five months pending trial for alleged violations of the DMCA arising from work he had done in Russia, where the work was legal. In 2007, the cryptographic keys responsible for Blu-ray and HD DVD content scrambling were discovered and released onto the Internet. In both cases, the Motion Picture Association of America sent out numerous DMCA takedown notices, and there was a massive Internet backlash[10] triggered by the perceived impact of such notices on fair use and free speech.
In the United Kingdom, the Regulation of Investigatory Powers Act gives UK police the powers to force suspects to decrypt files or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security.[8] Successful prosecutions have occurred under the Act; the first, in 2009,[80] resulted in a term of 13 months' imprisonment.[81] Similar forced disclosure laws in Australia, Finland, France, and India compel individual suspects under investigation to hand over encryption keys or passwords during a criminal investigation.
In the United States, the federal criminal case of United States v. Fricosu addressed whether a search warrant can compel a person to reveal an encryption passphrase or password.[82] The Electronic Frontier Foundation (EFF) argued that this is a violation of the protection from self-incrimination given by the Fifth Amendment.[83] In 2012, the court ruled that under the All Writs Act, the defendant was required to produce an unencrypted hard drive for the court.[84]
In many jurisdictions, the legal status of forced disclosure remains unclear.
As a potential counter-measure to forced disclosure some cryptographic software supports plausible deniability, where the encrypted data is indistinguishable from unused random data (for example such as that of a drive which has been securely wiped).
Computer security, cybersecurity (cyber security), or information technology security (IT security) is the protection of computer systems and networks from attack by malicious actors that may result in unauthorized information disclosure, theft of, or damage to hardware, software, or data, as well as from the disruption or misdirection of the services they provide.[1][2]
The field has become of significance due to the expanded reliance on computer systems, the Internet,[3] and wireless network standards such as Bluetooth and Wi-Fi, and due to the growth of smart devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity is one of the most significant challenges of the contemporary world, due to both the complexity of information systems and the societies they support. Security is of especially high importance for systems that govern large-scale systems with far-reaching physical effects, such as power distribution, elections, and finance.[4][5]
Since the Internet's arrival and with the digital transformation initiated in recent years, the notion of cybersecurity has become a familiar subject in both our professional and personal lives. Cybersecurity and cyber threats have been consistently present for the last 50 years of technological change. In the 1970s and 1980s, computer security was mainly limited to academia until the conception of the Internet, where, with increased connectivity, computer viruses and network intrusions began to take off. After the spread of viruses in the 1990s, the 2000s marked the institutionalization[clarification needed] of cyber threats and cybersecurity.
The April 1967 session organized by Willis Ware at the Spring Joint Computer Conference, and the later publication of the Ware Report, were foundational moments in the history of the field of computer security.[6] Ware's work straddled the intersection of material, cultural, political, and social concerns.[6]
A 1977 NIST publication[7] introduced the CIA triad of confidentiality, integrity, and availability as a clear and simple way to describe key security goals.[8] While still relevant, many more elaborate frameworks have since been proposed.[9][10]
However, in the 1970s and 1980s, there were no grave computer threats because computers and the internet were still developing, and security threats were easily identifiable. Most often, threats came from malicious insiders who gained unauthorized access to sensitive documents and files. Although malware and network breaches existed during the early years, they did not use them for financial gain. By the second half of the 1970s, established computer firms like IBM started offering commercial access control systems and computer security software products.[11]
One of the earliest examples of an attack on a computer network was the computer worm Creeper written by Bob Thomas at BBN, which propagated through the ARPANET in 1971. The program was purely experimental in nature and carried no malicious payload. A later program, Reaper, was created by Ray Tomlinson in 1972 and used to destroy Creeper.
Between September 1986 and June 1987, a group of German hackers performed the first documented case of cyber espionage. The group hacked into American defense contractors, universities, and military base networks and sold gathered information to the Soviet KGB. The group was led by Markus Hess, who was arrested on 29 June 1987. He was convicted of espionage (along with two co-conspirators) on 15 Feb 1990.
In 1988, one of the first computer worms, called the Morris worm, was distributed via the Internet. It gained significant mainstream media attention.
In 1993, Netscape started developing the protocol SSL, shortly after the National Center for Supercomputing Applications (NCSA) launched Mosaic 1.0, the first web browser, in 1993. Netscape had SSL version 1.0 ready in 1994, but it was never released to the public due to many serious security vulnerabilities. These weaknesses included replay attacks and a vulnerability that allowed hackers to alter unencrypted communications sent by users. However, in February 1995, Netscape launched Version 2.0.
The National Security Agency (NSA) is responsible for the protection of U.S. information systems and also for collecting foreign intelligence.[12]
The agency analyzes commonly used software in order to find security flaws, which it reserves for offensive purposes against competitors of the United States. The agency seldom takes defensive action by reporting the flaws to software producers so that they can eliminate them.[13]
NSA contractors created and sold click-and-shoot attack tools to US agencies and close allies, but eventually, the tools made their way to foreign adversaries. In 2016, NSAs own hacking tools were hacked, and they have been used by Russia and North Korea.  NSA's employees and contractors have been recruited at high salaries by adversaries, anxious to compete in cyberwarfare. In 2007, the United States and Israel began exploiting security flaws in the Microsoft Windows operating system to attack and damage equipment used in Iran to refine nuclear materials. Iran responded by heavily investing in their own cyberwarfare capability, which they began using against the United States.[13]
A vulnerability is a weakness in design, implementation, operation, or internal control. Most of the vulnerabilities that have been discovered are documented in the Common Vulnerabilities and Exposures (CVE) database.[citation needed] An exploitable vulnerability is one for which at least one working attack or exploit exists.[14] Vulnerabilities can be researched, reverse-engineered, hunted, or exploited using automated tools or customized scripts.[15][16] To secure a computer system, it is important to understand the attacks that can be made against it, and these threats can typically be classified into one of these categories below:
A backdoor in a computer system, a cryptosystem or an algorithm, is any secret method of bypassing normal authentication or security controls. They may exist for many reasons, including original design or poor configuration. They may have been added by an authorized party to allow some legitimate access, or by an attacker for malicious reasons; but regardless of the motives for their existence, they create a vulnerability.  Backdoors can be very hard to detect, and backdoors are usually discovered by someone who has access to application source code or intimate knowledge of the operating system of the computer.
An unauthorized user gaining physical access to a computer is most likely able to directly copy data from it. They may also compromise security by making operating system modifications, installing software worms, keyloggers, covert listening devices or using wireless microphones. Even when the system is protected by standard security measures, these may be bypassed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.
Eavesdropping is the act of surreptitiously listening to a private computer conversation (communication), typically between hosts on a network. For instance, programs such as Carnivore and NarusInSight have been used by the Federal Bureau of Investigation (FBI) and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact to the outside world) can be eavesdropped upon by monitoring the faint electromagnetic transmissions generated by the hardware. TEMPEST is a specification by the NSA referring to these attacks.
Surfacing in 2017, a new class of multi-vector,[18] polymorphic[19] cyber threats combined several types of attacks and changed form to avoid cybersecurity controls as they spread.
Phishing is the attempt of acquiring sensitive information such as usernames, passwords, and credit card details directly from users by deceiving the users.[20] Phishing is typically carried out by email spoofing or instant messaging, and it often directs users to enter details at a fake website whose look and feel are almost identical to the legitimate one. The fake website often asks for personal information, such as login details and passwords. This information can then be used to gain access to the individual's real account on the real website. Preying on a victim's trust, phishing can be classified as a form of social engineering.  Attackers are using creative ways to gain access to real accounts.  A common scam is for attackers to send fake electronic invoices[21] to individuals showing that they recently purchased music, apps, or others, and instructing them to click on a link if the purchases were not authorized. A more strategic type of phishing is spear-phishing which leverages personal or organization-specific details to make the attacker appear like a trusted source. Spear-phishing attacks target specific individuals, rather than the broad net cast by phishing attempts.[22]
Privilege escalation describes a situation where an attacker with some level of restricted access is able to, without authorization, elevate their privileges or access level. For example, a standard computer user may be able to exploit a vulnerability in the system to gain access to restricted data; or even become root and have full unrestricted access to a system.
Any computational system affects its environment in some form. This effect it has on its environment, includes a wide range of criteria, which can range from electromagnetic radiation, to residual effect on RAM cells which as a consequence make a Cold boot attack possible, to hardware implementation faults that allow for access and or guessing of other values that normally should be inaccessible. In Side-channel attack scenarios, the attacker would gather such information about a system or network to guess its internal state and as a result access the information which is assumed by the victim to be secure.
Social engineering, in the context of computer security, aims to convince a user to disclose secrets such as passwords, card numbers, etc. or grant physical access by, for example, impersonating a senior executive, bank, a contractor, or a customer.[24] This generally involves exploiting peoples trust, and relying on their cognitive biases. A common scam involves emails sent to accounting and finance department personnel, impersonating their CEO and urgently requesting some action. In early 2016, the FBI reported that such business email compromise (BEC) scams had cost US businesses more than $2 billion in about two years.[25]
In May 2016, the Milwaukee Bucks NBA team was the victim of this type of cyber scam with a perpetrator impersonating the team's president Peter Feigin, resulting in the handover of all the team's employees' 2015 W-2 tax forms.[26]
Spoofing is an act of masquerading as a valid entity through the falsification of data (such as an IP address or username), in order to gain access to information or resources that one is otherwise unauthorized to obtain.[27][28] There are several types of spoofing, including:
In 2018, the cybersecurity firm Trellix published research on the life-threatening risk of spoofing in the healthcare industry.[30]
Tampering describes a malicious modification or alteration of data. An intentional but unauthorized act resulting in the modification of a system, components of systems, its intended behavior, or data. So-called Evil Maid attacks and security services planting of surveillance capability into routers are examples.[31]
Malicious software (malware) installed on a computer can leak any information, such as personal information, business information and passwords, can give control of the system to the attacker, and can corrupt or delete data permanently.[32]
Employee behavior can have a big impact on information security in organizations. Cultural concepts can help different segments of the organization work effectively or work against effectiveness toward information security within an organization. Information security culture is the "...totality of patterns of behavior in an organization that contributes to the protection of information of all kinds."[34]
Andersson and Reimers (2014) found that employees often do not see themselves as part of their organization's information security effort and often take actions that impede organizational changes.[35] Indeed, the Verizon Data Breach Investigations Report 2020, which examined 3,950 security breaches, discovered 30% of cybersecurity incidents involved internal actors within a company.[36] Research shows information security culture needs to be improved continuously. In "Information Security Culture from Analysis to Change", authors commented, "It's a never-ending process, a cycle of evaluation and change or maintenance." To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.[37]
The growth in the number of computer systems and the increasing reliance upon them by individuals, businesses, industries, and governments means that there are an increasing number of systems at risk.
The computer systems of financial regulators and financial institutions like the U.S. Securities and Exchange Commission, SWIFT, investment banks, and commercial banks are prominent hacking targets for cybercriminals interested in manipulating markets and making illicit gains.[38] Websites and apps that accept or store credit card numbers, brokerage accounts, and bank account information are also prominent hacking targets, because of the potential for immediate financial gain from transferring money, making purchases, or selling the information on the black market.[39] In-store payment systems and ATMs have also been tampered with in order to gather customer account data and PINs.
Computers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.[40]
The aviation industry is very reliant on a series of complex systems which could be attacked.[41] A simple power outage at one airport can cause repercussions worldwide,[42] much of the system relies on radio transmissions which could be disrupted,[43] and controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore.[44] There is also potential for attack from within an aircraft.[45]
In Europe, with the (Pan-European Network Service)[46] and NewPENS,[47] and in the US with the NextGen program,[48] air navigation service providers are moving to create their own dedicated networks.
The consequences of a successful attack range from loss of confidentiality to loss of system integrity, air traffic control outages, loss of aircraft, and even loss of life.
Desktop computers and laptops are commonly targeted to gather passwords or financial account information or to construct a botnet to attack another target. Smartphones, tablet computers, smart watches, and other mobile devices such as quantified self devices like activity trackers have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. WiFi, Bluetooth, and cell phone networks on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.[49]
The increasing number of home automation devices such as the Nest thermostat are also potential targets.[49]
Large corporations are common targets. In many cases attacks are aimed at financial gain through identity theft and involve data breaches. Examples include the loss of millions of clients' credit card and financial details by Home Depot,[50] Staples,[51] Target Corporation,[52] and Equifax.[53]
Medical records have been targeted in general identify theft, health insurance fraud, and impersonating patients to obtain prescription drugs for recreational purposes or resale.[54] Although cyber threats continue to increase, 62% of all organizations did not increase security training for their business in 2015.[55]
Not all attacks are financially motivated, however: security firm HBGary Federal had a serious series of attacks in 2011 from hacktivist group Anonymous in retaliation for the firm's CEO claiming to have infiltrated their group,[56][57] and Sony Pictures was hacked in 2014 with the apparent dual motive of embarrassing the company through data leaks and crippling the company by wiping workstations and servers.[58][59]
Vehicles are increasingly computerized, with engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver-assistance systems on many models. Additionally, connected cars may use WiFi and Bluetooth to communicate with onboard consumer devices and the cell phone network.[60] Self-driving cars are expected to be even more complex. All of these systems carry some security risk, and such issues have gained wide attention.[61][62][63]
Manufacturers are reacting in numerous ways, with Tesla in 2016 pushing out some security fixes over the air into its cars' computer systems.[67] In the area of autonomous vehicles, in September 2016 the United States Department of Transportation announced some initial safety standards, and called for states to come up with uniform policies.[68][69][70]
Government and military computer systems are commonly attacked by activists[71][72][73] and foreign powers.[74][75][76][77] Local and regional government infrastructure such as traffic light controls, police and intelligence agency communications, personnel records, student records,[78] and financial systems are also potential targets as they are now all largely computerized. Passports and government ID cards that control access to facilities which use RFID can be vulnerable to cloning.
The Internet of things (IoT) is the network of physical objects such as devices, vehicles, and buildings that are embedded with electronics, software, sensors, and network connectivity that enables them to collect and exchange data.[79] Concerns have been raised that this is being developed without appropriate consideration of the security challenges involved.[80][81]
While the IoT creates opportunities for more direct integration of the physical world into computer-based systems,[82][83]
it also provides opportunities for misuse. In particular, as the Internet of Things spreads widely, cyberattacks are likely to become an increasingly physical (rather than simply virtual) threat.[84] If a front door's lock is connected to the Internet, and can be locked/unlocked from a phone, then a criminal could enter the home at the press of a button from a stolen or hacked phone. People could stand to lose much more than their credit card numbers in a world controlled by IoT-enabled devices. Thieves have also used electronic means to circumvent non-Internet-connected hotel door locks.[85]
An attack that targets physical infrastructure and/or human lives is sometimes referred to as a cyber-kinetic attack. As IoT devices and appliances gain currency, cyber-kinetic attacks can become pervasive and significantly damaging.
In distributed generation systems, the risk of a cyber attack is real, according to Daily Energy Insider. An attack could cause a loss of power in a large area for a long period of time, and such an attack could have just as severe consequences as a natural disaster. The District of Columbia is considering creating a Distributed Energy Resources (DER) Authority within the city, with the goal being for customers to have more insight into their own energy use and giving the local electric utility, Pepco, the chance to better estimate energy demand. The D.C. proposal, however, would "allow third-party vendors to create numerous points of energy distribution, which could potentially create more opportunities for cyber attackers to threaten the electric grid."[102]
Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. "Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal."[103]
However, reasonable estimates of the financial cost of security breaches can actually help organizations make rational investment decisions. According to the classic Gordon-Loeb Model analyzing the optimal investment level in information security, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).[104]
As with physical security, the motivations for breaches of computer security vary between attackers. Some are thrill-seekers or vandals, some are activists, others are criminals looking for financial gain. State-sponsored attackers are now common and well resourced but started with amateurs such as Markus Hess who hacked for the KGB, as recounted by Clifford Stoll in The Cuckoo's Egg.
Additionally, recent attacker motivations can be traced back to extremist organizations seeking to gain political advantage or disrupt social agendas.[105] The growth of the internet, mobile technologies, and inexpensive computing devices have led to a rise in capabilities but also to the risk to environments that are deemed as vital to operations. All critical targeted environments are susceptible to compromise and this has led to a series of proactive studies on how to migrate the risk by taking into consideration motivations by these types of actors. Several stark differences exist between the hacker motivation and that of nation state actors seeking to attack based on an ideological preference.[106]
A standard part of threat modeling for any particular system is to identify what might motivate an attack on that system, and who might be motivated to breach it. The level and detail of precautions will vary depending on the system to be secured. A home personal computer, bank, and classified military network face very different threats, even when the underlying technologies in use are similar.[107]
In computer security, a countermeasure is an action, device, procedure or technique that reduces a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.[108][109][110]
Security by design, or alternately secure by design, means that the software has been designed from the ground up to be secure. In this case, security is considered as a main feature.
The Open Security Architecture organization defines IT security architecture as "the design artifacts that describe how the security controls (security countermeasures) are positioned, and how they relate to the overall information technology architecture. These controls serve the purpose to maintain the system's quality attributes: confidentiality, integrity, availability, accountability and assurance services".[111]
Techopedia defines security architecture as "a unified security design that addresses the necessities and potential risks involved in a certain scenario or environment. It also specifies when and where to apply security controls. The design process is generally reproducible." The key attributes of security architecture are:[112]
Practicing security architecture provides the right foundation to systematically address business, IT and security concerns in an organization.
A state of computer security is the conceptual ideal, attained by the use of the three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:
Today, computer security consists mainly of preventive measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real-time filtering and blocking. Another implementation is a so-called physical firewall, which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.
Some organizations are turning to big data platforms, such as Apache Hadoop, to extend data accessibility and machine learning to detect advanced persistent threats.[114]
However, relatively few organizations maintain computer systems with effective detection systems, and fewer still have organized response mechanisms in place. As a result, as Reuters points out: "Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets".[115] The primary obstacle to effective eradication of cybercrime could be traced to excessive reliance on firewalls and other automated detection systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.[citation needed]
In order to ensure adequate security, the confidentiality, integrity and availability of a network, better known as the CIA triad, must be protected and is considered the foundation to information security.[116] To achieve those objectives, administrative, physical and technical security measures should be employed. The amount of security afforded to an asset can only be determined when its value is known.[117]
Vulnerability management is the cycle of identifying, remediating or mitigating vulnerabilities,[118] especially in software and firmware. Vulnerability management is integral to computer security and network security.
Vulnerabilities can be discovered with a vulnerability scanner, which analyzes a computer system in search of known vulnerabilities,[119] such as open ports, insecure software configuration, and susceptibility to malware.  In order for these tools to be effective, they must be kept up to date with every new update the vendor release.  Typically, these updates will scan for the new vulnerabilities that were introduced recently.
Beyond vulnerability scanning, many organizations contract outside security auditors to run regular penetration tests against their systems to identify vulnerabilities. In some sectors, this is a contractual requirement.[120]
Two factor authentication is a method for mitigating unauthorized access to a system or sensitive information. It requires something you know; a password or PIN, and something you have; a card, dongle, cellphone, or another piece of hardware. This increases security as an unauthorized person needs both of these to gain access.
Social engineering and direct computer access (physical) attacks can only be prevented by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Training is often involved to help mitigate this risk, but even in highly disciplined environments (e.g. military organizations), social engineering attacks can still be difficult to foresee and prevent.
Inoculation, derived from inoculation theory, seeks to prevent social engineering and other fraudulent tricks or traps by instilling a resistance to persuasion attempts through exposure to similar or related attempts.[126]
It is possible to reduce an attacker's chances by keeping systems up to date with security patches and updates, using a security scanner[definition needed] and/or hiring people with expertise in security, though none of these guarantee the prevention of an attack. The effects of data loss/damage can be reduced by careful backing up and insurance.
While hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process,[127][128] hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.
One use of the term computer security refers to technology that is used to implement secure operating systems. In the 1980s, the United States Department of Defense (DoD) used the "Orange Book"[138] standards, but the current international standard ISO/IEC 15408, Common Criteria defines a number of progressively more stringent Evaluation Assurance Levels. Many common operating systems meet the EAL4 standard of being "Methodically Designed, Tested and Reviewed", but the formal verification required for the highest levels means that they are uncommon. An example of an EAL6 ("Semiformally Verified Design and Tested") system is INTEGRITY-178B, which is used in the Airbus A380[139]
and several military jets.[140]
In software engineering, secure coding aims to guard against the accidental introduction of security vulnerabilities. It is also possible to create software designed from the ground up to be secure. Such systems are secure by design. Beyond this, formal verification aims to prove the correctness of the algorithms underlying a system;[141]
important for cryptographic protocols for example.
Within computer systems, two of the main security models capable of enforcing privilege separation are access control lists (ACLs) and role-based access control (RBAC).
An access-control list (ACL), with respect to a computer file system, is a list of permissions associated with an object. An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed on given objects.
Role-based access control is an approach to restricting system access to authorized users,[142][143][144]  used by the majority of enterprises with more than 500 employees,[145] and can implement mandatory access control (MAC) or discretionary access control (DAC).
A further approach, capability-based security has been mostly restricted to research operating systems. Capabilities can, however, also be implemented at the language level, leading to a style of programming that is essentially a refinement of standard object-oriented design. An open-source project in the area is the E language.
The end-user is widely recognized as the weakest link in the security chain[146] and it is estimated that more than 90% of security incidents and breaches involve some kind of human error.[147][148] Among the most commonly recorded forms of errors and misjudgment are poor password management, sending emails containing sensitive data and attachments to the wrong recipient, the inability to recognize misleading URLs and to identify fake websites and dangerous email attachments.  A common mistake that users make is saving their user id/password in their browsers to make it easier to log in to banking sites.  This is a gift to attackers who have obtained access to a machine by some means.  The risk may be mitigated by the use of two-factor authentication.[149]
As the human component of cyber risk is particularly relevant in determining the global cyber risk[150] an organization is facing, security awareness training, at all levels, not only provides formal compliance with regulatory and industry mandates but is considered essential[151] in reducing cyber risk and protecting individuals and companies from the great majority of cyber threats.
The focus on the end-user represents a profound cultural change for many security practitioners, who have traditionally approached cybersecurity exclusively from a technical perspective, and moves along the lines suggested by major security centers[152] to develop a culture of cyber awareness within the organization, recognizing that a security-aware user provides an important line of defense against cyber attacks.
Related to end-user training, digital hygiene or cyber hygiene is a fundamental principle relating to information security and, as the analogy with personal hygiene shows, is the equivalent of establishing simple routine measures to minimize the risks from cyber threats. The assumption is that good cyber hygiene practices can give networked users another layer of protection, reducing the risk that one vulnerable node will be used to either mount attacks or compromise another node or network, especially from common cyberattacks.[153] Cyber hygiene should also not be mistaken for proactive cyber defence, a military term.[154]
As opposed to a purely technology-based defense against threats, cyber hygiene mostly regards routine measures that are technically simple to implement and mostly dependent on discipline[155] or education.[156] It can be thought of as an abstract list of tips or measures that have been demonstrated as having a positive effect on personal and/or collective digital security. As such, these measures can be performed by laypeople, not just security experts.
Cyber hygiene relates to personal hygiene as computer viruses relate to biological viruses (or pathogens). However, while the term computer virus was coined almost simultaneously with the creation of the first working computer viruses,[157] the term cyber hygiene is a much later invention, perhaps as late as 2000[158] by Internet pioneer Vint Cerf. It has since been adopted by the Congress[159] and Senate of the United States,[160] the FBI,[161] EU institutions[153] and heads of state.[154]
Responding to attempted security breaches is often very difficult for a variety of reasons, including:
Where an attack succeeds and a breach occurs, many jurisdictions now have in place mandatory security breach notification laws.
Incident response is an organized approach to addressing and managing the aftermath of a computer security incident or compromise with the goal of preventing a breach or thwarting a cyberattack. An incident that is not identified and managed at the time of intrusion typically escalates to a more damaging event such as a data breach or system failure. The intended outcome of a computer security incident response plan is to contain the incident, limit damage and assist recovery to business as usual. Responding to compromises quickly can mitigate exploited vulnerabilities, restore services and processes and minimize losses.[162]
Incident response planning allows an organization to establish a series of best practices to stop an intrusion before it causes damage. Typical incident response plans contain a set of written instructions that outline the organization's response to a cyberattack. Without a documented plan in place, an organization may not successfully detect an intrusion or compromise and stakeholders may not understand their roles, processes and procedures during an escalation, slowing the organization's response and resolution.
There are four key components of a computer security incident response plan:
Some illustrative examples of different types of computer security breaches are given below.
In 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.[165]
In early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion[166] and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.[167]
In early 2013, documents provided by Edward Snowden were published by The Washington Post and The Guardian[173][174] exposing the massive scale of NSA global surveillance. There were also indications that the NSA may have inserted a backdoor in a NIST standard for encryption.[175] This standard was later withdrawn due to widespread criticism.[176] The NSA additionally were revealed to have tapped the links between Google's data centers.[177]
In July 2015, a hacker group is known as The Impact Team successfully breached the extramarital relationship website Ashley Madison, created by Avid Life Media. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company's CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently.[185] When Avid Life Media did not take the site offline the group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned; but the website remained to function.
In June 2021, the cyber attack took down the largest fuel pipeline in the U.S. and led to shortages across the East Coast.[186]
The role of the government is to make regulations to force companies and organizations to protect their systems, infrastructure and information from any cyberattacks, but also to protect its own national infrastructure such as the national power-grid.[189]
The government's regulatory role in cyberspace is complicated. For some, cyberspace was seen as a virtual space that was to remain free of government intervention, as can be seen in many of today's libertarian blockchain and bitcoin discussions.[190]
Many government officials and experts think that the government should do more and that there is a crucial need for improved regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the "industry only responds when you threaten regulation. If the industry doesn't respond (to the threat), you have to follow through."[191] On the other hand, executives from the private sector agree that improvements are necessary, but think that government intervention would affect their ability to innovate efficiently. Daniel R. McCarthy analyzed this public-private partnership in cybersecurity and reflected on the role of cybersecurity in the broader constitution of political order.[192]
On 14 April 2016, the European Parliament and Council of the European Union adopted The General Data Protection Regulation (GDPR) (EU) 2016/679. GDPR, which became enforceable beginning 25 May 2018, provides for data protection and privacy for all individuals within the European Union (EU) and the European Economic Area (EEA). GDPR requires that business processes that handle personal data be built with data protection by design and by default. GDPR also requires that certain organizations appoint a Data Protection Officer (DPO).
Most countries have their own computer emergency response team to protect network security.
Since 2010, Canada has had a cybersecurity strategy.[199][200] This functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure.[201] The strategy has three main pillars: securing government systems, securing vital private cyber systems, and helping Canadians to be secure online.[200][201] There is also a Cyber Incident Management Framework to provide a coordinated response in the event of a cyber incident.[202][203]
The Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada's critical infrastructure and cyber systems. It provides support to mitigate cyber threats, technical support to respond & recover from targeted cyber attacks, and provides online tools for members of Canada's critical infrastructure sectors.[204] It posts regular cybersecurity bulletins[205] & operates an online reporting tool where individuals and organizations can report a cyber incident.[206]
To inform the general public on how to protect themselves online, Public Safety Canada has partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations,[207] and launched the Cyber Security Cooperation Program.[208][209] They also run the GetCyberSafe portal for Canadian citizens, and Cyber Security Awareness Month during October.[210]
Public Safety Canada aims to begin an evaluation of Canada's cybersecurity strategy in early 2015.[201]
Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000.[212]
The National Cyber Security Policy 2013 is a policy framework by the Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyberattacks, and safeguard "information, such as personal information (of web users), financial and banking information and sovereign data". CERT- In is the nodal agency which monitors the cyber threats in the country. The post of National Cyber Security Coordinator has also been created in the Prime Minister's Office (PMO).
The Indian Companies Act 2013 has also introduced cyber law and cybersecurity obligations on the part of Indian directors. Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000 Update in 2013.[213]
Following cyberattacks in the first half of 2013, when the government, news media, television stations, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart for these attacks, as well as incidents that occurred in 2009, 2011,[214] and 2012, but Pyongyang denies the accusations.[215]
In 2013, executive order 13636 Improving Critical Infrastructure Cybersecurity was signed, which prompted the creation of the NIST Cybersecurity Framework.
In response to the Colonial Pipeline ransomware attack[218] President Joe Biden signed Executive Order 14028[219] on May 12, 2021, to increase software security standards for sales to the government, tighten detection and security on existing systems, improve information sharing and training, establish a Cyber Safety Review Board, and improve incident response.
The General Services Administration (GSA) has[when?] standardized the penetration test service as a pre-vetted support service, to rapidly address potential vulnerabilities, and stop adversaries before they impact US federal, state and local governments. These services are commonly referred to as Highly Adaptive Cybersecurity Services (HACS).
The Department of Homeland Security has a dedicated division responsible for the response system, risk management program and requirements for cybersecurity in the United States called the National Cyber Security Division.[220][221] The division is home to US-CERT operations and the National Cyber Alert System.[221] The National Cybersecurity and Communications Integration Center brings together government organizations responsible for protecting computer networks and networked infrastructure.[222]
The third priority of the FBI is to: "Protect the United States against cyber-based attacks and high-technology crimes",[223] and they, along with the National White Collar Crime Center (NW3C), and the Bureau of Justice Assistance (BJA) are part of the multi-agency task force, The Internet Crime Complaint Center, also known as IC3.[224]
In addition to its own specific duties, the FBI participates alongside non-profit organizations such as InfraGard.[225][226]
The United States Cyber Command, also known as USCYBERCOM, "has the mission to direct, synchronize, and coordinate cyberspace planning and operations to defend and advance national interests in collaboration with domestic and international partners."[229] It has no role in the protection of civilian networks.[230][231]
The U.S. Federal Communications Commission's role in cybersecurity is to strengthen the protection of critical communications infrastructure, to assist in maintaining the reliability of networks during disasters, to aid in swift recovery after, and to ensure that first responders have access to effective communications services.[232]
The Food and Drug Administration has issued guidance for medical devices,[233] and the National Highway Traffic Safety Administration[234] is concerned with automotive cybersecurity. After being criticized by the Government Accountability Office,[235] and following successful attacks on airports and claimed attacks on airplanes, the Federal Aviation Administration has devoted funding to securing systems on board the planes of private manufacturers, and the Aircraft Communications Addressing and Reporting System.[236] Concerns have also been raised about the future Next Generation Air Transportation System.[237]
The US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.. [238]
Computer emergency response team is a name given to expert groups that handle computer security incidents. In the US, two distinct organizations exist, although they do work closely together.
There is growing concern that cyberspace will become the next theater of warfare. As Mark Clayton from The Christian Science Monitor wrote in a 2015 article titled "The New Cyber Arms Race":
In the future, wars will not just be fought by soldiers with guns or with planes that drop bombs. They will also be fought with the click of a mouse a half a world away that unleashes carefully weaponized computer programs that disrupt or destroy critical industries like utilities, transportation, communications, and energy. Such attacks could also disable military networks that control the movement of troops, the path of jet fighters, the command and control of warships.[240]
This has led to new terms such as cyberwarfare and cyberterrorism. The United States Cyber Command was created in 2009[241] and many other countries have similar forces.
There are a few critical voices that question whether cybersecurity is as significant a threat as it is made out to be.[242][243][244]
Cybersecurity is a fast-growing field of IT concerned with reducing organizations' risk of hack or data breaches.[245] According to research from the Enterprise Strategy Group, 46% of organizations say that they have a "problematic shortage" of cybersecurity skills in 2016, up from 28% in 2015.[246] Commercial, government and non-governmental organizations all employ cybersecurity professionals. The fastest increases in demand for cybersecurity workers are in industries managing increasing volumes of consumer data such as finance, health care, and retail.[247] However, the use of the term cybersecurity is more prevalent in government job descriptions.[248]
Student programs are also available for people interested in beginning a career in cybersecurity.[255][256] Meanwhile, a flexible and effective option for information security professionals of all experience levels to keep studying is online security training, including webcasts.[257][258] A wide range of certified courses are also available.[259]
In the United Kingdom, a nationwide set of cybersecurity forums, known as the U.K Cyber Security Forum, were established supported by the Government's cybersecurity strategy[260] in order to encourage start-ups and innovation and to address the skills gap[261] identified by the U.K Government.
In Singapore, the Cyber Security Agency has issued a Singapore Operational Technology (OT) Cybersecurity Competency Framework (OTCCF). The framework defines emerging cybersecurity roles in Operational Technology. The OTCCF was endorsed by the Infocomm Media Development Authority (IMDA). It outlines the different OT cybersecurity job positions as well as the technical skills and core competencies necessary. It also depicts the many career paths available, including vertical and lateral advancement opportunities.[262]
The following terms used with regards to computer security are explained below:
Vulnerabilities are flaws in a computer system that weaken the overall security of the device/system. Vulnerabilities can be weaknesses in either the hardware itself, or the software that runs on the hardware. Vulnerabilities can be exploited by a threat actor, such as an attacker, to cross privilege boundaries (i.e. perform unauthorized actions) within a computer system. To exploit a vulnerability, an attacker must have at least one applicable tool or technique that can connect to a system weakness. In this frame, vulnerabilities are also known as the attack surface.
Vulnerability management is a cyclical practice that varies in theory but contains common processes which include: discover all assets, prioritize assets, assess or perform a complete vulnerability scan, report on results, remediate vulnerabilities, verify remediation - repeat. This practice generally refers to software vulnerabilities in computing systems.[1] Agile vulnerability management refers preventing attacks by identifying all vulnerabilities as quickly as possible.[2]
Security bug (security defect) is a narrower concept. There are vulnerabilities that are not related to software: hardware, site, personnel vulnerabilities are examples of vulnerabilities that are not software security bugs.
Constructs in programming languages that are difficult to use properly can manifest large numbers of vulnerabilities.
The Committee on National Security Systems  of United States of America defined vulnerability  in CNSS Instruction No. 4009 dated 26 April 2010 National Information Assurance Glossary:[6]
Many NIST publications define vulnerability  in IT context in different publications: FISMApedia[7] term[8] provide a list. Between them SP 800-30,[9] give a broader one:
According to FAIR vulnerability is related to Control Strength, i.e. the strength of control as compared to a standard measure of force and the threat Capabilities, i.e. the probable level of force that a threat agent is capable of applying against an asset.
Matt Bishop and Dave Bailey[13] give the following definition of computer vulnerability:
A resource (either physical or logical) may have one or more vulnerabilities that can be exploited by a threat actor. The result can potentially compromise the confidentiality, integrity or availability of resources (not necessarily the vulnerable one) belonging to an organization and/or other parties involved (customers, suppliers). The so-called CIA triad is a cornerstone of Information Security.
An attack can be active when it attempts to alter system resources or affect their operation, compromising integrity or availability. A "passive attack" attempts to learn or make use of information from the system but does not affect system resources, compromising confidentiality.[5]
OWASP (see figure) depicts the same phenomenon in slightly different terms: a threat agent through an attack vector exploits a weakness (vulnerability) of the system and the related security controls, causing a technical impact on an IT resource (asset) connected to a business impact.
The overall picture represents the risk factors of the risk scenario.[16]
A set of policies concerned with the information security management system (ISMS), has been developed to manage, according to Risk management principles, the countermeasures to ensure a security strategy is set up following the rules and regulations applicable to a given organization. These countermeasures are also called Security controls, but when applied to the transmission of information, they are called security services.[17]
Vulnerabilities are classified according to the asset class they are related to:[3]
The research has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human:[26] so humans should be considered in their different roles as asset, threat, information resources. Social engineering is an increasing security concern.
The impact of a security breach can be very high.[27] Most legislation sees the failure of IT managers to address IT systems and applications vulnerabilities if they are known to them as misconduct; IT managers have a responsibility to manage IT risk.[28] Privacy law forces managers to act to reduce the impact or likelihood of that security risk. Information technology security audit is a way to let other independent people certify that the IT environment is managed properly and lessen the responsibilities, at least having demonstrated the good faith. Penetration test is a form of verification of the weakness and countermeasures adopted by an organization: a White hat hacker tries to attack an organization's information technology assets, to find out how easy or difficult it is to compromise the IT security.[29] The proper way to professionally manage the IT risk is to adopt an Information Security Management System, such as ISO/IEC 27002 or Risk IT and follow them, according to the security strategy set forth by the upper management.[17]
One of the key concept of information security is the principle of defence in depth, i.e. to set up a multilayer defense system that can:[27]
Intrusion detection system is an example of a class of systems used to detect attacks.
Physical security is a set of measures to physically protect an information asset: if somebody can get physical access to the information asset, it is widely accepted that an attacker can access any information on it or make the resource unavailable to its legitimate users.
Some sets of criteria to be satisfied by a computer, its operating system and applications in order to meet a good security level have been developed: ITSEC and Common criteria are two examples.
Coordinated disclosure (some refer to it as 'responsible disclosure' but that is considered a biased term by others) of vulnerabilities is a topic of great debate. As reported by The Tech Herald in August 2010, "Google, Microsoft, TippingPoint, and Rapid7 have issued guidelines and statements addressing how they will deal with disclosure going forward."[30] The other method is typically full disclosure, when all the details of a vulnerability is publicized, sometimes with the intent to put pressure on the software author to publish a fix more quickly. In January 2014 when Google revealed a Microsoft vulnerability before Microsoft released a patch to fix it, a Microsoft representative called for coordinated practices among software companies in revealing disclosures.[31]
Mitre Corporation maintains an incomplete list of publicly disclosed vulnerabilities in a system called Common Vulnerabilities and Exposures. This information is immediately shared with the National Institute of Standards and Technology (NIST), where each vulnerability is given a risk score using Common Vulnerability Scoring System (CVSS), Common Platform Enumeration (CPE) scheme, and Common Weakness Enumeration. 
Cloud service providers often don't list security issues in their services using the CVE system.[32] There is currently no universal standard for cloud computing vulnerability enumeration, severity assessment, and no unified tracking mechanism.[33] The Open CVDB initiative is a community-driven centralized cloud vulnerability database that catalogs CSP vulnerabilities, and lists the steps users can take to detect or prevent these issues in their own environments.[34]
OWASP maintains a list of vulnerability classes with the aim of educating system designers and programmers, therefore reducing the likelihood of vulnerabilities being written unintentionally into the software.[35]
The time of disclosure of a vulnerability is defined differently in the security community and industry. It is most commonly referred to as "a kind of public disclosure of security information by a certain party". Usually, vulnerability information is discussed on a mailing list or published on a security web site and results in a security advisory afterward.
The time of disclosure is the first date a security vulnerability is described on a channel where the disclosed information on the vulnerability has to fulfill the following requirement:
Many software tools exist that can aid in the discovery (and sometimes removal) of vulnerabilities in a computer system. Though these tools can provide an auditor with a good overview of possible vulnerabilities present, they can not replace human judgment. Relying solely on scanners will yield false positives and a limited-scope view of the problems present in the system.
Vulnerabilities have been found in every major operating system [36] including Windows, macOS, various forms of Unix and Linux, OpenVMS, and others. The only way to reduce the chance of a vulnerability being used against a system is through constant vigilance, including careful system maintenance (e.g. applying software patches), best practices in deployment (e.g. the use of firewalls and access controls) and auditing (both during development and throughout the deployment lifecycle).
It is evident that a pure technical approach cannot always protect physical assets: one should have administrative procedure to let maintenance personnel to enter the facilities and people with adequate knowledge of the procedures, motivated to follow it with proper care. However, technical protections do not necessarily stop Social engineering (security) attacks.
Common types of software flaws that lead to vulnerabilities include:
Some set of coding guidelines have been developed and a large number of static code analyzers has been used to verify that the code follows the guidelines.
Computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content.  Although the term often refers to the study of three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing. 
Computer graphics studies manipulation of visual and geometric information using computational techniques.  It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues.  Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities.
There are several international conferences and journals where the most significant results in computer graphics are published. Among them are the SIGGRAPH and Eurographics conferences and the Association for Computing Machinery (ACM) Transactions on Graphics journal. The joint Eurographics and ACM SIGGRAPH symposium series features the major venues for the more specialized sub-fields: Symposium on Geometry Processing,[1] Symposium on Rendering, Symposium on Computer Animation,[2] and High Performance Graphics.[3]
As in the rest of computer science, conference publications in computer graphics are generally more significant than journal publications (and subsequently have lower acceptance rates).[4][5][6][7]
A broad classification of major subfields in computer graphics might be:
The subfield of geometry studies the representation of three-dimensional objects in a discrete digital setting.  Because the appearance of an object depends largely on its exterior, boundary representations are most commonly used.  Two dimensional surfaces are a good representation for most objects, though they may be non-manifold.  Since surfaces are not finite, discrete digital approximations are used. Polygonal meshes (and to a lesser extent subdivision surfaces) are by far the most common representation, although point-based representations have become more popular recently (see for instance the Symposium on Point-Based Graphics).[8] These representations are Lagrangian, meaning the spatial locations of the samples are independent.  Recently, Eulerian surface descriptions (i.e., where spatial samples are fixed) such as level sets have been developed into a useful representation for deforming surfaces which undergo many topological changes (with fluids being the most notable example).[9]
The subfield of animation studies descriptions for surfaces (and other phenomena) that move or deform over time.  Historically, most work in this field has focused on parametric and data-driven models, but recently physical simulation has become more popular as computers have become more powerful computationally.
Rendering generates images from a model.  Rendering may simulate light transport to create realistic images or it may create images that have a particular artistic style in non-photorealistic rendering.  The two basic operations in realistic rendering are transport (how much light passes from one place to another) and scattering (how surfaces interact with light).  See Rendering (computer graphics) for more information.
Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.
Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.
The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.
Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), and computer vision (3D reconstruction).
Although most algorithms of computational geometry have been developed (and are being developed) for electronic computers, some algorithms were developed for unconventional computers (e.g. optical computers [3])
The primary goal of research in combinatorial computational geometry is to develop efficient algorithms and data structures for solving problems stated in terms of basic geometrical objects: points, line segments, polygons, polyhedra, etc.
Some of these problems seem so simple that they were not regarded as problems at all until the advent of computers. Consider, for example, the Closest pair problem:
One could compute the distances between all the pairs of points, of which there are n(n-1)/2, then pick the pair with the smallest distance. This brute-force algorithm takes O(n2) time; i.e. its execution time is proportional to the square of the number of points. A classic result in computational geometry was the formulation of an algorithm that takes O(n log n). Randomized algorithms that take O(n) expected time,[4] as well as a deterministic algorithm that takes O(n log log n) time,[5] have also been discovered.
The core problems in computational geometry may be classified in different ways, according to various criteria. The following general classes may be distinguished.
In the problems of this category, some input is given and the corresponding output needs to be constructed or found. Some fundamental problems of this type are:
The computational complexity for this class of problems is estimated by the time and space (computer memory) required to solve a given problem instance.
In geometric query problems, commonly known as geometric search problems, the input consists of two parts: the search space part and the query part, which varies over the problem instances. The search space typically needs to be preprocessed, in a way that multiple queries can be answered efficiently.
If the search space is fixed, the computational complexity for this class of problems is usually estimated by:
For the case when the search space is allowed to vary, see "Dynamic problems".
Yet another major class is the dynamic problems, in which the goal is to find an efficient algorithm for finding a solution repeatedly after each incremental modification of the input data (addition or deletion input geometric elements). Algorithms for problems of this type typically involve dynamic data structures. Any of the computational geometric problems may be converted into a dynamic one, at the cost of increased processing time. For example, the range searching problem may be converted into the dynamic range searching problem by providing for addition and/or deletion of the points. The dynamic convex hull problem is to keep track of the convex hull, e.g., for the dynamically changing set of points, i.e., while the input points are inserted or deleted.
The computational complexity for this class of problems is estimated by:
Some problems may be treated as belonging to either of the categories, depending on the context. For example, consider the following problem.
In many applications this problem is treated as a single-shot one, i.e., belonging to the first class. For example, in many applications of computer graphics a common problem is to find which area on the screen is clicked by a pointer. However, in some applications, the polygon in question is invariant, while the point represents a query. For example, the input polygon may represent a border of a country and a point is a position of an aircraft, and the problem is to determine whether the aircraft violated the border. Finally, in the previously mentioned example of computer graphics, in CAD applications the changing input data are often stored in dynamic data structures, which may be exploited to speed-up the point-in-polygon queries.
In some contexts of query problems there are reasonable expectations on the sequence of the queries, which may be exploited either for efficient data structures or for tighter computational complexity estimates. For example, in some cases it is important to know the worst case for the total time for the whole sequence of N queries, rather than for a single query. See also "amortized analysis".
This branch is also known as geometric modelling and computer-aided geometric design (CAGD).
Application areas of computational geometry include shipbuilding, aircraft, and automotive industries.
Below is the list of the major journals that have been publishing research in geometric algorithms. Please notice with the appearance of journals specifically dedicated to computational geometry, the share of geometric publications in general-purpose computer science and computer graphics journals decreased.
Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages. Programming language theory is closely related to other fields including mathematics, software engineering, and linguistics. There are a number of academic conferences and journals in the area.
In some ways, the history of programming language theory predates even the development of programming languages themselves. The lambda calculus, developed by Alonzo Church and Stephen Cole Kleene in the 1930s, is considered by some to be the world's first programming language, even though it was intended to model computation rather than being a means for programmers to describe algorithms to a computer system. Many modern functional programming languages have been described as providing a "thin veneer" over the lambda calculus,[2] and many are easily described in terms of it.
Some other key events in the history of programming language theory since then:
There are several fields of study which either lie within programming language theory, or which have a profound influence on it; many of these have considerable overlap. In addition, PLT makes use of many other branches of mathematics, including computability theory, category theory, and set theory.
Formal semantics is the formal specification of the behaviour of computer programs and programming languages. Three common approaches to describe the semantics or "meaning" of a computer program are denotational semantics, operational semantics and axiomatic semantics.
Type theory is the study of type systems; which are "a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute".[4] Many programming languages are distinguished by the characteristics of their type systems.
Program analysis is the general problem of examining a program and determining key characteristics (such as the absence of classes of program errors). Program transformation is the process of transforming a program in one form (language) to another form.
Comparative programming language analysis seeks to classify programming languages into different types based on their characteristics; broad categories of programming languages are often known as programming paradigms.
Metaprogramming is the generation of higher-order programs which, when executed, produce programs (possibly in a different language, or in a subset of the original language) as a result.
Domain-specific languages are languages constructed to efficiently solve problems of a particular part of domain.
Compiler theory is the theory of writing compilers (or more generally, translators); programs which translate a program written in one language into another form. The actions of a compiler are traditionally broken up into syntax analysis (scanning and parsing), semantic analysis (determining what a program should do), optimization (improving the performance of a program as indicated by some metric; typically execution speed) and code generation (generation and output of an equivalent program in some target language; often the instruction set of a CPU).
Run-time systems refer to the development of programming language runtime environments and their components, including virtual machines, garbage collection, and foreign function interfaces.
Conferences are the primary venue for presenting research in programming languages. The most well known conferences include the Symposium on Principles of Programming Languages (POPL), Programming Language Design and Implementation (PLDI), the International Conference on Functional Programming (ICFP), the International Conference on Object Oriented Programming, Systems, Languages and Applications (OOPSLA) and the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS).
Notable journals that publish PLT research include the ACM Transactions on Programming Languages and Systems (TOPLAS), Journal of Functional Programming (JFP), Journal of Functional and Logic Programming, and Higher-Order and Symbolic Computation.
In computing, a database is an organized collection of data stored and accessed electronically. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
A database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term "database" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.
Formally, a "database" refers to a set of related data and the way it is organized. Access to this data is usually provided by a "database management system" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
Because of the close relationship between them, the term "database" is often used casually to refer to both a database and the DBMS used to manipulate it.
Outside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.[1]
Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:
Both a database and its DBMS conform to the principles of a particular database model.[5] "Database system" refers collectively to the database model, database management system, and database.[6]
Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.[citation needed]
Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.[7]
Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.
The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational,[8] SQL/relational, and post-relational.
The two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another.
The relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018[update] they remain dominant: IBM Db2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS.[9] The dominant database language, standardized SQL for the relational model, has influenced database languages for other data models.[citation needed]
The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term "data-base" in a specific technical sense.[10]
As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.
The CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods:
Later systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However, CODASYL databases were complex and required significant training and effort to produce useful applications.
IBM also had its own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL's network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman's 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems' TOTAL databases are classified as network databases. IMS remains in use as of 2014[update].[11]
Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that were primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a "search" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.[12]
In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd's idea was to organize the data as a number of "tables", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each "fact" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated.
Codd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based.
The use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.
In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a "repeating group" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys.
For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.
As well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.
Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a "language" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.
IBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs.
In 1970, the University of Michigan began development of the MICRO Information Management System[13] based on D.L. Childs' Set-Theoretic Data model.[14][15][16] MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System.[17] The system remained in production until 1998.
In the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.
Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata).
Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R. Though Oracle V1 implementations were completed in 1978, it wasn't until Oracle Version 2 when Ellison beat IBM to market in 1979.[18]
Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).
In Sweden, Codd's paper was also read and Mimer SQL was developed in the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise.
The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation."[19] dBASE was one of the top selling software titles in the 1980s and early 1990s.
XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.
NoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.
In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.
NewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.
Databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).
Databases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.
One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.
Connolly and Begg define database management system (DBMS) as a "software system that enables users to define, create, maintain and control access to the database".[24] Examples of DBMS's include MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.
The functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:[25]
It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities.[26] The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.
Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.
The large major enterprise DBMSs have tended to increase in size and functionality and have involved up to thousands of human years of development effort throughout their lifetime.[a]
A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.
External interaction with the database will be via an application program that interfaces with the DBMS.[29] This can range from a database tool that allows users to execute SQL queries textually or graphically, to a website that happens to use a database to store and search information.
A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.
Database languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:
Database languages are specific to a particular data model. Notable examples include:
Database storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, "data about the data", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Databases as digital objects contain three layers of information which must be stored: the data, the structure, and the semantics. Proper storage of all three layers is needed for future preservation and longevity of the database.[33] Putting data into permanent storage is generally the responsibility of the database engine a.k.a. "storage engine". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems' file systems as intermediates for storage layout), storage properties and configuration settings are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look at the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).
Some DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.
Various low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.
Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.
Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.
With data virtualization, the data used remains in its original locations and real-time access is established to allow analytics across multiple sources. This can aid in resolving some technical difficulties such as compatibility problems when combining data from various platforms, lowering the risk of error caused by faulty data, and guaranteeing that the newest data is used. Furthermore, avoiding the creation of a new database containing personal information can make it easier to comply with privacy regulations. However, with data virtualization, the connection to all necessary data sources must be operational as there is no local copy of the data, which is one of the main drawbacks of the approach.[34]
Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).
Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.
This may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called "subschemas". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.
Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).
Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).
The acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.
A database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database's transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database's conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This is in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help import databases from other popular DBMSs.
After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).
When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.
After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.
Sometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database's data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.
Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques.[36] The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc.
Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as "DevOps for database".[37]
Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.
Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).
The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary "fact" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.
The final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.
Another aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.
A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.
A database management system provides three views of the database data:
While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database.
The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model.[39] The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.
The conceptual view provides a level of indirection between internal and external. On the one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.
Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more.
The database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).
Due to the multidisciplinary nature of HCI, people with different backgrounds contribute to its success.
Poorly designed human-machine interfaces can lead to many unexpected problems. A classic example is the Three Mile Island accident, a nuclear meltdown accident, where investigations concluded that the design of the human-machine interface was at least partly responsible for the disaster.[7][8][9] Similarly, accidents in aviation have resulted from manufacturers' decisions to use non-standard flight instruments or throttle quadrant layouts: even though the new designs were proposed to be superior in basic human-machine interaction, pilots had already ingrained the "standard" layout. Thus, the conceptually good idea had unintended results.
Visions of what researchers in the field seek to achieve might vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities. When pursuing a post-cognitivist perspective, researchers of HCI may seek to align computer interfaces with existing social practices or existing sociocultural values.
Researchers in HCI are interested in developing design methodologies, experimenting with devices, prototyping software and hardware systems, exploring interaction paradigms, and developing models and theories of interaction.
The following experimental design principles are considered, when evaluating a current user interface, or designing a new user interface:
The iterative design process is repeated until a sensible, user-friendly interface is created.[15]
Displays are human-made artifacts designed to support the perception of relevant system variables and facilitate further processing of that information. Before a display is designed, the task that the display is intended to support must be defined (e.g., navigating, controlling, decision making, learning, entertaining, etc.). A user or operator must be able to process whatever information a system generates and displays; therefore, the information must be displayed according to principles to support perception, situation awareness, and understanding.
Christopher Wickens et al. defined 13 principles of display design in their book An Introduction to Human Factors Engineering.[19]
These principles of human perception and information processing can be utilized to create an effective display design. A reduction in errors, a reduction in required training time, an increase in efficiency, and an increase in user satisfaction are a few of the many potential benefits that can be achieved by utilizing these principles.
Certain principles may not apply to different displays or situations. Some principles may also appear to be conflicting, and there is no simple solution to say that one principle is more important than another. The principles may be tailored to a specific design or situation. Striking a functional balance among the principles is critical for an effective design.[20]
Social computing is an interactive and collaborative behavior considered between technology and people. In recent years, there has been an explosion of social science research focusing on interactions as the unit of analysis, as there are a lot of social computing technologies that include blogs, emails,  social networking, quick messaging, and various others. Much of this research draws from psychology, social psychology, and sociology. For example, one study found out that people expected a computer with a man's name to cost more than a machine with a woman's name.[21] Other research finds that individuals perceive their interactions with computers more negatively than humans, despite behaving the same way towards these machines.[22]
In human and computer interactions, a semantic gap usually exists between human and computer's understandings towards mutual behaviors. Ontology, as a formal representation of domain-specific knowledge, can be used to address this problem by solving the semantic ambiguities between the two parties.[23]
Security interactions are the study of interaction between humans and computers specifically as it pertains to information security.  Its aim, in plain terms, is to improve the usability of security features in end user applications.  
Unlike HCI, which has roots in the early days of Xerox PARC during the 1970s, HCISec is a nascent field of study by comparison. Interest in this topic tracks with that of Internet security, which has become an area of broad public concern only in very recent years.
When security features exhibit poor usability, the following are common reasons:
As of 2010[update] the future for HCI is expected[26] to include the following characteristics:
There are also dozens of other smaller, regional, or specialized HCI-related conferences held around the world each year, including:[27]
Software engineering is a systematic engineering approach to software development.[1][2][3]
A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. The term programmer is sometimes used as a synonym, but may also lack connotations of engineering education or skills.
Engineering techniques are used to inform the software development process,[1][4] which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management,[1][4] which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.
Beginning in the 1960s, software engineering was seen as its own type of engineering. Additionally, the development of software engineering was seen as a struggle. It was difficult to keep up with the hardware which caused many problems for software engineers. Problems included software that was over budget, exceeded deadlines, required extensive de-bugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968 NATO held the first Software Engineering conference where issues related to software were addressed: guidelines and best practices for the development of software were established.[5]
In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.  The Process Maturity Levels introduced would become the Capability Maturity Model Integration for Development(CMMI-DEV), which has defined how the US Government evaluates the abilities of a software development team.
Modern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK).[15] Software engineering is considered one of major computing disciplines.[16]
Margaret Hamilton promoted the term "software engineering" during her work on the Apollo program. The term "engineering" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term:
When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new "term" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.[26]
Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering.[27][28] Steve McConnell has said that it is not, but that it should be.[29] Donald Knuth has said that programming is an art and a science.[30] Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused[improper synthesis?]  and should be considered harmful, particularly in the United States.[31]
Requirements engineering is about the elicitation, analysis, specification, and validation of requirements for software. Software requirements can be of three different types. There are functional requirements, non-functional requirements, and domain requirements. The operation of the software should be performed and the proper output should be expected for the user to use. Non-functional requirements deal with issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.[32]
Software design is about the process of defining the architecture, components, interfaces, and other characteristics of a system or component. This is also called software architecture. Software design is divided into three different levels of design. The three levels are interface design, architectural design, and detailed design. Interface design is the interaction between a system and its environment. This happens at a high level of abstraction along with the inner workings of the system. Architectural design has to do with the major components of a system and their responsibilities, properties, interfaces, and their relationships and interactions that occur between them. Detailed design is the internal elements of all the major system components, their properties, relationships, processing, and usually their algorithms and the data structures.[33]
Software construction, the main activity of software development,[1][4] is the combination of programming, unit testing, integration testing, and debugging. Testing during this phase is generally performed by the programmer while the software is under construction, to verify what was just written and decide when the code is ready to be sent to the next step.
Software testing[1][4] is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the product or service under test, with different approaches such as unit testing and integration testing. It is one aspect of software quality. As a separate phase in software development, it is typically performed by quality assurance staff or a developer other than the one who wrote the code.
Software analysis is the process of analyzing the behavior of computer programs regarding a property such as performance, robustness, and security It can be performed without executing the program (static program analysis), during runtime (dynamic program analysis) or in a combination of both.
Software maintenance[1][4] refers to the activities required to provide cost-effective support after shipping the software product. Software maintenance is modifying and updating software applications after distribution to correct faults and to improve its performance. Software has a lot to do with the real world and when the real world changes, software maintenance is required. Software maintenance includes: error correction, optimization, deletion of unused and discarded features, and enhancement of features that already exist. Usually, maintenance takes up about 40% to 80% of the project cost therefore, focusing on maintenance keeps the costs down.[34]
Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.[35]
Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014.[36] A number of universities have Software Engineering degree programs; as of 2010[update], there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.
In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.
Half of all practitioners today have degrees in computer science, information systems, or information technology.[citation needed] A small, but growing, number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering Bachelor's degree in the UK and the world; in the following year, the University of Sheffield established a similar program.[37]  In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States, however, it did not obtain ABET accreditation until 2003, the same time as Rice University, Clarkson University, Milwaukee School of Engineering and Mississippi State University obtained theirs.[38] In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.[citation needed]
Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004[update], in the U.S., about 50 universities offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering Master's degree was established at Seattle University in 1979. Since then graduate software engineering degrees have been made available from many more universities.  Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.
Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer.  In some areas of Canada, such as Alberta, British Columbia, Ontario,[41] and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title.
The United States, since 2013, has offered an NCEES Professional Engineer exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized.[42] NCEES will end the exam after April 2019 due to lack of participation.[43] Mandatory licensing is currently still largely debated, and perceived as controversial.[citation needed] In some parts of the US such as Texas, the use of the term Engineer is regulated by law and reserved only for use by individuals who have a Professional Engineer license.[citation needed]
There are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016.[46][47]
Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, educators, and researchers.
Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008.[48] Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.[49]
The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture.[60] IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies.[61] These certification programs are tailored to the institutions that would employ people who use these technologies.
Broader certification of general software engineering skills is available through various professional societies. As of 2006[update], the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP).[62] In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA).[63] The ACM had a professional certification program in the early 1980s,[citation needed] which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.[64]
In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the Institution of Engineering and Technology and so qualify for Chartered Engineer status. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP).[65] In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng.[66] The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.
The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers.[67] Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected.[68][69] Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations.[70] When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.
There are several prizes in the field of software engineering:[72]
Software engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls[by whom?] for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.
Software engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.
One of the core issues in software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a "theoretical environment."
Edsger Dijkstra, the founder of many of the concepts used within software development today, rejected the idea of "software engineering" up until his death in 2002, arguing that those terms were poor analogies for what
he called the "radical novelty" of computer science:
A number of these phenomena have been bundled under the name "Software Engineering". As economics is known as "The Miserable Science", software engineering should be known as "The Doomed Discipline", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter "How to program if you cannot."[73]
An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.
Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.
The dominant general-purpose personal computer operating system is Microsoft Windows with a market share of around 74.99%. macOS by Apple Inc. is in second place (14.84%), and the varieties of Linux are collectively in third place (2.81%).[3] In the mobile sector (including smartphones and tablets), Android's share is 70.82% in the year 2020.[4] According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent.[5] Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems),[6][7] such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.
Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. USB stick).
Single-user operating systems have no facilities to distinguish users but may allow multiple programs to run in tandem.[8] A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users.
A distributed operating system manages a group of distinct, networked computers and makes them appear to be a single computer, as all computations are distributed (divided amongst the constituent computers).[9]
Embedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines with less autonomy (e.g. PDAs). They are very compact and extremely efficient by design and are able to operate with a limited amount of resources. Windows CE and Minix 3 are some examples of embedded operating systems.
A real-time operating system is an operating system that guarantees to process events or data by a specific moment in time. A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. Such an event-driven system switches between tasks based on their priorities or external events, whereas time-sharing operating systems switch tasks based on clock interrupts.
A library operating system is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries and composed with the application and configuration code to construct a unikernel: a specialized, single address space, machine image that can be deployed to cloud or embedded environments[further explanation needed].
Early computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing. Operating systems did not exist in their modern and more complex forms until the early 1960s.[10]   Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.
In the 1940s, the earliest electronic digital systems had no operating systems.  Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plugboards.  These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards.  After programmable general-purpose computers were invented, machine languages(consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that sped up the programming process (Stern, 1981).[full citation needed]
In the early 1950s, a computer could execute only one program at a time.  Each user had sole use of the computer for a limited period and would arrive at a scheduled time with their program and data on punched paper cards or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the universal Turing machine.[10]
Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and compiling (generating machine code from human-readable symbolic code). This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England, the job queue was at one time a washing line (clothesline) from which tapes were hung with different colored clothes-pegs to indicate job priority.[citation needed]
By the late 1950s, programs that one would recognize as an operating system were beginning to appear. Often pointed to as the earliest recognizable example is GM-NAA I/O, released in 1956 on the IBM 704. The first known example that actually referred to itself was the SHARE Operating System, a development of GM-NAA I/O, released in 1959. In a May 1960 paper describing the system, George Ryckman noted:
The development of computer operating systems have materially aided the problem of getting a program or series of programs on and off the computer efficiently.[11]
One of the more famous examples that is often found in discussions of early systems is the Atlas Supervisor, running on the  Atlas in 1962.[12] It was referred to as such in a December 1961 article describing the system, but the context of "the Operating System" is more along the lines of "the system operates in the fashion". The Atlas team itself used the term "supervisor",[13] which was widely used along with "monitor". Brinch Hansen described it as "the most significant breakthrough in the history of operating systems."[14]
Through the 1950s, many major features were pioneered in the field of operating systems on mainframe computers, including batch processing, input/output interrupting, buffering, multitasking, spooling, runtime libraries, link-loading, and programs for sorting records in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications.  In 1959, the SHARE Operating System was released as an integrated utility for the IBM 704, and later in the 709 and 7090 mainframes, although it was quickly supplanted by IBSYS/IBJOB on the 709, 7090 and 7094, which in turn influenced the later 7040-PR-150 (7040/7044) and 1410-PR-155 (1410/7010) operating systems.
During the 1960s, IBM's OS/360 introduced the concept of a single OS spanning an entire product line, which was crucial for the success of the System/360 machines. IBM's current mainframe operating systems are distant descendants of this original system and modern machines are backward compatible with applications written for OS/360.[citation needed]
OS/360 also pioneered the concept that the operating system keeps track of all of the system resources that are used, including program and data space allocation in main memory and file space in secondary storage, and file locking during updates. When a process is terminated for any reason, all of these resources are re-claimed by the operating system.
The alternative CP-67 system for the S/360-67 started a whole line of IBM operating systems focused on the concept of virtual machines. Other operating systems used on IBM S/360 series mainframes included systems developed by IBM: DOS/360[a] (Disk Operating System), TSS/360 (Time Sharing System), TOS/360 (Tape Operating System), BOS/360 (Basic Operating System), and ACP (Airline Control Program), as well as a few non-IBM systems: MTS (Michigan Terminal System), MUSIC (Multi-User System for Interactive Computing), and ORVYL (Stanford Timesharing System).
Control Data Corporation developed the SCOPE operating system in the 1960s, for batch processing. In cooperation with the University of Minnesota, the Kronos and later the NOS operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was an extension of the Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois developed the PLATO operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games.
In 1961, Burroughs Corporation introduced the B5000 with the MCP (Master Control Program) operating system. The B5000 was a stack machine designed to exclusively support high-level languages with no assembler;[b] indeed, the MCP was the first OS to be written exclusively in a high-level language (ESPOL, a dialect of ALGOL). MCP also introduced many other ground-breaking innovations, such as being the first commercial implementation of virtual memory. During development of the AS/400, IBM made an approach to Burroughs to license MCP to run on the AS/400 hardware. This proposal was declined by Burroughs management to protect its existing hardware production. MCP is still in use today in the Unisys company's MCP/ClearPath line of computers.
UNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems.[15][16][17] Like all early main-frame systems, this batch-oriented system managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.
General Electric developed General Electric Comprehensive Operating Supervisor (GECOS), which primarily supported batch processing. After its acquisition by Honeywell, it was renamed General Comprehensive Operating System (GCOS).
Bell Labs,[c] General Electric and MIT developed Multiplexed Information and Computing Service (Multics), which introduced the concept of ringed security privilege levels.
Digital Equipment Corporation developed many operating systems for its various computer lines, including TOPS-10 and TOPS-20 time-sharing systems for the 36-bit PDP-10 class systems. Before the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early ARPANET community. RT-11 was a single-user real-time OS for the PDP-11 class minicomputer, and RSX-11 was the corresponding multi-user OS.
From the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in a series. In fact, most 360s after the 360/40 (except the 360/44, 360/75, 360/91, 360/95 and 360/195) were microprogrammed implementations.
The enormous investment in software for these systems made since the 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. Notable supported mainframe operating systems include:
The first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minis; minimalistic operating systems were developed, often loaded from ROM and known as monitors. One notable early disk operating system was CP/M, which was supported on many early microcomputers and was closely imitated by Microsoft's MS-DOS, which became widely popular as the operating system chosen for the IBM PC (IBM's version of it was called IBM DOS or PC DOS). In the 1980s, Apple Computer Inc. (now Apple Inc.) abandoned its popular Apple II series of microcomputers to introduce the Apple Macintosh computer with an innovative graphical user interface (GUI) to the Mac OS.
The introduction of the Intel 80386 CPU chip in October 1985,[18] with 32-bit architecture and paging capabilities, provided personal computers with the ability to run multitasking operating systems like those of earlier minicomputers and mainframes. Microsoft responded to this progress by hiring Dave Cutler, who had developed the VMS operating system for Digital Equipment Corporation. He would lead the development of the Windows NT operating system, which continues to serve as the basis for Microsoft's operating systems line. Steve Jobs, a co-founder of Apple Inc., started NeXT Computer Inc., which developed the NEXTSTEP operating system. NEXTSTEP would later be acquired by Apple Inc. and used, along with code from FreeBSD as the core of Mac OS X (macOS after latest name change).
The GNU Project was started by activist and programmer Richard Stallman with the goal of creating a complete free software replacement to the proprietary UNIX operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the GNU Hurd kernel proved to be unproductive. In 1991, Finnish computer science student Linus Torvalds, with cooperation from volunteers collaborating over the Internet, released the first version of the Linux kernel. It was soon merged with the GNU user space components and system software to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply "Linux" by the software industry, a naming convention that Stallman and the Free Software Foundation remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as BSD, is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and ported to many minicomputers, it eventually also gained a following for use on PCs, mainly as FreeBSD, NetBSD and OpenBSD.
Unix was originally written in assembly language.[19] Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project. B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see History).
The Unix-like family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. The name "UNIX" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. "UNIX-like" is commonly used to refer to the large set of operating systems which resemble the original UNIX.
Unix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas.
Five operating systems are certified by The Open Group (holder of the Unix trademark) as Unix. HP's HP-UX and IBM's AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor's hardware. In contrast, Sun Microsystems's Solaris can run on multiple types of hardware, including x86 and SPARC servers, and PCs. Apple's macOS, a replacement for Apple's earlier (non-Unix) classic Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD. IBM's z/OS UNIX System Services includes a shell and utilities based on Mortice Kerns' InterOpen products.
Unix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.
A subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NeXTSTEP.
In 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T.
Steve Jobs, upon leaving Apple Inc. in 1985, formed NeXT Inc., a company that manufactured high-end computers running on a variation of BSD called NeXTSTEP. One of these computers was used by Tim Berners-Lee as the first webserver to create the World Wide Web.
Developers like Keith Bostic encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. After two years of legal disputes, the BSD project spawned a number of free derivatives, such as NetBSD and FreeBSD (both in 1993), and OpenBSD (from NetBSD in 1995).
macOS (formerly "Mac OS X" and later "OS X")  is a line of open core graphical operating systems developed, marketed, and sold by Apple Inc., the latest of which is pre-loaded on all currently shipping Macintosh computers. macOS is the successor to the original classic Mac OS, which had been Apple's primary operating system since 1984. Unlike its predecessor, macOS is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997.
The operating system was first released in 1999 as Mac OS X Server 1.0, followed in March 2001 by a client version (Mac OS X v10.0 "Cheetah"). Since then, six more distinct "client" and "server" editions of macOS have been released, until the two were merged in OS X 10.7 "Lion".
First introduced as the OpenEdition upgrade to MVS/ESA System Product Version 4 Release 3, announced[21] February 1993 with support for POSIX and other standards.[22][23][24] z/OS UNIX System Services is built on top of MVS services and cannot run independently. While IBM initially introduced OpenEdition to satisfy FIPS requirements, several z/OS component now require UNIX services, e.g., TCP/IP.
The Linux kernel originated in 1991, as a project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel.
Linux is Unix-like, but was developed without any Unix code, unlike BSD and its variants. Because of its open license model, the Linux kernel code is available for study and modification, which resulted in its use on a wide range of computing machinery from supercomputers to smartwatches. Although estimates suggest that Linux is used on only 2.81% of all "desktop" (or laptop) PCs,[3] it has been widely adopted for use in servers[29] and embedded systems[30] such as cell phones. Linux has superseded Unix on many platforms and is used on most supercomputers including the top 385.[31] Many of the same computers are also on Green500 (but in different order), and Linux runs on the top 10. Linux is also commonly used on other small energy-efficient computers, such as smartphones and smartwatches. The Linux kernel is used in some popular distributions, such as Red Hat, Debian, Ubuntu, Linux Mint and Google's Android, ChromeOS, and ChromiumOS.
Microsoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to Intel architecture based computers, with an estimated 88.9 percent total usage share on Web connected computers.[32][33][34][35] The latest version is Windows 11.
In 2011, Windows 7 overtook Windows XP as the most common version in use.[36][37][38]
Microsoft Windows was first released in 1985, as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS[39][40] and 16-bit Windows 3.x[41] drivers. Windows ME, released in 2000, was the last version in the Win9x family. Later versions have all been based on the Windows NT kernel. Current client versions of Windows run on IA-32, x86-64 and ARM microprocessors.[42] In addition Itanium is still supported in older server version Windows Server 2008 R2. In the past, Windows NT supported additional architectures.
Server editions of Windows are widely used, however, Windows' usage on servers is not as widespread as on personal computers as Windows competes against Linux and BSD for server market share.[43][44]
There have been many operating systems that were significant in their day but are no longer so, such as AmigaOS; OS/2 from IBM and Microsoft; classic Mac OS, the non-Unix precursor to Apple's macOS; BeOS; XTS-300; RISC OS; MorphOS; Haiku; BareMetal and FreeMint. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications.
The z/OS operating system for IBM z/Architecture mainframe computers is still being used and developed, and 
OpenVMS, formerly from DEC, is still under active development by VMS Software Inc.  The IBM i operating system for IBM AS/400 and IBM Power Systems midrange computers is also still being used and developed.
Other operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs' Plan 9.
The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.
With the aid of firmware and device drivers, the kernel provides the most basic level of control over all of the computer's hardware devices. It manages memory access for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU's operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc.
The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system.  The operating system is also a set of services which simplify development and execution of application programs. Executing an application program typically involves the creation of a process by the operating system kernel, which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program, which then interacts with the user and with hardware devices. However, in some systems an application can request that the operating system execute another application within the same process, either as a subroutine or in a separate thread, e.g., the LINK and ATTACH facilities of OS/360 and successors..
An interrupt (also known as abort, exception, fault, signal[45] and trap)[46] provides an efficient way for most operating systems to react to the environment. Interrupts cause the central processing unit (CPU) to have a control flow change away from the currently running program to an interrupt handler, also known as an interrupt service routine (ISR).[47][48] An interrupt service routine may cause the central processing unit (CPU) to have a context switch[49] [d]. The details of how a computer processes an interrupt vary from architecture to architecture, and the details of how interrupt service routines behave vary from operating system to operating system.[50] However, several interrupt functions are common.[50] The architecture and operating system must:[50]
Software interrupts may be normally occurring events. It is expected that a time slice will occur, so the kernel will have to perform a context switch.[53] A computer program may set a timer to go off after a few seconds in case too much data causes an algorithm to take too long.[54]
Software interrupts may be error conditions, such as a malformed machine instruction.[54] However, the most common error conditions are division by zero and accessing an invalid memory address.[54]
Users can send messages to the kernel to modify the behavior of a currently running process.[54] For example, in the command-line environment, pressing the interrupt character (usually Control-C) might terminate the currently running process.[54]
To generate software interrupts for x86 CPUs, the INT assembly language instruction is available.[55] The syntax is INT X, where X is the offset number (in hexadecimal format) to the interrupt vector table.
To generate software interrupts in Unix-like operating systems, the kill(pid,signum) system call will send a signal to another process.[56] pid is the process identifier of the receiving process. signum is the signal number (in mnemonic format)[e] to be sent. (The abrasive name of kill was chosen because early implementations only terminated the process.)[57]
In Unix-like operating systems, signals inform processes of the occurrence of asynchronous events.[56] To communicate asynchronously, interrupts are required.[58] One reason a process needs to asynchronously communicate to another process solves a variation of the classic reader/writer problem.[59] The writer receives a pipe from the shell for its output to be sent to the reader's input stream.[60] The command-line syntax is alpha | bravo. alpha will write to the pipe when its computation is ready and then sleep in the wait queue.[61] bravo will then be moved to the ready queue and soon will read from its input stream.[62] The kernel will generate software interrupts to coordinate the piping.[62]
Signals may be classified into 7 categories.[56] The categories are:
Input/Output (I/O) devices are slower than the CPU. Therefore, it would slow down the computer if the CPU had to wait for each I/O to finish. Instead, a computer may implement interrupts for I/O completion, avoiding the need for polling or busy waiting.[63]
Some computers require an interrupt for each character or word, costing a significant amount of CPU time. Direct memory access (DMA) is an architecture feature to allow devices to bypass the CPU and access main memory directly.[64] (Separate from the architecture, a device may perform direct memory access[f] to and from main memory either directly or via a bus.)[65][g]
When a computer user types a key on the keyboard, typically the character appears immediately on the screen. Likewise, when a user moves a mouse, the cursor immediately moves across the screen. Each keystroke and mouse movement generates an interrupt called Interrupt-driven I/O. An interrupt-driven I/O occurs when a process causes an interrupt for every character[65] or word[66] transmitted.
Devices such as hard disk drives, solid state drives, and magnetic tape drives can transfer data at a rate high enough that interrupting the CPU for every byte or word transferred, and having the CPU transfer the byte or word between the device and memory, would require too much CPU time.  Data is, instead, transferred between the device and memory independently of the CPU by hardware such as a channel or a direct memory access controller; an interrupt is delivered only when all the data is transferred.[67]
If a computer program executes a system call to perform a block I/O write operation, then the system call might execute the following instructions:
While the writing takes place, the operating system will context switch to other processes as normal. When the device finishes writing, the device will interrupt the currently running process by asserting an interrupt request. The device will also place an integer onto the data bus.[71] Upon accepting the interrupt request, the operating system will:
When the writing process has its time slice expired, the operating system will:[72]
With the program counter now reset, the interrupted process will resume its time slice.[50]
Modern computers support multiple modes of operation. CPUs with this capability offer at least two modes: user mode and supervisor mode. In general terms, supervisor mode operation allows unrestricted access to all machine resources, including all MPU instructions.  User mode operation sets limits on instruction use and typically disallows direct access to machine resources. CPUs might have other modes similar to user mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one.
At power-on or reset, the system begins in supervisor mode. Once an operating system kernel has been loaded and started, the boundary between user mode and supervisor mode (also known as kernel mode) can be established.
Supervisor mode is used by the kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is accessed, and communicating with devices such as disk drives and video display devices. User mode, in contrast, is used for almost everything else. Application programs, such as word processors and database managers, operate within user mode, and can only access machine resources by turning control over to the kernel, a process which causes a switch to supervisor mode.  Typically, the transfer of control to the kernel is achieved by executing a software interrupt instruction, such as the Motorola 68000 TRAP instruction.  The software interrupt causes the processor to switch from user mode to supervisor mode and begin executing code that allows the kernel to take control.
In user mode, programs usually have access to a restricted set of processor instructions, and generally cannot execute any instructions that could potentially cause disruption to the system's operation.  In supervisor mode, instruction execution restrictions are typically removed, allowing the kernel unrestricted access to all machine resources.
The term "user mode resource" generally refers to one or more CPU registers, which contain information that the running program isn't allowed to alter. Attempts to alter these resources generally cause a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting; for example, by forcibly terminating ("killing") the program.
Among other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by the programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.
Cooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel's memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program's memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.
Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which doesn't exist in all computers.
In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt, which causes the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error.
Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway.
The use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.
If a program tries to access memory that isn't in its current range of accessible memory, but nonetheless has been allocated to it, the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a page fault.
When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has actually been allocated yet.
In modern operating systems, memory which is accessed less frequently can be temporarily stored on a disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.
"Virtual memory" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.[73]
Multitasking refers to the running of multiple independent computer programs on the same computer, giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute.
An operating system kernel contains a scheduling program which determines how much time each process spends executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the CPU and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a context switch.
An early model which governed the allocation of time to programs was called cooperative multitasking. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an infinite loop.
Modern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well.
The philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A protected mode timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.)
On many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. AmigaOS is an exception, having preemptive multitasking from its first version. Windows NT was the first version of Microsoft Windows which enforced preemptive multitasking, but it didn't reach the home user market until Windows XP (since Windows NT was targeted at professionals).
Access to data stored on disks is a central feature of all operating systems. Computers store data on disks using files, which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use of the drive's available space. The specific way in which files are stored on a disk is called a file system, and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a directory tree.
Early operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system.
While many simpler operating systems support a limited range of options for accessing storage systems, operating systems like UNIX and Linux support a technology known as a virtual file system or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or file systems, allowing them to be accessed through a common application programming interface (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific device drivers and file system drivers.
A connected storage device, such as a hard drive, is accessed through a device driver. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of block devices.
When the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates.
Various differences between file systems make supporting all file systems difficult. Allowed characters in file names, case sensitivity, and the presence of various kinds of file attributes makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, NTFS in Windows and ReiserFS, Reiser4, ext3, ext4 and Btrfs in Linux. However, in practice, third party drivers are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through NTFS-3g, and ext2/3 and ReiserFS are available in Windows through third-party software).
Support for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use ISO 9660 or UDF, and as of Windows Vista, NTFS is the only file system which the operating system can be installed on.  It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD, DVD...), a USB flash drive, or even contained within a file located on another file system.
A device driver is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to and/or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.
The key design goal of device drivers is abstraction. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver ensures that the device appears to operate as usual from the operating system's point of view.
Under versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do.
Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer's command line interface.
Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's IP address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.
Many operating systems support one or more vendor-specific or open networking protocols as well, for example, SNA on IBM systems, DECnet on systems from Digital Equipment Corporation, and Microsoft-specific protocols (SMB) on Windows. Specific protocols for specific tasks may also be supported such as NFS for file access. Protocols like ESound, or esd can be easily extended over the network to provide sound from local applications, on a remote system's sound hardware.
A computer being secure depends on a number of technologies working properly. A modern operating system provides access to a number of resources, which are available to software running on the system, and to external devices like networks via the kernel.[74]
The operating system must be capable of distinguishing between requests which should be allowed to be processed, and others which should not be processed. While some systems may simply distinguish between "privileged" and "non-privileged", systems commonly have a form of requester identity, such as a user name. To establish identity there may be a process of authentication. Often a username must be quoted, and each username may have a password. Other methods of authentication, such as magnetic cards or biometric data, might be used instead. In some cases, especially connections from the network, resources may be accessed with no authentication at all (such as reading files over a network share). Also covered by the concept of requester identity is authorization; the particular services and resources accessible by the requester once logged into a system are tied to either the requester's user account or to the variously configured groups of users to which the requester belongs.[citation needed]
In addition to the allow or disallow model of security, a system with a high level of security also offers auditing options. These would allow tracking of requests for access to resources (such as, "who has been reading this file?"). Internal security, or security from an already running program is only possible if all possibly harmful requests must be carried out through interrupts to the operating system kernel. If programs can directly access hardware and resources, they cannot be secured.[citation needed]
External security involves a request from outside the computer, such as a login at a connected console or some kind of network connection. External requests are often passed through device drivers to the operating system's kernel, where they can be passed onto applications, or carried out directly. Security of operating systems has long been a concern because of highly sensitive data held on computers, both of a commercial and military nature. The United States Government Department of Defense (DoD) created the Trusted Computer System Evaluation Criteria (TCSEC) which is a standard that sets basic requirements for assessing the effectiveness of security. This became of vital importance to operating system makers, because the TCSEC was used to evaluate, classify and select trusted operating systems being considered for the processing, storage and retrieval of sensitive or classified information.
Network services include offerings such as file sharing, print services, email, web sites, and file transfer protocols (FTP), most of which can have compromised security. At the front line of security are hardware devices known as firewalls or intrusion detection/prevention systems. At the operating system level, there are a number of software firewalls available, as well as intrusion detection/prevention systems. Most modern operating systems include a software firewall, which is enabled by default. A software firewall can be configured to allow or deny network traffic to or from a service or application running on the operating system. Therefore, one can install and be running an insecure service, such as Telnet or FTP, and not have to be threatened by a security breach because the firewall would deny all traffic trying to connect to the service on that port.
An alternative strategy, and the only sandbox strategy available in systems that do not meet the Popek and Goldberg virtualization requirements, is where the operating system is not running user programs as native code, but instead either emulates a processor or provides a host for a p-code based system such as Java.
Internal security is especially relevant for multi-user systems; it allows each user of the system to have private files that the other users cannot tamper with or read. Internal security is also vital if auditing is to be of any use, since a program can potentially bypass the operating system, inclusive of bypassing auditing.
Every computer that is to be operated by an individual requires a user interface. The user interface is usually referred to as a shell and is essential if human interaction is to be supported.  The user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices, such as a keyboard, mouse or credit card reader, and requests operating system services to display prompts, status messages and such on output hardware devices, such as a video monitor or printer. The two most common forms of a user interface have historically been the command-line interface, where computer commands are typed out line-by-line, and the graphical user interface, where a visual environment (most commonly a WIMP) is present.
Most of the modern computer systems support graphical user interfaces (GUI), and often include them. In some computer systems, such as the original implementation of the classic Mac OS, the GUI is integrated into the kernel.
While technically a graphical user interface is not an operating system service, incorporating support for one into the operating system kernel can allow the GUI to be more responsive by reducing the number of context switches required for the GUI to perform its output functions. Other operating systems are modular, separating the graphics subsystem from the kernel and the Operating System. In the 1980s UNIX, VMS and many others had operating systems that were built this way. Linux and macOS are also built this way. Modern releases of Microsoft Windows such as Windows Vista implement a graphics subsystem that is mostly in user-space; however the graphics drawing routines of versions between Windows NT 4.0 and Windows Server 2003 exist mostly in kernel space. Windows 9x had very little distinction between the interface and the kernel.
Numerous Unix-based GUIs have existed over time, most derived from X11. Competition among the various vendors of Unix (HP, IBM, Sun) led to much fragmentation, though an effort to standardize in the 1990s to COSE and CDE failed for various reasons, and were eventually eclipsed by the widespread adoption of GNOME and K Desktop Environment. Prior to free software-based toolkits and desktop environments, Motif was the prevalent toolkit/desktop combination (and was the basis upon which CDE was developed).
A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.
An early example of a large-scale real-time operating system was Transaction Processing Facility developed by American Airlines and IBM for the Sabre Airline Reservations System.
Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase.[76] Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b.
Some embedded systems use operating systems such as Palm OS, BSD, and Linux, although such operating systems do not support real-time computing.
A hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.[citation needed]
In some cases, hobby development is in support of a "homebrew" computing device, for example, a simple single-board computer powered by a 6502 microprocessor.  Or, development may be for an architecture already in widespread use.  Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system.  In either case, the hobbyist is her/his own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.
Examples of a hobby operating system include Syllable and TempleOS.
Application software is generally written for use on a specific operating system, and sometimes even for specific hardware.[citation needed] When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.
Unix was the first operating system not written in assembly language, making it very portable to systems different from its native PDP-11.[77]
This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.
Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.
A computer network is a set of computers sharing resources located on or provided by network nodes. The computers use common communication protocols over digital interconnections to communicate with each other. These interconnections are made up of telecommunication network technologies, based on physically wired, optical, and wireless radio-frequency methods that may be arranged in a variety of network topologies.
The nodes of a computer network can include personal computers, servers, networking hardware, or other specialized or general-purpose hosts. They are identified by network addresses, and may have hostnames. Hostnames serve as memorable labels for the nodes, rarely changed after initial assignment. Network addresses serve for locating and identifying the nodes by communication protocols such as the Internet Protocol.
Computer networks may be classified by many criteria, including the transmission medium used to carry signals, bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanism, and organizational intent[citation needed].
Computer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications.
Computer networking may be considered a branch of computer science, computer engineering, and telecommunications, since it relies on the theoretical and practical application of the related disciplines. Computer networking was influenced by a wide array of technology developments and historical milestones.
A computer network extends interpersonal communications by electronic means with various technologies, such as email, instant messaging, online chat, voice and video telephone calls, and video conferencing. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer or use of a shared storage device. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. Distributed computing uses computing resources across a network to accomplish tasks.
Most modern computer networks use protocols based on packet-mode transmission. A network packet is a formatted unit of data carried by a packet-switched network.
Packets consist of two types of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.
With packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from other users, and so the cost can be shared, with relatively little interference, provided the link isn't overused. Often the route a packet needs to take through a network is not immediately available. In that case, the packet is queued and waits until a link is free.
The physical link technologies of packet network typically limit the size of packets to a certain maximum transmission unit (MTU). A longer message may be fragmented before it is transferred and once the packets arrive, they are reassembled to construct the original message.
The physical or geographic locations of network nodes and links generally have relatively little effect on a network, but the topology of interconnections of a network can significantly affect its throughput and reliability. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the more expensive it is to install. Therefore, most network diagrams are arranged by their network topology which is the map of logical interconnections of network hosts.
The physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with FDDI, the network topology is a ring, but the physical topology is often a star, because all neighboring connections can be routed via a central physical location. Physical layout is not completely irrelevant, however, as common ducting and equipment locations can represent single points of failure due to issues like fires, power failures and flooding.
An overlay network is a virtual network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links.  Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks.  They are organized as nodes of a virtual system of links that run on top of the Internet.[28]
Overlay networks have been around since the invention of networking when computer systems were connected over telephone lines using modems before any data network existed.
The most striking example of an overlay network is the Internet itself. The Internet itself was initially built as an overlay on the telephone network.[28] Even today, each Internet node can communicate with virtually any other through an underlying mesh of sub-networks of wildly different topologies and technologies. Address resolution and routing are the means that allow mapping of a fully connected IP overlay network to its underlying network.
Another example of an overlay network is a distributed hash table, which maps keys to nodes in the network. In this case, the underlying network is an IP network, and the overlay network is a table (actually a map) indexed by keys.
Overlay networks have also been proposed as a way to improve Internet routing, such as through quality of service guarantees achieve higher-quality streaming media. Previous proposals such as IntServ, DiffServ, and IP multicast have not seen wide acceptance largely because they require modification of all routers in the network.[citation needed]  On the other hand, an overlay network can be incrementally deployed on end-hosts running the overlay protocol software, without cooperation from Internet service providers.  The overlay network has no control over how packets are routed in the underlying network between two overlay nodes, but it can control, for example, the sequence of overlay nodes that a message traverses before it reaches its destination[citation needed].
For example, Akamai Technologies manages an overlay network that provides reliable, efficient content delivery (a kind of multicast).  Academic research includes end system multicast,[29] resilient routing and quality of service studies, among others.
A widely adopted family that uses copper and fiber media in local area network (LAN) technology are collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3.  Wireless LAN standards use radio waves, others use infrared signals as a transmission medium. Power line communication uses a building's power cabling to transmit data.
The following classes of wired technologies are used in computer networking.
Network connections can be established wirelessly using radio or other electromagnetic means of communication.
The last two cases have a large round-trip delay time, which gives slow two-way communication but doesn't prevent sending large amounts of information (they can have high throughput).
Apart from any physical transmission media, networks are built from additional basic system building blocks, such as network interface controllers (NICs), repeaters, hubs, bridges, switches, routers, modems, and firewalls. Any particular piece of equipment will frequently contain multiple building blocks and so may perform multiple functions.
A network interface controller (NIC) is computer hardware that connects the computer to the network media and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.
A repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal is retransmitted at a higher power level, or to the other side of obstruction so that the signal can cover longer distances without degradation. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.
Repeaters work on the physical layer of the OSI model but still require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance and may affect proper function. As a result, many network architectures limit the number of repeaters used in a network, e.g., the Ethernet 5-4-3 rule.
An Ethernet repeater with multiple ports is known as an Ethernet hub. In addition to reconditioning and distributing network signals, a repeater hub assists with collision detection and fault isolation for the network. Hubs and repeaters in LANs have been largely obsoleted by modern network switches.
Network bridges and network switches are distinct from a hub in that they only forward frames to the ports involved in the communication whereas a hub forwards to all ports.[33] Bridges only have two ports but a switch can be thought of as a multi-port bridge. Switches normally have numerous ports, facilitating a star topology for devices, and for cascading additional switches.
Bridges and switches operate at the data link layer (layer 2) of the OSI model and bridge traffic between two or more network segments to form a single local network. Both are devices that forward frames of data between ports based on the destination MAC address in each frame.[34]
They learn the association of physical ports to MAC addresses by examining the source addresses of received frames and only forward the frame when necessary. If an unknown destination MAC is targeted, the device broadcasts the request to all ports except the source, and discovers the location from the reply.
Bridges and switches divide the network's collision domain but maintain a single broadcast domain. Network segmentation through bridging and switching helps break down a large, congested network into an aggregation of smaller, more efficient networks.
A router is an internetworking device that forwards packets between networks by processing the addressing or routing information included in the packet.  The routing information is often processed in conjunction with the routing table.  A router uses its routing table to determine where to forward packets and does not require broadcasting packets which is inefficient for very big networks.
Modems (modulator-demodulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Early modems modulated audio signals sent over a standard voice telephone line. Modems are still commonly used for telephone lines, using a digital subscriber line technology and cable television systems using DOCSIS technology.
A firewall is a network device or software for controlling network security and access rules. Firewalls are inserted in connections between secure internal networks and potentially insecure external networks such as the Internet. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.
A communication protocol is a set of rules for exchanging information over a network. Communication protocols have various characteristics.  They may be connection-oriented or connectionless, they may use circuit mode or packet switching, and they may use hierarchical addressing or flat addressing.
In a protocol stack, often constructed per the OSI model, communications functions are divided up into protocol layers, where each layer leverages the services of the layer below it until the lowest layer controls the hardware that sends information across the media. The use of protocol layering is ubiquitous across the field of computer networking. An important example of a protocol stack is HTTP (the World Wide Web protocol) running over TCP over IP (the Internet protocols) over IEEE 802.11 (the Wi-Fi protocol). This stack is used between the wireless router and the home user's personal computer when the user is surfing the web.
There are many communication protocols, a few of which are described below.
The Internet Protocol Suite, also called TCP/IP, is the foundation of all modern networking. It offers connection-less and connection-oriented services over an inherently unreliable network traversed by datagram transmission using Internet protocol (IP). At its core, the protocol suite defines the addressing, identification, and routing specifications for Internet Protocol Version 4 (IPv4) and for IPv6, the next generation of the protocol with a much enlarged addressing capability. The Internet Protocol Suite is the defining set of protocols for the Internet.[35]
IEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks. The complete IEEE 802 protocol suite provides a diverse set of networking capabilities. The protocols have a flat addressing scheme. They operate mostly at layers 1 and 2 of the OSI model.
Ethernet is a family of technologies used in wired LANs. It is described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers.
Wireless LAN based on the IEEE 802.11 standards, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. IEEE 802.11 shares many properties with wired Ethernet.
Synchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support circuit-switched digital telephony. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames.
Asynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks.  It uses asynchronous time-division multiplexing and encodes data into small, fixed-sized cells. This differs from other protocols such as the Internet Protocol Suite or Ethernet that use variable-sized packets or frames. ATM has similarities with both circuit and packet switched networking.  This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, low-latency content such as voice and video. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins.
ATM still plays a role in the last mile, which is the connection between an Internet service provider and the home user.[38][needs update]
There are a number of different digital cellular standards, including: Global System for Mobile Communications (GSM), General Packet Radio Service (GPRS), cdmaOne, CDMA2000, Evolution-Data Optimized (EV-DO), Enhanced Data Rates for GSM Evolution (EDGE), Universal Mobile Telecommunications System (UMTS), Digital Enhanced Cordless Telecommunications (DECT), Digital AMPS (IS-136/TDMA), and Integrated Digital Enhanced Network (iDEN).[39]
Routing is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks.
In packet-switched networks, routing protocols direct packet forwarding through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though because they lack specialized hardware, may offer limited performance. The routing process directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.
Routing can be contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices.  In large networks, the structured addressing used by routers outperforms unstructured addressing used by bridging. Structured IP addresses are used on the Internet. Unstructured MAC addresses are used for bridging on Ethernet and similar local area networks.
Networks may be characterized by many properties or features, such as physical capacity, organizational purpose, user authorization, access rights, and others. Another distinct classification method is that of the physical extent or geographic scale.
A nanoscale network has key components implemented at the nanoscale, including message carriers, and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for other communication techniques.[40]
A personal area network (PAN) is a computer network used for communication among computers and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters.[41] A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN.
A local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Wired LANs are most commonly based on Ethernet technology.  Other networking technologies such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.[42]
A home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable Internet access or digital subscriber line (DSL) provider.
A storage area network (SAN) is a dedicated network that provides access to consolidated, block-level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the storage appears as locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.[citation needed]
A campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, Cat5 cabling, etc.) are almost entirely owned by the campus tenant or owner (an enterprise, university, government, etc.).
For example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls.
A backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or subnetworks.  A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area. When designing a network backbone, network performance and network congestion are critical factors to take into account.  Normally, the backbone network's capacity is greater than that of the individual networks connected to it.
For example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone. Another example of a backbone network is the Internet backbone, which is a massive, global system of fiber-optic cable and optical networking that carry the bulk of data between wide area networks (WANs), metro, regional, national and transoceanic networks.
A metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area.
A wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances.  A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and airwaves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI model: the physical layer, the data link layer, and the network layer.
An enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources.
A virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features.
VPN may have best-effort performance or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider.
A global area network (GAN) is a network used for supporting mobile across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off user communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs.[44]
Networks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity.
An intranet is a set of networks that are under the control of a single administrative entity.  The intranet uses the IP protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits the use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information. An intranet is also anything behind the router on a local area network.
An extranet is a network that is also under the administrative control of a single organization but supports a limited connection to a specific external network.  For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers.  These other entities are not necessarily trusted from a security standpoint.  Network connection to an extranet is often, but not always, implemented via WAN technology.
An internetwork is the connection of multiple different types of computer networks to form a single computer network by layering on top of the different networking software and connecting them together using routers.
The Internet is the largest example of internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet Protocol Suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet utilizes copper communications and the optical networking backbone to enable the World Wide Web (WWW), the Internet of Things, video transfer, and a broad range of information services.
Participants on the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet Protocol Suite and an addressing system (IP addresses) administered by the Internet Assigned Numbers Authority and address registries. Service providers and large enterprises exchange information about the reachability of their address spaces through the Border Gateway Protocol (BGP), forming a redundant worldwide mesh of transmission paths.
Darknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference.[46]
Network services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate.
The World Wide Web, E-mail,[47] printing and network file sharing are examples of well-known network services. Network services such as DNS (Domain Name System) give names for IP and MAC addresses (people remember names like "nm.lan" better than numbers like "210.121.67.18"),[48] and DHCP to ensure that the equipment on the network has a valid IP address.[49]
Services are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service.
Bandwidth in bit/s may refer to consumed bandwidth, corresponding to achieved throughput or goodput, i.e., the average rate of successful data transfer through a communication path. The throughput is affected by technologies such as bandwidth shaping, bandwidth management, bandwidth throttling, bandwidth cap, bandwidth allocation (for example bandwidth allocation protocol and dynamic bandwidth allocation), etc. A bit stream's bandwidth  is proportional to the average consumed signal bandwidth in hertz (the average spectral bandwidth of the analog signal representing the bit stream) during a studied time interval.
Network delay is a design and performance characteristic of a telecommunications network. It specifies the latency for a bit of data to travel across the network from one communication endpoint to another. It is typically measured in multiples or fractions of a second. Delay may differ slightly, depending on the location of the specific pair of communicating endpoints. Engineers usually report both the maximum and average delay, and they divide the delay into several parts:
A certain minimum level of delay is experienced by signals due to the time it takes to transmit a packet serially through a link. This delay is extended by more variable levels of delay due to network congestion. IP network delays can range from a few milliseconds to several hundred milliseconds.
Depending on the installation requirements, network performance is usually measured by the quality of service of a telecommunications product. The parameters that affect this typically can include throughput, jitter, bit error rate and latency.
The following list gives examples of network performance measures for a circuit-switched network and one type of packet-switched network, viz. ATM:
There are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.[52]
Network congestion occurs when a link or node is subjected to a greater data load than it is rated for, resulting in a deterioration of its quality of service. When networks are congested and queues become too full, packets have to be discarded, and so networks rely on re-transmission. Typical effects of congestion include queueing delay, packet loss or the blocking of new connections.  A consequence of these latter two is that incremental increases in offered load lead either to only a small increase in the network throughput or to a reduction in network throughput.
Network resilience is "the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation."[53]
Computer networks are also used by security hackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial-of-service attack.
Network Security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources.[54] Network security is the authorization of access to data in a network, which is controlled by the network administrator. Users are assigned an ID and password that allows them access to information and programs within their authority.  Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies, and individuals.
Network surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency.
Computer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity.
Surveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent/investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high-speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens.[55]
End-to-end encryption (E2EE) is a digital communications paradigm of uninterrupted protection of data traveling between two communicating parties. It involves the originating party encrypting data so only the intended recipient can decrypt it, with no dependency on third parties. End-to-end encryption prevents intermediaries, such as Internet providers or application service providers, from discovering or tampering with communications. End-to-end encryption generally protects both confidentiality and integrity.
Examples of end-to-end encryption include HTTPS for web traffic, PGP for email, OTR for instant messaging, ZRTP for telephony, and TETRA for radio.
Typical server-based communications systems do not include end-to-end encryption. These systems can only guarantee the protection of communications between clients and servers, not between the communicating parties themselves. Examples of non-E2EE systems are Google Talk, Yahoo Messenger, Facebook, and Dropbox. Some such systems, for example, LavaBit and SecretInk, have even described themselves as offering "end-to-end" encryption when they do not. Some systems that normally offer end-to-end encryption have turned out to contain a back door that subverts negotiation of the encryption key between the communicating parties, for example Skype or Hushmail.
The end-to-end encryption paradigm does not directly address risks at the endpoints of the communication themselves, such as the technical exploitation of clients, poor quality random number generators, or key escrow. E2EE also does not address traffic analysis, which relates to things such as the identities of the endpoints and the times and quantities of messages that are sent.
The introduction and rapid growth of e-commerce on the World Wide Web in the mid-1990s made it obvious that some form of authentication and encryption was needed. Netscape took the first shot at a new standard. At the time, the dominant web browser was Netscape Navigator. Netscape created a standard called secure socket layer (SSL). SSL requires a server with a certificate. When a client requests access to an SSL-secured server, the server sends a copy of the certificate to the client. The SSL client checks this certificate (all web browsers come with an exhaustive list of CA root certificates preloaded), and if the certificate checks out, the server is authenticated and the client negotiates a symmetric-key cipher for use in the session. The session is now in a very secure encrypted tunnel between the SSL server and the SSL client.[30]
Users and network administrators typically have different views of their networks. Users can share printers and some servers from a workgroup, which usually means they are in the same geographic location and are on the same LAN, whereas a Network Administrator is responsible to keep that network up and running.  A community of interest has less of a connection of being in a local area and should be thought of as a set of arbitrarily located users who share a set of servers, and possibly also communicate via peer-to-peer technologies.
Network administrators can see networks from both physical and logical perspectives. The physical perspective involves geographic locations, physical cabling, and the network elements (e.g., routers, bridges and application layer gateways) that interconnect via the transmission media. Logical networks, called, in the TCP/IP architecture, subnets, map onto one or more transmission media. For example, a common practice in a campus of buildings is to make a set of LAN cables in each building appear to be a common subnet, using VLAN technology.
Both users and administrators are aware, to varying extents, of the trust and scope characteristics of a network. Again using TCP/IP architectural terminology, an intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees).[59]  Intranets do not have to be connected to the Internet, but generally have a limited connection.  An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers).[59]
Unofficially, the Internet is the set of users, enterprises, and content providers that are interconnected by Internet Service Providers (ISP). From an engineering viewpoint, the Internet is the set of subnets, and aggregates of subnets, that share the registered IP address space and exchange information about the reachability of those IP addresses using the Border Gateway Protocol. Typically, the human-readable names of servers are translated to IP addresses, transparently to users, via the directory function of the Domain Name System (DNS).
Over the Internet, there can be  business-to-business (B2B),  business-to-consumer (B2C) and consumer-to-consumer (C2C) communications. When money or sensitive information is exchanged, the communications are apt to be protected by some form of communications security mechanism.  Intranets and extranets can be securely superimposed onto the Internet, without any access by general Internet users and administrators, using secure Virtual Private Network (VPN) technology.
Modern embedded systems are often based on microcontrollers (i.e. microprocessors with integrated memory and peripheral interfaces), but ordinary microprocessors (using external chips for memory and peripheral interface circuits) are also common, especially in more complex systems. In either case, the processor(s) used may be types ranging from general purpose to those specialized in a certain class of computations, or even custom designed for the application at hand. A common standard class of dedicated processors is the digital signal processor (DSP).
Since the embedded system is dedicated to specific tasks, design engineers can optimize it to reduce the size and cost of the product and increase its reliability and performance. Some embedded systems are mass-produced, benefiting from economies of scale.
Embedded systems range in size from portable personal devices such as digital watches and MP3 players to bigger machines like home appliances, industrial assembly lines, robots, transport vehicles, traffic light controllers, and medical imaging systems. Often they constitute subsystems of other machines like avionics in aircraft and astrionics in spacecraft. Large installations like factories, pipelines and electrical grids rely on multiple embedded systems networked together. Generalized through software customization, embedded systems such as programmable logic controllers frequently comprise their functional units.
Embedded systems range from those low in complexity, with a single microcontroller chip, to very high with multiple units, peripherals and networks, which may reside in equipment racks or across large geographical areas connected via long-distance communications lines.
The first multi-chip microprocessors, the Four-Phase Systems AL1 in 1969 and the Garrett AiResearch MP944 in 1970, were developed with multiple MOS LSI chips. The first single-chip microprocessor was the Intel 4004, released in 1971. It was developed by Federico Faggin, using his silicon-gate MOS technology, along with Intel engineers Marcian Hoff and Stan Mazor, and Busicom engineer Masatoshi Shima.[6]
One of the first recognizably modern embedded systems was the Apollo Guidance Computer,[citation needed] developed ca. 1965 by Charles Stark Draper at the MIT Instrumentation Laboratory. At the project's inception, the Apollo guidance computer was considered the riskiest item in the Apollo project as it employed the then newly developed monolithic integrated circuits to reduce the computer's size and weight.
An early mass-produced embedded system was the Autonetics D-17 guidance computer for the Minuteman missile, released in 1961. When the Minuteman II went into production in 1966, the D-17 was replaced with a new computer that represented the first high-volume use of integrated circuits.
Since these early applications in the 1960s, embedded systems have come down in price and there has been a dramatic rise in processing power and functionality. An early microprocessor, the Intel 4004 (released in 1971), was designed for calculators and other small systems but still required external memory and support chips. By the early 1980s, memory, input and output system components had been integrated into the same chip as the processor forming a microcontroller. Microcontrollers find applications where a general-purpose computer would be too costly. As the cost of microprocessors and microcontrollers fell, the prevalence of embedded systems increased.
Today, a comparatively low-cost microcontroller may be programmed to fulfill the same role as a large number of separate components. With microcontrollers, it became feasible to replace, even in consumer products, expensive knob-based analog components such as potentiometers and variable capacitors with up/down buttons or knobs read out by a microprocessor. Although in this context an embedded system is usually more complex than a traditional solution, most of the complexity is contained within the microcontroller itself. Very few additional components may be needed and most of the design effort is in the software. Software prototype and test can be quicker compared with the design and construction of a new circuit not using an embedded processor.
Embedded systems are commonly found in consumer, industrial, automotive, home appliances, medical, telecommunication, commercial, aerospace and military applications.
Telecommunications systems employ numerous embedded systems from telephone switches for the network to cell phones at the end user. Computer networking uses dedicated routers and network bridges to route data.
Consumer electronics include MP3 players, television sets, mobile phones, video game consoles, digital cameras, GPS receivers, and printers. Household appliances, such as microwave ovens, washing machines and dishwashers, include embedded systems to provide flexibility, efficiency and features. Advanced heating, ventilation, and air conditioning (HVAC) systems use networked thermostats to more accurately and efficiently control temperature that can change by time of day and season. Home automation uses wired- and wireless-networking that can be used to control lights, climate, security, audio/visual, surveillance, etc., all of which use embedded devices for sensing and controlling.
Medical equipment uses embedded systems for monitoring, and various medical imaging (positron emission tomography (PET), single-photon emission computed tomography (SPECT), computed tomography (CT), and magnetic resonance imaging (MRI) for non-invasive internal inspections. Embedded systems within medical equipment are often powered by industrial computers.[8]
Embedded systems are used for safety-critical systems in aerospace and defense industries. Unless connected to wired or wireless networks via on-chip 3G cellular or other methods for IoT monitoring and control purposes, these systems can be isolated from hacking and thus be more secure.[citation needed] For fire safety, the systems can be designed to have a greater ability to handle higher temperatures and continue to operate. In dealing with security, the embedded systems can be self-sufficient and be able to deal with cut electrical and communication systems.
Miniature wireless devices called motes are networked wireless sensors. Wireless sensor networking makes use of miniaturization made possible by advanced integrated circuit (IC) design to couple full wireless subsystems to sophisticated sensors, enabling people and companies to measure a myriad of things in the physical world and act on this information through monitoring and control systems. These motes are completely self-contained and will typically run off a battery source for years before the batteries need to be changed or charged.
Embedded systems are designed to do some specific task, rather than be a general-purpose computer for multiple tasks. Some also have real-time performance constraints that must be met, for reasons such as safety and usability; others may have low or no performance requirements, allowing the system hardware to be simplified to reduce costs.
Embedded systems are not always standalone devices. Many embedded systems consist of small parts within a larger device that serves a more general purpose. For example, the Gibson Robot Guitar features an embedded system for tuning the strings, but the overall purpose of the Robot Guitar is, of course, to play music.[9] Similarly, an embedded system in an automobile provides a specific function as a subsystem of the car itself.
The program instructions written for embedded systems are referred to as firmware, and are stored in read-only memory or flash memory chips. They run with limited computer hardware resources: little memory, small or non-existent keyboard or screen.
Embedded systems range from no user interface at all, in systems dedicated only to one task, to complex graphical user interfaces that resemble modern computer desktop operating systems. Simple embedded devices use buttons, light-emitting diodes (LED), graphic or character liquid-crystal displays (LCD) with a simple menu system. More sophisticated devices that use a graphical screen with touch sensing or screen-edge soft keys provide flexibility while minimizing space used: the meaning of the buttons can change with the screen, and selection involves the natural behavior of pointing at what is desired.
Some systems provide user interface remotely with the help of a serial (e.g. RS-232) or network (e.g. Ethernet) connection. This approach extends the capabilities of the embedded system, avoids the cost of a display, simplifies the board support package (BSP) and allows designers to build a rich user interface on the PC. A good example of this is the combination of an embedded HTTP server running on an embedded device (such as an IP camera or a network router). The user interface is displayed in a web browser on a PC connected to the device.
Examples of properties of typical embedded computers when compared with general-purpose counterparts, are low power consumption, small size, rugged operating ranges, and low per-unit cost. This comes at the price of limited processing resources.
Numerous microcontrollers have been developed for embedded systems use. General-purpose microprocessors are also used in embedded systems, but generally, require more support circuitry than microcontrollers.
PC/104 and PC/104+ are examples of standards for ready-made computer boards intended for small, low-volume embedded and ruggedized systems. These are mostly x86-based and often physically small compared to a standard PC, although still quite large compared to most simple (8/16-bit) embedded systems. They may use DOS, Linux, NetBSD, or an embedded real-time operating system (RTOS) such as MicroC/OS-II, QNX or VxWorks.
In certain applications, where small size or power efficiency are not primary concerns, the components used may be compatible with those used in general-purpose x86 personal computers. Boards such as the VIA EPIA range help to bridge the gap by being PC-compatible but highly integrated, physically smaller or have other attributes making them attractive to embedded engineers. The advantage of this approach is that low-cost commodity components may be used along with the same software development tools used for general software development. Systems built in this way are still regarded as embedded since they are integrated into larger devices and fulfill a single role. Examples of devices that may adopt this approach are automated teller machines (ATM) and arcade machines, which contain code specific to the application.
However, most ready-made embedded systems boards are not PC-centered and do not use the ISA or PCI busses. When a system-on-a-chip processor is involved, there may be little benefit to having a standardized bus connecting discrete components, and the environment for both hardware and software tools may be very different.
One common design style uses a small system module, perhaps the size of a business card, holding high density BGA chips such as an ARM-based system-on-a-chip processor and peripherals, external flash memory for storage, and DRAM for runtime memory. The module vendor will usually provide boot software and make sure there is a selection of operating systems, usually including Linux and some real-time choices. These modules can be manufactured in high volume, by organizations familiar with their specialized testing issues, and combined with much lower volume custom mainboards with application-specific external peripherals. Prominent examples of this approach include Arduino and Raspberry Pi.
A system on a chip (SoC) contains a complete system - consisting of multiple processors, multipliers, caches, even different types of memory and commonly various peripherals like interfaces for wired or wireless communication on a single chip. Often graphics processing units (GPU) and DSPs are included such chips. SoCs can be implemented as an application-specific integrated circuit (ASIC) or using a field-programmable gate array (FPGA) which typically can be reconfigured.
ASIC implementations are common for very-high-volume embedded systems like mobile phones and smartphones. ASIC or FPGA implementations may be used for not-so-high-volume embedded systems with special needs in kind of signal processing performance, interfaces and reliability, like in avionics.
Embedded systems talk with the outside world via peripherals, such as:
As with other software, embedded system designers use compilers, assemblers, and debuggers to develop embedded system software. However, they may also use more specific tools:
As the complexity of embedded systems grows, higher-level tools and operating systems are migrating into machinery where it makes sense. For example, cellphones, personal digital assistants and other consumer computers often need significant software that is purchased or provided by a person other than the manufacturer of the electronics. In these systems, an open programming environment such as Linux, NetBSD, OSGi or Embedded Java is required so that the third-party software provider can sell to a large market.
Embedded debugging may be performed at different levels, depending on the facilities available. Considerations include: does it slow down the main application, how close is the debugged system or application to the actual system or application, how expressive are the triggers that can be set for debugging (e.g., inspecting the memory when a particular program counter value is reached), and what can be inspected in the debugging process (such as, only memory, or memory and registers, etc.).
From simplest to most sophisticated debugging techniques and systems be roughly grouped into the following areas:
Unless restricted to external debugging, the programmer can typically load and run software through the tools, view the code running in the processor, and start or stop its operation. The view of the code may be as high-level programming language, assembly code or mixture of both.
Real-time operating systems often support tracing of operating system events. A graphical view is presented by a host PC tool, based on a recording of the system behavior. The trace recording can be performed in software, by the RTOS, or by special tracing hardware. RTOS tracing allows developers to understand timing and performance issues of the software system and gives a good understanding of the high-level system behaviors. Trace recording in embedded systems can be achieved using hardware or software solutions. Software-based trace recording does not require specialized debugging hardware and can be used to record traces in deployed devices, but it can have an impact on CPU and RAM usage.[13] One example of a software-based tracing method used in RTOS environments is the use of empty macros which are invoked by the operating system at strategic places in the code, and can be implemented to serve as hooks.
Embedded systems often reside in machines that are expected to run continuously for years without error, and in some cases recover by themselves if an error occurs. Therefore, the software is usually developed and tested more carefully than that for personal computers, and unreliable mechanical moving parts such as disk drives, switches or buttons are avoided.
For high-volume systems such as mobile phones, minimizing cost is usually the primary design consideration. Engineers typically select hardware that is just good enough to implement the necessary functions.
For low-volume or prototype embedded systems, general-purpose computers may be adapted by limiting the programs or by replacing the operating system with an RTOS.
There are several different types of software architecture in common use today.
In this design, the software simply has a loop which monitors the input devices. The loop calls subroutines, each of which manages a part of the hardware or software. Hence it is called a simple control loop or programmed input-output.
Some embedded systems are predominantly controlled by interrupts. This means that tasks performed by the system are triggered by different kinds of events; an interrupt could be generated, for example, by a timer at a predefined interval, or by a serial port controller receiving data.
This architecture is used if event handlers need low latency, and the event handlers are short and simple. These systems run a simple task in a main loop also, but this task is not very sensitive to unexpected delays. Sometimes the interrupt handler will add longer tasks to a queue structure. Later, after the interrupt handler has finished, these tasks are executed by the main loop. This method brings the system close to a multitasking kernel with discrete processes.
Cooperative multitasking is very similar to the simple control loop scheme, except that the loop is hidden in an API.[3][1] The programmer defines a series of tasks, and each task gets its own environment to run in. When a task is idle, it calls an idle routine which passes control to another task.
The advantages and disadvantages are similar to that of the control loop, except that adding new software is easier, by simply writing a new task, or adding to the queue.
In this type of system, a low-level piece of code switches between tasks or threads based on a timer invoking an interrupt. This is the level at which the system is generally considered to have an operating system kernel. Depending on how much functionality is required, it introduces more or less of the complexities of managing multiple tasks running conceptually in parallel.
As any code can potentially damage the data of another task (except in systems using a memory management unit) programs must be carefully designed and tested, and access to shared data must be controlled by some synchronization strategy such as message queues, semaphores or a non-blocking synchronization scheme.
Because of these complexities, it is common for organizations to use an off-the-shelf RTOS, allowing the application programmers to concentrate on device functionality rather than operating system services. The choice to include an RTOS brings in its own issues, however, as the selection must be made prior to starting the application development process. This timing forces developers to choose the embedded operating system for their device based on current requirements and so restricts future options to a large extent.[19]
The level of complexity in embedded systems is continuously growing as devices are required to manage peripherals and tasks such as serial, USB, TCP/IP, Bluetooth, Wireless LAN, trunk radio, multiple channels, data and voice, enhanced graphics, multiple states, multiple threads, numerous wait states and so on. These trends are leading to the uptake of embedded middleware in addition to an RTOS.
A microkernel allocates memory and switches the CPU to different threads of execution. User-mode processes implement major functions such as file systems, network interfaces, etc.
Exokernels communicate efficiently by normal subroutine calls. The hardware and all the software in the system are available to and extensible by application programmers.
In this case, a relatively large kernel with sophisticated capabilities is adapted to suit an embedded environment. This gives programmers an environment similar to a desktop operating system like Linux or Microsoft Windows, and is therefore very productive for development; on the downside, it requires considerably more hardware resources, is often more expensive, and, because of the complexity of these kernels, can be less predictable and reliable.
Common examples of embedded monolithic kernels are embedded Linux, VXWorks and Windows CE.
Despite the increased cost in hardware, this type of embedded system is increasing in popularity, especially on the more powerful embedded devices such as wireless routers and GPS navigation systems. Here are some of the reasons:
In addition to the core operating system, many embedded systems have additional upper-layer software components. These components consist of networking protocol stacks like CAN, TCP/IP, FTP, HTTP, and HTTPS, and also included storage capabilities like FAT and flash memory management systems. If the embedded device has audio and video capabilities, then the appropriate drivers and codecs will be present in the system. In the case of the monolithic kernels, many of these software layers are included. In the RTOS category, the availability of the additional software components depends upon the commercial offering.
In the automotive sector, AUTOSAR is a standard architecture for embedded software.
Collective intelligence
Collective action
Self-organized criticality
Herd mentality
Phase transition
Agent-based modelling
Synchronization
Ant colony optimization
Particle swarm optimization
Swarm behaviour
Evolutionary computation
Genetic algorithms
Genetic programming
Artificial life
Machine learning
Evolutionary developmental biology
Artificial intelligence
Evolutionary robotics
A complex system is a system composed of many components which may interact with each other. Examples of complex systems are Earth's global climate, organisms, the human brain, infrastructure such as power grid, transportation or communication systems, complex software and electronic systems, social and economic organizations (like cities), an ecosystem, a living cell, and ultimately the entire universe.
Complex systems are systems whose behavior is intrinsically difficult to model due to the dependencies, competitions, relationships, or other types of interactions between their parts or between a given system and its environment. Systems that are "complex" have distinct properties that arise from these relationships, such as nonlinearity, emergence, spontaneous order, adaptation, and feedback loops, among others. Because such systems appear in a wide variety of fields, the commonalities among them have become the topic of their independent area of research. In many cases, it is useful to represent such a system as a network where the nodes represent the components and links to their interactions.
The term complex systems often refers to the study of complex systems, which is an approach to science that investigates how relationships between a system's parts give rise to its collective behaviors and how the system interacts and forms relationships with its environment.[1] The study of complex systems regards collective, or system-wide, behaviors as the fundamental object of study; for this reason, complex systems can be understood as an alternative paradigm to reductionism, which attempts to explain systems in terms of their constituent parts and the individual interactions between them.
As an interdisciplinary domain, complex systems draws contributions from many different fields, such as the study of self-organization and critical phenomena from physics, that of spontaneous order from the social sciences, chaos from mathematics, adaptation from biology, and many others. Complex systems is therefore often used as a broad term encompassing a research approach to problems in many diverse disciplines, including statistical physics, information theory, nonlinear dynamics, anthropology, computer science, meteorology, sociology, economics, psychology, and biology.
Complex systems are chiefly concerned with the behaviors and properties of systems. A system, broadly defined, is a set of entities that, through their interactions, relationships, or dependencies, form a unified whole. It is always defined in terms of its boundary, which determines the entities that are or are not part of the system. Entities lying outside the system then become part of the system's environment.
A system can exhibit properties that produce behaviors which are distinct from the properties and behaviors of its parts; these system-wide or global properties and behaviors are characteristics of how the system interacts with or appears to its environment, or of how its parts behave (say, in response to external stimuli) by virtue of being within the system. The notion of behavior implies that the study of systems is also concerned with processes that take place over time (or, in mathematics, some other phase space parameterization). Because of their broad, interdisciplinary applicability, systems concepts play a central role in complex systems.
As a field of study, complex systems is a subset of systems theory. General systems theory focuses similarly on the collective behaviors of interacting entities, but it studies a much broader class of systems, including non-complex systems where traditional reductionist approaches may remain viable. Indeed, systems theory seeks to explore and describe all classes of systems, and the invention of categories that are useful to researchers across widely varying fields is one of the systems theory's main objectives.
As it relates to complex systems, systems theory contributes an emphasis on the way relationships and dependencies between a system's parts can determine system-wide properties. It also contributes to the interdisciplinary perspective of the study of complex systems: the notion that shared properties link systems across disciplines, justifying the pursuit of modeling approaches applicable to complex systems wherever they appear. Specific concepts important to complex systems, such as emergence, feedback loops, and adaptation, also originate in systems theory.
For a system to exhibit complexity means that the systems' behaviors cannot be easily inferred from its properties. Any modeling approach that ignores such difficulties or characterizes them as noise will necessarily produce models that are neither accurate nor useful. As yet no fully general theory of complex systems has emerged for addressing these problems, so researchers must solve them in domain-specific contexts. Researchers in complex systems address these problems by viewing the chief task of modeling to be capturing, rather than reducing, the complexity of their respective systems of interest.
While no generally accepted exact definition of complexity exists yet, there are many archetypal examples of complexity. Systems can be complex if, for instance, they have chaotic behavior (behavior that exhibits extreme sensitivity to initial conditions, among other properties), or if they have emergent properties (properties that are not apparent from their components in isolation but which result from the relationships and dependencies they form when placed together in a system), or if they are computationally intractable to model (if they depend on a number of parameters that grows too rapidly with respect to the size of the system).
The interacting components of a complex system form a network, which is a collection of discrete objects and relationships between them, usually depicted as a graph of vertices connected by edges. Networks can describe the relationships between individuals within an organization, between logic gates in a circuit, between genes in gene regulatory networks, or between any other set of related entities.
Networks often describe the sources of complexity in complex systems. Studying complex systems as networks, therefore, enables many useful applications of graph theory and network science. Many complex systems, for example, are also complex networks, which have properties such as phase transitions and power-law degree distributions that readily lend themselves to emergent or chaotic behavior. The fact that the number of edges in a complete graph grows quadratically in the number of vertices sheds additional light on the source of complexity in large networks: as a network grows, the number of relationships between entities quickly dwarfs the number of entities in the network.
Complex systems often have nonlinear behavior, meaning they may respond in different ways to the same input depending on their state or context. In mathematics and physics, nonlinearity describes systems in which a change in the size of the input does not produce a proportional change in the size of the output. For a given change in input, such systems may yield significantly greater than or less than proportional changes in output, or even no output at all, depending on the current state of the system or its parameter values.
Of particular interest to complex systems are nonlinear dynamical systems, which are systems of differential equations that have one or more nonlinear terms. Some nonlinear dynamical systems, such as the Lorenz system, can produce a mathematical phenomenon known as chaos. Chaos, as it applies to complex systems, refers to the sensitive dependence on initial conditions, or "butterfly effect", that a complex system can exhibit. In such a system, small changes to initial conditions can lead to dramatically different outcomes. Chaotic behavior can, therefore, be extremely hard to model numerically, because small rounding errors at an intermediate stage of computation can cause the model to generate completely inaccurate output. Furthermore, if a complex system returns to a state similar to one it held previously, it may behave completely differently in response to the same stimuli, so chaos also poses challenges for extrapolating from experience.
Another common feature of complex systems is the presence of emergent behaviors and properties: these are traits of a system that are not apparent from its components in isolation but which result from the interactions, dependencies, or relationships they form when placed together in a system. Emergence broadly describes the appearance of such behaviors and properties, and has applications to systems studied in both the social and physical sciences. While emergence is often used to refer only to the appearance of unplanned organized behavior in a complex system, emergence can also refer to the breakdown of an organization; it describes any phenomena which are difficult or even impossible to predict from the smaller entities that make up the system.
One example of a complex system whose emergent properties have been studied extensively is cellular automata. In a cellular automaton, a grid of cells, each having one of the finitely many states, evolves according to a simple set of rules. These rules guide the "interactions" of each cell with its neighbors. Although the rules are only defined locally, they have been shown capable of producing globally interesting behavior, for example in Conway's Game of Life.
When emergence describes the appearance of unplanned order, it is spontaneous order (in the social sciences) or self-organization (in physical sciences). Spontaneous order can be seen in herd behavior, whereby a group of individuals coordinates their actions without centralized planning. Self-organization can be seen in the global symmetry of certain crystals, for instance the apparent radial symmetry of snowflakes, which arises from purely local attractive and repulsive forces both between water molecules and their surrounding environment.
Complex adaptive systems are special cases of complex systems that are adaptive in that they have the capacity to change and learn from experience. Examples of complex adaptive systems include the stock market, social insect and ant colonies, the biosphere and the ecosystem, the brain and the immune system, the cell and the developing embryo, the cities, manufacturing businesses and any human social group-based endeavor in a cultural and social system such as political parties or communities.[3]
Although arguably, humans have been studying complex systems for thousands of years, the modern scientific study of complex systems is relatively young in comparison to established fields of science such as physics and chemistry. The history of the scientific study of these systems follows several different research trends.
In the area of mathematics, arguably the largest contribution to the study of complex systems was the discovery of chaos in deterministic systems, a feature of certain dynamical systems that is strongly related to nonlinearity.[20]  The study of neural networks was also integral in advancing the mathematics needed to study complex systems.
The notion of self-organizing systems is tied with work in nonequilibrium thermodynamics, including that pioneered by chemist and Nobel laureate Ilya Prigogine in his study of dissipative structures. Even older is the work by Hartree-Fock on the quantum chemistry equations and later calculations of the structure of molecules which can be regarded as one of the earliest examples of emergence and emergent wholes in science.
One complex system containing humans is the classical political economy of the Scottish Enlightenment, later developed by the Austrian school of economics, which argues that order in market systems is spontaneous (or emergent) in that it is the result of human action, but not the execution of any human design.[21][22]
Upon this, the Austrian school developed from the 19th to the early 20th century the economic calculation problem, along with the concept of dispersed knowledge, which were to fuel debates against the then-dominant Keynesian economics. This debate would notably lead economists, politicians, and other parties to explore the question of computational complexity.[citation needed]
A pioneer in the field, and inspired by Karl Popper's and Warren Weaver's works, Nobel prize economist and philosopher Friedrich Hayek dedicated much of his work, from early to the late 20th century, to the study of complex phenomena,[23] not constraining his work to human economies but venturing into other fields such as psychology,[24] biology and cybernetics. Cybernetician Gregory Bateson played a key role in establishing the connection between anthropology and systems theory; he recognized that the interactive parts of cultures function much like ecosystems.
While the explicit study of complex systems dates at least to the 1970s,[25] the first research institute focused on complex systems, the Santa Fe Institute, was founded in 1984.[26][27] Early Santa Fe Institute participants included physics Nobel laureates Murray Gell-Mann and Philip Anderson, economics Nobel laureate Kenneth Arrow, and Manhattan Project scientists George Cowan and Herb Anderson.[28] Today, there are over 50 institutes and research centers focusing on complex systems.[citation needed]
Since the late 1990s, the interest of mathematical physicists in researching economic phenomena has been on the rise. The proliferation of cross-disciplinary research with the application of solutions originated from the physics epistemology has entailed a gradual paradigm shift in the theoretical articulations and methodological approaches in economics, primarily in financial economics. The development has resulted in the emergence of a new branch of discipline, namely "econophysics," which is broadly defined as a cross-discipline that applies statistical physics methodologies which are mostly based on the complex systems theory and the chaos theory for economics analysis.[29]
The 2021 Nobel Prize in Physics was awarded to Syukuro Manabe, Klaus Hasselmann, and Giorgio Parisi for their work to understand complex systems. Their work was used to create more accurate computer models of the effect of global warming on the Earth's climate.[30]
The traditional approach to dealing with complexity is to reduce or constrain it. Typically, this involves compartmentalization: dividing a large system into separate parts. Organizations, for instance, divide their work into departments that each deal with separate issues. Engineering systems are often designed using modular components. However, modular designs become susceptible to failure when issues arise that bridge the divisions.
As projects and acquisitions become increasingly complex, companies and governments are challenged to find effective ways to manage mega-acquisitions such as the Army Future Combat Systems.  Acquisitions such as the FCS rely on a web of interrelated parts which interact unpredictably.  As acquisitions become more network-centric and complex, businesses will be forced to find ways to manage complexity while governments will be challenged to provide effective governance to ensure flexibility and resiliency.[31]
Over the last decades, within the emerging field of complexity economics, new predictive tools have been developed to explain economic growth. Such is the case with the models built by the Santa Fe Institute in 1989 and the more recent economic complexity index (ECI), introduced by the MIT physicist Cesar A. Hidalgo and the Harvard economist Ricardo Hausmann. Based on the ECI, Hausmann, Hidalgo and their team of The Observatory of Economic Complexity have produced GDP forecasts for the year 2020.[citation needed]
Recurrence quantification analysis has been employed to detect the characteristic of business cycles and economic development. To this end, Orlando et al.[32] developed the so-called recurrence quantification correlation index (RQCI) to test correlations of RQA on a sample signal and then investigated the application to business time series. The said index has been proven to detect hidden changes in time series. Further, Orlando et al.,[33] over an extensive dataset, shown that recurrence quantification analysis may help in anticipating transitions from laminar (i.e. regular) to turbulent (i.e. chaotic) phases such as USA GDP in 1949, 1953, etc. Last but not least, it has been demonstrated that recurrence quantification analysis can detect differences between macroeconomic variables and highlight hidden features of economic dynamics.
Focusing on issues of student persistence with their studies, Forsman, Moll and Linder explore the "viability of using complexity science as a frame to extend methodological applications for physics education research", finding that "framing a social network analysis within a complexity science perspective offers a new and powerful applicability across a broad range of PER topics".[34]
Complexity science has been applied to living organisms, and in particular to biological systems. One of the areas of research is the emergence and evolution of intelligent systems [35] Within the emerging field of fractal physiology, bodily signals, such as heart rate or brain activity, are characterized using entropy or fractal indices. The goal is often to assess the state and the health of the underlying system, and diagnose potential disorders and illnesses.
One of Friedrich Hayek's main contributions to early complexity theory is his distinction between the human capacity to predict the behavior of simple systems and its capacity to predict the behavior of complex systems through modeling. He believed that economics and the sciences of complex phenomena in general, which in his view included biology, psychology, and so on, could not be modeled after the sciences that deal with essentially simple phenomena like physics.[36] Hayek would notably explain that complex phenomena, through modeling, can only allow pattern predictions, compared with the precise predictions that can be made out of non-complex phenomena.[37]
The emergence of complexity theory shows a domain between deterministic order and randomness which is complex.[41] This is referred to as the "edge of chaos".[42]
When one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails. As stated by Colander,[43] the study of complexity is the opposite of the study of chaos. Complexity is about how a huge number of extremely complicated and dynamic sets of relationships can generate some simple behavioral patterns, whereas chaotic behavior, in the sense of deterministic chaos, is the result of a relatively small number of non-linear interactions.[41] For recent examples in economics and business see Stoop et al.[44] who discussed Android's market position, Orlando [45] who explained the corporate dynamics in terms of mutual synchronization and chaos regularization of bursts in a group of chaotically bursting cells and Orlando et al.[46]  who modelled financial data (Financial Stress Index, swap and equity, emerging and developed, corporate and government, short and long maturity) with a low-dimensional deterministic model.
Therefore, the main difference between chaotic systems and complex systems is their history.[47] Chaotic systems do not rely on their history as complex ones do. Chaotic behavior pushes a system in equilibrium into chaotic order, which means, in other words, out of what we traditionally define as 'order'.[clarification needed] On the other hand, complex systems evolve far from equilibrium at the edge of chaos. They evolve at a critical state built up by a history of irreversible and unexpected events, which physicist Murray Gell-Mann called "an accumulation of frozen accidents".[48] In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence. Many real complex systems are, in practice and over long but finite periods, robust. However, they do possess the potential for radical qualitative change of kind whilst retaining systemic integrity. Metamorphosis serves as perhaps more than a metaphor for such transformations.
A complex system is usually composed of many components and their interactions. Such a system can be represented by a network where nodes represent the components and links represent their interactions.[49][50] For example, the Internet can be represented as a network composed of nodes (computers) and links (direct connections between computers). Other examples of complex networks include social networks, financial institution interdependencies,[51] airline networks,[52] and biological networks.
In computer engineering, computer architecture is a description of the structure of a computer system made from component parts.[1] It can sometimes be a high-level description that ignores details of the implementation.[2] At a more detailed level, the description may include the instruction set architecture design, microarchitecture design, logic design, and implementation.[3]
The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. When building the computer Z1 in 1936, Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept.[4][5] Two other early and important examples are:
The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959. Johnson had the opportunity to write a proprietary research communication about the Stretch, an IBM-developed supercomputer for Los Alamos National Laboratory (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of "system architecture", a term that seemed more useful than "machine organization".[8]
Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, "Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints."[9]
Brooks went on to help develop the IBM System/360 (now called the IBM zSeries) line of computers, in which "architecture" became a noun defining "what the user needs to know".[10] Later, computer users came to use the term in many less explicit ways.[11]
There are other technologies in computer architecture. The following technologies are used in bigger companies like Intel, and were estimated in 2002[14] to count for 1% of all of computer architecture:
Computer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. The case of instruction set architecture can be used to illustrate the balance of these competing factors. More complex instruction sets enable programmers to write more space efficient programs, since a single instruction can encode some higher-level abstraction (such as the x86 Loop instruction).[17] However, longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways.
The implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with compilers, operating systems to logic design, and packaging.[18]
An instruction set architecture (ISA) is the interface between the computer's software and hardware and also can be viewed as the programmer's view of the machine. Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand.
The ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler.  An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form.  Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs.
ISAs vary in quality and completeness.  A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time).  Memory organization defines how instructions interact with the memory, and how memory interacts with itself.
During design emulation, emulators can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine whether a particular ISA is meeting its goals.
Computer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite a detailed analysis of the computer's organization.  For example, in an SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.
Computer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well.  For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.
Once an instruction set and micro-architecture have been designed, a practical machine must be developed. This design process is called the implementation. Implementation is usually not considered architectural design, but rather hardware design engineering. Implementation can be further broken down into several steps:
For CPUs, the entire implementation process is organized differently and is often referred to as CPU design.
The exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.
The most common scheme does an in-depth power analysis and figures out how to keep power consumption low while maintaining adequate performance.
Modern computer performance is often described in instructions per cycle (IPC), which measures the efficiency of the architecture at any clock frequency; a faster IPC rate means the computer is faster. Older computers had IPC counts as low as 0.1 while modern processors easily reach nearly 1. Superscalar processors may reach three to five IPC by executing several instructions per clock cycle.[citation needed]
Counting machine-language instructions would be misleading because they can do varying amounts of work in different ISAs. The "instruction" in the standard measurements is not a count of the ISA's machine-language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture.
Many people used to measure a computer's speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance.
Other factors influence speed, such as the mix of functional units, bus speeds, available memory, and the type and order of instructions in the programs.
There are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time.  Interrupt latency is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data).
Benchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn't be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don't offer similar advantages to general tasks.
Power efficiency is another important measurement in modern computers. Higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt).
Modern circuits have less power required per transistor as the number of transistors per chip grows.[19] This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However, the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible.[20] In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency.
AI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).[1]
As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of AI, a phenomenon known as the AI effect.[2] For instance, optical character recognition is frequently excluded from things considered to be AI,[3] having become a routine technology.[4]
Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,[5][6] followed by disappointment and the loss of funding (known as an "AI winter"),[7][8] followed by new approaches, success and renewed funding.[6][9] AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.[9][10]
The field was founded on the assumption that human intelligence "can be so precisely described that a machine can be made to simulate it".[b] This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[13] Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals.[c]
By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known as Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the "heuristic search" approach, which likened intelligence to a problem of exploring a space of possibilities for answers.
The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.[20] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.[21]
The field of AI research was born at a workshop at Dartmouth College in 1956.[d][24] The attendees became the founders and leaders of AI research.[e] They and their students produced programs that the press described as "astonishing":[f] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[g][26]
By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[27] and laboratories had been established around the world.[28]
They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[32] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an "AI winter", a period when obtaining funding for AI projects was difficult.[7]
In the early 1980s, AI research was revived by the commercial success of expert systems,[33] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[6] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[8]
Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into "sub-symbolic" approaches to specific AI problems.[34] Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.[h]
Interest in neural networks and "connectionism" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s.[39] Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.
AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).[40] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence".[10]
Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.[41] According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a "sporadic usage" in 2012 to more than 2,700 projects.[i] He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.[9]
Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s.[11]
The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[a]
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[44] By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[45]
Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a "combinatorial explosion": they became exponentially slower as the problems grew larger.[46] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[47]
Knowledge representation and knowledge engineering[48] allow AI programs to answer questions intelligently and make deductions about real-world facts.
A representation of "what exists" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.[49] The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.[50]
AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects;[50] situations, events, states and time;[51] causes and effects;[52] knowledge about knowledge (what we know about what other people know);.[53] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[54] as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous);[55] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express verbally).[47]
Formal knowledge representations are used in content-based indexing and retrieval,[56] scene interpretation,[57] clinical decision support,[58] knowledge discovery (mining "interesting" and actionable inferences from large databases),[59] and other areas.[60]
Machine learning (ML), a fundamental concept of AI research since the field's inception,[j] is the study of computer algorithms that improve automatically through experience.[k]
In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.[65]
Transfer learning is when the knowledge gained from one problem is applied to a new problem.[66]
Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[67]
Natural language processing (NLP)[68]
allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.[69]
Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic[46] and the breadth of commonsense knowledge.[55] Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), "Keyword spotting" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others.[70] They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.[71]
Machine perception[72]
is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[73]
facial recognition, and object recognition.[74]
Computer vision is the ability to analyze visual input.[75]
A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.[80]
Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, "master algorithm" that could lead to AGI.[81]
Others believe that anthropomorphic features like an artificial brain[82]
or simulated child development[l]
will someday reach a critical point where general intelligence emerges.
AI can solve many problems by intelligently searching through many possible solutions.[83] Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.[84] Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[85] Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.[86]
Simple exhaustive searches[87]
are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use "heuristics" or "rules of thumb" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called "pruning the search tree"). Heuristics supply the program with a "best guess" for the path on which the solution lies.[88]
Heuristics limit the search for solutions into a smaller sample size.[89]
A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing.[90] Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming.[91] Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[92]
Logic[93]
is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning[94]
and inductive logic programming is a method for learning.[95]
Several different forms of logic are used in AI research. Propositional logic[96] involves truth functions such as "or" and "not". First-order logic[97]
adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a "degree of truth" (between 0 and 1) to vague statements such as "Alice is old" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.[98]
Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.[54]
Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;[50]
situation calculus, event calculus and fluent calculus (for representing events and time);[51]
causal calculus;[52]
belief calculus (belief revision); and modal logics.[53]
Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.[99]
Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[100]
Bayesian networks[101]
are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),[m][103]
learning (using the expectation-maximization algorithm),[n][105]
planning (using decision networks)[106] and perception (using dynamic Bayesian networks).[107]
Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[107]
A key concept from the science of economics is "utility", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[108]
and information value theory.[109] These tools include models such as Markov decision processes,[110] dynamic decision networks,[107] game theory and mechanism design.[111]
The simplest AI applications can be divided into two types: classifiers ("if shiny then diamond") and controllers ("if diamond then pick up"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[112]
A classifier can be trained in various ways; there are many statistical and machine learning approaches.
The decision tree is the simplest and most widely used symbolic machine learning algorithm.[113]
K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.[114]
Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[115]
The naive Bayes classifier is reportedly the "most widely used learner"[116] at Google, due in part to its scalability.[117]
Neural networks are also used for classification.[118]
Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as "naive Bayes" on most practical data sets.[119]
Neural networks[118]
were inspired by the architecture of neurons in the human brain. A simple "neuron" N accepts input from other neurons, each of which, when activated (or "fired"), casts a weighted "vote" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed "fire together, wire together") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.
The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.[122]
Deep learning[124]
uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.[125] Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification[126] and others.
Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons,[127] and creates a hierarchy similar to the organization of the animal visual cortex.[128]
In a recurrent neural network (RNN) the signal will propagate through a layer more than once;[129]
thus, an RNN is an example of deep learning.[130]
RNNs can be trained by gradient descent,[131]
however long-term gradients which are back-propagated can "vanish" (that is, they can tend to zero) or "explode" (that is, they can tend to infinity), known as the vanishing gradient problem.[132]
The long short term memory (LSTM) technique can prevent this in most cases.[133]
Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing.
AI is relevant to any intellectual task.[134]
Modern artificial intelligence techniques are pervasive and are too numerous to list here.[135]
Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[136]
In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),
targeting online advertisements,[137] recommendation systems (offered by Netflix, YouTube or Amazon),
driving internet traffic,[138][139] targeted advertising (AdSense, Facebook),
virtual assistants (such as Siri or Alexa),[140] autonomous vehicles (including drones, ADAS and self-driving cars),
automatic language translation (Microsoft Translator, Google Translate),
facial recognition (Apple's Face ID or Microsoft's DeepFace),
image labeling (used by Facebook, Apple's iPhoto and TikTok)
, spam filtering and chatbots (such as Chat GPT).
There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage,[141] deepfakes,[142] medical diagnosis, military logistics, or supply chain management.
Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[143] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[144]
In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[145] Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus[o] and Cepheus.[147] DeepMind in the 2010s developed a "generalized artificial intelligence" that could learn many diverse Atari games on its own.[148]
By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.[149]
DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[150]
Other applications predict the result of judicial decisions,[151] create art (such as poetry or painting) and prove mathematical theorems.
Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[152]
Alan Turing wrote in 1950 "I propose to consider the question 'can machines think'?"[155]
He advised changing the question from whether a machine "thinks", to "whether or not it is possible for machinery to show intelligent behaviour".[155]
He devised the Turing test, which measures the ability of a machine to simulate human conversation.[156] Since we can only observe the behavior of the machine, it does not matter if it is "actually" thinking or literally has a "mind". Turing notes that we can not determine these things about other people[p] but "it is usual to have a polite convention that everyone thinks"[157]
Russell and Norvig agree with Turing that AI must be defined in terms of "acting" and not "thinking".[158] However, they are critical that the test compares machines to people. "Aeronautical engineering texts," they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"[159] AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence".[160]
No established unifying theory or paradigm has guided AI research for most of its history.[q] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.
Symbolic AI (or "GOFAI")[165] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at "intelligent" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."[166]
However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level "intelligent" tasks were easy for AI, but low level "instinctive" tasks were extremely difficult.[167]
Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a "feel" for the situation, rather than explicit symbolic knowledge.[168]
Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.[r][47]
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[170][171] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.
"Neats" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). "Scruffies" expect that it necessarily requires solving a large number of unrelated problems (especially in areas like common sense reasoning). This issue was actively discussed in the 70s and 80s,[172]
but in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed "the victory of the neats".[173]
Finding a provably correct or optimal solution is intractable for many important problems.[46] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.
AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[174][175]
General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.
David Chalmers identified two problems in understanding the mind, which he named the "hard" and "easy" problems of consciousness.[177] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[178]
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[179]
Philosopher John Searle characterized this position as "strong AI": "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."[s]
Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.[182]
If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so, then it could also suffer, and thus it would be entitled to certain rights.[183]
Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.[184]
This issue has been considered in fiction for centuries,[185]
and is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.[186]
A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.[175]
If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.[187]
Its intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the "singularity".[188]
Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.[189]
Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.[190]
Edward Fredkin argues that "artificial intelligence is the next stage in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[191]
In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI.[192]
A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[193]
Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classifies only 9% of U.S. jobs as "high risk".[t][195]
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously".[196]
Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[197]
AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.[198]
Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.[199]
Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.[200]
AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.[201]
Bias can be inadvertently introduced by the way training data is selected.[202]
It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair.[203] An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.[204]
Health equity issues may also be exacerbated when many-to-many mapping are done without taking steps to ensure equity for populations at risk for bias. At this time equity-focused tools and regulations are not in place to ensure equity application representation and usage.[205] Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.
At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[206]
Superintelligent AI may be able to improve itself to the point that humans could not control it. This could, as physicist Stephen Hawking puts it, "spell the end of the human race".[207] Philosopher Nick Bostrom argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's, it might need to harm humanity to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or "friendly" its stated goals might be.[208]
Political scientist Charles T. Rubin argues that "any sufficiently advanced benevolence may be indistinguishable from malevolence." Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would share our system of morality.[209]
AI's decisions making abilities raises the questions of legal responsibility and copyright status of created works. This issues are being refined in various jurisdictions.[216]
Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[217]
Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[218]
Machine ethics is also called machine morality, computational ethics or computational morality,[218]
and was founded at an AAAI symposium in 2005.[219]
Other approaches include Wendell Wallach's "artificial moral agents"[220]
and Stuart J. Russell's three principles for developing provably beneficial machines.[221]
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.[222]
The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[223]
Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[43]
Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[43]
The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[43] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[224]
Thought-capable artificial beings have appeared as storytelling devices since antiquity,[14]
and have been a persistent theme in science fiction.[16]
A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[225]
Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the "Multivac" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;[226]
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[227]
Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune.
These were the four the most widely used AI textbooks in 2008:
The two most widely used textbooks in 2021.Open Syllabus: Explorer
Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.[3][4]
A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[6][7]
Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.[8][9]
In its application across business problems, machine learning is also referred to as predictive analytics.
Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as "since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well". They can be nuanced, such as "X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist".[10]
Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.[11]
The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.[11]
The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[12][13] The synonym self-teaching computers was also used in this time period.[14][15]
By the early 1960s an experimental "learning machine" with punched tape memory, called CyberTron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively "trained" by a human operator/teacher to recognize patterns and equipped with a "goof" button to cause it to re-evaluate incorrect decisions.[16] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[17] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[18] In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[19]
Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E."[20] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".[21]
Modern-day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[22]
Machine learning (ML), reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[27]
The difference between ML and AI is frequently misunderstood. ML learns and predicts based on passive observations, whereas AI implies an agent interacting with the environment to learn and take actions that maximize its chance of successfully achieving its goals.[28]
As of 2020, many sources continue to assert that ML remains a subfield of AI.[29][30][27] Others have the view that not all ML is part of AI, but only an 'intelligent subset' of ML should be considered AI.[5][31][32]
Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples).[33]
The difference between optimization and machine learning arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples. Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.
Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[34] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[35] He also suggested the term data science as a placeholder to call the overall field.[35]
Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[29] wherein "algorithmic model" means more or less the machine learning algorithms like Random Forest.
Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[30]
Analytical and computational techniques derived from statistical physics of disordered systems, can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks.[36] Statistical physics is thus finding applications in the area of medical diagnostics.[37]
A core objective of a learner is to generalize from its experience.[5][31] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[32]
In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.
Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the "signal" or "feedback" available to the learning system:
Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[38] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[39] An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[20]
Types of supervised-learning algorithms include active learning, classification and regression.[28] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email.
Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.
Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, such as finding the probability density function.[40] Though unsupervised learning encompasses other domains involving summarizing and explaining data features.
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.
Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.
In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[41]
Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques.[42] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.
Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[43] In other words, it is a process of reducing the dimension of the feature set, also called the "number of features". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). This results in a smaller dimension of data (2D instead of 3D), while keeping all original variables in the model without changing the data.[44]
The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.
Other approaches have been developed which don't fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.[45]
As of 2022, deep learning is the dominant approach for much ongoing work in the field of machine learning.[11]
Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[46] It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[47]
The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 
It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.[48]
Several learning algorithms aim at discovering better representations of the inputs provided during training.[49] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[50] and various forms of clustering.[51][52][53]
Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[54] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[55]
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[56] A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[57]
In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[58] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[59]
In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[60]
Three broad categories of anomaly detection techniques exist.[61] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.
Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[62][63] and finally meta-learning (e.g. MAML).
Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of "interestingness".[64]
Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves "rules" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[65] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[67]
Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.
Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[68][69][70] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[71] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.
Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems.
Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
An ANN is a model based on a collection of connected units or nodes called "artificial neurons", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a "signal", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called "edges". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[72]
Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.
Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[73]  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[74]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.
A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.
Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.
A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[76][77] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[78]
Typically, machine learning models require a high quantity of reliable data in order for the models to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Bias models may result in detrimental outcomes thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably be integrated within machine learning engineering teams.
Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[79]
In 2006, the media-services provider Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[81] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.[82] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[83] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[84] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists.[85] In 2019 Springer Nature published the first research book created using machine learning.[86] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[87] Machine learning was recently applied to predict the pro-environmental behavior of travelers.[88] Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone.[89][90][91]
Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[92][93][94] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[95]
In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[96] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[97][98]
Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[99]
Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the "black box" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.
Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.[10]
Learners can also disappoint by "learning the wrong lesson". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[111] A real-world example is that, unlike humans, current image classifiers often don't primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in "adversarial" images that the system misclassifies.[112][113]
Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. Some systems are so brittle that changing a single adversarial pixel predictably induces misclassification.[citation needed] Machine learning models are often vulnerable to manipulation and/or evasion via adversarial machine learning.[114]
Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories "spam" and well-visible "not spam" of posts) machine learning models which are often developed and/or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[115][116][117]
Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[118]
In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).[119]
Machine learning poses a host of ethical questions. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[120] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to be either women or had non-European sounding names.[100] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[121][122] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.
AI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on the objectivity and logical reasoning.[123] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[124][125]
Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines.[126] This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increase profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[127]
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units.[128] By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[129] OpenAI estimated the hardware computing used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[130][131]
A physical neural network or Neuromorphic computer  is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. "Physical" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.[132][133]
Embedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers.[134][135][136] Running machine learning model in embedded devices removes the need for transferring and storing data on cloud servers for further processing, henceforth, reducing data breaches and privacy leaks happening because of transferring data, and also minimizes theft of intellectual properties, personal data and business secrets. Embedded Machine Learning could be applied through several techniques including hardware acceleration,[137][138] using approximate computing,[139] optimization of machine learning models and many more.[140][141]
Software suites containing a variety of machine learning algorithms include the following:
Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.
In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.
Given a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state).
The difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions.
The simplest possible planning problem, known as the Classical Planning Problem, is determined by:
Since the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning.
Further, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed.
With nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree.
When full observability is replaced by partial observability, planning corresponds to partially observable Markov decision process (POMDP).
If there are more than one agent, we have multi-agent planning, which is closely related to game theory.
In AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called "domain independent" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner.
The most commonly used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion.
An alternative language for describing planning problems is that of hierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks.
Temporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related to scheduling problems when uncertainty is involved and can also be understood in terms of timed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied. [2]
Probabilistic planning can be solved with iterative methods such as value iteration and policy iteration, when the state space is sufficiently small.
With partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states.
In preference-based planning, the objective is not only to produce a plan but also to satisfy user-specified preferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value.
Deterministic planning was introduced with the STRIPS planning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generated behavior tree.[3] The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but no loops or if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to a control flow, known from other programming languages like Pascal. It is very similar to program synthesis, which means a planner generates sourcecode which can be executed by an interpreter.[4]
We speak of "contingent planning" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but a decision tree because each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning.[8] The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it.
Michael L. Littman showed in 1998 that with branching actions, the planning problem becomes EXPTIME-complete.[9][10] A particular case of contiguous planning is represented by FOND problems - for "fully-observable and non-deterministic". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete[11] and 2EXPTIME-complete if the goal is specified with LDLf.
Conformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning,[12][13] but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning is EXPSPACE-complete,[14] and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.[10]
Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.
The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.
Sub-domains of computer vision include scene reconstruction, object detection, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. 
Adopting computer vision technology might be painstaking for organizations as there is no single point solution for it. There are very few companies that provide a unified and distributed platform or an Operating System where computer vision applications can be easily deployed and managed. 
Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[5][6][7] "Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding."[8] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[9] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.
In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior.[10] In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it "describe what it saw".[11][12]
What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[10]
The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[13]
By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[10]
Recent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks.[14][15] 
The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification,[16] segmentation and optical flow has surpassed prior methods.[citation needed][17]
Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible or infrared light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.[10] Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids.
Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing visual stimuli in both humans and various animals. This has led to a coarse, yet convoluted, description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology.  The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex.
Yet another field related to computer vision is signal processing. Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.
Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment.[19] A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot.
Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.[citation needed]
The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input is an image and the output is an image as well, whereas in computer vision, an image or a video is taken as an input and the output could be an enhanced image, an understanding of the content of an image or even behavior of a computer system based on such understanding.
Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data.[20] There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.
The following characterizations appear relevant but should not be taken as universally accepted:
Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.
Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:
A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable food stuff from bulk material, a process called optical sorting.[25]
Military applications are probably one of the largest areas of computer vision[citation needed]. The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as "battlefield awareness", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.
One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles and/or automatically ensuring navigational safety.[26] It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars, but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.
Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting micro undulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins is being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface.[27] Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.[28]
Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.
Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[33]
The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.[34]
Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition.[35] Performance of convolutional neural networks on the ImageNet tests is now close to that of humans.[35] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.[citation needed]
Several tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are:
Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.[20]
Image restoration comes into picture when the original image is degraded or damaged due to some external factors like lens wrong positioning , transmission interference , low lighting or motion blurs and etc.. which is referred to as noise. When the images are degraded or damaged the information to be extracted from that also gets damaged. Therefore we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look, to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.
The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.
Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.
The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.
While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[41]
There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories such as camera supports, cables and connectors.
Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).
A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures "images" that are then processed often using the same computer vision algorithms used to process visible-light images.
While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.[42]
Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.
As of 2016, vision processing units are emerging as a new class of processor, to complement CPUs and graphics processing units (GPUs) in this role.[43]
Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The goal is a computer capable of "understanding" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.
Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[7]
In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[8][9] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[10] and parsing.[11][12] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care.[13]
In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[14][15] such as by writing grammars or devising heuristic rules for stemming.
More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: 
Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used:
Since the so-called "statistical revolution"[16][17] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.
Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of "features" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature (complex-valued embeddings,[18] and neural networks in general have also been proposed, for e.g. speech[19]). Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.
Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.
Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.
A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[20] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).
The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[41]
Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
Cognition refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses."[42] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[43] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[44] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.
As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[45] with two defining aspects:
Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[47] functional grammar,[48] construction grammar,[49] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[50] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of "cognitive AI".[51] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit).[52]
The ACM A. M. Turing Award is an annual prize given by the Association for Computing Machinery (ACM) for contributions of lasting and major technical importance to computer science.[2] It is generally recognized as the highest distinction in computer science and is colloquially known as or often referred to as the "Nobel Prize of Computing".[3][4][5][6]
The award is named after Alan Turing, who was a British mathematician and reader in mathematics at the University of Manchester. Turing is often credited as being the key founder of theoretical computer science and artificial intelligence.[7] From 2007 to 2013, the award was accompanied by an additional prize of US$250,000, with financial support provided by Intel and Google.[2] Since 2014, the award has been accompanied by a prize of US$1 million, with financial support provided by Google.[1][8]
The first recipient, in 1966, was Alan Perlis, of Carnegie Mellon University. The first female recipient was Frances E. Allen of IBM in 2006.[9] The latest recipient, in 2021, is Jack Dongarra, of the University of Tennessee.
The history of computer science began long before the modern discipline of computer science, usually appearing in forms like mathematics or physics. Developments in previous centuries alluded to the discipline that we now know as computer science.[1] This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.[2]
The Antikythera mechanism is believed to be an early mechanical analog computer.[7]  It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.[7]
When John Napier discovered logarithms for computational purposes in the early 17th century,[15] there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624.[16] Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria.[17] Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.[18]
In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.[19]
Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.
In 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. Leibniz simplified the binary system and articulated logical properties such as conjunction, disjunction, negation, identity, inclusion, and the empty set.[21] He anticipated Lagrangian interpolation and algorithmic information theory. His calculus ratiocinator anticipated aspects of the universal Turing machine. In 1961, Norbert Wiener suggested that Leibniz should be considered the patron saint of cybernetics.[22] Wiener is quoted with "Indeed, the general idea of a computing machine is nothing but a mechanization of Leibniz's Calculus Ratiocinator."[23] But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled.[24]
By this time, the first mechanical devices driven by a binary pattern had been invented. The industrial revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard's loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard's loom was far from being a computer, but it did illustrate that machines could be driven by binary systems.[24]
Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his "Analytical Engine", the first mechanical computer.[26] During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers,[27] although this is arguable as Charles was the first to design the difference engine and consequently its corresponding difference based algorithms, making him the first computer algorithm designer.  Moreover, Lovelace's work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not.[28] While she was never able to see the results of her work, as the "Analytical Engine" was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.[29]
Following Babbage, although at first unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.[30][31] Two other inventors, Leonardo Torres y Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1913) Torres y Quevedo designed a Babbage type of calculating machine that used electromechanical parts which included floating point number representations and built a prototype in 1920. Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.[32]
Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as a logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935 to 1938).
Up to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor.  This changed with switching circuit theory in the 1930s. From 1934 to 1936, Akira Nakashima, Claude Shannon, and Viktor Shetakov published a series of papers showing that the two-valued Boolean algebra, can describe the operation of switching circuits.[37][38][39][40] This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.[40]
While taking an undergraduate philosophy class, Shannon had been exposed to Boole's work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems. His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.[41]
Before the 1920s, computers (sometimes computors) were human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Many of these clerks who served as human computers were women.[42][43][44][45] Some performed astronomical calculations for calendars, others ballistic tables for the military.[46]
After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.
Machines that computed with continuous values became known as the analog kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.
Digital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.
The phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.
Since the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described "purely mechanical." The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.
In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine. This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use.[49] These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability. If a Turing machine can complete the task, it is considered Turing computable.[50]
The Los Alamos physicist Stanley Frankel, has described John von Neumann's view of the fundamental importance of Turing's 1936 paper, in a letter:[49]
In 1948, the Manchester Baby was completed; it was the world's first electronic digital computer that ran programs stored in its memory, like almost all modern computers.[49] The influence on Max Newman of Turing's seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Baby.[49]
Claude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit.  This work is one of the theoretical foundations for many areas of study, including data compression and cryptography.[56]
From experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for "steersman." He published "Cybernetics" in 1948, which influenced artificial intelligence. Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.[57]
In 1946, a model for computer architecture was introduced and became known as Von Neumann architecture. Since 1950, the von Neumann model provided uniformity in subsequent computer designs. The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space.[58]  The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU). In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.[59]
The term artificial intelligence was credited by John McCarthy to explain the research that they were doing for a proposal for the Dartmouth Summer Research. The naming of artificial intelligence also led to the birth of a new field in computer science.[61] On August 31, 1955, a research project was proposed consisting of John McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon. The official project began in 1956 that consisted of several significant parts they felt would help them better understand artificial intelligence's makeup.[62]
McCarthy and his colleagues' ideas behind automatic computers was while a machine is capable of completing a task, then the same should be confirmed with a computer by compiling a program to perform the desired results. They also discovered that the human brain was too complex to replicate, not by the machine itself but by the program. The knowledge to produce a program that sophisticated was not there yet.[62]
The concept behind this was looking at how humans understand our own language and structure of how we form sentences, giving different meaning and rule sets and comparing them to a machine process.[62] The way computers can understand is at a hardware level. This language is written in binary (1s and 0's). This has to be written in a specific format that gives the computer the ruleset to run a particular hardware piece.[63]
Minsky's process determined how these artificial neural networks could be arranged to have similar qualities to the human brain. However, he could only produce partial results and needed to further the research into this idea.[62]
McCarthy and Shannon's idea behind this theory was to develop a way to use complex problems to determine and measure the machine's efficiency through mathematical theory and computations.[64] However, they were only to receive partial test results.[62]
The idea behind self-improvement is how a machine would use self-modifying code to make itself smarter. This would allow for a machine to grow in intelligence and increase calculation speeds.[65] The group believed they could study this if a machine could improve upon the process of completing a task in the abstractions part of their research.[62]
The group thought that research in this category could be broken down into smaller groups. This would consist of sensory and other forms of information about artificial intelligence.[62] Abstractions in computer science can refer to mathematics and programming language.[66]
Their idea of computational creativity is how the program or a machine can be seen in having similar ways of human thinking.[67] They wanted to see if a machine could take a piece of incomplete information and improve upon it to fill in the missing details as the human mind can do. If this machine could do this; they needed to think of how did the machine determine the outcome.[62]
The history of computing is longer than the history of computing hardware and modern computing technology and includes the history of methods intended for pen and paper or for chalk and slate, with or without the aid of tables.
Digital computing is intimately tied to the representation of numbers.[1] But long before abstractions like the number arose, there were mathematical concepts to serve the purposes of civilization. These concepts are implicit in concrete practices such as:
Eventually, the concept of numbers became concrete and familiar enough for counting to arise, at times with sing-song mnemonics to teach sequences to others. All known human languages, except the Piraha language, have words for at least "one" and "two", and even some animals like the blackbird can distinguish a surprising number of items.[5]
Advances in the numeral system and mathematical notation eventually led to the discovery of mathematical operations such as addition, subtraction, multiplication, division, squaring, square root, and so forth. Eventually the operations were formalized, and concepts about the operations became understood well enough to be stated formally, and even proven. See, for example, Euclid's algorithm for finding the greatest common divisor of two numbers.
Mathematical statements need not be abstract only; when a statement can be illustrated with actual numbers, the numbers can be communicated and a community can arise. This allows the repeatable, verifiable statements which are the hallmark of mathematics and science. These kinds of statements have existed for thousands of years, and across multiple civilizations, as shown below:
In the 3rd century BC, Archimedes used the mechanical principle of balance (see Archimedes Palimpsest#Mathematical content) to calculate mathematical problems, such as the number of grains of sand in the universe (The sand reckoner), which also required a recursive notation for numbers (e.g., the myriad myriad).
The Antikythera mechanism is believed to be the earliest known mechanical analog computer.[10]  It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.
Indeed, when John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. The apex of this early era of formal computing can be seen in the difference engine and its successor the analytical engine  both by Charles Babbage. Babbage never completed constructing either engine, but in 2002 Doron Swade and a group of other engineers at the Science Museum in London completed Babbage's difference engine using only materials that would have been available in the 1840s.[17]  By following Babbage's detailed design they were able to build a functioning engine, allowing historians to say, with some confidence, that if Babbage would have been able to complete his difference engine it would have worked.[18] The additionally advanced analytical engine combined concepts from his previous work and that of others to create a device that, if constructed as designed, would have possessed many properties of a modern electronic computer, such as an internal "scratch memory" equivalent to RAM, multiple forms of output including a bell, a graph-plotter, and simple printer, and a programmable input-output "hard" memory of punch cards which it could modify as well as read.  The key advancement which Babbage's devices possessed beyond those created before his was that each component of the device was independent of the rest of the machine, much like the components of a modern electronic computer. This was a fundamental shift in thought; previous computational devices served only a single purpose, but had to be at best disassembled and reconfigured to solve a new problem.  Babbage's devices could be reprogramed to solve new problems by the entry of new data, and act upon previous calculations within the same series of instructions. Ada Lovelace took this concept one step further, by creating a program for the analytical engine to calculate Bernoulli numbers, a complex calculation requiring a recursive algorithm.  This is considered to be the first example of a true computer program, a series of instructions that act upon data not known in full until the program is run.
Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history.[19] Two other inventors, Leonardo Torres y Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1913) Torres y Quevedo designed a Babbage type of calculating machine that used electromechanical parts which included floating point number representations and built a prototype in 1920. Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.[20]
Several examples of analog computation survived into recent times.  A planimeter is a device which does integrals, using distance as the analog quantity. Until the 1980s, HVAC systems used air both as the analog quantity and the controlling element. Unlike modern digital computers, analog computers are not very flexible, and need to be reconfigured (i.e., reprogrammed) manually to switch them from working on one problem to another. Analog computers had an advantage over early digital computers in that they could be used to solve complex problems using behavioral analogues while the earliest attempts at digital computers were quite limited.
Since computers were rare in this era, the solutions were often hard-coded into paper forms such as nomograms,[21] which could then produce analog solutions to these problems, such as the distribution of pressures and temperatures in a heating system.
The "brain" [computer] may one day come down to our level [of the common people] and help with our income-tax and book-keeping calculations. But this is speculation and there is no sign of it so far.
None of the early computational devices were really computers in the modern sense, and it took considerable advancement in mathematics and theory before the first modern computers could be designed.
In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits.[23] During 1880-81 he showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but this work on it was unpublished until 1933.[24] The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called Peirce's arrow.[25] Consequently, these gates are sometimes called universal logic gates.[26]
Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as a logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935 to 1938).
The first recorded idea of using digital electronics for computing was the 1931 paper "The Use of Thyratrons for High Speed Automatic Counting of Physical Phenomena" by C. E. Wynn-Williams.[27] From 1934 to 1936, NEC engineer Akira Nakashima, Claude Shannon, and Victor Shestakov published papers introducing switching circuit theory, using digital electronics for Boolean algebraic operations.[28][29][30][31]
In 1935 Alan Turing wrote his seminal paper On Computable Numbers, with an Application to the Entscheidungsproblem[32] in which he modeled computation in terms of a one-dimensional storage tape, leading to the idea of the Universal Turing machine and Turing-complete systems.
The first digital electronic computer was developed in the period April 1936 - June 1939, in the IBM Patent Department, Endicott, New York by Arthur Halsey Dickinson.[33][34][35] In this computer IBM introduced for the first time, a calculating device with keyboard, processor and electronic output (display). Competitor to IBM was the digital electronic computer NCR3566, developed in NCR, Dayton, Ohio by Joseph Desch and Robert Mumma in the period April 1939 - August 1939.[36][37] The IBM and NCR machines were decimal, executing addition and subtraction in binary position code.
During World War II, ballistics computing was done by women, who were hired as "computers." The term computer remained one that referred to mostly women (now seen as "operator") until 1945, after which it took on the modern definition of machinery it presently holds.[40]
The ENIAC (Electronic Numerical Integrator And Computer) was the first electronic general-purpose computer, announced to the public in 1946. It was Turing-complete,[citation needed] digital, and capable of being reprogrammed to solve a full range of computing problems. Women implemented the programming for machines like the ENIAC, and men created the hardware.[40]
The Manchester Baby was the first electronic stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[41]
William Shockley, John Bardeen and Walter Brattain at Bell Labs invented the first working transistor, the point-contact transistor, in 1947, followed by the bipolar junction transistor in 1948.[42][43] At the University of Manchester in 1953, a team under the leadership of Tom Kilburn designed and built the first transistorized computer, called the Transistor Computer, a machine using the newly developed transistors instead of valves.[44] The first stored-program transistor computer was the ETL Mark III, developed by Japan's Electrotechnical Laboratory[45][46][47] from 1954[48] to 1956.[46] However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications.[49]
In 1954, 95% of computers in service were being used for engineering and scientific purposes.[50]
The silicon-gate MOS integrated circuit was developed by Federico Faggin at Fairchild Semiconductor in 1968.[58] This led to the development of the first single-chip microprocessor, the Intel 4004.[59] The Intel 4004 was developed as a single-chip microprocessor from 1969 to 1970, led by Intel's Federico Faggin, Marcian Hoff, and Stanley Mazor, and Busicom's Masatoshi Shima.[60] The chip was mainly designed and realized by Faggin, with his silicon-gate MOS technology.[59] The microprocessor led to the microcomputer revolution, with the development of the microcomputer, which would later be called the personal computer (PC).
Most early microprocessors, such as the Intel 8008 and Intel 8080, were 8-bit. Texas Instruments released the first fully 16-bit microprocessor, the TMS9900 processor, in June 1976.[61] They used the microprocessor in the TI-99/4 and TI-99/4A computers.
The 1980s brought about significant advances with microprocessor that greatly impacted the fields of engineering and other sciences. The Motorola 68000 microprocessor had a processing speed that was far superior to the other microprocessors being used at the time. Because of this, having a newer, faster microprocessor allowed for the newer microcomputers that came along after to be more efficient in the amount of computing they were able to do. This was evident in the 1983 release of the Apple Lisa. The Lisa was one of the first personal computers with a graphical user interface (GUI) that was sold commercially. It ran on the Motorola 68000 CPU and used both dual floppy disk drives and a 5 MB hard drive for storage. The machine also had 1MB of RAM used for running software from disk without rereading the disk persistently.[62] After the failure of the Lisa in terms of sales, Apple released its first Macintosh computer, still running on the Motorola 68000 microprocessor, but with only 128KB of RAM, one floppy drive, and no hard drive in order to lower the price.
Starting with known special cases, the calculation of logarithms and trigonometric functions can be performed by looking up numbers in a mathematical table, and interpolating between known cases.  For small enough differences, this linear operation was accurate enough for use in navigation and astronomy in the Age of Exploration. The uses of interpolation have thrived in the past 500 years: by the twentieth century Leslie Comrie and W.J. Eckert systematized the use of interpolation in tables of numbers for punch card calculation.
By the late 1960s, computer systems could perform symbolic algebraic manipulations well enough to pass college-level calculus courses.[citation needed]
Women are often underrepresented in STEM fields when compared to their male counterparts.[69] In the modern era prior to the 1960s, computing was widely seen as "women's work," since it was associated with the operation of tabulating machines and other mechanical office work.[70][71] The accuracy of this association varied from place to place. In America, Margaret Hamilton recalled an environment dominated by men,[72] while Elsie Shutt recalled surprise at seeing even half of the computer operators at Raytheon were men.[73] Machine operators in Britain were mostly women into the early 1970s.[74] As these perceptions changed and computing became a high-status career, the field became more dominated by men.[75][76][77] Professor Janet Abbate, in her book Recoding Gender, writes:
Some notable examples of women in the history of computing are:
Click on a date/time to view the file as it appeared at that time.
Computer hardware includes the physical parts of a computer, such as the case, central processing unit (CPU), random access memory (RAM), monitor, mouse, keyboard, computer data storage, graphics card, sound card, speakers and motherboard.[1][2]
By contrast, software is the set of instructions that can be stored and run by hardware. Hardware is so-termed because it is "hard" or rigid with respect to changes, whereas software is "soft" because it is easy to change.
Hardware is typically directed by the software to execute any command or instruction. A combination of hardware and software forms a usable computing system, although other systems exist with only hardware.
The template for all modern computers is the Von Neumann architecture, detailed in a 1945 paper by Hungarian mathematician John von Neumann. This describes a design architecture for an electronic digital computer with subdivisions of a processing unit consisting of an arithmetic logic unit and processor registers, a control unit containing an instruction register and program counter, a memory to store both data and instructions, external mass storage, and input and output mechanisms.[3] The meaning of the term has evolved to mean a stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus. This is referred to as the Von Neumann bottleneck and often limits the performance of the system.[4]
The personal computer is one of the most common types of computer due to its versatility and relatively low price. Desktop personal computers have  a monitor, a keyboard, a mouse, and a computer case. The computer case holds the motherboard, fixed or removable disk drives for data storage, the power supply, and may contain other peripheral devices such as modems or network interfaces.  Some models of desktop computers integrated the monitor and keyboard into the same case as the processor and power supply. Separating the elements allows the user to arrange the components in a pleasing, comfortable array, at the cost of managing power and data cables between them.
Laptops are designed for portability but operate similarly to desktop PCs.[5] They may use lower-power or reduced size components, with lower performance than a similarly priced desktop computer.[6] Laptops contain the keyboard, display, and processor in one case. The monitor in the folding upper cover of the case can be closed for transportation, to protect the screen and keyboard.  Instead of a mouse, laptops may have a touchpad or pointing stick.
Tablets are portable computers that use a touch screen as the primary input device. Tablets generally weigh less and are smaller than laptops.
Some tablets include fold-out keyboards, or offer connections to separate external keyboards. Some models of laptop computers have a detachable keyboard, which allows the system to be configured as a touch-screen tablet. They are sometimes called "2-in-1 detachable laptops" or "tablet-laptop hybrids".[7]
The computer case encloses most of the components of the system. It provides mechanical support and protection for internal elements such as the motherboard, disk drives, and power supplies, and controls and directs the flow of cooling air over internal components. The case is also part of the system to control electromagnetic interference radiated by the computer and protects internal parts from electrostatic discharge. Large tower cases provide space for multiple disk drives or other peripherals and usually stand on the floor, while desktop cases provide less expansion room. All-in-one style designs include a video display built into the same case. Portable and laptop computers require cases that provide impact protection for the unit. Hobbyists may decorate the cases with colored lights, paint, or other features, in an activity called case modding.
Components directly attached to or to part of the motherboard include:
An expansion card in computing is a printed circuit board that can be inserted into an expansion slot of a computer motherboard or backplane to add functionality to a computer system via the expansion bus. Expansion cards can be used to obtain or expand on features not offered by the motherboard.
A storage device is any computing hardware and digital media that is used for storing, porting and extracting data files and objects. It can hold and store information both temporarily and permanently and can be internal or external to a computer, server or any similar computing device. Data storage is a core function and fundamental component of computers. Dedicated storage devices include RAIDs and tape libraries.
Data is stored by a computer using a variety of media. Hard disk drives (HDDs) are found in virtually all older computers, due to their high capacity and low cost, but solid-state drives (SSDs) are faster and more power efficient, although currently more expensive than hard drives in terms of dollar per gigabyte,[11] so are often found in personal computers built post-2007.[12] SSDs use flash memory, which stores data on MOS memory chips consisting of floating-gate MOSFET memory cells. Some systems may use a disk array controller for greater performance or reliability.
To transfer data between computers, an external flash memory device (such as a memory card or USB flash drive) or optical disc (such as a CD-ROM, DVD-ROM or BD-ROM) may be used. Their usefulness depends on being readable by other systems; the majority of machines have an optical disk drive (ODD), and virtually all have at least one Universal Serial Bus (USB) port. Additionally, USB sticks are typically pre-formatted with the FAT32 file system, which is widely supported across operating systems.
Input and output devices are typically housed externally to the main computer chassis. The following are either standard or very common to many computer systems.
Input devices allow the user to enter information into the system, or control its operation. Most personal computers have a mouse and keyboard, but laptop systems typically use a touchpad instead of a mouse. Other input devices include webcams, microphones, joysticks, and image scanners.
Output devices are designed around the senses of human beings. For example, monitors display text that can be read, speakers produce sound that can be heard.[13] Such devices also could include printers or a Braille embosser.
A mainframe computer is a much larger computer that typically fills a room and may cost many hundreds or thousands of times as much as a personal computer. They are designed to perform large numbers of calculations for governments and large enterprises.
In the 1960s and 1970s, more and more departments started to use cheaper and dedicated systems for specific purposes like process control and laboratory automation. A minicomputer, or colloquially mini, is a class of smaller computers that was developed in the mid-1960s[14][15] and sold for much less than mainframe[16] and mid-size computers from IBM and its direct competitors.
A supercomputer is superficially similar to a mainframe but is instead intended for extremely demanding computational tasks. As of November 2021[update], the fastest supercomputer on the TOP500 supercomputer list is Fugaku, in Japan, with a LINPACK benchmark score of 415 PFLOPS, superseding the second fastest, Summit, in the United States, by around 294 PFLOPS.
The term supercomputer does not refer to a specific technology. Rather it indicates the fastest computations available at any given time. In mid-2011, the fastest supercomputers boasted speeds exceeding one petaflop, or 1 quadrillion (10^15 or 1,000 trillion) floating-point operations per second.
Supercomputers are fast but extremely costly, so they are generally used by large organizations to execute computationally demanding tasks involving large data sets. Supercomputers typically run military and scientific applications. Although costly, they are also being used for commercial applications where huge amounts of data must be analyzed. For example, large banks employ supercomputers to calculate the risks and returns of various investment strategies, and healthcare organizations use them to analyze giant databases of patient data to determine optimal treatments for various diseases and problems incurring to the country.
When using computer hardware, an upgrade means adding new or additional hardware to a computer that improves its performance, increases its capacity, or adds new features. For example, a user could perform a hardware upgrade to replace the hard drive with a faster one or a Solid State Drive (SSD) to get a boost in performance. The user may also install more Random Access Memory (RAM) so the computer can store additional temporary data, or retrieve such data at a faster rate. The user may add a USB 3.0 expansion card to fully use USB 3.0 devices, or could upgrade the Graphics Processing Unit (GPU) for cleaner, more advanced graphics, or more monitors. Performing such hardware upgrades may be necessary for aged computers to meet a new, or updated program's system requirements.
In large organizations, hardware upgrades are handled by administrators who are also in charge of keeping networks running smoothly. They replace network devices like servers, routers and storage devices based on new demands and capacities.
Global revenue from computer hardware in 2016 reached 408 billion Euros.[17]
Because computer parts contain hazardous materials, there is a growing movement to recycle old and outdated parts.[18] Computer hardware contain dangerous chemicals such as: lead, mercury, nickel, and cadmium. According to the EPA these e-wastes have a harmful effect on the environment unless they are disposed of properly. Making hardware requires energy, and recycling parts will reduce air pollution, water pollution, as well as greenhouse gas emissions.[19] Disposing unauthorized computer equipment is in fact illegal. Legislation makes it mandatory to recycle computers through the government approved facilities. Recycling a computer can be made easier by taking out certain reusable parts. For example, the RAM, DVD drive, the graphics card, hard drive or SSD, and other similar removable parts can be reused.
Many materials used in computer hardware can be recovered by recycling for use in future production. Reuse of tin, silicon, iron, aluminium, and a variety of plastics that are present in bulk in computers or other electronics can reduce the costs of constructing new systems. Components frequently contain copper, gold, tantalum,[20][21] silver, platinum, palladium, and lead as well as other valuable materials suitable for reclamation.[22][23]
The central processing unit contains many toxic materials. It contains lead and chromium in the metal plates. Resistors, semi-conductors, infrared detectors, stabilizers, cables, and wires contain cadmium. The circuit boards in a computer contain mercury, and chromium.[24] When these types of materials, and chemicals are disposed improperly will become hazardous for the environment.
According to the United States Environmental Protection Agency only around 15% of the e-waste actually is recycled. When e-waste byproducts leach into groundwater, are burned, or get mishandled during recycling, it causes harm. Health problems associated with such toxins include impaired mental development, cancer, and damage to the lungs, liver, and kidneys.[25] That's why even wires have to be recycled. Different companies have different techniques to recycle a wire. The most popular one is the grinder that separates the copper wires from the plastic/rubber casing. When the processes are done there are two different piles left; one containing the copper powder, and the other containing plastic/rubber pieces.[26] Computer monitors, mice, and keyboards all have a similar way of being recycled. For example, first, each of the parts are taken apart then all of the inner parts get separated and placed into its own bin.[27]
Computer components contain many toxic substances, like dioxins, polychlorinated biphenyls (PCBs), cadmium, chromium, radioactive isotopes and mercury. Circuit boards contain considerable quantities of lead-tin solders that are more likely to leach into groundwater or create air pollution due to incineration. In US landfills, about 40% of the lead content levels are from e-waste.[28] The processing (e.g. incineration and acid treatments) required to reclaim these precious substances may release, generate, or synthesize toxic byproducts.
Recycling of computer hardware is considered environmentally friendly because it prevents hazardous waste, including heavy metals and carcinogens, from entering the atmosphere, landfill or waterways. While electronics consist a small fraction of total waste generated, they are far more dangerous. There is stringent legislation designed to enforce and encourage the sustainable disposal of appliances, the most notable being the Waste Electrical and Electronic Equipment Directive of the European Union and the United States National Computer Recycling Act.[29]
As computer hardware contain a wide number of metals inside, the United States Environmental Protection Agency (EPA) encourages the collection and recycling of computer hardware. "E-cycling", the recycling of computer hardware, refers to the donation, reuse, shredding and general collection of used electronics. Generically, the term refers to the process of collecting, brokering, disassembling, repairing and recycling the components or metals contained in used or discarded electronic equipment, otherwise known as electronic waste (e-waste). "E-cyclable" items include, but are not limited to: televisions, computers, microwave ovens, vacuum cleaners, telephones and cellular phones, stereos, and VCRs and DVDs just about anything that has a cord, light or takes some kind of battery.[30]
Recycling a computer is made easier by a few of the national services, such as Dell and Apple. Both companies will take back the computer of their make or any other make. Otherwise a computer can be donated to Computer Aid International which is an organization that recycles and refurbishes old computers for hospitals, schools, universities, etc.[31]
The history of computing hardware covers the developments from early simple devices to aid calculation to modern day computers.
The first aids to computation were purely mechanical devices which required the operator to set up the initial values of an elementary arithmetic operation, then manipulate the device to obtain the result. Later, computers represented numbers in a continuous form (e.g. distance along a scale, rotation of a shaft, or a voltage). Numbers could also be represented in the form of digits, automatically manipulated by a mechanism. Although this approach generally required more complex mechanisms, it greatly increased the precision of results. The development of transistor technology and then the integrated circuit chip led to a series of breakthroughs, starting with transistor computers and then integrated circuit computers, causing digital computers to largely replace analog computers. Metal-oxide-semiconductor (MOS) large-scale integration (LSI) then enabled semiconductor memory and the microprocessor, leading to another key breakthrough, the miniaturized personal computer (PC), in the 1970s. The cost of computers gradually became so low that personal computers by the 1990s, and then mobile computers (smartphones and tablets) in the 2000s, became ubiquitous.
Scottish mathematician and physicist John Napier discovered that the multiplication and division of numbers could be performed by the addition and subtraction, respectively, of the logarithms of those numbers. While producing the first logarithmic tables, Napier needed to perform many tedious multiplications. It was at this point that he designed his 'Napier's bones', an abacus-like device that greatly simplified calculations that involved multiplication and division.[d]
Since real numbers can be represented as distances or intervals on a line, the slide rule was invented in the 1620s, shortly after Napier's work, to allow multiplication and division operations to be carried out significantly faster than was previously possible.[13] Edmund Gunter built a calculating device with a single logarithmic scale at the University of Oxford. His device greatly simplified arithmetic calculations, including multiplication and division. William Oughtred greatly improved this in 1630 with his circular slide rule. He followed this up with the modern slide rule in 1632, essentially a combination of two Gunter rules, held together with the hands. Slide rules were used by generations of engineers and other mathematically involved professional workers, until the invention of the pocket calculator.[14]
In 1609 Guidobaldo del Monte made a mechanical multiplier to calculate fractions of a degree. Based on a system of four gears, the rotation of an index on one quadrant corresponds to 60 rotations of another index on an opposite quadrant.[15] Thanks to this machine, errors in the calculation of first, second, third and quarter degrees can be avoided. Guidobaldo is the first to document the use of gears for mechanical calculation.
Wilhelm Schickard, a German polymath, designed a calculating machine in 1623 which combined a mechanized form of Napier's rods with the world's first mechanical adding machine built into the base. Because it made use of a single-tooth gear there were circumstances in which its carry mechanism would jam.[16] A fire destroyed at least one of the machines in 1624 and it is believed Schickard was too disheartened to build another.
In 1642, while still a teenager, Blaise Pascal started some pioneering work on calculating machines and after three years of effort and 50 prototypes[17] he invented a mechanical calculator.[18][19] He built twenty of these machines (called Pascal's calculator or Pascaline) in the following ten years.[20] Nine Pascalines have survived, most of which are on display in European museums.[21] A continuing debate exists over whether Schickard or Pascal should be regarded as the "inventor of the mechanical calculator" and the range of issues to be considered is discussed elsewhere.[22]
Gottfried Wilhelm von Leibniz invented the stepped reckoner and his famous stepped drum mechanism around 1672. He attempted to create a machine that could be used not only for addition and subtraction but would use a moveable carriage to enable multiplication and division. Leibniz once said "It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used."[23] However, Leibniz did not incorporate a fully successful carry mechanism. Leibniz also described the binary numeral system,[24] a central ingredient of all modern computers. However, up to the 1940s, many subsequent designs (including Charles Babbage's machines of the 1822 and even ENIAC of 1945) were based on the decimal system.[e]
Around 1820, Charles Xavier Thomas de Colmar created what would over the rest of the century become the first successful, mass-produced mechanical calculator, the Thomas Arithmometer. It could be used to add and subtract, and with a moveable carriage the operator could also multiply, and divide by a process of long multiplication and long division.[25] It utilised a stepped drum similar in conception to that invented by Leibniz. Mechanical calculators remained in use until the 1970s.
In 1804, French weaver Joseph Marie Jacquard developed a loom in which the pattern being woven was controlled by a paper tape constructed from punched cards. The paper tape could be changed without changing the mechanical design of the loom. This was a landmark achievement in programmability. His machine was an improvement over similar weaving looms. Punched cards were preceded by punch bands, as in the machine proposed by Basile Bouchon. These bands would inspire information recording for automatic pianos and more recently numerical control machine tools.
In the late 1880s, the American Herman Hollerith invented data storage on punched cards that could then be read by a machine.[26] To process these punched cards, he invented the tabulator and the keypunch machine. His machines used electromechanical relays and counters.[27] Hollerith's method was used in the 1890 United States Census. That census was processed two years faster than the prior census had been.[28] Hollerith's company eventually became the core of IBM.
By 1920, electromechanical tabulating machines could add, subtract, and print accumulated totals.[29] Machine functions were directed  by inserting dozens of wire jumpers into removable control panels. When the United States instituted Social Security in 1935, IBM punched-card systems were used to process records of 26 million workers.[30] Punched cards became ubiquitous in industry and government for accounting and administration.
Leslie Comrie's articles on punched-card methods and W. J. Eckert's publication of Punched Card Methods in Scientific Computation in 1940, described punched-card techniques sufficiently advanced to solve some differential equations[31] or perform multiplication and division using floating-point representations, all on punched cards and unit record machines. Such machines were used during World War II for cryptographic statistical processing, as well as a vast number of administrative uses. The Astronomical Computing Bureau, Columbia University, performed astronomical calculations representing the state of the art in computing.[32][33]
Companies like Friden, Marchant Calculator and Monroe made desktop mechanical calculators from the 1930s that could add, subtract, multiply and divide.[36] In 1948, the Curta was introduced by Austrian inventor Curt Herzstark. It was a small, hand-cranked mechanical calculator and as such, a descendant of Gottfried Leibniz's Stepped Reckoner and Thomas' Arithmometer.
Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[39] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. It employed ordinary base-10 fixed-point arithmetic.
The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[40][41]
There was to be a store, or memory, capable of holding 1,000 numbers of 40 decimal digits each (ca. 16.7 kB). An arithmetical unit, called the "mill", would be able to perform all four arithmetic operations, plus comparisons and optionally square roots. Initially it was conceived as a difference engine curved back upon itself, in a generally circular layout,[42] with the long store exiting off to one side. (Later drawings depict a regularized grid layout.)[43] Like the central processing unit (CPU) in a modern computer, the mill would rely on its own internal procedures, roughly equivalent to microcode in modern CPUs, to be stored in the form of pegs inserted into rotating drums called "barrels", to carry out some of the more complex instructions the user's program might specify.[44]
The programming language to be employed by users was akin to modern day assembly languages. Loops and conditional branching were possible, and so the language as conceived would have been Turing-complete as later defined by Alan Turing. Three different types of punch cards were used: one for arithmetical operations, one for numerical constants, and one for load and store operations, transferring numbers from the store to the arithmetical unit or back. There were three separate readers for the three types of cards.
Following Babbage, although at first unaware of his earlier work, was Percy Ludgate, a clerk to a corn merchant in Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.[46][47] Two other inventors, Leonardo Torres y Quevedo and Vannevar Bush, also did follow on research based on Babbage's work. In his Essays on Automatics (1913) Torres y Quevedo designed a Babbage type of calculating machine that used electromechanical parts which included floating-point number representations and built an early prototype in 1920. Bush's paper Instrumental Analysis (1936) discussed using existing IBM punch card machines to implement Babbage's design. In the same year he started the Rapid Arithmetical Machine project to investigate the problems of constructing an electronic digital computer.[48]
In the first half of the 20th century, analog computers were considered by many to be the future of computing. These devices used the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved, in contrast to digital computers that represented varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines.[49]
The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson, later Lord Kelvin, in 1872. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location and was of great utility to navigation in shallow waters. His device was the foundation for further developments in analog computing.[50]
The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin. He explored the possible construction of such calculators, but was stymied by the limited output torque of the ball-and-disk integrators.[51] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output.
An important advance in analog computing was the development of the first fire-control systems for long range ship gunlaying. When gunnery ranges increased dramatically in the late 19th century it was no longer a simple matter of calculating the proper aim point, given the flight times of the shells. Various spotters on board the ship would relay distance measures and observations to a central plotting station. There the fire direction teams fed in the location, speed and direction of the ship and its target, as well as various adjustments for Coriolis effect, weather effects on the air, and other adjustments; the computer would then output a firing solution, which would be fed to the turrets for laying. In 1912, British engineer Arthur Pollen developed the first electrically powered mechanical analogue computer (called at the time the Argo Clock).[citation needed] It was used by the Imperial Russian Navy in World War I.[citation needed] The alternative Dreyer Table fire control system was fitted to British capital ships by mid-1916.
Mechanical devices were also used to aid the accuracy of aerial bombing. Drift Sight was the first such aid, developed by Harry Wimperis in 1916 for the Royal Naval Air Service; it measured the wind speed from the air, and used that measurement to calculate the wind's effects on the trajectory of the bombs. The system was later improved with the Course Setting Bomb Sight, and reached a climax with World War II bomb sights, Mark XIV bomb sight (RAF Bomber Command) and the Norden[52] (United States Army Air Forces).
The art of mechanical analog computing reached its zenith with the differential analyzer,[53] built by H. L. Hazen and Vannevar Bush at MIT starting in 1927, which built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious; the most powerful was constructed at the University of Pennsylvania's Moore School of Electrical Engineering, where the ENIAC was built.
By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but hybrid analog computers, controlled by digital electronics, remained in substantial use into the 1950s and 1960s, and later in some specialized applications.
He also introduced the notion of a "universal machine" (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[58] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.
The Z2 was one of the earliest examples of an electromechanical relay computer, and was created by German engineer Konrad Zuse in 1940. It was an improvement on his earlier Z1; although it used the same mechanical memory, it replaced the arithmetic and control logic with electrical relay circuits.[59]
In the same year, electro-mechanical devices called bombes were built by British cryptologists to help decipher German Enigma-machine-encrypted secret messages during World War II. The bombe's initial design was created in 1939 at the UK Government Code and Cypher School (GC&CS) at Bletchley Park by Alan Turing,[60] with an important refinement devised in 1940 by Gordon Welchman.[61] The engineering design and construction was the work of Harold Keen of the British Tabulating Machine Company. It was a substantial development from a device that had been designed in 1938 by Polish Cipher Bureau cryptologist Marian Rejewski, and known as the "cryptologic bomb" (Polish: "bomba kryptologiczna").
Zuse suffered setbacks during World War II when some of his machines were destroyed in the course of Allied bombing campaigns. Apparently his work remained largely unknown to engineers in the UK and US until much later, although at least IBM was aware of it as it financed his post-war startup company in 1946 in return for an option on Zuse's patents.
In 1944, the Harvard Mark I was constructed at IBM's Endicott laboratories.[67] It was a similar general purpose electro-mechanical computer to the Z3, but was not quite Turing-complete.
Engineer Tommy Flowers joined the telecommunications branch of the General Post Office in 1926. While working at the research station in Dollis Hill in the 1930s, he began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[50]
Computers whose logic was primarily built using vacuum tubes are now known as first generation computers.
The Germans also developed a series of teleprinter encryption systems, quite different from Enigma. The Lorenz SZ 40/42 machine was used for high-level Army communications, code-named "Tunny" by the British. The first intercepts of Lorenz messages began in 1941. As part of an attack on Tunny, Max Newman and his colleagues developed the Heath Robinson, a fixed-function machine to aid in code breaking.[83] Tommy Flowers, a senior engineer at the Post Office Research Station[84] was recommended to Max Newman by Alan Turing[85] and spent eleven months from early February 1943 designing and building the more flexible Colossus computer (which superseded the Heath Robinson).[86][87] After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944[88] and attacked its first message on 5 February.[89]
Most of the use of Colossus was in determining the start positions of the Tunny rotors for a message, which was called "wheel setting". Colossus included the first-ever use of shift registers and systolic arrays, enabling five simultaneous tests, each involving up to 100 Boolean calculations. This enabled five different possible start positions to be examined for one transit of the paper tape.[92] As well as wheel setting some later Colossi included mechanisms intended to help determine pin patterns known as "wheel breaking". Both models were programmable using switches and plug panels in a way their predecessors had not been. Ten Mk 2 Colossi were operational by the end of the war.
Without the use of these machines, the Allies would have been deprived of the very valuable intelligence that was obtained from reading the vast quantity of enciphered high-level telegraphic messages between the German High Command (OKW) and their army commands throughout occupied Europe. Details of their existence, design, and use were kept secret well into the 1970s. Winston Churchill personally issued an order for their destruction into pieces no larger than a man's hand, to keep secret that the British were capable of cracking Lorenz SZ cyphers (from German rotor stream cipher machines) during the oncoming Cold War. Two of the machines were transferred to the newly formed GCHQ and the others were destroyed. As a result, the machines were not included in many histories of computing.[f] A reconstructed working copy of one of the Colossus machines is now on display at Bletchley Park.
The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing-complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were women who had been trained as mathematicians.[94]
It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High-speed memory was limited to 20 words (equivalent to about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.[95] One of its major engineering feats was to minimize the effects of tube burnout, which was a common problem in machine reliability at that time. The machine was in almost constant use for the next ten years.
The theoretical basis for the stored-program computer had been proposed by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began his work on developing an electronic stored-program digital computer. His 1945 report 'Proposed Electronic Calculator' was the first specification for such a device.
Meanwhile, John von Neumann at the Moore School of Electrical Engineering, University of Pennsylvania, circulated his First Draft of a Report on the EDVAC in 1945. Although substantially similar to Turing's design and containing comparatively little engineering detail, the computer architecture it outlined became known as the "von Neumann architecture". Turing presented a more detailed paper to the National Physical Laboratory (NPL) Executive Committee in 1946, giving the first reasonably complete design of a stored-program computer, a device he called the Automatic Computing Engine (ACE). However, the better-known EDVAC design of John von Neumann, who knew of Turing's theoretical work, received more publicity, despite its incomplete nature and questionable lack of attribution of the sources of some of the ideas.[50]
Turing thought that the speed and the size of computer memory were crucial elements, so he proposed a high-speed memory of what would today be called 25 KB, accessed at a speed of 1 MHz. The ACE implemented subroutine calls, whereas the EDVAC did not, and the ACE also used Abbreviated Computer Instructions, an early form of programming language.
The machine was not intended to be a practical computer but was instead designed as a testbed for the Williams tube, the first random-access digital storage device.[98] Invented by Freddie Williams and Tom Kilburn[99][100] at the University of Manchester in 1946 and 1947, it was a cathode-ray tube that used an effect called secondary emission to temporarily store electronic binary data, and was used successfully in several early computers.
Described as small and primitive in a 1998 retrospective, the Baby was the first working machine to contain all of the elements essential to a modern electronic computer.[101] As soon as it had demonstrated the feasibility of its design, a project was initiated at the university to develop the design into a more usable computer, the Manchester Mark 1. The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.[102]
The SSEM led to the development of the Manchester Mark 1 at the University of Manchester.[103] Work began in August 1948, and the first version was operational by April 1949; a program written to search for Mersenne primes ran error-free for nine hours on the night of 16/17 June 1949.
The machine's successful operation was widely reported in the British press, which used the phrase "electronic brain" in describing it to their readers.
The other contender for being the first recognizably modern digital stored-program computer[104] was the EDSAC,[105] designed and constructed by Maurice Wilkes and his team at the University of Cambridge Mathematical Laboratory in England at the University of Cambridge in 1949. The machine was inspired by John von Neumann's seminal First Draft of a Report on the EDVAC and was one of the first usefully operational electronic digital stored-program computer.[g]
The "brain" [computer] may one day come down to our level [of the common people] and help with our income-tax and book-keeping calculations. But this is speculation and there is no sign of it so far.
ENIAC inventors John Mauchly and J. Presper Eckert proposed the EDVAC's construction in August 1944, and design work for the EDVAC commenced at the University of Pennsylvania's Moore School of Electrical Engineering, before the ENIAC was fully operational.  The design implemented a number of important architectural and logical improvements conceived during the ENIAC's construction, and a high-speed serial-access memory.[111] However, Eckert and Mauchly left the project and its construction floundered.
It was finally delivered to the U.S. Army's Ballistics Research Laboratory at the Aberdeen Proving Ground in August 1949, but due to a number of problems, the computer only began operation in 1951, and then only on a limited basis.
The first commercial computer was the Ferranti Mark 1, built by Ferranti and delivered to the University of Manchester in February 1951. It was based on the Manchester Mark 1. The main improvements over the Manchester Mark 1 were in the size of the primary storage (using random access Williams tubes), secondary storage (using a magnetic drum), a faster multiplier, and additional instructions. The basic cycle time was 1.2 milliseconds, and a multiplication could be completed in about 2.16 milliseconds. The multiplier used almost a quarter of the machine's 4,050 vacuum tubes (valves).[112] A second machine was purchased by the University of Toronto, before the design was revised into the Mark 1 Star. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[113]
In 1951, British scientist Maurice Wilkes developed the concept of microprogramming from the realisation that the central processing unit of a computer could be controlled by a miniature, highly specialized computer program in high-speed ROM. Microprogramming allows the base instruction set to be defined or extended by built-in programs (now called firmware or microcode).[120] This concept greatly simplified CPU development. He first described this at the University of Manchester Computer Inaugural Conference in 1951, then published in expanded form in IEEE Spectrum in 1955.[citation needed]
It was widely used in the CPUs and floating-point units of mainframe and other computers; it was implemented for the first time in EDSAC 2,[121] which also used multiple identical "bit slices" to simplify design. Interchangeable, replaceable tube assemblies were used for each bit of the processor.[j]
Magnetic drum memories were developed for the US Navy during WW II with the work continuing at Engineering Research Associates (ERA) in 1946 and 1947. ERA, then a part of Univac included a drum memory in its 1103, announced in February 1953. The first mass-produced computer, the IBM 650, also announced in 1953 had about 8.5 kilobytes of drum memory.
Magnetic core memory patented in 1949[123] with its first usage demonstrated for the Whirlwind computer in August 1953.[124] Commercialization followed quickly. Magnetic core was used in peripherals of the IBM 702 delivered in July 1955, and later in the 702 itself. The IBM 704 (1955) and the Ferranti Mercury (1957) used magnetic-core memory. It went on to dominate the field into the 1970s, when it was replaced with semiconductor memory.  Magnetic core peaked in volume about 1975 and declined in usage and market share thereafter.[125]
As late as 1980, PDP-11/45 machines using magnetic-core main memory and drums for swapping were still in use at many of the original UNIX sites.
The bipolar transistor was invented in 1947. From 1955 onward transistors replaced vacuum tubes in computer designs,[127] giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer service life.  Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. Transistors greatly reduced computers' size, initial cost, and operating cost. Typically, second-generation computers were composed of large numbers of printed circuit boards such as the IBM Standard Modular System,[128] each carrying one to four logic gates or flip-flops.
The Manchester University Transistor Computer's design was adopted by the local engineering firm of Metropolitan-Vickers in their Metrovick 950, the first commercial transistor computer anywhere.[137] Six Metrovick 950s were built, the first completed in 1956. They were successfully deployed within various departments of the company and were in use for about five years.[131] A second generation computer, the IBM 1401, captured about one third of the world market. IBM installed more than ten thousand 1401s between 1960 and 1964.
Transistorized electronics improved not only the CPU (Central Processing Unit), but also the peripheral devices. The second generation disk data storage units were able to store tens of millions of letters and digits.  Next to the fixed disk storage units, connected to the CPU via high-speed data transmission, were removable disk data storage units. A removable disk pack can be easily exchanged with another pack in a few seconds. Even if the removable disks' capacity is smaller than fixed disks, their interchangeability guarantees a nearly unlimited quantity of data close at hand. Magnetic tape provided archival capability for this data, at a lower cost than disk.
Many second-generation CPUs delegated peripheral device communications to a secondary processor. For example, while the communication processor controlled card reading and punching, the main CPU executed calculations and binary branch instructions. One databus would bear data between the main CPU and core memory at the CPU's fetch-execute cycle rate, and other databusses would typically serve the peripheral devices. On the PDP-1, the core memory's cycle time was 5 microseconds; consequently most arithmetic instructions took 10 microseconds (100,000 operations per second) because most operations took at least two memory cycles; one for the instruction, one for the operand data fetch.
The "third-generation" of digital electronic computers used integrated circuit (IC) chips as the basis of their logic.
The idea of an integrated circuit was conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer.
The first working integrated circuits were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.[146] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.[147] Kilby's invention was a hybrid integrated circuit (hybrid IC).[148] It had external wire connections, which made it difficult to mass-produce.[149]
Noyce came up with his own idea of an integrated circuit half a year after Kilby.[150] Noyce's invention was a monolithic integrated circuit (IC) chip.[151][149] His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. The basis for Noyce's monolithic IC was Fairchild's planar process, which allowed integrated circuits to be laid out using the same principles as those of printed circuits. The planar process was developed by Noyce's colleague Jean Hoerni in early 1959, based on Mohamed M. Atalla's work on semiconductor surface passivation by silicon dioxide at Bell Labs in the late 1950s.[152][153][154]
The "fourth-generation" of digital electronic computers used microprocessors as the basis of their logic. The microprocessor has origins in the MOS integrated circuit (MOS IC) chip.[162] Due to rapid MOSFET scaling, MOS IC chips rapidly increased in complexity at a rate predicted by Moore's law, leading to large-scale integration (LSI) with hundreds of transistors on a single MOS chip by the late 1960s. The application of MOS LSI chips to computing was the basis for the first microprocessors, as engineers began recognizing that a complete computer processor could be contained on a single MOS LSI chip.[162]
The subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor". The earliest multi-chip microprocessors were the Four-Phase Systems AL-1 in 1969 and Garrett AiResearch MP944 in 1970, developed with multiple MOS LSI chips.[162] The first single-chip microprocessor was the Intel 4004,[163] developed on a single PMOS LSI chip.[162] It was designed and realized by Ted Hoff, Federico Faggin, Masatoshi Shima and Stanley Mazor at Intel, and released in 1971.[m] Tadashi Sasaki and Masatoshi Shima at Busicom, a calculator manufacturer, had the initial insight that the CPU could be a single MOS LSI chip, supplied by Intel.[165][163]
During the 1960s there was considerable overlap between second and third generation technologies.[n] IBM implemented its IBM Solid Logic Technology modules in hybrid circuits for the IBM System/360 in 1964. As late as 1975, Sperry Univac continued the manufacture of second-generation machines such as the UNIVAC 494. The Burroughs large systems such as the B5000 were stack machines, which allowed for simpler programming. These pushdown automatons were also implemented in minicomputers and microprocessors later, which influenced programming language design. Minicomputers served as low-cost computer centers for industry, business and universities.[166] It became possible to simulate analog circuits with the simulation program with integrated circuit emphasis, or SPICE (1971) on minicomputers, one of the programs for electronic design automation (EDA). The microprocessor led to the development of microcomputers, small, low-cost computers that could be owned by individuals and small businesses. Microcomputers, the first of which appeared in the 1970s, became ubiquitous in the 1980s and beyond.
From 1975 to 1977, most microcomputers, such as the MOS Technology KIM-1, the Altair 8800, and some versions of the Apple I, were sold as kits for do-it-yourselfers. Pre-assembled systems did not gain much ground until 1977, with the introduction of the Apple II, the Tandy TRS-80, the first SWTPC computers, and the Commodore PET. Computing has evolved with microcomputer architectures, with features added from their larger brethren, now dominant in most market segments.
A NeXT Computer and its object-oriented development tools and libraries were used by Tim Berners-Lee and Robert Cailliau at CERN to develop the world's first web server software, CERN httpd, and also used to write the first web browser, WorldWideWeb.
In the 21st century, multi-core CPUs became commercially available.[171] Content-addressable memory (CAM)[172] has become inexpensive enough to be used in networking, and is frequently used for on-chip cache memory in modern microprocessors, although no computer system has yet implemented hardware CAMs for use in programming languages. Currently, CAMs (or associative arrays) in software are programming-language-specific. Semiconductor memory cell arrays are very regular structures, and manufacturers prove their processes on them; this allows price reductions on memory products. During the 1980s, CMOS logic gates developed into devices that could be made as fast as other circuit types; computer power consumption could therefore be decreased dramatically. Unlike the continuous current draw of a gate based on other logic types, a CMOS gate only draws significant current during the 'transition' between logic states, except for leakage.
CMOS circuits have allowed computing to become a commodity which is now ubiquitous, embedded in many forms, from greeting cards and telephones to satellites. The thermal design power which is dissipated during operation has become as essential as computing speed of operation. In 2006 servers consumed 1.5% of the total energy budget of the U.S.[173] The energy consumption of computer data centers was expected to double to 3% of world consumption by 2011. The SoC (system on a chip) has compressed even more of the integrated circuitry into a single chip; SoCs are enabling phones and PCs to converge into single hand-held wireless mobile devices.[174]
Quantum computing is an emerging technology in the field of computing. MIT Technology Review reported 10 November 2017 that IBM has created a 50-qubit computer; currently its quantum state lasts 50 microseconds.[175] Google researchers have been able to extend the  50 microsecond time limit, as reported 14 July 2021 in Nature;[176] stability has been extended 100-fold by spreading a single logical qubit over chains of data qubits for quantum error correction.[176] Physical Review X reported a technique for 'single-gate sensing as a viable readout method for spin qubits' (a singlet-triplet spin state in silicon) on 26 November 2018.[177] A Google team has succeeded in operating their RF pulse modulator chip at 3 Kelvin, simplifying the cryogenics of their 72-qubit computer, which is set up to operate at 0.3 Kelvin; but the readout circuitry and another driver remain to be brought into the cryogenics.[178][o] See: Quantum supremacy[180][181] Silicon qubit systems have demonstrated entanglement at non-local distances.[182]
Computing hardware and its software have even become a metaphor for the operation of the universe.[183]
An indication of the rapidity of development of this field can be inferred from the history of the seminal 1947 article by Burks, Goldstine and von Neumann.[184] By the time that anyone had time to write anything down, it was obsolete. After 1945, others read John von Neumann's  First Draft of a Report on the EDVAC, and immediately started implementing their own systems. To this day, the rapid pace of development has continued, worldwide.[185][p]
For the purposes of this article, the term "second generation" refers to computers using discrete transistors, even when the vendors referred to them as "third-generation". By 1960 transistorized computers were replacing vacuum tube computers, offering lower cost, higher speeds, and reduced power consumption. The marketplace was dominated by IBM and the seven dwarfs:
Some examples of 1960s second generation computers from those vendors are:
However, some smaller companies made significant contributions. Also, towards the end of the second generation Digital Equipment Corporation (DEC) was a serious contender in the small and medium machine marketplace.
Meanwhile, second-generation computers were also being developed in the USSR as, e.g., the Razdan family of general-purpose digital computers created at the Yerevan Computer Research and Development Institute.
The second-generation computer architectures initially varied; they included character-based decimal computers, sign-magnitude decimal computers with a 10-digit word, sign-magnitude binary computers, and ones' complement binary computers, although Philco, RCA, and Honeywell, for example, had some computers that were character-based binary computers and Digital Equipment Corporation (DEC) and Philco, for example, had two's complement computers. With the advent of the IBM System/360, two's complement became the norm for new product lines.
The most common word sizes for binary mainframes were 36 and 48 bits, although entry-level and midrange machines used smaller words, e.g., 12 bits, 18 bits, 24 bits, 30 bits. All but the smallest machines had asynchronous I/O channels and interrupts. Typically binary computers with word size up to 36 bits had one instruction per word, binary computers with 48 bits per word had two instructions per word and the CDC 60-bit machines could have two, three, or four instructions per word, depending on the instruction mix; the Burroughs B5000, B6500/B7500 and B8500 lines are notable exceptions to this.
First-generation computers with data channels (I/O channels) had a basic DMA interface to the channel cable. The second generation saw both simpler, e.g., channels on the CDC 6000 series had no DMA, and more sophisticated designs, e.g., the 7909 on the IBM 7090 had limited computational, conditional branching and interrupt system.
By 1960 magnetic core was the dominant memory technology, although there were still some new machines using drums and delay lines during the 1960s.
Magnetic thin film and rod memory were used on some second-generation machines, but advances in core technology meant they remained niche players until semiconductor memory displaced both core and thin film.
In the first generation, word-oriented computers typically had a single accumulator and an extension, referred to as, e.g., Upper and Lower Accumulator, Accumulator and Multiplier-Quotient (MQ) register. In the second generation, it became common for computers to have multiple addressable accumulators. On some computers, e.g., PDP-6, the same registers served as accumulators and index registers, making them an early example of general-purpose registers.
In the second generation there was considerable development of new address modes, including truncated addressing on, e.g., the Philco TRANSAC S-2000, the UNIVAC III, and automatic index register incrementing on, e.g., the RCA 601, UNIVAC 1107, GE 635. Although index registers were introduced in the first generation under the name B-line, their use became much more common in the second generation. Similarly, indirect addressing became more common in the second generation, either in conjunction with index registers or instead of them. While first-generation computers typically had a small number of index registers or none, several lines of second-generation computers had large numbers of index registers, e.g., Atlas, Bendix G-20, IBM 7070.
The first generation had pioneered the use of special facilities for calling subroutines, e.g., TSX on the IBM 709. In the second generation, such facilities were ubiquitous. In the descriptions below, NSI is the next sequential instruction, the return address. Some examples are:
The second generation saw the introduction of features intended to support multiprogramming and multiprocessor configurations, including master/slave (supervisor/problem) mode, storage protection keys, limit registers, protection associated with address translation, and atomic instructions.
The mass increase in the use of computers accelerated with 'Third Generation' computers starting around 1966 in the commercial market. These generally relied on early (sub-1000 transistor) integrated circuit technology.  The third generation ends with the microprocessor-based fourth generation.
In 1958, Jack Kilby at Texas Instruments invented the hybrid integrated circuit (hybrid IC),[1] which had external wire connections, making it difficult to mass-produce.[2] In 1959, Robert Noyce at Fairchild Semiconductor invented the monolithic integrated circuit (IC) chip.[3][2] It was made of silicon, whereas Kilby's chip was made of germanium. The basis for Noyce's monolithic IC was Fairchild's planar process, which allowed integrated circuits to be laid out using the same principles as those of printed circuits. The planar process was developed by Noyce's colleague Jean Hoerni in early 1959, based on the silicon surface passivation and thermal oxidation processes developed by Mohamed M. Atalla at Bell Labs in the late 1950s.[4][5][6]
Computers using IC chips began to appear in the early 1960s. For example, the 1961 Semiconductor Network Computer (Molecular Electronic Computer, Mol-E-Com),[7][8][9] the first monolithic integrated circuit[10][11][12] general purpose computer (built for demonstration purposes, programmed to simulate a desk calculator) was built by Texas Instruments for the US Air Force.[13][14][15]
Some of their early uses were in embedded systems, notably used by NASA for the Apollo Guidance Computer, by the military in the LGM-30 Minuteman intercontinental ballistic missile, the Honeywell ALERT airborne computer,[16][17] and in the Central Air Data Computer used for flight control in the US Navy's F-14A Tomcat fighter jet.
An early commercial use was the 1965 SDS 92.[18][19]  IBM first used ICs in computers for the logic of the System/360 Model 85 shipped in 1969 and then made extensive use of ICs in its System/370 which began shipment in 1971.  
The integrated circuit enabled the development of much smaller computers. The minicomputer was a significant innovation in the 1960s and 1970s. It brought computing power to more people, not only through more convenient physical size but also through broadening the computer vendor field. Digital Equipment Corporation became the number two computer company behind IBM with their popular PDP and VAX computer systems. Smaller, affordable hardware also brought about the development of important new operating systems such as Unix.
In 1969, Data General introduced the Nova and shipped a total of 50,000 at $8,000 each. The popularity of 16-bit computers, such as the Hewlett-Packard 21xx series and the Data General Nova, led the way toward word lengths that were multiples of the 8-bit byte. The Nova was first to employ medium-scale integration (MSI) circuits from Fairchild Semiconductor, with subsequent models using large-scale integrated (LSI) circuits. Also notable was that the entire central processor was contained on one 15-inch printed circuit board.
Large mainframe computers used ICs to increase storage and processing abilities.  The 1965 IBM System/360 mainframe computer family are sometimes called third-generation computers; however, their logic consisted primarily of SLT hybrid circuits, which contained discrete transistors and diodes interconnected on a substrate with printed wires and printed passive components; the S/360 M85 and M91 did use ICs for some of their circuits.  IBM's 1971 System/370 used ICs for their logic.
By 1971, the Illiac IV supercomputer was the fastest computer in the world, using about a quarter-million small-scale ECL logic gate integrated circuits to make up sixty-four parallel data processors.[25]
Third-generation computers were offered well into the 1990s; for example the IBM ES9000 9X2 announced April 1994[26] used 5,960 ECL chips to make a 10-way processor.[27]  Other third-generation computers offered in the 1990s included the DEC VAX 9000 (1989), built from ECL gate arrays and custom chips,[28] and the Cray T90 (1995).
Third-generation minicomputers were essentially scaled-down versions of mainframe computers, whereas the fourth generation's origins are fundamentally different.[clarification needed] The basis of the fourth generation is the microprocessor, a computer processor contained on a single large-scale integration (LSI) MOS integrated circuit chip.[29]
Microprocessor-based computers were originally very limited in their computational ability and speed and were in no way an attempt to downsize the minicomputer.  They were addressing an entirely different market.
Processing power and storage capacities have grown beyond all recognition since the 1970s, but the underlying technology has remained basically the same of large-scale integration (LSI) or very-large-scale integration (VLSI) microchips, so it is widely regarded that most of today's computers still belong to the fourth generation.
The microprocessor has origins in the MOS integrated circuit (MOS IC) chip.[29] The MOS IC was fabricated by Fred Heiman and Steven Hofstein at RCA in 1962.[37] Due to rapid MOSFET scaling, MOS IC chips rapidly increased in complexity at a rate predicted by Moore's law, leading to large-scale integration (LSI) with hundreds of transistors on a single MOS chip by the late 1960s. The application of MOS LSI chips to computing was the basis for the first microprocessors, as engineers began recognizing that a complete computer processor could be contained on a single MOS LSI chip.[29]
The earliest multi-chip microprocessors were the Four-Phase Systems AL1 in 1969 and Garrett AiResearch MP944 in 1970, each using several MOS LSI chips.[29] On November 15, 1971, Intel released the world's first single-chip microprocessor, the 4004, on a single MOS LSI chip. Its development was led by Federico Faggin, using silicon-gate MOS technology, along with Ted Hoff, Stanley Mazor and Masatoshi Shima.[38] It was developed for a Japanese calculator company called Busicom as an alternative to hardwired circuitry, but computers were developed around it, with much of their processing abilities provided by one small microprocessor chip. The dynamic RAM (DRAM) chip was based on the MOS DRAM memory cell developed by Robert Dennard of IBM, offering kilobits of memory on one chip. Intel coupled the RAM chip with the microprocessor, allowing fourth generation computers to be smaller and faster than prior computers. The 4004 was only capable of 60,000 instructions per second, but its successors brought ever-growing speed and power to computers, including the Intel 8008, 8080 (used in many computers using the CP/M operating system), and the 8086/8088 family. (The IBM personal computer (PC) and compatibles use processors that are still backward-compatible with the 8086.) Other producers also made microprocessors which were widely used in microcomputers.
The following table shows a timeline of significant microprocessor development.
The powerful supercomputers of the era were at the other end of the computing spectrum from the microcomputers, and they also used integrated circuit technology. In 1976, the Cray-1 was developed by Seymour Cray, who had left Control Data in 1972 to form his own company. This machine was the first supercomputer to make vector processing practical. It had a characteristic horseshoe shape to speed processing by shortening circuit paths. Vector processing uses one instruction to perform the same operation on many arguments; it has been a fundamental supercomputer processing method ever since.  The Cray-1 could calculate 150 million floating-point operations per second (150 megaflops). 85 were shipped at a price of $5 million each. The Cray-1 had a CPU that was mostly constructed of SSI and MSI ECL ICs.
A more interactive form of computer use developed commercially by the middle 1960s. In a time-sharing system, multiple teleprinter and display terminals let many people share the use of one mainframe computer processor, with the operating system assigning time slices to each user's jobs. This was common in business applications and in science and engineering.
A different model of computer use was foreshadowed by the way in which early, pre-commercial, experimental computers were used, where one user had exclusive use of a processor.[39] Some of the first computers that might be called "personal" were early minicomputers such as the LINC and PDP-8, and later on VAX and larger minicomputers from Digital Equipment Corporation (DEC), Data General, Prime Computer, and others. They originated as peripheral processors for mainframe computers, taking on some routine tasks and freeing the processor for computation.
By today's standards, they were physically large (about the size of a refrigerator) and costly (typically tens of thousands of US dollars), and thus were rarely purchased by individuals. However, they were much smaller, less expensive, and generally simpler to operate than the mainframe computers of the time, and thus affordable by individual laboratories and research projects. Minicomputers largely freed these organizations from the batch processing and bureaucracy of a commercial or university computing center.
In addition, minicomputers were more interactive than mainframes, and soon had their own operating systems. The minicomputer Xerox Alto (1973) was a landmark step in the development of personal computers, because of its graphical user interface, bit-mapped high-resolution screen, large internal and external memory storage, mouse, and special software.[40]
In the minicomputer ancestors of the modern personal computer, processing was carried out by circuits with large numbers of components arranged on multiple large printed circuit boards. Minicomputers were consequently physically large and expensive to produce compared with later microprocessor systems. After the "computer-on-a-chip" was commercialized, the cost to produce a computer system dropped dramatically. The arithmetic, logic, and control functions that previously occupied several costly circuit boards were now available in one integrated circuit which was very expensive to design but cheap to produce in large quantities. Concurrently, advances in developing solid state memory eliminated the bulky, costly, and power-hungry magnetic core memory used in prior generations of computers.
The development of the single-chip microprocessor was an enormous catalyst to the popularization of cheap, easy to use, and truly personal computers. The Altair 8800, introduced in a Popular Electronics magazine article in the January 1975 issue, at the time set a new low price point for a computer, bringing computer ownership to an admittedly select market in the 1970s. This was followed by the IMSAI 8080 computer, with similar abilities and limitations. The Altair and IMSAI were essentially scaled-down minicomputers and were incomplete: to connect a keyboard or teleprinter to them required heavy, expensive "peripherals". These machines both featured a front panel with switches and lights, which communicated with the operator in binary. To program the machine after switching it on the bootstrap loader program had to be entered, without error, in binary, then a paper tape containing a BASIC interpreter loaded from a paper-tape reader. Keying the loader required setting a bank of eight switches up or down and pressing the "load" button, once for each byte of the program, which was typically hundreds of bytes long. The computer could run BASIC programs once the interpreter had been loaded.
The MITS Altair, the first commercially successful microprocessor kit, was featured on the cover of Popular Electronics magazine in January 1975. It was the world's first mass-produced personal computer kit, as well as the first computer to use an Intel 8080 processor. It was a commercial success with 10,000 Altairs being shipped. The Altair also inspired the software development efforts of Paul Allen and his high school friend Bill Gates who developed a BASIC interpreter for the Altair, and then formed Microsoft.
The MITS Altair 8800 effectively created a new industry of microcomputers and computer kits, with many others following, such as a wave of small business computers in the late 1970s based on the Intel 8080, Zilog Z80 and Intel 8085 microprocessor chips. Most ran the CP/M-80 operating system developed by Gary Kildall at Digital Research. CP/M-80 was the first popular microcomputer operating system to be used by many different hardware vendors, and many software packages were written for it, such as WordStar and dBase II.
Many hobbyists during the mid-1970s designed their own systems, with various degrees of success, and sometimes banded together to ease the job. Out of these house meetings, the Homebrew Computer Club developed, where hobbyists met to talk about what they had done, exchange schematics and software, and demonstrate their systems. Many people built or assembled their own computers as per published designs. For example, many thousands of people built the Galaksija home computer later in the early 1980s.
It was arguably the Altair computer that spawned the development of Apple, as well as Microsoft which produced and sold the Altair BASIC programming language interpreter, Microsoft's first product. The second generation of microcomputers, those that appeared in the late 1970s, sparked by the unexpected demand for the kit computers at the electronic hobbyist clubs, were usually known as home computers. For business use these systems were less capable and in some ways less versatile than the large business computers of the day. They were designed for fun and educational purposes, not so much for practical use. And although you could use some simple office/productivity applications on them, they were generally used by computer enthusiasts for learning to program and for running computer games, for which the personal computers of the period were less suitable and much too expensive. For the more technical hobbyists home computers were also used for electronically interfacing to external devices, such as controlling model railroads, and other general hobbyist pursuits.
The advent of the microprocessor and solid-state memory made home computing affordable. Early hobby microcomputer systems such as the Altair 8800 and Apple I introduced around 1975 marked the release of low-cost 8-bit processor chips, which had sufficient computing power to be of interest to hobby and experimental users. By 1977 pre-assembled systems such as the Apple II, Commodore PET, and TRS-80 (later dubbed the "1977 Trinity" by Byte Magazine)[47] began the era of mass-market home computers; much less effort was required to obtain an operating computer, and applications such as games, word processing, and spreadsheets began to proliferate. Distinct from computers used in homes, small business systems were typically based on CP/M, until IBM introduced the IBM PC, which was quickly adopted. The PC was heavily cloned, leading to mass production and consequent cost reduction throughout the 1980s. This expanded the PC's presence in homes, replacing the home computer category during the 1990s and leading to the current monoculture of architecturally identical personal computers.
Software is a set of computer programs and associated documentation and data.[1] This is in contrast to hardware, from which the system is built and which actually performs the work.
The majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages.[2] High-level languages are translated into machine language using a compiler or an interpreter or a combination of the two. Software may also be written in a low-level assembly language, which has a strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.
An algorithm for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine.[3] She created proofs to show how the engine would calculate Bernoulli numbers.[3] Because of the proofs and the algorithm, she is considered the first computer programmer.[4][5]
The first theory about software, prior to the creation of computers as we know them today, was proposed by Alan Turing in his 1936 essay, On Computable Numbers, with an Application to the Entscheidungsproblem (decision problem).[6] This eventually led to the creation of the academic fields of computer science and software engineering; both fields study software and its creation.[citation needed] Computer science is the theoretical study of computer and software (Turing's essay is an example of computer science), whereas software engineering is the application of engineering principles to development of software.[7]
In 2000, Fred Shapiro, a librarian at the Yale Law School, published a letter revealing that John Wilder Tukey's 1958 paper "The Teaching of Concrete Mathematics"[8][9] contained the earliest known usage of the term "software" found in a search of JSTOR's electronic archives, predating the Oxford English Dictionary's citation by two years.[10] This led many to credit Tukey with coining the term, particularly in obituaries published that same year,[11] although Tukey never claimed credit for any such coinage. In 1995, Paul Niquette claimed he had originally coined the term in October 1953, although he could not find any documents supporting his claim.[12] The earliest known publication of the term "software" in an engineering context was in August 1953 by Richard R. Carhart, in a Rand Corporation Research Memorandum.[13]
On virtually all computer platforms, software can be grouped into a few broad categories.
Based on the goal, computer software can be divided into:
Software is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools.[citation needed] IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way.[citation needed] An IDE can make it easier to do specific tasks, such as searching in files in a particular project.[citation needed] Many programming language implementations provide the option of using both individual tools or an IDE.[citation needed]
People who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.[citation needed]
Data movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly; this is sometimes avoided by using "pointers" to data instead.[citation needed] Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together.[citation needed]
Software quality is very important, especially for commercial and system software. If software is faulty, it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called "bugs" which are often discovered during alpha and beta testing.[citation needed] Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.[citation needed]
The software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.[citation needed]
Open-source software comes with a free software license, granting the recipient the rights to modify and redistribute the software.[21]
Design and implementation of software vary depending on the complexity of the software. For instance, the design and creation of Microsoft Word took much more time than designing and developing Microsoft Notepad because the former has much more basic functionality.[citation needed]
Software is usually developed in integrated development environments (IDE) like Eclipse, IntelliJ and Microsoft Visual Studio that can simplify the process and compile the software.[citation needed] As noted in a different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing.[citation needed] Libraries (APIs) can be categorized by their purpose. For instance, the Spring Framework is used for implementing enterprise applications, the Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services.[citation needed] When a program is designed, it relies upon the API. For instance, a Microsoft Windows desktop application might call API functions in the .NET Windows Forms library like Form1.Close() and Form1.Show()[23] to close or open the application. Without these APIs, the programmer needs to write these functionalities entirely themselves. Companies like Oracle and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.[citation needed]
Data structures such as hash tables, arrays, and binary trees, and algorithms such as quicksort, can be useful for creating software.
Computer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.[specify][24][25]
Software is a set of programmed instructions stored in the memory of stored-program digital computers for execution by the processor.  Software is a recent development in human history, and it is fundamental to the Information Age.
Ada Lovelace's programs for Charles Babbage's Analytical Engine in the 19th century is often considered the founder of the discipline, though the mathematician's efforts remained theoretical only, as the technology of Lovelace and Babbage's day proved insufficient to build his computer. Alan Turing is credited with being the first person to come up with a theory for software in 1935, which led to the two academic fields of computer science and software engineering.
The first generation of software for early stored-program digital computers in the late 1940s had its instructions written directly in binary code, generally written for mainframe computers.  Later, the development of modern programming languages alongside the advancement of the home computer would greatly widen  the scope and breadth of available software, beginning with assembly language, and continuing on through functional programming and object-oriented programming paradigms.
Computing as a concept goes back to ancient times, with devices such as the abacus, the Antikythera mechanism, Astrolabes, Mechanical Astronomical clocks and Mechanical Calculators.[1] The Antikythera mechanism is an example for a highly complex ancient mechanical Astronomical device.[2]
However, these devices were pure hardware and had no software - their computing powers were directly tied to their specific form and engineering.
Software requires the concept of a general-purpose processor - what is now described as a Turing machine - as well as computer memory in which reusable sets of routines and mathematical functions comprising programs can be stored, started, and stopped individually, and only appears recently in human history.
The first known computer algorithm was written by Ada Lovelace in the 19th century for the Analytical Engine, to translate Luigi Menabrea's work on Bernoulli numbers for machine instruction.[3][3] However, this remained theoretical only - the lesser state of engineering in the lifetime of these two mathematicians proved insufficient[citation needed] to construct the Analytical Engine.
The first modern theory of software was proposed by Alan Turing in his 1935 essay Computable numbers with an application to the Entscheidungsproblem (decision problem).[4]
This eventually led to the creation of the twin academic fields of computer science and software engineering, which both study software and its creation. Computer science is more theoretical (Turing's essay is an example of computer science), whereas software engineering is focused on more practical concerns.
Grace Hopper worked as one of the first programmers of the Harvard Mark I.[12] She later created a 500-page manual for the computer.[13] Hopper is often falsely credited with coining the terms "bug" and "debugging," when she found a moth in the Mark II, causing a malfunction;[14] however, the term was in fact already in use when she found the moth.[14] Hopper developed the first compiler and brought her idea from working on the Mark computers to working on UNIVAC in the 1950s.[15] Hopper also developed the programming language FLOW-MATIC to program the UNIVAC.[14] Frances E. Holberton, also working at UNIVAC, developed a code[clarification needed], C-10, which let programmers use keyboard inputs and created the Sort-Merge Generator in 1951.[16][17] Adele Mildred Koss and Hopper also created the precursor to a report generator.[16]
The very first time a stored-program computer held a piece of software in electronic memory and executed it successfully, was 11 am 21 June 1948, at the University of Manchester, on the Manchester Baby computer.  It was written by Tom Kilburn, and calculated the highest factor of the integer 2^18 = 262,144. Starting with a large trial divisor, it performed division of 262,144 by repeated subtraction then checked if the remainder was zero. If not, it decremented the trial divisor by one and repeated the process.  Google released a tribute to the Manchester Baby, celebrating it as the "birth of software".
FORTRAN was developed by a team led by John Backus at IBM in the 1950s.  The first compiler was released in 1957.  The language proved so popular for scientific and technical computing that by 1963 all major manufacturers had implemented or announced FORTRAN for their computers.[18][19]
COBOL was first conceived of when Mary K. Hawes convened a meeting (which included Grace Hopper) in 1959 to discuss how to create a computer language to be shared between businesses.[16] Hopper's innovation with COBOL was developing a new symbolic way to write programming.[13] Her programming was self-documenting.[20] Betty Holberton helped edit the language which was submitted to the Government Printing Office in 1960.[21] FORMAC was developed by Jean E. Sammet in the 1960s.[21] Her book, Programming Languages: History and Fundamentals (1969), became an influential text.[21][22]
The Apollo Mission to the moon depended on software to program the computers in the landing modules.[23][24]  The computers were programmed with a language called "Basic" (no relation with the BASIC  programming language developed at Dartmouth at about the same time).[25] The software also had an interpreter which was made up of a series of routines and an executive (like a modern-day operating system), which specified which programs to run and when.[25] Both were designed by Hal Laning.[25] Margaret Hamilton, who had previously been involved with software reliability issues when working on the US SAGE air defense system, was also part of the Apollo software team.[23][26] Hamilton was in charge of the onboard flight software for the Apollo computers.[23] Hamilton felt that software operations were not just part of the machine, but also intricately involved with the people who operated the software.[25] Hamilton also coined the term "software engineering" while she was working at NASA.[27]
The actual "software" for the computers in the Apollo missions was made up of wires that were threaded through magnetic cores.[28] Where the wire went through a magnetic core, that represented a "1" and where the wire went around the core, that represented a "0."[28] Each core stored 64 bits of information.[28] Hamilton and others would create the software by punching holes in punch cards, which were then later processed on a Honeywell mainframe where the software could be simulated.[23] When the code was "solid," then it was sent to be woven into the magnetic cores at Raytheon, where women known as "Little Old Ladies" worked on the wires.[23] The program itself was "indestructible" and could even withstand lightning strikes, which happened to Apollo 12.[28] Wiring the computers took several weeks to do, freezing software development during that time.[29]
While using the simulators to test the programming, Hamilton discovered ways that code could produce dangerous errors when human mistakes were made while using it.[23] NASA believed that the astronauts would not make mistakes due to their training.[30] Hamilton was not allowed to program code to prevent errors that would lead to system crash, so she annotated the code in the program documentation.[23] Her ideas to add error-checking code was rejected as "excessive."[23] However, exactly what Hamilton predicted would happen occurred on the Apollo 8 flight, when human error caused the computer to wipe out all of the navigational data.[23]
Later, software was sold to multiple customers by being bundled with the hardware by original equipment manufacturers (OEMs) such as Data General, Digital Equipment and IBM. When a customer bought a minicomputer, at that time the smallest computer on the market, the computer did not come with pre-installed software, but needed to be installed by engineers employed by the OEM.[citation needed]
This bundling attracted the attention of US antitrust regulators, who sued IBM for improper "tying" in 1969, alleging that it was an antitrust violation that customers who wanted to obtain its software had to also buy or lease its hardware in order to do so. However, the case was dropped by the US Justice Department, after many years of attrition, as it concluded it was "without merit".[31]
In 2008, Psystar Corporation was sued by Apple Inc. for distributing unauthorized Macintosh clones with OS X preinstalled, and countersued. One of the arguments in the countersuit - citing the Data General case - was that Apple dominates the market for OS X compatible computers by illegally tying the operating system to Apple computers. District Court Judge William Alsup rejected this argument, saying, as the District Court had ruled in the Data General case over 20 years prior, that the relevant market was not simply one operating system (Mac OS) but all PC operating systems, including Mac OS, and noting that Mac OS did not enjoy a dominant position in that broader market. Alsup's judgement also noted that the surprising Data General precedent that tying of copyrighted products was always illegal had since been "implicitly overruled" by the verdict in the Illinois Tool Works Inc. v. Independent Ink, Inc. case.[34]
An industry producing independently packaged software - software that was neither produced as a "one-off" for an individual customer, nor "bundled" with computer hardware - started to develop in the late 1960s.[35]
Unix was an early operating system which became popular and very influential, and still exists today. The most popular variant of Unix today is macOS (previously called OS X and Mac OS X), while Linux is closely related to Unix.
In January 1975, Micro Instrumentation and Telemetry Systems began selling its Altair 8800 microcomputer kit by mail order.  Microsoft released its first product Altair BASIC later that year, and hobbyists began developing programs to run on these kits.  Tiny BASIC was published as a type-in program in Dr. Dobb's Journal, and developed collaboratively.
In 1976, Peter R. Jennings for instance created his Microchess program for MOS Technology's KIM-1 kit, but since it did not come with a tape drive, he would send the source code in a little booklet to his mail-order customers, and they would have to type the whole program in by hand.  In 1978, Kathe and Dan Spracklen released the source of their Sargon (chess) program in a computer magazine.  Jennings later switched to selling paper tape, and eventually compact cassettes with the program on it.
Even with the spread of cartridges and cassette tapes in the 1980s for distribution of commercial software, free programs (such as simple educational programs for the purpose of teaching programming techniques) were still often printed, because it was cheaper than making and attaching cassette tapes to magazines.
However, eventually a combination of four factors brought this practice of printing complete source code listings of entire programs in computer magazines to an end:
Very quickly, commercial software started to be pirated, and commercial software producers were very unhappy at this. Bill Gates, cofounder of Microsoft, was an early moraliser against software piracy with his famous Open Letter to Hobbyists in 1976.[36]
Applications for mobile devices (cellphones and tablets) have been termed "apps" in recent years. Apple chose to funnel iPhone and iPad app sales through their App Store, and thus both vet apps, and get a cut of every paid app sold. Apple does not allow apps which could be used to circumvent their app store (e.g. virtual machines such as the Java or Flash virtual machines).
The Android platform, by contrast, has multiple app stores available for it, and users can generally select which to use (although Google Play requires a compatible or rooted device).
This move was replicated for desktop operating systems with GNOME Software (for Linux), the Mac App Store (for macOS), and the Windows Store (for Windows). All of these platforms remain, as they have always been, non-exclusive: they allow applications to be installed from outside the app store, and indeed from other app stores.
The explosive rise in popularity of apps, for the iPhone in particular but also for Android, led to a kind of "gold rush", with some hopeful programmers dedicating a significant amount of time to creating apps in the hope of striking it rich. As in real gold rushes, not all of these hopeful entrepreneurs were successful.
The development of curricula in computer science has resulted in improvements in software development. Components of these curricula include:
As more and more programs enter the realm of firmware, and the hardware itself becomes smaller, cheaper and faster as predicted by Moore's law, an increasing number of types of functionality of computing first carried out by software, have joined the ranks of hardware, as for example with graphics processing units. (However, the change has sometimes gone the other way for cost or other reasons, as for example with softmodems and microcode.)
Most hardware companies today have more software programmers on the payroll than hardware designers[citation needed], since software tools have automated many tasks of printed circuit board (PCB) engineers.
The following tables include year by year development of many different aspects of computer software including:
What we wanted to preserve was not just a good environment in which to do programming, but a system around which a fellowship could form. We knew from experience that the essence of communal computing, as supplied by remote-access, time-shared machines, is not just to type programs into a terminal instead of a keypunch, but to encourage close communication.
In the late 1960s, Bell Labs was involved in a project with MIT and General Electric to develop a time-sharing system, called Multiplexed Information and Computing Service (Multics), allowing multiple users to access a mainframe simultaneously. Dissatisfied with the project's progress, Bell Labs management ultimately withdrew.
Ken Thompson, a programmer in the Labs' computing research department, had worked on Multics. He decided to write his own operating system. While he still had access to the Multics environment, he wrote simulations for the new file and paging system[citation needed] on it. He also programmed a game called Space Travel, but it needed a more efficient and less expensive machine to run on, and eventually he found a little-used Digital Equipment Corporation PDP-7 at Bell Labs.[4][5] On the PDP-7, in 1969, a team of Bell Labs researchers led by Thompson and Ritchie, including Rudd Canaday, implemented a hierarchical file system, the concepts of computer processes and device files, a command-line interpreter, and some small utility programs, modeled on the corresponding features in Multics, but simplified.[3] The resulting system, much smaller and simpler than Multics, was to become Unix. In about a month's time, in August 1969, Thompson had implemented a self-hosting operating system with an assembler, editor and shell, using a GECOS machine for bootstrapping.[6]
Douglas McIlroy then ported TMG compiler-compiler to PDP-7 assembly, creating the first high-level language running on Unix. Thompson used this tool to develop the first version of his B programming language.[3]
The new operating system was initially without organizational backing, and also without a name. At this stage, the new operating system was a singletasking operating system,[3] not a multitasking one such as Multics. The name Unics (Uniplexed Information and Computing Service, pronounced as "eunuchs"), a pun on Multics (Multiplexed Information and Computer Services), was initially suggested for the project in 1970. Brian Kernighan claims the coining for himself, and adds that "no one can remember" who came up with the final spelling Unix.[7] Dennis Ritchie and Doug McIlroy also credit Kernighan.[3][8]
When the Computing Sciences Research Center wanted to use Unix on a machine larger than the PDP-7, while another department needed a word processor, Thompson and Ritchie added text processing capabilities to Unix and received funding for a PDP-11.[5] For the first time in 1970, the Unix operating system was officially named and ran on the PDP-11. A text-formatting program called roff and a text editor were added. All three were written in PDP-11 assembly language. Bell Labs used this initial text-processing system, consisting of Unix, roff, and the editor, for text processing of patent applications. Roff soon evolved into troff, the first electronic publishing program with full typesetting capability.
As the system grew in complexity and the research team wanted more users, the need for a manual grew apparent. The UNIX Programmer's Manual was published on 3 November 1971; commands were documented in the "man page" format that is still used, offering terse reference information about usage as well as bugs in the software, and listing the authors of programs to channel questions to them.[8]
After other Bell Labs departments purchased DEC PDP-11s, they also chose to run Unix instead of DEC's own operating system. By Version 4 it was widely used within the laboratory and a Unix Support Group was formed, helping the operating system survive by formalizing its distribution.[5][8]
In 1973, Version 4 Unix was rewritten in the higher-level language C, contrary to the general notion at the time that an operating system's complexity and sophistication required it to be written in assembly language.[9][5] The C language appeared as part of Version 2. Thompson and Ritchie were so influential on early Unix that McIlroy estimated that they wrote and debugged about 100,000 lines of code that year, stating that "[their names] may safely be assumed to be attached to almost everything not otherwise attributed".[8] Although assembly did not disappear from the man pages until Version 8,[8] the migration to C suggested portability of the software, requiring only a relatively small amount of machine-dependent code to be replaced when porting Unix to other computing platforms. Version 4 Unix, however, still had considerable PDP-11-dependent code and was not suitable for porting. The first port to other platform was made five years later (1978) for Interdata 8/32.[10]
In 1973, AT&T released Version 5 Unix and licensed it to educational institutions, and licensed 1975's Version 6 to companies for the first time.[12]  While commercial users were rare because of the US$20,000 (equivalent to $100,717 in 2021) cost, the latter was the most widely used version into the early 1980s. Anyone could purchase a license, but the terms were very restrictive; licensees only received the source code, on an as-is basis.[12] The licenses also included the machine-dependent parts of the kernel, written in PDP-11 assembly language. Copies of the Lions' Commentary on UNIX 6th Edition, with Source Code circulated widely, which led to considerable use of Unix as an educational example. The first meeting of Unix users took place in New York in 1974, attracting a few dozen people; this would later grow into the USENIX organization. The importance of the user group stemmed from the fact that Unix was entirely unsupported by AT&T.[6]
Versions of the Unix system were determined by editions of its user manuals;[12] for example, "Fifth Edition UNIX" and "UNIX Version 5" have both been used to designate the same version. The Bell Labs developers did not think in terms of "releases" of the operating system, instead using a model of continuous development, and sometimes distributing tapes with patches (without AT&T lawyers' approval).[6] Development expanded, adding the concept of pipes, which led to the development of a more modular code base, and quicker development cycles. Version 5, and especially Version 6, led to a plethora of different Unix versions both inside and outside Bell Labs, including PWB/UNIX and the first commercial Unix, IS/1.
Unix still only ran on DEC systems.[12] As more of the operating system was rewritten in C (and the C language extended to accommodate this), portability also increased; in 1977, Bell Labs procured an Interdata 8/32 with the aim of porting Unix to a computer that was as different from the PDP-11 as possible, making the operating system more machine-independent in the process. Unix next ran as a guest operating system inside a VM/370 hypervisor at Princeton. Simultaneously, a group at the University of Wollongong ported Unix to the similar Interdata 7/32.[13] Target machines of further Bell Labs ports for research and AT&T-internal use included an Intel 8086-based computer (with custom-built MMU) and the UNIVAC 1100.[14][5]
In May 1975, ARPA documented the benefits of the Unix time-sharing system which "presents several interesting capabilities" as an ARPA network mini-host in RFC 681.
In 1978, UNIX/32V was released for DEC's then new VAX system. By this time, over 600 machines were running Unix in some form. Version 7 Unix, the last version of Research Unix to be released widely, was released in 1979. In Version 7, the number of system calls was only around 50, although later Unix and Unix-like systems would add many more:[15]
Version 7 of the Research UNIX System provided about 50 system calls, 4.4BSD provided about 110, and SVR4 had around 120. The exact number of system calls varies depending on the operating system version. More recent systems have seen incredible growth in the number of supported system calls. Linux 3.2.0 has 380 system calls and FreeBSD 8.0 has over 450.
A microprocessor port of Unix, to the LSI-11, was completed in 1978,[16] and an Intel 8086 version was reported to be "in progress" the same year.[13] The first microcomputer versions of Unix, and Unix-like operating systems like Whitesmiths' Idris, appeared in the late 1970s.[12]
Bell developed multiple versions of Unix for internal use, such as CB UNIX (with improved support for databases) and PWB/UNIX, the "Programmer's Workbench", aimed at large groups of programmers. It advertised the latter version, as well as 32V and V7, stating that "more than 800 systems are already in use outside the Bell System" in 1980,[17] and "more than 2000" the following year.[18] Research Unix versions 8, 9, and 10 were developed through the 1980s but were only released to a few universities, though they did generate papers describing the new work. This research focus then shifted to the development of Plan 9 from Bell Labs, a new portable distributed system.
Microcomputer Unix became commercially available in 1980, when Onyx Systems released its Zilog Z8000-based C8002[12] and Microsoft announced its first Unix for 16-bit microcomputers called Xenix, which the Santa Cruz Operation (SCO) ported to the 8086 processor in 1983. Other companies began to offer commercial versions of Unix for their own minicomputers and workstations. Many of these new Unix flavors were developed from the System V base under a license from AT&T; others were based on BSD. One of the leading developers of BSD, Bill Joy, went on to co-found Sun Microsystems in 1982 and created SunOS for its workstations.
In 1983, the U.S. Department of Justice settled its second antitrust case against AT&T, causing the breakup of the Bell System. This relieved AT&T of the 1956 consent decree that had prevented the company from commercializing Unix. AT&T promptly introduced Unix System V into the market. The newly created competition nearly destroyed the long-term viability of Unix, because it stifled the free exchanging of source code and led to fragmentation and incompatibility.[11] The GNU Project was founded in the same year by Richard Stallman.
Since the newer commercial UNIX licensing terms were not as favorable for academic use as the older versions of Unix, the Berkeley researchers continued to develop BSD as an alternative to UNIX System III and V. Many contributions to Unix first appeared in BSD releases, notably the C shell with job control (modelled on ITS). Perhaps the most important aspect of the BSD development effort was the addition of TCP/IP network code to the mainstream Unix kernel. The BSD effort produced several significant releases that contained network code: 4.1cBSD, 4.2BSD, 4.3BSD, 4.3BSD-Tahoe ("Tahoe" being the nickname of the Computer Consoles Inc. Power 6/32 architecture that was the first non-DEC release of the BSD kernel), Net/1, 4.3BSD-Reno (to match the "Tahoe" naming, and that the release was something of a gamble), Net/2, 4.4BSD, and 4.4BSD-lite. The network code found in these releases is the ancestor of much TCP/IP network code in use today, including code that was later released in AT&T System V UNIX and early versions of Microsoft Windows. The accompanying Berkeley sockets API is a de facto standard for networking APIs and has been copied on many platforms.
During this period, many observers expected that UNIX, with its portability, rich capabilities, and support from companies like DEC and IBM, was likely to become an industry-standard operating system for microcomputers.[21][22] Citing its much smaller software library and installed base than that of MS-DOS and the IBM PC, others expected that customers would prefer personal computers on local area networks to Unix multiuser systems.[23] Microsoft planned to make Xenix MS-DOS's multiuser successor;[12] by 1983 a Xenix-based Altos 586 with 512 KB RAM and 10 MB hard drive cost US$8,000 (equivalent to $21,765 in 2021).[24] BYTE reported that the Altos "under moderate load approaches DEC VAX performance for most tasks that a user would normally invoke", while other computers from Sun and MASSCOMP were much more expensive but equaled the VAX. The magazine added that both PC/IX and Venix on the IBM PC outperformed Venix on the PDP-11/23.[21] uNETix, a commercial microcomputer Unix, implemented the first Unix color windowing system.[citation needed]
Thompson and Ritchie were given the Turing Award in 1983 for their work on Unix.
During this time a number of vendors including Digital Equipment, Sun, Addamax and others began building trusted versions of UNIX for high security applications, mostly designed for military and law enforcement applications.
AT&T responded by issuing a standard, the System V Interface Definition (SVID, 1985), and required conformance for operating systems to be branded "System V".
In 1984, several European computer vendors established the X/Open consortium with the goal of creating an open system specification based on Unix (and eventually the SVID).[30]
Yet another standardization effort was the IEEE's POSIX specification (1988), designed as a compromise API readily implemented on both BSD and System V platforms. In 1993, POSIX was mandated by the United States government for many of its own systems.[31]
In the spring of 1988, AT&T took the standardization a step further. First, it collaborated with SCO to merge System V and Xenix into System V/386.[29] Next, it sought collaboration with Sun Microsystems (vendor of the 4.2BSD derivative SunOS and its Network File System) to merge System V, BSD/SunOS and Xenix into a single unified Unix, which would become System V Release 4. AT&T and Sun, as UNIX International (UI), acted independently of X/Open and drew ire from other vendors, which started the Open Software Foundation to work on their own unified Unix, OSF/1, ushering in a new phase of the Unix wars.[29]
The Unix wars continued into the 1990s, but turned out to be less of a threat than originally thought: AT&T and Sun went their own ways after System V.4, while OSF/1's schedule slipped behind.[29] 
By 1993, most commercial vendors changed their variants of Unix to be based on System V with many BSD features added. The creation of the Common Open Software Environment (COSE) initiative that year, by the major players in Unix, marked the end of the most notorious phase of the Unix wars, and was followed by the merger of UI and OSF in 1994. The new combined entity retained the OSF name and stopped work on OSF/1. By that time the only vendor using it was Digital Equipment Corporation, which continued its own development, rebranding their product Digital UNIX in early 1995.
POSIX became the unifying standard for Unix systems (and some other operating systems).[29]
Meanwhile, the BSD world saw its own developments. The group at Berkeley moved its operating system toward POSIX compliance and released a stripped-down version of its networking code, supposedly without any code that was the property of AT&T. In 1991, a group of BSD developers (Donn Seeley, Mike Karels, Bill Jolitz, and Trent Hein) left the University of California to found Berkeley Software Design, Inc. (BSDi), which sold a fully functional commercial version of BSD Unix for the Intel platform, which they advertised as free of AT&T code. They ran into legal trouble when AT&T's Unix subsidiary sued BSDi for copyright infringement and various other charges in relation to BSD; subsequently, the University of California countersued.[32]
Shortly after it was founded, Bill Jolitz left BSDi to pursue distribution of 386BSD, the free software ancestor of FreeBSD, OpenBSD, and NetBSD.
In 1991, USL and Novell teamed up to develop a version of System 4 for i386 and i486 computers that would combine TCP/IP and Novell's IPX/SPX networking protocols, called the Destiny Project. They formed the Univel subsidiary as a jointly owned company to develop, market, and support the product, which was given the official name UnixWare (aka SRV4.2) and was sold as Univel UnixWare 1.0 that same year. Shortly after Univel released UnixWare 1.0 produced, AT&T sold USL and it's share of Univel to Novell meaning Novell now owned all of AT&T's rights to Unix. Dennis Ritchie likened this sale to the Biblical story of Esau selling his birthright for the mess of pottage.[33]  Novell tried to use the fact that UnixWare combined both TCP/IP and NetWare networking technologies as a marketing tool against Windows NT, but their core markets suffered considerably. It also quickly settled the court battles with BSDi and Berkeley.[32]
In 1993, Novell decided to transfer the UNIX trademark and certification rights to the X/Open Consortium.[34] In 1996, X/Open merged with OSF, creating the Open Group. Various standards by the Open Group now define what is and what is not a UNIX operating system, notably the post-1998 Single UNIX Specification.
In 1993 Unix was used as a minor plot element in the blockbuster film Jurassic Park.[35]
In 1995, the business of administering and supporting the existing UNIX licenses, plus rights to further develop the System V code base, were sold by Novell to the Santa Cruz Operation.[36] Whether Novell also sold the copyrights would later become the subject of litigation (see below).
In 1997, Apple sought a new foundation for its Macintosh operating system and chose NeXTSTEP, an operating system developed by NeXT. The core operating system, which was based on BSD and the Mach kernel, was renamed Darwin after Apple acquired it. The deployment of Darwin in Mac OS X makes it, according to a statement made by an Apple employee at a USENIX conference, the most widely used Unix-based system in the desktop computer market.[citation needed]
In 2000, SCO sold its entire UNIX business and assets to Caldera Systems, which later changed its name to The SCO Group.
In 2003, the SCO Group started legal action against various users and vendors of Linux. SCO had alleged that Linux contained copyrighted Unix code now owned by the SCO Group. Other allegations included trade-secret violations by IBM, or contract violations by former Santa Cruz customers who had since converted to Linux. However, Novell disputed the SCO Group's claim to hold copyright on the UNIX source base. According to Novell, SCO (and hence the SCO Group) are effectively franchise operators for Novell, which also retained the core copyrights, veto rights over future licensing activities of SCO, and 95% of the licensing revenue. The SCO Group disagreed with this, and the dispute resulted in the SCO v. Novell lawsuit.  On 10 August 2007, a major portion of the case was decided in Novell's favor (that Novell had the copyright to UNIX, and that the SCO Group had improperly kept money that was due to Novell). The court also ruled that "SCO is obligated to recognize Novell's waiver of SCO's claims against IBM and Sequent". After the ruling, Novell announced they have no interest in suing people over Unix and stated, "We don't believe there is Unix in Linux".[39][40][41]  SCO successfully got the 10th Circuit Court of Appeals to partially overturn this decision on 24 August 2009 which sent the lawsuit back to the courts for a jury trial.[42][43][44]
On 30 March 2010, following a jury trial, Novell, and not The SCO Group, was "unanimously [found]" to be the owner of the UNIX and UnixWare copyrights.[45] The SCO Group, through bankruptcy trustee Edward Cahn, decided to continue the lawsuit against IBM for causing a decline in SCO revenues.[46] On March 1, 2016, SCO's lawsuit against IBM was dismissed with prejudice.
In 2005, Sun Microsystems released the bulk of its Solaris system code (based on UNIX System V Release 4) into an open source project called OpenSolaris. New Sun OS technologies, notably the ZFS file system, were first released as open source code via the OpenSolaris project. Soon afterwards, OpenSolaris spawned several non-Sun distributions. In 2010, after Oracle acquired Sun, OpenSolaris was officially discontinued, but the development of derivatives continued.
Since the early 2000s, Linux is the leading Unix-like operating system, with other variants of Unix (apart from macOS) having only a negligible market share (see Usage share of operating systems).
In the 1950s and 1960s, computer operating software and compilers were delivered as a part of hardware purchases without separate fees. At the time, source code, the human-readable form of software, was generally distributed with the software providing the ability to fix bugs or add new functions.[1] Universities were early adopters of computing technology. Many of the modifications developed by universities were openly shared, in keeping with the academic principles of sharing knowledge, and organizations sprung up to facilitate sharing. As large-scale operating systems matured, fewer organizations allowed modifications to the operating software, and eventually such operating systems were closed to modification. However, utilities and other added-function applications are still shared and new organizations have been formed to promote the sharing of software.
The concept of free sharing of technological information existed long before computers. For example, in the early years of automobile development, one enterprise owned the rights to a 2-cycle gasoline engine patent originally filed by George B. Selden.[2] By controlling this patent, they were able to monopolize the industry and force car manufacturers to adhere to their demands, or risk a lawsuit. In 1911, independent automaker Henry Ford won a challenge to the Selden patent. The result was that the Selden patent became virtually worthless and a new association (which would eventually become the Motor Vehicle Manufacturers Association) was formed.[2] The new association instituted a cross-licensing agreement among all US auto manufacturers: although each company would develop technology and file patents, these patents were shared openly and without the exchange of money between all the manufacturers.[2] By the time the US entered World War 2, 92 Ford patents and 515 patents from other companies were being shared between these manufacturers, without any exchange of money (or lawsuits).[2][improper synthesis?]
In the 1950s and into the 1960s almost all software was produced by academics and corporate researchers working in collaboration,[3] often shared as public-domain software. As such, it was generally distributed under the principles of openness and cooperation long established in the fields of academia, and was not seen as a commodity in itself. Such communal behavior later became a central element of the so-called hacking culture (a term with a positive connotation among open-source programmers). At this time, source code, the human-readable form of software, was generally distributed with the software machine code because users frequently modified the software themselves, because it would not run on different hardware or OS without modification, and also to fix bugs or add new functions.[4][5][failed verification] The first example of free and open-source software is believed to be the A-2 system, developed at the UNIVAC division of Remington Rand in 1953,[6] which was released to customers with its source code. They were invited to send their improvements back to UNIVAC.[7] Later, almost all IBM mainframe software was also distributed with source code included. User groups such as that of the IBM 701, called SHARE, and that of Digital Equipment Corporation (DEC), called DECUS, were formed to facilitate the exchange of software. The SHARE Operating System, originally developed by General Motors, was distributed by SHARE for the IBM 709 and 7090 computers. Some university computer labs even had a policy requiring that all programs installed on the computer had to come with published source-code files.[8]
In 1969 the Advanced Research Projects Agency Network (ARPANET), a transcontinental, high-speed computer network was constructed. The network (later succeeded by the Internet) simplified the exchange of software code.[4]
Some free software which was developed in the 1970s continues to be developed and used, such as TeX (developed by Donald Knuth)[9] and SPICE.[10]
By the late 1960s change was coming: as operating systems and programming language compilers evolved, software production costs were dramatically increasing relative to hardware. A growing software industry was competing with the hardware manufacturers' bundled software products (the cost of bundled products was included in the hardware cost), leased machines required software support while providing no revenue for software, and some customers, able to better meet their own needs,[11] did not want the costs of manufacturer's software to be bundled with hardware product costs. In the United States vs. IBM antitrust suit, filed 17 January 1969, the U.S. government charged that bundled software was anticompetitive.[12] While some software continued to come at no cost, there was a growing amount of software that was for sale only under restrictive licenses.
In the early 1970s AT&T distributed early versions of Unix at no cost to government and academic researchers, but these versions did not come with permission to redistribute or to distribute modified versions, and were thus not free software in the modern meaning of the phrase. After Unix became more widespread in the early 1980s, AT&T stopped the free distribution and charged for system patches. As it is quite difficult to switch to another architecture, most researchers paid for a commercial license.
Software was not considered copyrightable before the 1974 US Commission on New Technological Uses of Copyrighted Works (CONTU) decided that "computer programs, to the extent that they embody an author's original creation, are proper subject matter of copyright".[13][14] Therefore, software had no licenses attached and was shared as public-domain software, typically with source code. The CONTU decision plus later court decisions such as Apple v. Franklin in 1983 for object code, gave computer programs the copyright status of literary works and started the licensing of software and the shrink-wrap closed source software business model.[15]
In the late 1970s and early 1980s, computer vendors and software-only companies began routinely charging for software licenses, marketing software as "Program Products" and imposing legal restrictions on new software developments, now seen as assets, through copyrights, trademarks, and leasing contracts. In 1976 Bill Gates wrote an essay entitled "Open Letter to Hobbyists", in which he expressed dismay at the widespread sharing of Microsoft's product Altair BASIC by hobbyists without paying its licensing fee. In 1979, AT&T began to enforce its licenses when the company decided it might profit by selling the Unix system.[16] In an announcement letter dated 8 February 1983 IBM inaugurated a policy of no longer distributing sources with purchased software.[17][18]
To increase revenues, a general trend began to no longer distribute source code (easily readable by programmers), and only distribute the executable machine code that was compiled from the source code. One person especially distressed by this new practice was Richard Stallman. He was concerned that he could no longer study or further modify programs initially written by others. Stallman viewed this practice as ethically wrong. In response, he founded the GNU Project in 1983 so that people could use computers using only free software.[1] He established a non-profit organization, the Free Software Foundation, in 1985, to more formally organize the project. He invented copyleft, a legal mechanism to preserve the "free" status of a work subject to copyright, and implemented this in the GNU General Public License. Copyleft licenses allow authors to grant a number of rights to users (including rights to use a work without further charges, and rights to obtain, study and modify the program's complete corresponding source code) but requires derivatives to remain under the same license or one without any additional restrictions. Since derivatives include combinations with other original programs, downstream authors are prevented from turning the initial work into proprietary software, and invited to contribute to the copyleft commons.[4] Later, variations of such licenses were developed by others.
However, there were still those who wished to share their source code with other programmers and/or with users on a free basis, then called "hobbyists" and "hackers".[19] Before the introduction and widespread public use of the internet, there were several alternative ways available to do this, including listings in computer magazines (like Dr. Dobb's Journal, Creative Computing, SoftSide, Compute!, Byte, etc.) and in computer programming books, like the bestseller BASIC Computer Games.[20] Though still copyrighted, annotated source code for key components of Atari 8-bit family system software was published in mass market books, including The Atari BASIC Source Book[21] (full source for Atari BASIC) and Inside Atari DOS (full source for Atari DOS).[22]
The SHARE users group, founded in 1955, began collecting and distributing free software. The first documented distribution from SHARE was dated 17 October 1955.[23] The "SHARE Program Library Agency" (SPLA) distributed information and software, notably on magnetic tape.
In the early 1980s, the so-called DECUS tapes[24] were a worldwide system for transmission of free software for users of DEC equipment. Operating systems were usually proprietary software, but many tools like the TECO editor, Runoff text formatter, or List file listing utility, etc. were developed to make users' lives easier, and distributed on the DECUS tapes. These utility packages benefited DEC, which sometimes incorporated them into new releases of their proprietary operating system. Even compilers could be distributed and for example Ratfor (and Ratfiv) helped researchers to move from Fortran coding to structured programming (suppressing the GO TO statement). The 1981 Decus tape was probably the most innovative by bringing the Lawrence Berkeley Laboratory Software Tools Virtual Operating System which permitted users to use a Unix-like system on DEC 16-bit PDP-11s and 32-bit VAXes running under the VMS operating system. It was similar to the current cygwin system for Windows. Binaries and libraries were often distributed, but users usually preferred to compile from source code.[citation needed]
In the 1980s, parallel to the free software movement, software with source code was shared on BBS networks. This was sometimes a necessity; software written in BASIC and other interpreted languages could only be distributed as source code, and much of it was freeware. When users began gathering such source code, and setting up boards specifically to discuss its modification, this was a de facto open-source system.
One of the most obvious examples of this is one of the most-used BBS systems and networks, WWIV, developed initially in BASIC by Wayne Bell. A culture of "modding" his software, and distributing the mods, grew up so extensively that when the software was ported to first Pascal, then C++, its source code continued to be distributed to registered users, who would share mods and compile their own versions of the software. This may have contributed to it being a dominant system and network, despite being outside the Fidonet umbrella that was shared by so many other BBS makers.
Meanwhile, the advent of Usenet and UUCPNet in the early 1980s further connected the programming community and provided a simpler way for programmers to share their software and contribute to software others had written.[25]
In 1983, Richard Stallman launched the GNU Project to write a complete operating system free from constraints on use of its source code. Particular incidents that motivated this include a case where an annoying printer couldn't be fixed because the source code was withheld from users.[26] Stallman also published the GNU Manifesto in 1985 to outline the GNU Project's purpose and explain the importance of free software. Another probable inspiration for the GNU project and its manifesto was a disagreement between Stallman and Symbolics, Inc. over MIT's access to updates Symbolics had made to its Lisp machine, which was based on MIT code.[27] Soon after the launch, he[19] used[clarification needed] the existing term "free software" and founded the Free Software Foundation to promote the concept. The Free Software Definition was published in February 1986.
In 1989, the first version of the GNU General Public License was published.[28] A slightly updated version 2 was published in 1991.  In 1989, some GNU developers formed the company Cygnus Solutions.[29] The GNU project's kernel, later called "GNU Hurd", was continually delayed, but most other components were completed by 1991. Some of these, especially the GNU Compiler Collection, had become market leaders[clarification needed] in their own right. The GNU Debugger and GNU Emacs were also notable successes.
The Linux kernel, started by Linus Torvalds, was released as freely modifiable source code in 1991. The license was not a free software license, but with version 0.12 in February 1992, Torvalds relicensed the project under the GNU General Public License.[30] Much like Unix, Torvalds' kernel attracted attention from volunteer programmers.
Until this point, the GNU project's lack of a kernel meant that no complete free software operating systems existed. The development of Torvalds' kernel closed that last gap. The combination of the almost-finished GNU operating system and the Linux kernel made the first complete free software operating system.
Since 1996, the Linux kernel has included proprietary licensed components, so that it was no longer entirely free software.[32] Therefore, the Free Software Foundation Latin America released in 2008 a modified version of the Linux-kernel called Linux-libre, where all proprietary and non-free components were removed.
Many businesses offer customized Linux-based products, or distributions, with commercial support. The naming remains controversial. Referring to the complete system as simply "Linux" is common usage. However, the Free Software Foundation, and many others,[citation needed] advocate the use of the term "GNU/Linux", saying that it is a more accurate name for the whole operating system.[33]
Linux adoption grew among businesses and governments in the 1990s and 2000s. In the English-speaking world at least, Ubuntu and its derivatives became a relatively popular group of Linux distributions.
When the USL v. BSDi lawsuit was settled out of court in 1993, FreeBSD and NetBSD (both derived from 386BSD) were released as free software. In 1995, OpenBSD forked from NetBSD. In 2004, Dragonfly BSD forked from FreeBSD.
In the mid to late 90s, when many website-based companies were starting up, free software became a popular choice for web servers. The Apache HTTP Server became the most-used web-server software, a title that still holds as of 2015.[34] Systems based on a common "stack" of software with the Linux kernel at the base, Apache providing web services, the MySQL database engine for data storage, and the PHP programming language for providing dynamic pages, came to be termed LAMP systems. In actuality, the programming language that predated PHP and dominated the web in the mid and late 1990s was Perl. Web forms were processed on the server side through Common Gateway Interface scripts written in Perl.
The term "open source," as related to free software, was in common use by 1995.[35] Other recollection have it in use during the 1980s.[36]
In 1997, Eric S. Raymond published "The Cathedral and the Bazaar", a reflective analysis of the hacker community and free software principles. The paper received significant attention in early 1998 and was one factor in motivating Netscape Communications Corporation to release their popular Netscape Communicator Internet suite as free software.[37]
Netscape's act prompted Raymond and others to look into how to bring free software principles and benefits to the commercial-software industry. They concluded that FSF's social activism was not appealing to companies like Netscape, and looked for a way to rebrand the free software movement to emphasize the business potential of the sharing of source code.[38]
The label "open source" was adopted by some people in the free software movement at a strategy session[39] held at Palo Alto, California, in reaction to Netscape's January 1998 announcement of a source code release for Navigator. The group of individuals at the session included Christine Peterson who suggested "open source",[1] Todd Anderson, Larry Augustin, Jon Hall, Sam Ockman, Michael Tiemann, and Eric S. Raymond. Over the next week, Raymond and others worked on spreading the word. Linus Torvalds gave an all-important sanction the following day. Phil Hughes offered a pulpit in Linux Journal. Richard Stallman, pioneer of the free software movement, flirted with adopting the term, but changed his mind.[39] Those people who adopted the term used the opportunity before the release of Navigator's source code to free themselves of the ideological and confrontational connotations of the term "free software". Netscape released its source code under the Netscape Public License and later under the Mozilla Public License.[40]
The term was given a big boost at an event organized in April 1998 by technology publisher Tim O'Reilly. Originally titled the "Freeware Summit" and later named the "Open Source Summit",[41] the event brought together the leaders of many of the most important free and open-source projects, including Linus Torvalds, Larry Wall, Brian Behlendorf, Eric Allman, Guido van Rossum, Michael Tiemann, Paul Vixie, Jamie Zawinski of Netscape, and Eric Raymond. At that meeting, the confusion caused by the name free software was brought up. Tiemann argued for "sourceware" as a new term, while Raymond argued for "open source". The assembled developers took a vote, and the winner was announced at a press conference that evening. Five days later, Raymond made the first public call to the free software community to adopt the new term.[42] The Open Source Initiative was formed shortly thereafter.[1][39] According to the OSI Richard Stallman initially flirted with the idea of adopting the open source term.[43] But as the enormous success of the open source term buried Stallman's free software term and his message on social values and computer users' freedom,[44][45][46] later Stallman and his FSF strongly objected to the OSI's approach and terminology.[47] Due to Stallman's rejection of the term "open-source software", the FOSS ecosystem is divided in its terminology; see also Alternative terms for free software. For example, a 2002 FOSS developer survey revealed that 32.6% associated themselves with OSS, 48% with free software, and 19.4% in between or undecided.[48] Stallman still maintained, however, that users of each term were allies in the fight against proprietary software.
On 13 October 2000, Sun Microsystems released the StarOffice office suite as free software under the GNU Lesser General Public License. The free software version was renamed OpenOffice.org, and coexisted with StarOffice.
By the end of the 1990s, the term "open source" gained much traction in public media[49] and acceptance in software industry in context of the dotcom bubble and the open-source software driven Web 2.0.
The X Window System was created in 1984, and became the de facto standard window system in desktop free software operating systems by the mid-1990s.  X runs as a server, and is responsible for communicating with graphics hardware on behalf of clients (which are individual software applications).  It provides useful services such as having multiple virtual desktops for the same monitor, and transmitting visual data across the network so a desktop can be accessed remotely.
Initially, users or system administrators assembled their own environments from X and available window managers (which add standard controls to application windows; X itself does not do this), pagers, docks and other software. While X can be operated without a window manager, having one greatly increases convenience and ease of use.
Two key "heavyweight" desktop environments for free software operating systems emerged in the 1990s that were widely adopted: KDE and GNOME.
KDE was founded in 1996 by Matthias Ettrich. At the time, he was troubled by the inconsistencies in the user interfaces of UNIX applications. He proposed a new desktop environment. He also wanted to make this desktop easy to use. His initial Usenet post spurred a lot of interest.[50]
Ettrich chose to use the Qt toolkit for the KDE project. At the time, Qt did not use a free software license. Members of the GNU project became concerned with the use of such a toolkit for building a free software desktop environment. In August 1997, two projects were started in response to KDE: the Harmony toolkit (a free replacement for the Qt libraries) and GNOME (a different desktop without Qt and built entirely on top of free software).[51] GTK+ was chosen as the base of GNOME in place of the Qt toolkit.
In November 1998, the Qt toolkit was licensed under the free/open source Q Public License (QPL) but debate continued about compatibility with the GNU General Public License (GPL). In September 2000, Trolltech made the Unix version of the Qt libraries available under the GPL, in addition to the QPL, which has eliminated the concerns of the Free Software Foundation.  KDE has since been split into KDE Plasma Workspaces, a desktop environment, and KDE Software Compilation, a much broader set of software that includes the desktop environment.
Both KDE and GNOME now participate in freedesktop.org, an effort launched in 2000 to standardize Unix desktop interoperability, although there is still competition between them.[52]
Since 2000, software written for X almost always uses some widget toolkit written on top of X, like Qt or GTK.[citation needed]
In 2010, Canonical released the first version of Unity, a replacement for the prior default desktop environment for Ubuntu, GNOME. This change to a new, under-development desktop environment and user interface was initially somewhat controversial among Ubuntu users.
In 2011, GNOME 3 was introduced, which largely discarded the desktop metaphor in favor of a more mobile-oriented interface. The ensuing controversy led Debian to consider making the Xfce environment default on Debian 7. Several independent projects were begun to keep maintaining the GNOME 2 code.
Fedora did not adopt Unity, retaining its existing offering of a choice of GNOME, KDE and LXDE with GNOME being the default, and hence Red Hat Enterprise Linux (for which Fedora acts as the "initial testing ground") did not adopt Unity either. A fork of Ubuntu was made by interested third-party developers that kept GNOME and discarded Unity. In March 2017, Ubuntu announced that it will be abandoning Unity in favour of GNOME 3 in future versions, and ceasing its efforts in developing Unity-based smartphones and tablets.[53][54]
When Google built the Linux-based Android operating system, mostly for phone and tablet devices, it replaced X with the purpose-built SurfaceFlinger.
Open-source developers also criticized X as obsolete, carrying many unused or overly complicated elements in its protocol and libraries, while missing modern functionality, e.g., compositing, screen savers, and functions provided by window managers.[55] Several attempts have been made or are underway to replace X for these reasons, including:
As free software became more popular, industry incumbents such as Microsoft started to see it as a serious threat. This was shown in a leaked 1998 document, confirmed by Microsoft as genuine, which came to be called the first of the Halloween Documents.
Steve Ballmer once compared the GPL to "a cancer", but has since stopped using this analogy. Indeed, Microsoft has softened its public stance towards open source[citation needed] in general, with open source since becoming an important part of the Microsoft Windows ecosystem. However, at the same time, behind the scenes, Microsoft's actions have been less favourable toward the open-source community.
In 2003, a proprietary Unix vendor and former Linux distribution vendor called SCO alleged that Unix intellectual property had been inappropriately copied into the Linux kernel, and sued IBM, claiming that it bore responsibility for this. Several related lawsuits and countersuits followed, some originating from SCO, some from others suing SCO. However, SCO's allegations lacked specificity, and while some in the media reported them as credible, many critics of SCO believed the allegations to be highly dubious at best.
Over the course of the SCO v. IBM case, it emerged that not only had SCO been distributing the Linux kernel for years under the GPL, and continued to do so (thus rendering any claims hard to sustain legally), but that SCO did not even own the copyrights to much of the Unix code that it asserted copyright over, and had no right to sue over them on behalf of the presumed owner, Novell.
This was despite SCO's CEO, Darl McBride, having made many wild and damaging claims of inappropriate appropriation to the media, many of which were later shown to be false, or legally irrelevant even if true.
The blog Groklaw was one of the most forensic examiners of SCO's claims and related events, and gained its popularity from covering this material for many years.
SCO suffered defeat after defeat in SCO v. IBM and its various other court cases, and filed for Chapter 11 bankruptcy in 2007. However, despite the courts finding that SCO did not own the copyrights (see above), and SCO's lawsuit-happy CEO Darl McBride no longer running the company, the bankruptcy trustee in charge of SCO-in-bankruptcy decided to press on with some portions he claimed remained relevant in the SCO v. IBM lawsuit. He could apparently afford to do this because SCO's main law firm in SCO v. IBM had signed an agreement at the outset to represent SCO for a fixed amount of money no matter how long the case took to complete.
In 2004, the Alexis de Tocqueville Institution (ADTI) announced its intent to publish a book, Samizdat: And Other Issues Regarding the 'Source' of Open Source Code, showing that the Linux kernel was based on code stolen from Unix, in essence using the argument that it was impossible to believe that Linus Torvalds could produce something as sophisticated as the Linux kernel. The book was never published, after it was widely criticised and ridiculed, including by people supposedly interviewed for the book. It emerged that some of the people were never interviewed, and that ADTI had not tried to contact Linus Torvalds, or ever put the allegations to him to allow a response. Microsoft attempted to draw a line under this incident, stating that it was a "distraction".
Many suspected that some or all of these legal and fear, uncertainty and doubt (FUD) attacks against the Linux kernel were covertly arranged by Microsoft, although this has never been proven. Both ADTI and SCO, however, received funding from Microsoft.
In 2004 the European Commission found Microsoft guilty of anti-competitive behaviour with respect to interoperability in the workgroup software market. Microsoft had formerly settled United States v. Microsoft in 2001, in a case which charged that it illegally abused its monopoly power to force computer manufacturers to preinstall Internet Explorer.
The Commission demanded that Microsoft produce full documentation of its workgroup protocols to allow competitors to interoperate with its workgroup software, and imposed fines of 1.5 million euros per day for Microsoft's failure to comply. The commission had jurisdiction because Microsoft sells the software in question in Europe.
Microsoft, after a failed attempt to appeal the decision through the Court of Justice of the European Union, eventually complied with the demand, producing volumes of detailed documentation.
The Samba project, as Microsoft's sole remaining competitor in the workgroup software market, was the key beneficiary of this documentation.
In 2008 the International Organization for Standardization published Microsoft's Office Open XML as an international standard, which crucially meant that it, and therefore Microsoft Office, could be used in projects where the use of open standards were mandated by law or by policy. Critics of the standardisation process, including some members of ISO national committees involved in the process itself, alleged irregularities and procedural violations in the process, and argued that the ISO should not have approved OOXML as a standard because it made reference to undocumented Microsoft Office behaviour.
As of 2012[update], no correct open-source implementation of OOXML exists, which validates the critics' remarks about OOXML being difficult to implement and underspecified. Presently, Google cannot yet convert Office documents into its own proprietary Google Docs format correctly. This suggests that OOXML is not a true open standard, but rather a partial document describing what Microsoft Office does, and only involving certain file formats.
In 2006 Microsoft launched its CodePlex open source code hosting site, to provide hosting for open-source developers targeting Microsoft platforms. In July 2009 Microsoft even open sourced some Hyper-V-supporting patches to the Linux kernel, because they were required to do so by the GNU General Public License,[57][58] and contributed them to the mainline kernel. Note that Hyper-V itself is not open source. Microsoft's F# compiler, created in 2002, has also been released as open source under the Apache license. The F# compiler is a commercial product, as it has been incorporated into Microsoft Visual Studio, which is not open source.
Microsoft representatives have made regular appearances at various open source and Linux conferences for many years.
In 2012, Microsoft launched a subsidiary named Microsoft Open Technologies Inc., with the aim of bridging the gap between proprietary Microsoft technologies and non-Microsoft technologies by engaging with open-source standards.[59] This subsidiary was subsequently folded back into Microsoft as Microsoft's position on open source and non-Windows platforms became more favourable.
In January 2016 Microsoft released Chakra as open source under the MIT License; the code is available on GitHub.[60]
Microsoft's stance on open source has shifted as the company began endorsing more open-source software. In 2016, Steve Balmer, former CEO of Microsoft, has retracted his statement that Linux is a malignant cancer.[61] In 2017, the company became a platinum supporter of the Linux Foundation. By 2018, shortly before acquiring GitHub, Microsoft led the charts in the number of paid staff contributing to open-source projects there.[62] While Microsoft may or may not endorse the original philosophy of free software, data shows that it does endorse open source strategically.[original research?]
Critics have noted that, in March 2019, Microsoft sued Foxconn's subsidiary over a 2013 patent contract;[63] in 2013, Microsoft had announced a patent agreement with Foxconn related to Foxconn's use of the Linux-based Android and ChromeOS.[64]
The vast majority of programming languages in use today have a free software implementation available.
Since the 1990s, the release of major new programming languages in the form of open-source compilers and/or interpreters has been the norm, rather than the exception. Examples include Python in 1991, Ruby in 1995, and Scala in 2003. In recent times, the most notable exceptions have been Java, ActionScript, C#, and Apple's Swift until version 2.2 was proprietary. Partly compatible open-source implementations have been developed for most, and in the case of Java, the main open-source implementation is by now very close to the commercial version.
Since its first public release in 1996, the Java platform had not been open source, although the Java source code portion of the Java runtime was included in Java Development Kits (JDKs), on a purportedly "confidential" basis, despite it being freely downloadable by the general public in most countries. Sun later expanded this "confidential" source code access to include the full source code of the Java Runtime Environment via a separate program which was open to members of the public, and later made the source of the Java compiler javac available also. Sun also made the JDK source code available confidentially to the Blackdown Java project, which was a collection of volunteers who ported early versions of the JDK to Linux, or improved on Sun's Linux ports of the JDK. However, none of this was open source, because modification and redistribution without Sun's permission were forbidden in all cases. Sun stated at the time that they were concerned about preventing forking of the Java platform.
However, several independent partial reimplementations of the Java platform had been created, many of them by the open-source community, such as the GNU Compiler for Java (GCJ). Sun never filed lawsuits against any of the open source clone projects. GCJ notably caused a bad user experience for Java on free software supporting distributions such as Fedora and Ubuntu which shipped GCJ at the time as their Java implementation. How to replace GCJ with the Sun JDK was a frequently asked question by users, because GCJ was an incomplete implementation, incompatible and buggy.
In 2006 Jonathan I. Schwartz became CEO of Sun Microsystems, and signalled his commitment to open source. On 8 May 2007, Sun Microsystems released the Java Development Kit as OpenJDK under the GNU General Public License. Part of the class library (4%) could not be released as open source due to them being licensed from other parties and were included as binary plugs.[citation needed] Because of this, in June 2007, Red Hat launched IcedTea to resolve the encumbered components with the equivalents from GNU Classpath implementation. Since the release, most of the encumbrances have been solved, leaving only the audio engine code and colour management system (the latter is to be resolved using Little CMS).
The first open-source distributed revision control system (DVCS) was 'tla' in 2001 (since renamed to GNU arch); however, it and its successors 'baz' and 'bzr' (Bazaar) never became very popular, and GNU arch was discontinued, although Bazaar still continues and is used by Canonical.
However, other DVCS projects sprung up, and some started to get significant adoption.
Git, the most popular DVCS, was created in 2005.[65] Some developers of the Linux kernel started to use a proprietary DVCS called BitKeeper, notably Linux founder Linus Torvalds, although some other kernel developers never used it due to its proprietary nature. The unusual situation whereby Linux kernel development involved the use by some of proprietary software "came to a head" when Andrew Tridgell started to reverse-engineer BitKeeper with the aim of producing an open-source tool which could provide some of the same functionality as the commercial version. BitMover, the company that developed BitKeeper, in response, in 2005 revoked the special free of-charge license it had granted to certain kernel developers.
As a result of the removal of the BitKeeper license, Linus Torvalds decided to write his own DVCS, called git, because he thought none of the existing open-source DVCSs were suitable for his particular needs as a kernel maintainer (which was why he had adopted BitKeeper in the first place). A number of other developers quickly jumped in and helped him, and git over time grew from a relatively simple "stupid content tracker" (on which some developers developed "porcelain" extensions) into the sophisticated and powerful DVCS that it is today. Torvalds no longer maintains git himself, however; it has been maintained by Junio Hamano for many years, and has continued receiving contributions from many developers.
The increasing popularity of open-source DVCSs such as git, and then, later, DVCS hosting sites, the most popular of which is GitHub (founded 2008), incrementally reduced the barriers to participation in free software projects still further. With sites like GitHub, no longer did potential contributors have to do things like hunt for the URL for the source code repository (which could be in different places on each website, or sometimes tucked away in a README file or developer documentation), or work out how to generate a patch, and if necessary subscribe to the right mailing list so that their patch email would get to the right people. Contributors can simply fork their own copy of a repository with one click, and issue a pull request from the appropriate branch when their changes are ready. GitHub has become the most popular hosting site in the world for open-source software, and this, together with the ease of forking and the visibility of forks has made it a popular way for contributors to make changes, large and small.
While copyright is the primary legal mechanism that FOSS authors use to ensure license compliance for their software, other mechanisms such as legislation, software patents, and trademarks have uses also. In response to legal issues with patents and the DMCA, the Free Software Foundation released version 3 of its GNU Public License in 2007 that explicitly addressed the DMCA's digital rights management (DRM) provisions and patent rights.
After the development of the GNU GPLv3, as copyright holder of many pieces of the GNU system, such as the GNU Compiler Collection (GCC) software, the FSF updated most[citation needed] of the GNU programs' licenses from GPLv2 to GPLv3. Apple, a user of GCC and a heavy user of both DRM and patents, decided to switch the compiler in its Xcode IDE from GCC to Clang, another FOSS compiler,[66] but which is under a permissive license.[67] LWN speculated that Apple was motivated partly by a desire to avoid GPLv3.[66] The Samba project also switched to GPLv3, which Apple replaced in their software suite with a closed-source, proprietary software alternative.[68]
Recent mergers have affected major open-source software. Sun Microsystems (Sun) acquired MySQL AB, owner of the popular open-source MySQL database, in 2008.[69]
Oracle in turn purchased Sun in January 2010, acquiring their copyrights, patents, and trademarks. This made Oracle the owner of both the most popular proprietary database and the most popular open-source database (MySQL).[citation needed] Oracle's attempts to commercialize the open-source MySQL database have raised concerns in the FOSS community.[70] Partly in response to uncertainty about the future of MySQL, the FOSS community forked the project into new database systems outside of Oracle's control. These include MariaDB, Percona, and Drizzle.[71] All of these have distinct names; they are distinct projects and cannot use the trademarked name MySQL.[72]
In September 2008, Google released the first version of Android, a new smartphone operating system, as open source (some Google applications that are sometimes but not always bundled with Android are not open source). Initially, the operating system was given away for free by Google, and was eagerly adopted by many handset makers; Google later bought Motorola Mobility and produced its own "vanilla" Android phones and tablets, while continuing to allow other manufacturers to use Android. Android is now the world's most popular mobile platform.[73]
Because Android is based on the Linux kernel, this means that Linux is now the dominant kernel on both mobile platforms (via Android), and supercomputers,[74] and a key player in server operating systems too.
In August 2010, Oracle sued Google claiming that its use of Java in Android infringed on Oracle's copyrights and patents. The initial Oracle v. Google trial ended in May 2012, with the finding that Google did not infringe on Oracle's patents, and the trial judge ruled that the structure of the Java application programming interfaces (APIs) used by Google was not copyrightable. The jury found that Google made a trivial ("de minimis") copyright infringement, but the parties stipulated that Google would pay no damages, because it was so trivial.[75] However, Oracle appealed to the Federal Circuit, and Google filed a cross-appeal on the literal copying claim.[76] The Federal Circuit ruled that the small copyright infringement acknowledged by Google was not de minimis, and sent the fair use issue back to the trial judge for reconsideration. In 2016, the case was retried and a jury found for Google, on the grounds of fair use.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
To protect the wiki against automated account creation, we kindly ask you to enter the words that appear below in the box (more info):
To protect the wiki against automated account creation, we kindly ask you to enter the words that appear below in the box (more info):
Full help contents page
Training for students
A single-page guide to contributing
A training adventure game
Resources for new editors
People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using.
Many IP addresses change periodically, and are often shared by several people. You may create an account or log in to avoid future confusion with other logged out users. Creating an account also hides your IP address.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
Wikipedia is a compendium of the world's knowledge. If you know what you are looking for, type it into Wikipedia's search box. If, however, you need a bird's eye view of what Wikipedia has to offer, see its main contents pages below, which in turn list more specific pages.
Wikipedia's main contents systems are arranged into these subject classifications. Each subject is further divided into subtopics. 
Timelines list events chronologically, sometimes including links to articles with more detail.  There are several ways to find timelines:
You can help us keep Wikipedia up to date! The list below is for encyclopedia entries that describe and pertain to events happening on a current basis. 
Speaking of reference works, various third-party classification systems have been mapped to Wikipedia articles, which can be accessed from these pages:
Bibliographies list sources on a given topic, for verification or further reading outside Wikipedia:
Overview articles summarize in prose a broad topic like biology, and also have illustrations and links to subtopics like cell biology, biographies like Carl Linnaeus, and other related articles like Human Genome Project.
Outline pages have trees of topics in an outline format, which in turn are linked to further outlines and articles providing more detail.  Outlines show how important subtopics relate to each other based on how they are arranged in the tree, and they are useful as a more condensed, non-prose alternative to overview articles.
List pages enumerate items of a particular type, such as the List of sovereign states or List of South Africans.  Wikipedia has "lists of lists" when there are too many items to fit on a single page, when the items can be sorted in different ways, or as a way of navigating lists on a topic (for example Lists of countries and territories or Lists of people).  There are several ways to find lists:
Portals contain featured articles and images, news, categories, excerpts of key articles, links to related portals, and to-do lists for editors.  There are two ways to find portals:
Glossaries are lists of terms with definitions. Wikipedia includes hundreds of alphabetical glossaries; they can be found in two ways:
Wikipedia's collection of category pages is a classified index system.  It is automatically generated from category tags at the bottoms of articles and most other pages. Nearly all of the articles available so far on the website can be found through these subject indexes.
If you are simply looking to browse articles by topic, there are two top-level pages to choose from:
Category:Contents is technically at the top of the category hierarchy, but contains many categories useful to editors but not readers. Special:Categories lists every category alphabetically.
Vital articles are lists of subjects for which the English Wikipedia should have corresponding high-quality articles. They serve as centralized watchlists to track the quality status of Wikipedia's most important articles and to give editors guidance on which articles to prioritize for improvement.
Featured content represents the best of Wikipedia, and has undergone a thorough review process to ensure that it meets the highest encyclopedic standards. Presented by type:
Good articles are articles that meet a core set of editorial standards, the good article criteria, and successfully pass through the good article nomination process. They are well written, contain factually accurate and verifiable information, are broad in coverage, neutral in point of view, stable, and illustrated, where possible, by relevant images with suitable copyright licenses.
Growing collections of Wikipedia articles are starting to become available as spoken word recordings as well.
However, with the exodus of the Islamic Arab tribes to the coastal areas and the border areas within the framework of the conquest, the Christians of Lebanon began to gather in the fortified mountainous areas along the coast. Subsequently, from time to time, they used to communicate with the Byzantines, who took a spearhead from them to attack the Muslims since the days of the Umayyad Caliph Muawiyah bin Abi Sufyan, taking advantage of his preoccupation with the events that accompanied his disagreement with Ali bin Abi Talib. The christian anti-islamic rebellion reached its climax during the reign of Abdul Malik bin Marwan during the events that culminated in the Hijaz and Iraq against the background of the revolution launched by Abdullah bin Al-Zubayr, So Abd al-Malik had to stop resisting the Mardaites and had to pay their leaders tributes in the form of dinars.[14][15][16][17][18]
Other principal towns and cities of the Jund Dimashq were Beirut, Sidon, Tyre (the tax proceeds of which went to Jund al-Urdunn), Tripoli and Byblos along the coast. The coastal cities and their immediate surroundings formed their own small districts.[19]
Wikipedia is a dynamic free online encyclopedia that anyone can edit in good faith, and tens of millions already have!
Wikipedia's purpose is to benefit readers by containing information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, whose articles also have numerous links to guide readers to more information.
Written collaboratively by largely anonymous volunteers, anyone with Internet access and not blocked, can write and make changes to Wikipedia articles (except in limited cases where editing is restricted to prevent disruption or vandalism). Since its creation on January 15, 2001, Wikipedia has grown into the world's largest reference website, attracting over a billion visitors monthly. It currently has more than sixty million articles in more than 300 languages, including 6,623,677 articles in English with 129,357 active contributors in the past month.
The fundamental principles of Wikipedia are summarized in its five pillars. The Wikipedia community has developed many policies and guidelines, but you do not need to be familiar with every one of them before contributing.
Anyone can edit Wikipedia's text, references, and images. What is written is more important than who writes it. The content must conform with Wikipedia's policies, including being verifiable by published sources. Editors' opinions, beliefs, personal experiences, unreviewed research, libelous material, and copyright violations will not remain. Wikipedia's software allows easy reversal of errors, and experienced editors watch and patrol bad edits.
Wikipedia differs from printed references in important ways. It is continually created and updated, and encyclopedic articles on new events appear within minutes rather than months or years. Because anyone can improve Wikipedia, it has become more comprehensive, clear, and balanced than any other encyclopedia. Its contributors improve the quality and quantity of the articles as well as remove misinformation, errors, and vandalism. Any reader can fix a mistake or add more information to articles (see Researching with Wikipedia). 
Wikipedia has tested the wisdom of the crowd since 2001 and found that it succeeds.
We could not find the above page on our servers.
Alternatively, you can visit the Main Page or read more information about this type of error.
This page provides help with the most common questions about Wikipedia.
You can also search Wikipedia's help pages using the search box below, or browse the Help menu or the Help directory.
The Readers' FAQ and our about page contain the most commonly sought information about Wikipedia.
There are other ways to browse and explore Wikipedia articles; many can be found at Wikipedia:Contents. See our disclaimer for cautions about Wikipedia's limitations.
For mobile access, press the mobile view link at the very bottom of every desktop view page.
Contributing is easy: see how to edit a page. For a quick summary on participating, see contributing to Wikipedia, and for a friendly tutorial, see our introduction. For a listing of introductions and tutorials by topic, see getting started. The Simplified Manual of Style and Cheatsheet can remind you of basic wiki markup.
The simple guide to vandalism cleanup can help you undo malicious edits.
If you're looking for places you can help out, the Task Center is the place to go, or check out what else is happening at the community portal. You can practice editing and experiment in a sandboxyour sandbox.
If there is a problem with an article about yourself, a family member, a friend or a colleague, please read Biographies of living persons/Help.
If you spot a problem with an article, you can fix it directly, by clicking on the "Edit" link at the top of that page. See the "edit an article" section of this page for more information.
If you don't feel ready to fix the article yourself, post a message on the article's talk page. This will bring the matter to the attention of others who work on that article. There is a "Talk" link at the beginning of every article page.
Check Your first article to see if your topic is appropriate, then the Article wizard will walk you through creating the article.
Once you have created an article, see Writing better articles for guidance on how to improve it and what to include (like reference citations).
For contributing images, audio or video files, see the Introduction to uploading images. Then the Upload wizard will guide you through that process.
Answers to common problems can be found at frequently asked questions.
Or check out where to ask questions or make comments.
New users having problems editing Wikipedia should ask at the Teahouse. More complex questions can be posed at the Help desk. Volunteers will respond as soon as they're able.
Or ask for help on your talk page and a volunteer will visit you there!
You can get live help with editing in the help chatroom.
For help with technical issues, ask at the Village pump.
If searching Wikipedia has not answered your question (for example, questions like "Which country has the world's largest fishing fleet?"), try the Reference Desk. Volunteers there will attempt to answer your questions on any topic, or point you toward the information you need.
Screen readers are a form of assistive technology for people with disabilities. A list of screen readers is available including a section, Software aids for people with reading difficulties.
Reader software examples include Spoken Web, JAWS, and NonVisual Desktop Access (NVDA). In addition, Fangs screen reader emulator is an open-source extension for the Pale Moon browser that simulates how a web page would look in JAWS.
Full help contents page
Training for students
A single-page guide to contributing
A training adventure game
Resources for new editors
This page provides a listing of current collaborations, tasks, and news about English Wikipedia. New to Wikipedia? See the contributing to Wikipedia page or our tutorial for everything you need to know to get started. For a listing of internal project pages of interest, see the department directory.
For a listing of ongoing discussions and current requests, see the Dashboard.
You can help improve the articles listed below! This list updates frequently, so check back here for more tasks to try. (See Wikipedia:Maintenance or the  Task Center for further information.)
Help counter systemic bias by creating new articles on important women.
Welcome to the community bulletin board, which is a page used for announcements from WikiProjects and other groups. Included here are coordinated efforts, events, projects, and other general announcements.
Also consider posting WikiProject, Task Force, and Collaboration news at The Signpost's WikiProject Report page.
Latest tech news from the Wikimedia technical community. Please tell other users about these changes. Not all changes will affect you. Translations are available.
Discussions in the following areas have requested wider attention via Requests for comment:
The School and university projects page collects information about Wikipedia editing projects for school and university classes, including an archive of many past class projects.
A list of current classes using Wikipedia can be found at current projects.
Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.
Uploads locally to Wikipedia; must comply with the non-free content criteria
Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript.
Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again.
Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. 
In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed.
The filename you chose seems to be very short, or overly generic. Please don't use:
If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
This should not be done, except in very rare exceptional cases.
Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.
Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the Wikimedia Commons.
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
Please note that by "entirely self-made" we really mean just that. 
Do not use this section for any of the following:
Editors who falsely declare such items as their "own work" will be blocked from editing.
Use this only if there is an explicit licensing statement in the source. 
The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
If the source website doesn't say so explicitly, please do not upload the file.
Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it.
 Please remember that you will need to demonstrate that:
Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually.
Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
If this is an article you are only planning to write, please write it first and upload the file afterwards.
The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
Please upload this file only if it is going to be used in an actual article.
If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
The page Example is not a real article, but a disambiguation page pointing to a number of other pages.
Please check and enter the exact title of the actual target article you meant.
If neither of these two statements applies, then please do not upload this image.
This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
In view of this, please explain how the use of this file will be minimal.
Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you.
This is the data that will be submitted to upload:
This might take a minute or two, depending on the size of the file and the speed of your internet connection.
Once uploading is completed, you will find your new file at this link:
Your file has been uploaded successfully and can now be found here:
Please follow the link and check that the image description page has all the information you meant to include.
If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
To insert this file into an article, you may want to use code similar to the following:
If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages.
Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page.
Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes.
Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.
Uploads locally to Wikipedia; must comply with the non-free content criteria
Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript.
Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again.
Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. 
In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed.
The filename you chose seems to be very short, or overly generic. Please don't use:
If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
This should not be done, except in very rare exceptional cases.
Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.
Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the Wikimedia Commons.
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
Please note that by "entirely self-made" we really mean just that. 
Do not use this section for any of the following:
Editors who falsely declare such items as their "own work" will be blocked from editing.
Use this only if there is an explicit licensing statement in the source. 
The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
If the source website doesn't say so explicitly, please do not upload the file.
Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it.
 Please remember that you will need to demonstrate that:
Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually.
Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
If this is an article you are only planning to write, please write it first and upload the file afterwards.
The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
Please upload this file only if it is going to be used in an actual article.
If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
The page Example is not a real article, but a disambiguation page pointing to a number of other pages.
Please check and enter the exact title of the actual target article you meant.
If neither of these two statements applies, then please do not upload this image.
This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
In view of this, please explain how the use of this file will be minimal.
Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you.
This is the data that will be submitted to upload:
This might take a minute or two, depending on the size of the file and the speed of your internet connection.
Once uploading is completed, you will find your new file at this link:
Your file has been uploaded successfully and can now be found here:
Please follow the link and check that the image description page has all the information you meant to include.
If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
To insert this file into an article, you may want to use code similar to the following:
If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages.
Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page.
This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators).
This is the current revision of this page, as edited by The Blade of the Northern Lights (talk | contribs) at 19:27, 5 October 2022 (Same issue, commented out text accidentally got split into two lines of markup). The present address (URL) is a permanent link to this version.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
Pages transcluded onto the current version of this page (help):
Please remember to check your manual of style, standards guide or instructor's guidelines for the exact syntax to suit your needs.  For more detailed advice, see Citing Wikipedia.
Wikipedia contributors. (2022, October 5). Main Page. In Wikipedia, The Free Encyclopedia. Retrieved 16:08, February 26, 2023, from https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180
Wikipedia contributors. "Main Page." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 5 Oct. 2022. Web. 26 Feb. 2023.
Wikipedia contributors, 'Main Page',  Wikipedia, The Free Encyclopedia, 5 October 2022, 19:27 UTC, <https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180> [accessed 26 February 2023]
Wikipedia contributors, "Main Page,"  Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180 (accessed February 26, 2023).
Wikipedia contributors. Main Page [Internet].  Wikipedia, The Free Encyclopedia;  2022 Oct 5, 19:27 UTC [cited 2023 Feb 26].  Available from: 
https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180.
Wikipedia contributors. Main Page. Wikipedia, The Free Encyclopedia. October 5, 2022, 19:27 UTC. Available at: https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180. Accessed February 26, 2023.
When using the LaTeX package url (\usepackage{url} somewhere in the preamble), which tends to give much more nicely formatted web addresses, the following may be preferred:
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
If you have a question related to the Main Page, please search the talk page archives first to check if it has previously been addressed:
001 002 003 004 005 006 007 008 009
010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099
100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206
To report an error in current or upcoming Main Page content, please add it to the appropriate section below.
Our friend the old style calendar again. She was born on 14 February 1869 old style.  That calendar was in use in Russia until 1918 so as per the guidelines at Wikipedia:Selected_anniversaries; "the event should have occurred on the day in question in the calendar in use at the time (per MOS:JG)", we should use the old style date - Dumelow (talk) 07:58, 24 February 2023 (UTC)Reply[reply]
Has been orange-tagged for almost a year for an update.  How much of an update you can give an article from 1600+ years ago, I don't know, but it should be resolved either way before main page appearance - Dumelow (talk) 06:47, 25 February 2023 (UTC)Reply[reply]
The date does not appear in the source cited - Dumelow (talk) 09:50, 26 February 2023 (UTC)Reply[reply]
Date of birth uncited - Dumelow (talk) 10:00, 26 February 2023 (UTC)Reply[reply]
"At the time, the chart was titled Best-Selling Popular Record Albums and was "based on reports received from more than 200 dealers" throughout the United States."
"That Bing Crosby's Merry Christmas "reached the top in December 1945 and peaked for two more weeks in January 1946, for a total of six consecutive weeks at number one." is largely uncited.  The table cites only the 1946 chart position - Dumelow (talk) 08:28, 23 February 2023 (UTC)Reply[reply]
"It again reached the top in late November for an additional five weeks, making it the longest reigning album of the year".  The table shows six entries for this song in November and December, so that's six weeks right? - Dumelow (talk) 08:29, 23 February 2023 (UTC)Reply[reply]
"The second longest-reigning album of the previous year, Glenn Miller, recorded by Glenn Miller & His Orchestra"
Overall, I think I can have the article itself fixed up and a proposal for fixes to the blurb by twelve hours from now. Should leave some time for an admin to make the update before Monday. Firefangledfeathers (talk / contribs) 17:25, 24 February 2023 (UTC)Reply[reply]
The hurricane animation is very distracting; almost like an advertisement. 
I would strongly urge the community to not turn Wikipedia's main page into some kind of Yahoo portal. Please keep it static. Yes, some kids may consider this "boring". I consider it a prerequisite for "informative". CapnZapp (talk) 06:29, 17 February 2023 (UTC)Reply[reply]
I'm more concerned with its copyright status, which I've raised on the Commons talk page. --Paul_012 (talk) 04:05, 18 February 2023 (UTC)Reply[reply]
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
You do not have permission to edit this page, for the following reasons:
Pages transcluded onto the current version of this page (help):
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
You do not have permission to edit this page, for the following reasons:
Pages transcluded onto the current version of this page (help):
Wikipedia[note 3] is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.[3] It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2022,[update] Wikipedia was ranked the 5th most popular site in the world.[4] It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.[5]
Wikipedia has been praised for its enablement of the democratization of knowledge, extent of coverage, unique structure, culture, and reduced degree of commercial bias. It has been criticized for exhibiting systemic bias, particularly gender bias against women and alleged ideological bias.[12][13] The reliability of Wikipedia was frequently criticized in the 2000s, but has improved over time, as Wikipedia has been generally praised in the late 2010s and early 2020s.[3][12][14] The website's coverage of controversial topics such as American politics and major events like the COVID-19 pandemic and the Russian invasion of Ukraine has received substantial media attention.[15][16][17] It has been censored by world governments, ranging from specific pages to the entire site.[18][19] On 3 April 2018, Facebook and YouTube announced that they would help users detect fake news by suggesting fact-checking links to related Wikipedia articles.[20][21] Articles on breaking news are often accessed as a source of frequently updated information about those events.[22][23]
Various collaborative online encyclopedias were attempted before the start of Wikipedia, but with limited success.[24] Wikipedia began as a complementary project for Nupedia, a free online English-language encyclopedia project whose articles were written by experts and reviewed under a formal process.[25] It was founded on March 9, 2000, under the ownership of Bomis, a web portal company. Its main figures were Bomis CEO Jimmy Wales and Larry Sanger, editor-in-chief for Nupedia and later Wikipedia.[1][26] Nupedia was initially licensed under its own Nupedia Open Content License, but before Wikipedia was founded, Nupedia switched to the GNU Free Documentation License at the urging of Richard Stallman.[27] Wales is credited with defining the goal of making a publicly editable encyclopedia,[28][29] while Sanger is credited with the strategy of using a wiki to reach that goal.[30] On January 10, 2001, Sanger proposed on the Nupedia mailing list to create a wiki as a "feeder" project for Nupedia.[31]
The domains wikipedia.com (later redirecting to wikipedia.org) and wikipedia.org were registered on January 12, 2001,[32] and January 13, 2001,[33] respectively, and Wikipedia was launched on January 15, 2001[25] as a single English-language edition at www.wikipedia.com,[34] and announced by Sanger on the Nupedia mailing list.[28] Its integral policy of "neutral point-of-view"[35] was codified in its first few months. Otherwise, there were initially relatively few rules, and it operated independently of Nupedia.[28] Bomis originally intended it as a business for profit.[36]
Citing fears of commercial advertising and lack of control, users of the Spanish Wikipedia forked from Wikipedia to create Enciclopedia Librecode: spa promoted to code: es  in February 2002.[42] Wales then announced that Wikipedia would not display advertisements, and changed Wikipedia's domain from wikipedia.com to wikipedia.org.[43][44]
In November 2009, a researcher at the Rey Juan Carlos University in Madrid, Spain found that the English Wikipedia had lost 49,000 editors during the first three months of 2009; in comparison, it lost only 4,900 editors during the same period in 2008.[51][52] The Wall Street Journal cited the array of rules applied to editing and disputes related to such content among the reasons for this trend.[53] Wales disputed these claims in 2009, denying the decline and questioning the study's methodology.[54] Two years later, in 2011, he acknowledged a slight decline, noting a decrease from "a little more than 36,000 writers" in June 2010 to 35,800 in June 2011. In the same interview, he also claimed the number of editors was "stable and sustainable".[55] A 2013 MIT Technology Review article, "The Decline of Wikipedia", questioned this claim, revealing that since 2007, Wikipedia had lost a third of its volunteer editors, and that those remaining had focused increasingly on minutiae.[56] In July 2012, The Atlantic reported that the number of administrators was also in decline.[57] In the November 25, 2013, issue of New York magazine, Katherine Ward stated, "Wikipedia, the sixth-most-used website, is facing an internal crisis."[58]
The number of active English Wikipedia editors has since remained steady after a long period of decline.[59][60]
On January 20, 2014, Subodh Varma reporting for The Economic Times indicated that not only had Wikipedia's growth stalled, it "had lost nearly ten percent of its page views last year. There was a decline of about two billion between December 2012 and December 2013. Its most popular versions are leading the slide: page-views of the English Wikipedia declined by twelve percent, those of German version slid by 17 percent and the Japanese version lost nine percent."[70] Varma added, "While Wikipedia's managers think that this could be due to errors in counting, other experts feel that Google's Knowledge Graphs project launched last year may be gobbling up Wikipedia users."[70] When contacted on this matter, Clay Shirky, associate professor at New York University and fellow at Harvard's Berkman Klein Center for Internet & Society said that he suspected much of the page-view decline was due to Knowledge Graphs, stating, "If you can get your question answered from the search page, you don't need to click [any further]."[70] By the end of December 2016, Wikipedia was ranked the fifth most popular website globally.[71]
As of January 2023, 55,791 English Wikipedia articles have been cited 92,300 times in scholarly journals,[78] from which cloud computing was the most cited page.[79]
On January 18, 2023, Wikipedia debuted a new website redesign, called 'Vector 2022".[80][81] It featured a redesigned menu bar, moving the table of contents to the left as a sidebar, and numerous changes in the locations of buttons like the language selection tool.[81][82] The update initially received backlash, most notably when editors of the Swahili Wikipedia unanimously voted to revert the changes.[80][83]
Unlike traditional encyclopedias, Wikipedia follows the procrastination principle regarding the security of its content, meaning that it waits until a problem arises to fix it.[84]
Due to Wikipedia's increasing popularity, some editions, including the English version, have introduced editing restrictions for certain cases. For instance, on the English Wikipedia and some other language editions, only registered users may create a new article.[85] On the English Wikipedia, among others, particularly controversial, sensitive, or vandalism-prone pages have been protected to varying degrees.[86][87] A frequently vandalized article can be "semi-protected" or "extended confirmed protected", meaning that only "autoconfirmed" or "extended confirmed" editors can modify it.[88] A particularly contentious article may be locked so that only administrators can make changes.[89] A 2021 article in the Columbia Journalism Review identified Wikipedia's page-protection policies as "perhaps the most important" means at its disposal to "regulate its market of ideas".[90]
In certain cases, all editors are allowed to submit modifications, but review is required for some editors, depending on certain conditions. For example, the German Wikipedia maintains "stable versions" of articles which have passed certain reviews.[91] Following protracted trials and community discussion, the English Wikipedia introduced the "pending changes" system in December 2012.[92] Under this system, new and unregistered users' edits to certain controversial or vandalism-prone articles are reviewed by established users before they are published.[93]
Although changes are not systematically reviewed, the software that powers Wikipedia provides tools allowing anyone to review changes made by others. Each article's History page links to each revision.[note 4][94] On most articles, anyone can undo others' changes by clicking a link on the article's History page. Anyone can view the latest changes to articles, and anyone registered may maintain a "watchlist" of articles that interest them so they can be notified of changes.[95] "New pages patrol" is a process where newly created articles are checked for obvious problems.[96]
In 2003, economics PhD student Andrea Ciffolilli argued that the low transaction costs of participating in a wiki created a catalyst for collaborative development, and that features such as allowing easy access to past versions of a page favored "creative construction" over "creative destruction".[97]
Any change or edit that manipulates content in a way that deliberately compromises Wikipedia's integrity is considered vandalism. The most common and obvious types of vandalism include additions of obscenities and crude humor; it can also include advertising and other types of spam.[98] Sometimes editors commit vandalism by removing content or entirely blanking a given page. Less common types of vandalism, such as the deliberate addition of plausible but false information, can be more difficult to detect. Vandals can introduce irrelevant formatting, modify page semantics such as the page's title or categorization, manipulate the article's underlying code, or use images disruptively.[99]
Obvious vandalism is generally easy to remove from Wikipedia articles; the median time to detect and fix it is a few minutes.[100][101] However, some vandalism takes much longer to detect and repair.[102]
In the Seigenthaler biography incident, an anonymous editor introduced false information into the biography of American political figure John Seigenthaler in May 2005, falsely presenting him as a suspect in the assassination of John F. Kennedy.[102] It remained uncorrected for four months.[102] Seigenthaler, the founding editorial director of USA Today and founder of the Freedom Forum First Amendment Center at Vanderbilt University, called Wikipedia co-founder Jimmy Wales and asked whether he had any way of knowing who contributed the misinformation. Wales said he did not, although the perpetrator was eventually traced.[103][104] After the incident, Seigenthaler described Wikipedia as "a flawed and irresponsible research tool".[102] The incident led to policy changes at Wikipedia for tightening up the verifiability of biographical articles of living people.[105]
In 2010, Daniel Tosh encouraged viewers of his show, Tosh.0, to visit the show's Wikipedia article and edit it at will. On a later episode, he commented on the edits to the article, most of them offensive, which had been made by the audience and had prompted the article to be locked from editing.[106][107]
Wikipedians often have disputes regarding content, which may result in repeated competing changes to an article, known as "edit warring".[108][109] It is widely seen as a resource-consuming scenario where no useful knowledge is added,[110] and criticized as creating a competitive[111] and conflict-based editing culture associated with traditional masculine gender roles.[112][113]
Content in Wikipedia is subject to the laws (in particular, copyright laws) of the United States and of the US state of Virginia, where the majority of Wikipedia's servers are located.[114][115] By using the site, one agrees to the Wikimedia Foundation Terms of Use and Privacy Policy; some of the main rules are that contributors are legally responsible for their edits and contributions, that they should follow the policies that govern each of the independent project editions, and they may not engage in activities, whether legal or illegal, that may be harmful to other users.[116][117] In addition to the terms, the Foundation has developed policies, described as the "official policies of the Wikimedia Foundation".[118]
The editorial principles of the Wikipedia community are embodied in the "Five pillars" and in numerous policies and guidelines intended to appropriately shape content.[119] The rules developed by the community are stored in wiki form, and Wikipedia editors write and revise the website's policies and guidelines.[120] Editors can enforce the rules by deleting or modifying non-compliant material.[121] Originally, rules on the non-English editions of Wikipedia were based on a translation of the rules for the English Wikipedia. They have since diverged to some extent.[91]
According to the rules on the English Wikipedia community, each entry in Wikipedia must be about a topic that is encyclopedic and is not a dictionary entry or dictionary-style.[122] A topic should also meet Wikipedia's standards of "notability", which generally means that the topic must have been covered in mainstream media or major academic journal sources that are independent of the article's subject.[123] Further, Wikipedia intends to convey only knowledge that is already established and recognized.[124] It must not present original research.[125] A claim that is likely to be challenged requires a reference to a reliable source, as do all quotations.[122] Among Wikipedia editors, this is often phrased as "verifiability, not truth" to express the idea that the readers, not the encyclopedia, are ultimately responsible for checking the truthfulness of the articles and making their own interpretations.[126] This can at times lead to the removal of information that, though valid, is not properly sourced.[127] Finally, Wikipedia must not take sides.[128]
Wikipedia's initial anarchy integrated democratic and hierarchical elements over time.[129][130] An article is not considered to be owned by its creator or any other editor, nor by the subject of the article.[131]
Editors in good standing in the community can request extra user rights, granting them the technical ability to perform certain special actions. In particular, editors can choose to run for "adminship",[132] which includes the ability to delete pages or prevent them from being changed in cases of severe vandalism or editorial disputes.[133] Administrators are not supposed to enjoy any special privilege in decision-making; instead, their powers are mostly limited to making edits that have project-wide effects and thus are disallowed to ordinary editors, and to implement restrictions intended to prevent disruptive editors from making unproductive edits.[133]
By 2012, fewer editors were becoming administrators compared to Wikipedia's earlier years, in part because the process of vetting potential administrators had become more rigorous.[134] In 2022, there was a particularly contentious request for adminship over the candidate's anti-Trump views; ultimately, they were granted adminship.[135]
Over time, Wikipedia has developed a semiformal dispute resolution process. To determine community consensus, editors can raise issues at appropriate community forums, seek outside input through third opinion requests, or initiate a more general community discussion known as a "request for comment".[136]
The Arbitration Committee presides over the ultimate dispute resolution process. Although disputes usually arise from a disagreement between two opposing views on how an article should read, the Arbitration Committee explicitly refuses to directly rule on the specific view that should be adopted.[138] Statistical analyses suggest that the committee ignores the content of disputes and rather focuses on the way disputes are conducted,[139] functioning not so much to resolve disputes and make peace between conflicting editors, but to weed out problematic editors while allowing potentially productive editors back in to participate.[138] Therefore, the committee does not dictate the content of articles, although it sometimes condemns content changes when it deems the new content violates Wikipedia policies (for example, if the new content is considered biased).[note 5] Commonly used solutions include cautions and probations (used in 63% of cases) and banning editors from articles (43%), subject matters (23%), or Wikipedia (16%).[138] Complete bans from Wikipedia are generally limited to instances of impersonation and anti-social behavior.[140] When conduct is not impersonation or anti-social, but rather edit warring and other violations of editing policies, solutions tend to be limited to warnings.[138]
Each article and each user of Wikipedia has an associated and dedicated "talk" page. These form the primary communication channel for editors to discuss, coordinate and debate.[141]
Wikipedia's community has been described as cultlike,[142] although not always with entirely negative connotations.[143] Its preference for cohesiveness, even if it requires compromise that includes disregard of credentials, has been referred to as "anti-elitism".[144]
The English Wikipedia has 6,623,857 articles, 45,088,151 registered editors, and 129,357 active editors. An editor is considered active if they have made one or more edits in the past 30 days.[150]
Editors who fail to comply with Wikipedia cultural rituals, such as signing talk page comments, may implicitly signal that they are Wikipedia outsiders, increasing the odds that Wikipedia insiders may target or discount their contributions. Becoming a Wikipedia insider involves non-trivial costs: the contributor is expected to learn Wikipedia-specific technological codes, submit to a sometimes convoluted dispute resolution process, and learn a "baffling culture rich with in-jokes and insider references".[151] Editors who do not log in are in some sense second-class citizens on Wikipedia,[151] as "participants are accredited by members of the wiki community, who have a vested interest in preserving the quality of the work product, on the basis of their ongoing participation",[152] but the contribution histories of anonymous unregistered editors recognized only by their IP addresses cannot be attributed to a particular editor with certainty.[152]
A 2008 study found that Wikipedians were less agreeable, open, and conscientious than others,[154] although a later commentary pointed out serious flaws, including that the data showed higher openness and that the differences with the control group and the samples were small.[155] According to a 2009 study, there is "evidence of growing resistance from the Wikipedia community to new content".[156]
Several studies have shown that most Wikipedia contributors are male. Notably, the results of a Wikimedia Foundation survey in 2008 showed that only 13 percent of Wikipedia editors were female.[157] Because of this, universities throughout the United States tried to encourage women to become Wikipedia contributors.[158] Similarly, many of these universities, including Yale and Brown, gave college credit to students who create or edit an article relating to women in science or technology.[158] Andrew Lih, a professor and scientist, said that the reason he thought the number of male contributors outnumbered the number of females so greatly was because identifying as a woman may expose oneself to "ugly, intimidating behavior".[citation needed][159] Data has shown that Africans are underrepresented among Wikipedia editors.[160]
Distribution of the 60,598,804 articles in different language editions (as of February 26, 2023)[161]
There are currently 329 language editions of Wikipedia (also called language versions, or simply Wikipedias). As of February 2023, the six largest, in order of article count, are the English, Cebuano, German, Swedish, French, and Dutch Wikipedias.[162] The second and fourth-largest Wikipedias owe their position to the article-creating bot Lsjbot, which as of 2013[update] had created about half the articles on the Swedish Wikipedia, and most of the articles in the Cebuano and Waray Wikipedias. The latter are both languages of the Philippines.
Since Wikipedia is based on the Web and therefore worldwide, contributors to the same language edition may use different dialects or may come from different countries (as is the case for the English edition). These differences may lead to some conflicts over spelling differences (e.g. colour versus color)[166] or points of view.[167]
Though the various language editions are held to global policies such as "neutral point of view", they diverge on some points of policy and practice, most notably on whether images that are not licensed freely may be used under a claim of fair use.[168][169]
Jimmy Wales has described Wikipedia as "an effort to create and distribute a free encyclopedia of the highest possible quality to every single person on the planet in their own language".[170] Though each language edition functions more or less independently, some efforts are made to supervise them all. They are coordinated in part by Meta-Wiki, the Wikimedia Foundation's wiki devoted to maintaining all its projects (Wikipedia and others).[171] For instance, Meta-Wiki provides important statistics on all language editions of Wikipedia,[172] and it maintains a list of articles every Wikipedia should have.[173] The list concerns basic content by subject: biography, history, geography, society, culture, science, technology, and mathematics.[173] It is not rare for articles strongly related to a particular language not to have counterparts in another edition. For example, articles about small towns in the United States might be available only in English, even when they meet the notability criteria of other language Wikipedia projects.[123]
Translated articles represent only a small portion of articles in most editions, in part because those editions do not allow fully automated translation of articles. Articles available in more than one language may offer "interwiki links", which link to the counterpart articles in other editions.[175][176]
A study published by PLOS One in 2012 also estimated the share of contributions to different editions of Wikipedia from different regions of the world. It reported that the proportion of the edits made from North America was 51% for the English Wikipedia, and 25% for the simple English Wikipedia.[174]
On March 1, 2014, The Economist, in an article titled "The Future of Wikipedia", cited a trend analysis concerning data published by the Wikimedia Foundation stating that "[t]he number of editors for the English-language version has fallen by a third in seven years."[177] The attrition rate for active editors in English Wikipedia was cited by The Economist as substantially in contrast to statistics for Wikipedia in other languages (non-English Wikipedia). The Economist reported that the number of contributors with an average of five or more edits per month was relatively constant since 2008 for Wikipedia in other languages at approximately 42,000 editors within narrow seasonal variances of about 2,000 editors up or down. The number of active editors in English Wikipedia, by sharp comparison, was cited as peaking in 2007 at approximately 50,000 and dropping to 30,000 by the start of 2014.[177]
In contrast, the trend analysis for Wikipedia in other languages (non-English Wikipedia) shows success in retaining active editors on a renewable and sustained basis, with their numbers remaining relatively constant at approximately 42,000. No comment was made concerning which of the differentiated edit policy standards from Wikipedia in other languages (non-English Wikipedia) would provide a possible alternative to English Wikipedia for effectively improving substantial editor attrition rates on the English-language Wikipedia.[177]
Various Wikipedians have criticized Wikipedia's large and growing regulation, which includes more than fifty policies and nearly 150,000 words as of 2014.[update][178][137]
Critics have stated that Wikipedia exhibits systemic bias. In 2010, columnist and journalist Edwin Black described Wikipedia as being a mixture of "truth, half-truth, and some falsehoods".[179] Articles in The Chronicle of Higher Education and The Journal of Academic Librarianship have criticized Wikipedia's "Undue Weight" policy, concluding that Wikipedia explicitly is not designed to provide correct information about a subject, but rather focus on all the major viewpoints on the subject, give less attention to minor ones, and creates omissions that can lead to false beliefs based on incomplete information.[180][181][182]
Journalists Oliver Kamm and Edwin Black alleged (in 2010 and 2011 respectively) that articles are dominated by the loudest and most persistent voices, usually by a group with an "ax to grind" on the topic.[179][183] A 2008 article in Education Next Journal concluded that as a resource about controversial topics, Wikipedia is subject to manipulation and spin.[184]
In 2020, Omer Benjakob and Stephen Harrison noted that "Media coverage of Wikipedia has radically shifted over the past two decades: once cast as an intellectual frivolity, it is now lauded as the 'last bastion of shared reality' online."[185]
Multiple news networks and pundits have accused Wikipedia of being ideologically biased. In February 2021, Fox News accused Wikipedia of whitewashing communism and socialism and having too much "leftist bias".[186] In 2022, libertarian John Stossel opined that Wikipedia, a site he financially supported at one time, appeared to have gradually taken a significant turn in bias to the political left, specifically on political topics.[187]
As a consequence of the open structure, Wikipedia "makes no guarantee of validity" of its content, since no one is ultimately responsible for any claims appearing in it.[196] Concerns have been raised by PC World in 2009 regarding the lack of accountability that results from users' anonymity,[197] the insertion of false information,[198] vandalism, and similar problems.
Wikipedia's open structure inherently makes it an easy target for Internet trolls, spammers, and various forms of paid advocacy seen as counterproductive to the maintenance of a neutral and verifiable online encyclopedia.[94][207]
In response to paid advocacy editing and undisclosed editing issues, Wikipedia was reported in an article in The Wall Street Journal to have strengthened its rules and laws against undisclosed editing.[208] The article stated that: "Beginning Monday [from the date of the article, June 16, 2014], changes in Wikipedia's terms of use will require anyone paid to edit articles to disclose that arrangement. Katherine Maher, the nonprofit Wikimedia Foundation's chief communications officer, said the changes address a sentiment among volunteer editors that, 'we're not an advertising service; we're an encyclopedia.'"[208][209][210][211][212] These issues, among others, had been parodied since the first decade of Wikipedia, notably by Stephen Colbert on The Colbert Report.[213]
Legal Research in a Nutshell (2011), cites Wikipedia as a "general source" that "can be a real boon" in "coming up to speed in the law governing a situation" and, "while not authoritative, can provide basic facts as well as leads to more in-depth resources".[214]
Some university lecturers discourage students from citing any encyclopedia in academic work, preferring primary sources;[215] some specifically prohibit Wikipedia citations.[216][217] Wales stresses that encyclopedias of any type are not usually appropriate to use as citable sources, and should not be relied upon as authoritative.[218] Wales once (2006 or earlier) said he receives about ten emails weekly from students saying they got failing grades on papers because they cited Wikipedia; he told the students they got what they deserved. "For God's sake, you're in college; don't cite the encyclopedia", he said.[219][220]
In February 2007, an article in The Harvard Crimson newspaper reported that a few of the professors at Harvard University were including Wikipedia articles in their syllabi, although without realizing the articles might change.[221] In June 2007, former president of the American Library Association Michael Gorman condemned Wikipedia, along with Google, stating that academics who endorse the use of Wikipedia are "the intellectual equivalent of a dietitian who recommends a steady diet of Big Macs with everything".[222]
Contrarily, a 2016 article in the Universal Journal of Educational Research argued that "Wikipedia can be used for serious student projects..." and that Wikipedia is a good place to learn academic writing styles.[223] A 2020 research study published in Studies in Higher Education argued that Wikipedia could be applied in the higher education "flipped classroom", an educational model where students learn before coming to class and apply it in classroom activities. The experimental group was instructed to learn before class and get immediate feedback before going in (the flipped classroom model), while the control group was given direct instructions in class (the conventional classroom model). The groups were then instructed to collaboratively develop Wikipedia entries, which would be graded in quality after the study. The results showed that the experimental group yielded more Wikipedia entries and received higher grades in quality. The study concluded that learning with Wikipedia in flipped classrooms was more effective than in conventional classrooms, proving that Wikipedia could be used as an educational tool in higher education.[224]
On March 5, 2014, Julie Beck writing for The Atlantic magazine in an article titled "Doctors' #1 Source for Healthcare Information: Wikipedia", stated that "Fifty percent of physicians look up conditions on the (Wikipedia) site, and some are editing articles themselves to improve the quality of available information."[225] Beck continued to detail in this article new programs of Amin Azzam at the University of San Francisco to offer medical school courses to medical students for learning to edit and improve Wikipedia articles on health-related issues, as well as internal quality control programs within Wikipedia organized by James Heilman to improve a group of 200 health-related articles of central medical importance up to Wikipedia's highest standard of articles using its Featured Article and Good Article peer-review evaluation process.[225] In a May 7, 2014 follow-up article in The Atlantic titled "Can Wikipedia Ever Be a Definitive Medical Text?", Julie Beck quotes WikiProject Medicine's James Heilman as stating: "Just because a reference is peer-reviewed doesn't mean it's a high-quality reference."[226] Beck added that: "Wikipedia has its own peer review process before articles can be classified as 'good' or 'featured'. Heilman, who has participated in that process before, says 'less than one percent' of Wikipedia's medical articles have passed."[226]
Wikipedia seeks to create a summary of all human knowledge in the form of an online encyclopedia, with each topic covered encyclopedically in one article. Since it has terabytes of disk space, it can have far more topics than can be covered by any printed encyclopedia.[227] The exact degree and manner of coverage on Wikipedia is under constant review by its editors, and disagreements are not uncommon (see deletionism and inclusionism).[228][229] Wikipedia contains materials that some people may find objectionable, offensive, or pornographic.[230] The "Wikipedia is not censored" policy has sometimes proved controversial: in 2008, Wikipedia rejected an online petition against the inclusion of images of Muhammad in the English edition of its Muhammad article, citing this policy.[231] The presence of politically, religiously, and pornographically sensitive materials in Wikipedia has led to the censorship of Wikipedia by national authorities in China[232] and Pakistan,[233] amongst other countries.[234][235][236]
A 2008 study conducted by researchers at Carnegie Mellon University and Palo Alto Research Center gave a distribution of topics as well as growth (from July 2006 to January 2008) in each field:[237]
These numbers refer only to the number of articles: it is possible for one topic to contain a large number of short articles and another to contain a small number of large ones. Through its "Wikipedia Loves Libraries" program, Wikipedia has partnered with major public libraries such as the New York Public Library for the Performing Arts to expand its coverage of underrepresented subjects and articles.[238]
A 2011 study conducted by researchers at the University of Minnesota indicated that male and female editors focus on different coverage topics. There was a greater concentration of females in the "people and arts" category, while males focus more on "geography and science".[239]
Research conducted by Mark Graham of the Oxford Internet Institute in 2009 indicated that the geographic distribution of article topics is highly uneven, Africa being the most underrepresented.[240] Across 30 language editions of Wikipedia, historical articles and sections are generally Eurocentric and focused on recent events.[241]
An editorial in The Guardian in 2014 claimed that more effort went into providing references for a list of female porn actors than a list of women writers.[242] Data has also shown that Africa-related material often faces omission; a knowledge gap that a July 2018 Wikimedia conference in Cape Town sought to address.[160]
When multiple editors contribute to one topic or set of topics, systemic bias may arise, due to the demographic backgrounds of the editors. In 2011, Wales claimed that the unevenness of coverage is a reflection of the demography of the editors, citing for example "biographies of famous women through history and issues surrounding early childcare".[55] The October 22, 2013, essay by Tom Simonite in MIT's Technology Review titled "The Decline of Wikipedia" discussed the effect of systemic bias and policy creep on the downward trend in the number of editors.[56]
Taha Yasseri of the University of Oxford, in 2013, studied the statistical trends of systemic bias at Wikipedia introduced by editing conflicts and their resolution.[243][244] His research examined the counterproductive work behavior of edit warring. Yasseri contended that simple reverts or "undo" operations were not the most significant measure of counterproductive behavior at Wikipedia and relied instead on the statistical measurement of detecting "reverting/reverted pairs" or "mutually reverting edit pairs". Such a "mutually reverting edit pair" is defined where one editor reverts the edit of another editor who then, in sequence, returns to revert the first editor in the "mutually reverting edit pairs". The results were tabulated for several language versions of Wikipedia. The English Wikipedia's three largest conflict rates belonged to the articles George W. Bush, anarchism, and Muhammad.[244] By comparison, for the German Wikipedia, the three largest conflict rates at the time of the Oxford study were for the articles covering Croatia, Scientology, and 9/11 conspiracy theories.[244]
Researchers from Washington University in St. Louis developed a statistical model to measure systematic bias in the behavior of Wikipedia's users regarding controversial topics. The authors focused on behavioral changes of the encyclopedia's administrators after assuming the post, writing that systematic bias occurred after the fact.[245][246]
Wikipedia has been criticized for allowing information about graphic content.[247] Articles depicting what some critics have called objectionable content (such as feces, cadaver, human penis, vulva, and nudity) contain graphic pictures and detailed information easily available to anyone with access to the internet, including children.[248]
The site also includes sexual content such as images and videos of masturbation and ejaculation, illustrations of zoophilia, and photos from hardcore pornographic films in its articles. It also has non-sexual photographs of nude children.[249]
In April 2010, Sanger wrote a letter to the Federal Bureau of Investigation, outlining his concerns that two categories of images on Wikimedia Commons contained child pornography, and were in violation of US federal obscenity law.[251][252] Sanger later clarified that the images, which were related to pedophilia and one about lolicon, were not of real children, but said that they constituted "obscene visual representations of the sexual abuse of children", under the PROTECT Act of 2003.[253] That law bans photographic child pornography and cartoon images and drawings of children that are obscene under American law.[253] Sanger also expressed concerns about access to the images on Wikipedia in schools.[254] Wikimedia Foundation spokesman Jay Walsh strongly rejected Sanger's accusation,[255] saying that Wikipedia did not have "material we would deem to be illegal. If we did, we would remove it."[255] Following the complaint by Sanger, Wales deleted sexual images without consulting the community. After some editors who volunteered to maintain the site argued that the decision to delete had been made hastily, Wales voluntarily gave up some of the powers he had held up to that time as part of his co-founder status. He wrote in a message to the Wikimedia Foundation mailing-list that this action was "in the interest of encouraging this discussion to be about real philosophical/content issues, rather than be about me and how quickly I acted".[256] Critics, including Wikipediocracy, noticed that many of the pornographic images deleted from Wikipedia since 2010 have reappeared.[257]
In January 2006, a German court ordered the German Wikipedia shut down within Germany because it stated the full name of Boris Floricic, aka "Tron", a deceased hacker. On February 9, 2006, the injunction against Wikimedia Deutschland was overturned, with the court rejecting the notion that Tron's right to privacy or that of his parents was being violated.[260]
Wikipedia has a "Volunteer Response Team" that uses Znuny, a free and open-source software fork of OTRS[261] to handle queries without having to reveal the identities of the involved parties. This is used, for example, in confirming the permission for using individual images and other media in the project.[262]
Wikipedia was described in 2015 as harboring a battleground culture of sexism and harassment.[263][264] The perceived toxic attitudes and tolerance of violent and abusive language were reasons put forth in 2013 for the gender gap in Wikipedia editorship.[265] Edit-a-thons have been held to encourage female editors and increase the coverage of women's topics.[266]
A comprehensive 2008 survey, published in 2016, by Julia B. Bear of Stony Brook University's College of Business and Benjamin Collier of Carnegie Mellon University found significant gender differences in confidence in expertise, discomfort with editing, and response to critical feedback. "Women reported less confidence in their expertise, expressed greater discomfort with editing (which typically involves conflict), and reported more negative responses to critical feedback compared to men."[271]
Maher served as executive director until April 2021.[281] Maryana Iskander was named the incoming CEO in September 2021, and took over that role in January 2022. She stated that one of her focuses would be increasing diversity in the Wikimedia community.[282]
The operation of Wikipedia depends on MediaWiki, a custom-made, free and open source wiki software platform written in PHP and built upon the MySQL database system.[284] The software incorporates programming features such as a macro language, variables, a transclusion system for templates, and URL redirection.[285] MediaWiki is licensed under the GNU General Public License (GPL) and it is used by all Wikimedia projects, as well as many other wiki projects.[284][286] Originally, Wikipedia ran on UseModWiki written in Perl by Clifford Adams (Phase I), which initially required CamelCase for article hyperlinks; the present double bracket style was incorporated later.[287] Starting in January 2002 (Phase II), Wikipedia began running on a PHP wiki engine with a MySQL database; this software was custom-made for Wikipedia by Magnus Manske. The Phase II software was repeatedly modified to accommodate the exponentially increasing demand. In July 2002 (Phase III), Wikipedia shifted to the third-generation software, MediaWiki, originally written by Lee Daniel Crocker.
Several MediaWiki extensions are installed to extend the functionality of the MediaWiki software.[288]
In April 2005, a Lucene extension[289][290] was added to MediaWiki's built-in search and Wikipedia switched from MySQL to Lucene for searching. Lucene was later replaced by CirrusSearch which is based on Elasticsearch.[291]
In July 2013, after extensive beta testing, a WYSIWYG (What You See Is What You Get) extension, VisualEditor, was opened to public use.[292][293][294] It was met with much rejection and criticism, and was described as "slow and buggy".[295] The feature was changed from opt-out to opt-in afterward.[296]
Computer programs called bots have often been used to perform simple and repetitive tasks, such as correcting common misspellings and stylistic issues, or to start articles such as geography entries in a standard format from statistical data.[297][298][299] One controversial contributor, Sverker Johansson, created articles with his bot Lsjbot, which was reported to create up to 10,000 articles on the Swedish Wikipedia on certain days.[300] Additionally, there are bots designed to automatically notify editors when they make common editing errors (such as unmatched quotes or unmatched parentheses).[301] Edits falsely identified by bots as the work of a banned editor can be restored by other editors. An anti-vandal bot is programmed to detect and revert vandalism quickly.[298] Bots are able to indicate edits from particular accounts or IP address ranges, as occurred at the time of the shooting down of the MH17 jet incident in July 2014 when it was reported that edits were made via IPs controlled by the Russian government.[302] Bots on Wikipedia must be approved before activation.[303]
According to Andrew Lih, the current expansion of Wikipedia to millions of articles would be difficult to envision without the use of such bots.[304]
As of 2021,[update] page requests are first passed to a front-end layer of Varnish caching servers and back-end layer caching is done by Apache Traffic Server.[305] Requests that cannot be served from the Varnish cache are sent to load-balancing servers running the Linux Virtual Server software, which in turn pass them to one of the Apache web servers for page rendering from the database.[305] The web servers deliver pages as requested, performing page rendering for all the language editions of Wikipedia. To increase speed further, rendered pages are cached in a distributed memory cache until invalidated, allowing page rendering to be skipped entirely for most common page accesses.[306]
Multiple Wikimedia projects have internal news publications. Wikimedia's online newspaper The Signpost was founded in 2005 by Michael Snow, a Wikipedia administrator who would join the Wikimedia Foundation's board of trustees in 2008.[317][318] The publication covers news and events from the English Wikipedia, the Wikimedia Foundation, and Wikipedia's sister projects.[319] Other past and present community news publications on English Wikipedia include the Wikiworld webcomic,[320] the Wikipedia Weekly podcast,[321] and newsletters of specific WikiProjects like The Bugle from WikiProject Military History[322] and the monthly newsletter from The Guild of Copy Editors.[323] There are also several publications from the Wikimedia Foundation and multilingual publications such as Wikimedia Diff[324] and This Month in Education.[325]
When the project was started in 2001, all text in Wikipedia was covered by the GNU Free Documentation License (GFDL), a copyleft license permitting the redistribution, creation of derivative works, and commercial use of content while authors retain copyright of their work.[329] The GFDL was created for software manuals that come with free software programs licensed under the GPL. This made it a poor choice for a general reference work: for example, the GFDL requires the reprints of materials from Wikipedia to come with a full copy of the GFDL text.[330] In December 2002, the Creative Commons license was released; it was specifically designed for creative works in general, not just for software manuals. The Wikipedia project sought the switch to the Creative Commons.[331] Because the GFDL and Creative Commons were incompatible, in November 2008, following the request of the project, the Free Software Foundation (FSF) released a new version of the GFDL designed specifically to allow Wikipedia to relicense its content to CC BY-SA by August 1, 2009.[332] In April 2009, Wikipedia and its sister projects held a community-wide referendum which decided the switch in June 2009.[333][334][335][336]
The handling of media files (e.g. image files) varies across language editions. Some language editions, such as the English Wikipedia, include non-free image files under fair use doctrine,[337] while the others have opted not to, in part because of the lack of fair use doctrines in their home countries (e.g. in Japanese copyright law). Media files covered by free content licenses (e.g. Creative Commons' CC BY-SA) are shared across language editions via Wikimedia Commons repository, a project operated by the Wikimedia Foundation.[338] Wikipedia's accommodation of varying international copyright laws regarding images has led some to observe that its photographic coverage of topics lags behind the quality of the encyclopedic text.[339]
The Wikimedia Foundation is not a licensor of content on Wikipedia or its related projects but merely a hosting service for contributors to and licensors of Wikipedia, a position which was successfully defended in 2004 in a court in France.[340][341]
Because Wikipedia content is distributed under an open license, anyone can reuse or re-distribute it at no charge.[342] The content of Wikipedia has been published in many forms, both online and offline, outside the Wikipedia website.
Thousands of "mirror sites" exist that republish content from Wikipedia; two prominent ones that also include content from other reference sources are Reference.com and Answers.com.[343][344] Another example is Wapedia, which began to display Wikipedia content in a mobile-device-friendly format before Wikipedia itself did.[345] Some web search engines make special use of Wikipedia content when displaying search results: examples include Microsoft Bing (via technology gained from Powerset)[346] and DuckDuckGo.
Collections of Wikipedia articles have been published on optical discs. An English version released in 2006 contained about 2,000 articles.[347] The Polish-language version from 2006 contains nearly 240,000 articles,[348] the German-language version from 2007/2008 contains over 620,000 articles,[349] and the Spanish-language version from 2011 contains 886,000 articles.[350] Additionally, "Wikipedia for Schools", the Wikipedia series of CDs / DVDs produced by Wikipedia and SOS Children, is a free selection from Wikipedia designed for education towards children eight to seventeen.[351]
There have been efforts to put a select subset of Wikipedia's articles into printed book form.[352][353] Since 2009, tens of thousands of print-on-demand books that reproduced English, German, Russian, and French Wikipedia articles have been produced by the American company Books LLC and by three Mauritian subsidiaries of the German publisher VDM.[354]
Obtaining the full contents of Wikipedia for reuse presents challenges, since direct cloning via a web crawler is discouraged.[359] Wikipedia publishes "dumps" of its contents, but these are text-only; as of 2023,[update] there is no dump available of Wikipedia's images.[360] Wikimedia Enterprise is a for-profit solution to this.[361]
Several languages of Wikipedia also maintain a reference desk, where volunteers answer questions from the general public. According to a study by Pnina Shachaf in the Journal of Documentation, the quality of the Wikipedia reference desk is comparable to a standard library reference desk, with an accuracy of 55 percent.[362]
Bloomberg Businessweek reported in July 2014 that Google's Android mobile apps have dominated the largest share of global smartphone shipments for 2013, with 78.6% of market share over their next closest competitor in iOS with 15.2% of the market.[363] At the time of the appointment of new Wikimedia Foundation executive Lila Tretikov, Wikimedia representatives made a technical announcement concerning the number of mobile access systems in the market seeking access to Wikipedia. Soon after, the representatives stated that Wikimedia would be applying an all-inclusive approach to accommodate as many mobile access systems as possible in its efforts for expanding general mobile access, including BlackBerry and the Windows Phone system, making market share a secondary issue.[278] The Android app for Wikipedia was released on July 23, 2014, to over 500,000 installs and generally positive reviews, scoring over four of a possible five in a poll of approximately 200,000 users downloading from Google.[364][365] The version for iOS was released on April 3, 2013, to similar reviews.[366]
Access to Wikipedia from mobile phones was possible as early as 2004, through the Wireless Application Protocol (WAP), via the Wapedia service.[345] In June 2007, Wikipedia launched en.mobile.wikipedia.org, an official website for wireless devices. In 2009, a newer mobile service was officially released, located at en.m.wikipedia.org, which caters to more advanced mobile devices such as the iPhone, Android-based devices, or WebOS-based devices.[367] Several other methods of mobile access to Wikipedia have emerged since. Many devices and applications optimize or enhance the display of Wikipedia content for mobile devices, while some also incorporate additional features such as use of Wikipedia metadata like geoinformation.[368][369]
Wikipedia Zero was an initiative of the Wikimedia Foundation to expand the reach of the encyclopedia to the developing countries by partnering with mobile operators to allow free access.[370][371] It was discontinued in February 2018 due to lack of participation from mobile operators.[370]
Andrew Lih and Andrew Brown both maintain editing Wikipedia with smartphones is difficult and this discourages new potential contributors.[372][373] Lih states that the number of Wikipedia editors has been declining after several years,[372] and Tom Simonite of MIT Technology Review claims the bureaucratic structure and rules are a factor in this. Simonite alleges some Wikipedians use the labyrinthine rules and guidelines to dominate others and those editors have a vested interest in keeping the status quo.[56] Lih alleges there is a serious disagreement among existing contributors on how to resolve this. Lih fears for Wikipedia's long-term future while Brown fears problems with Wikipedia will remain and rival encyclopedias will not replace it.[372][373]
Access to the Chinese Wikipedia has been blocked in mainland China since May 2015.[19][374][375] This was done after Wikipedia started to use HTTPS encryption, which made selective censorship more difficult.[376]
In 2017, Quartz reported that the Chinese government had begun creating an unofficial version of Wikipedia. However, unlike Wikipedia, the website's contents would only be editable by scholars from state-owned Chinese institutions. The article stated it had been approved by the State Council of the People's Republic of China in 2011.[377]
According to "Wikipedia Readership Survey 2011", the average age of Wikipedia readers is 36, with a rough parity between genders. Almost half of Wikipedia readers visit the site more than five times a month, and a similar number of readers specifically look for Wikipedia in search engine results. About 47 percent of Wikipedia readers do not realize that Wikipedia is a non-profit organization.[385]
Wikipedia has also been used as a source in journalism,[401][402] often without attribution, and several reporters have been dismissed for plagiarizing from Wikipedia.[403][404][405][406]
In 2006, Time magazine recognized Wikipedia's participation (along with YouTube, Reddit, MySpace, and Facebook) in the rapid growth of online collaboration and interaction by millions of people worldwide.[407] On September 16, 2007, The Washington Post reported that Wikipedia had become a focal point in the 2008 US election campaign, saying: "Type a candidate's name into Google, and among the first results is a Wikipedia page, making those entries arguably as important as any ad in defining a candidate. Already, the presidential entries are being edited, dissected and debated countless times each day."[408] An October 2007 Reuters article, titled "Wikipedia page the latest status symbol", reported the recent phenomenon of how having a Wikipedia article vindicates one's notability.[409]
One of the first times Wikipedia was involved in a governmental affair was on September 28, 2007, when Italian politician Franco Grillini raised a parliamentary question with the minister of cultural resources and activities about the necessity of freedom of panorama. He said that the lack of such freedom forced Wikipedia, "the seventh most consulted website", to forbid all images of modern Italian buildings and art, and claimed this was hugely damaging to tourist revenues.[410]
In 2007, readers of brandchannel.com voted Wikipedia as the fourth-highest brand ranking, receiving 15 percent of the votes in answer to the question "Which brand had the most impact on our lives in 2006?"[415]
In 2015, Wikipedia was awarded both the annual Erasmus Prize, which recognizes exceptional contributions to culture, society or social sciences,[417] and the Spanish Princess of Asturias Award on International Cooperation.[418] Speaking at the Asturian Parliament in Oviedo, the city that hosts the awards ceremony, Jimmy Wales praised the work of the Asturian Wikipedia users.[419]
Many parodies target Wikipedia's openness and susceptibility to inserted inaccuracies, with characters vandalizing or modifying the online encyclopedia project's articles.
In an April 2007 episode of the American television comedy The Office, office manager (Michael Scott) is shown relying on a hypothetical Wikipedia article for information on negotiation tactics to assist him in negotiating lesser pay for an employee.[422] Viewers of the show tried to add the episode's mention of the page as a section of the actual Wikipedia article on negotiation, but this effort was prevented by other users on the article's talk page.[423]
"My Number One Doctor", a 2007 episode of the television show Scrubs, played on the perception that Wikipedia is an unreliable reference tool with a scene in which Perry Cox reacts to a patient who says that a Wikipedia article indicates that the raw food diet reverses the effects of bone cancer by retorting that the same editor who wrote that article also wrote the Battlestar Galactica episode guide.[424]
In 2008, the comedy website CollegeHumor produced a video sketch named "Professor Wikipedia", in which the fictitious Professor Wikipedia instructs a class with a medley of unverifiable and occasionally absurd statements.[425]
The Dilbert comic strip from May 8, 2009, features a character supporting an improbable claim by saying "Give me ten minutes and then check Wikipedia."[426]
In July 2009, BBC Radio 4 broadcast a comedy series called Bigipedia, which was set on a website which was a parody of Wikipedia.[427] Some of the sketches were directly inspired by Wikipedia and its articles.[428]
On August 23, 2013, the New Yorker website published a cartoon with this caption: "Dammit, Manning, have you considered the pronoun war that this is going to start on your Wikipedia page?"[429] The cartoon referred to Chelsea Elizabeth Manning (born Bradley Edward Manning), an American activist, politician, and former United States Army soldier who had recently come out as a trans woman.[430]
In December 2015, John Julius Norwich stated, in a letter published in The Times newspaper, that as a historian he resorted to Wikipedia "at least a dozen times a day", and had never yet caught it out. He described it as "a work of reference as useful as any in existence", with so wide a range that it is almost impossible to find a person, place, or thing that it has left uncovered and that he could never have written his last two books without it.[431]
Wikipedia has spawned several sister projects, which are also wikis run by the Wikimedia Foundation. These other Wikimedia projects include Wiktionary, a dictionary project launched in December 2002,[432] Wikiquote, a collection of quotations created a week after Wikimedia launched,[433] Wikibooks, a collection of collaboratively written free textbooks and annotated texts,[434] Wikimedia Commons, a site devoted to free-knowledge multimedia,[435] Wikinews, for collaborative journalism,[436] and Wikiversity, a project for the creation of free learning materials and the provision of online learning activities.[437] Another sister project of Wikipedia, Wikispecies, is a catalogue of all species, but is not open for public editing.[438] In 2012, Wikivoyage, an editable travel guide,[439] and Wikidata, an editable knowledge base, launched.[440]
Wikipedia's influence on the biography publishing business has been a concern for some. Book publishing data tracker Nielsen BookScan stated in 2013 that biography sales were dropping "far more sharply".[446] Kathryn Hughes, professor of life writing at the University of East Anglia and author of two biographies wrote, "The worry is that, if you can get all that information from Wikipedia, what's left for biography?"[446]
Wikipedia has been widely used as a corpus for linguistic research in computational linguistics, information retrieval and natural language processing.[447][448] In particular, it commonly serves as a target knowledge base for the entity linking problem, which is then called "wikification",[449] and to the related problem of word-sense disambiguation.[450] Methods similar to wikification can in turn be used to find "missing" links in Wikipedia.[451]
A 2017 MIT study suggests that words used on Wikipedia articles end up in scientific publications.[457][458]
Studies related to Wikipedia have been using machine learning and artificial intelligence to support various operations. One of the most important areas is the automatic detection of vandalism[459][460] and data quality assessment in Wikipedia.[461]
In February 2022, civil servants from the UK's Department for Levelling Up, Housing and Communities were found to have used Wikipedia for research in the drafting of the Levelling Up White Paper after journalists at The Independent noted that parts of the document had been lifted directly from Wikipedia articles on Constantinople and the list of largest cities throughout history.[462]
Several interactive multimedia encyclopedias incorporating entries written by the public existed long before Wikipedia was founded. The first of these was the 1986 BBC Domesday Project, which included text (entered on BBC Micro computers) and photographs from more than a million contributors in the UK, and covered the geography, art, and culture of the UK. This was the first interactive multimedia encyclopedia (and was also the first major multimedia document connected through internal links), with the majority of articles being accessible through an interactive map of the UK. The user interface and part of the content of the Domesday Project were emulated on a website until 2008.[463]
Several free-content, collaborative encyclopedias were created around the same period as Wikipedia (e.g. Everything2),[464] with many later being merged into the project (e.g. GNE).[465] One of the most successful early online encyclopedias incorporating entries by the public was h2g2, which was created by Douglas Adams in 1999. The h2g2 encyclopedia is relatively lighthearted, focusing on articles which are both witty and informative.[466]
Subsequent collaborative knowledge websites have drawn inspiration from Wikipedia. Others use more traditional peer review, such as Encyclopedia of Life and the online wiki encyclopedias Scholarpedia and Citizendium.[467][468] The latter was started by Sanger in an attempt to create a reliable alternative to Wikipedia.[469][470]
Free content, libre content, libre information, or free information, is any kind of functional work, work of art, or other creative content that meets the definition of a free cultural work.[1]
A free cultural work is, according to the definition of Free Cultural Works, one that has no significant legal restriction on people's freedom to:
Free content encompasses all works in the public domain and also those copyrighted works whose licenses honor and uphold the freedoms mentioned above. Because the Berne Convention in most countries by default grants copyright holders monopolistic control over their creations, copyright content must be explicitly declared free, usually by the referencing or inclusion of licensing statements from within the work.
Although there are a great many different definitions in regular everyday use, free content is legally very similar, if not like an identical twin, to open content. An analogy is a use of the rival terms free software and open-source, which describe ideological differences rather than legal ones.[3][4][5] For instance, the Open Knowledge Foundation's Open Definition describes "open" as synonymous to the definition of free in the "Definition of Free Cultural Works" (as also in the Open Source Definition and Free Software Definition).[6] For such free/open content both movements recommend the same three Creative Commons licenses, the CC BY, CC BY-SA, and CC0.[7][8][9][10]
Copyright is a legal concept, which gives the author or creator of a work legal control over the duplication and public performance of their work. In many jurisdictions, this is limited by a time period after which the works then enter the public domain. Copyright laws are a balance between the rights of creators of intellectual and artistic works and the rights of others to build upon those works. During the time period of copyright the author's work may only be copied, modified, or publicly performed with the consent of the author, unless the use is a fair use. Traditional copyright control limits the use of the work of the author to those who either pay royalties to the author for usage of the author's content or limit their use to fair use. Secondly, it limits the use of content whose author cannot be found.[11] Finally, it creates a perceived barrier between authors by limiting derivative works, such as mashups and collaborative content.[12]
The public domain is a range of creative works whose copyright has expired or was never established, as well as ideas and facts[note 1] which are ineligible for copyright. A public domain work is a work whose author has either relinquished to the public or no longer can claim control over, the distribution and usage of the work. As such, any person may manipulate, distribute, or otherwise use the work, without legal ramifications. A work in the public domain or released under a permissive license may be referred to as "copycenter".[13]
Copyleft is a play on the word copyright and describes the practice of using copyright law to remove restrictions on distributing copies and modified versions of a work.[14] The aim of copyleft is to use the legal framework of copyright to enable non-author parties to be able to reuse and, in many licensing schemes, modify content that is created by an author. Unlike works in the public domain, the author still maintains copyright over the material, however, the author has granted a non-exclusive license to any person to distribute, and often modify, the work. Copyleft licenses require that any derivative works be distributed under the same terms and that the original copyright notices be maintained. A symbol commonly associated with copyleft is a reversal of the copyright symbol, facing the other way; the opening of the C points left rather than right. Unlike the copyright symbol, the copyleft symbol does not have a codified meaning.[15]
Projects that provide free content exist in several areas of interest, such as software, academic literature, general literature, music, images, video, and engineering. Technology has reduced the cost of publication and reduced the entry barrier sufficiently to allow for the production of widely disseminated materials by individuals or small groups. Projects to provide free literature and multimedia content have become increasingly prominent owing to the ease of dissemination of materials that are associated with the development of computer technology. Such dissemination may have been too costly prior to these technological developments.
In media, which includes textual, audio, and visual content, free licensing schemes such as some of the licenses made by Creative Commons have allowed for the dissemination of works under a clear set of legal permissions. Not all Creative Commons licenses are entirely free; their permissions may range from very liberal general redistribution and modification of the work to a more restrictive redistribution-only licensing. Since February 2008, Creative Commons licenses which are entirely free carry a badge indicating that they are "approved for free cultural works".[16] Repositories exist which exclusively feature free material and provide content such as photographs, clip art, music,[17] and literature.[18] While extensive reuse of free content from one website in another website is legal, it is usually not sensible because of the duplicate content problem. Wikipedia is amongst the most well-known databases of user-uploaded free content on the web. While the vast majority of content on Wikipedia is free content, some copyrighted material is hosted under fair-use criteria.
Free and open-source software, which is also often referred to as open source software and free software, is a maturing technology with major companies using free software to provide both services and technology to both end-users and technical consumers. The ease of dissemination has allowed for increased modularity, which allows for smaller groups to contribute to projects as well as simplifying collaboration. Open source development models have been classified as having a similar peer-recognition and collaborative benefit incentives that are typified by more classical fields such as scientific research, with the social structures that result from this incentive model decreasing production cost.[19] Given sufficient interest in a software component, by using peer-to-peer distribution methods, distribution costs of software may be reduced, removing the burden of infrastructure maintenance from developers. As distribution resources are simultaneously provided by consumers, these software distribution models are scalable, that is the method is feasible regardless of the number of consumers. In some cases, free software vendors may use peer-to-peer technology as a method of dissemination.[20] In general, project hosting and code distribution is not a problem for the most of free projects as a number of providers offer them these services free.
Free content principles have been translated into fields such as engineering, where designs and engineering knowledge can be readily shared and duplicated, in order to reduce overheads associated with project development. Open design principles can be applied in engineering and technological applications, with projects in mobile telephony, small-scale manufacture,[21] the automotive industry,[22][23] and even agricultural areas. Technologies such as distributed manufacturing can allow computer-aided manufacturing and computer-aided design techniques to be able to develop small-scale production of components for the development of new, or repair of existing, devices. Rapid fabrication technologies underpin these developments, which allow end-users of technology to be able to construct devices from pre-existing blueprints, using software and manufacturing hardware to convert information into physical objects.
In academic work, the majority of works are not free, although the percentage of works that are open access is growing rapidly. Open access refers to online research outputs that are free of all restrictions on access (e.g. access tolls) and free of many restrictions on use (e.g. certain copyright and license restrictions).[24] Authors may see open access publishing as a method of expanding the audience that is able to access their work to allow for greater impact of the publication, or may support it for ideological reasons.[25][26][27] Open access publishers such as PLOS and BioMed Central provide capacity for review and publishing of free works; though such publications are currently more common in science than humanities. Various funding institutions and governing research bodies have mandated that academics must produce their works to be open-access, in order to qualify for funding, such as the US National Institutes of Health, Research Councils UK (effective 2016) and the European Union (effective 2020).[28][29][30][31] At an institutional level some universities, such as the Massachusetts Institute of Technology, have adopted open access publishing by default by introducing their own mandates.[32] Some mandates may permit delayed publication and may charge researchers for open access publishing.[33][34]
Open content publication has been seen as a method of reducing costs associated with information retrieval in research, as universities typically pay to subscribe for access to content that is published through traditional means[10][35][36] whilst improving journal quality by discouraging the submission of research articles of reduced quality.[10] Subscriptions for non-free content journals may be expensive for universities to purchase, though the article are written and peer-reviewed by academics themselves at no cost to the publisher. This has led to disputes between publishers and some universities over subscription costs, such as the one which occurred between the University of California and the Nature Publishing Group.[37][38] For teaching purposes, some universities, including MIT, provide freely available course content, such as lecture notes, video resources and tutorials. This content is distributed via Internet resources to the general public. Publication of such resources may be either by a formal institution-wide program,[39] or alternately via informal content provided by individual academics or departments.
Open content describes any work that others can copy or modify freely by attributing to the original creator, but without needing to ask for permission. This has been applied to a range of formats, including textbooks, academic journals, films and music. The term was an expansion of the related concept of open-source software.[40] Such content is said to be under an open license.
The concept of applying free software licenses to content was introduced by Michael Stutz, who in 1997 wrote the paper "Applying Copyleft to Non-Software Information" for the GNU Project. The term "open content" was coined by David A. Wiley in 1998 and evangelized via the Open Content Project, describing works licensed under the Open Content License (a non-free share-alike license, see 'Free content' below) and other works licensed under similar terms.[40]
It has since come to describe a broader class of content without conventional copyright restrictions. The openness of content can be assessed under the '5Rs Framework' based on the extent to which it can be reused, revised, remixed and redistributed by members of the public without violating copyright law.[41] Unlike free content and content under open-source licenses, there is no clear threshold that a work must reach to qualify as 'open content'.
Although open content has been described as a counterbalance to copyright,[42] open content licenses rely on a copyright holder's power to license their work, as copyleft which also utilizes copyright for such a purpose.
In 2003 Wiley announced that the Open Content Project has been succeeded by Creative Commons and their licenses, where he joined as "Director of Educational Licenses".[43][44]
In 2005, the Open Icecat project was launched, in which product information for e-commerce applications was created and published under the Open Content License. It was embraced by the tech sector, which was already quite open source minded.
Another successor project is the Open Knowledge Foundation,[49] founded by Rufus Pollock in Cambridge, in 2004[50] as a global non-profit network to promote and share open content and data.[51] In 2007 the OKF gave an Open Knowledge Definition for "content such as music, films, books; data be it scientific, historical, geographic or otherwise; government and other administrative information".[52] In October 2014 with version 2.0 Open Works and Open Licenses were defined and "open" is described as synonymous to the definitions of open/free in the Open Source Definition, the Free Software Definition and the Definition of Free Cultural Works.[53] A distinct difference is the focus given to the public domain and that it focuses also on the accessibility (open access) and the readability (open formats). Among several conformant licenses, six are recommended, three own (Open Data Commons Public Domain Dedication and Licence, Open Data Commons Attribution License, Open Data Commons Open Database License) and the CC BY, CC BY-SA, and CC0 Creative Commons licenses.[54][55][56]
The website of the Open Content Project once defined open content as 'freely available for modification, use and redistribution under a license similar to those used by the open-source / free software community'.[40] However, such a definition would exclude the Open Content License because that license forbids charging for content; a right required by free and open-source software licenses.[citation needed]
The term since shifted in meaning. Open content is "licensed in a manner that provides users with free and perpetual permission to engage in the 5R activities."[41]
The 5Rs are put forward on the Open Content Project website as a framework for assessing the extent to which content is open:
This broader definition distinguishes open content from open-source software, since the latter must be available for commercial use by the public. However, it is similar to several definitions for open educational resources, which include resources under noncommercial and verbatim licenses.[57][58]
The later Open Definition by the Open Knowledge Foundation define open knowledge with open content and open data as sub-elements and draws heavily on the Open Source Definition; it preserves the limited sense of open content as free content,[59] unifying both.
"Open access" refers to toll-free or gratis access to content, mainly published originally peer-reviewed scholarly journals. Some open access works are also licensed for reuse and redistribution (libre open access), which would qualify them as open content.
Over the past decade, open content has been used to develop alternative routes towards higher education. Traditional universities are expensive, and their tuition rates are increasing.[60] Open content allows a free way of obtaining higher education that is "focused on collective knowledge and the sharing and reuse of learning and scholarly content."[61] There are multiple projects and organizations that promote learning through open content, including OpenCourseWare, Khan Academy and the Saylor Academy. Some universities, like MIT, Yale, and Tufts are making their courses freely available on the internet.[62]
The textbook industry is one of the educational industries in which open content can make the biggest impact.[63] Traditional textbooks, aside from being expensive, can also be inconvenient and out of date, because of publishers' tendency to constantly print new editions.[64] Open textbooks help to eliminate this problem, because they are online and thus easily updatable. Being openly licensed and online can be helpful to teachers, because it allows the textbook to be modified according to the teacher's unique curriculum.[63] There are multiple organizations promoting the creation of openly licensed textbooks. Some of these organizations and projects include the University of Minnesota's Open Textbook Library, Connexions, OpenStax College, the Saylor Academy, Open Textbook Challenge and Wikibooks.
According to the current definition of open content on the OpenContent website, any general, royalty-free copyright license would qualify as an open license because it 'provides users with the right to make more kinds of uses than those normally permitted under the law. These permissions are granted to users free of charge.'[41]
However, the narrower definition used in the Open Definition effectively limits open content to libre content, any free content license, defined by the Definition of Free Cultural Works, would qualify as an open content license. According to this narrower criteria, the following still-maintained licenses qualify:
Encyclopedias have existed for around 2,000 years and have evolved considerably during that time as regards language (written in a major international or a vernacular language), size (few or many volumes), intent (presentation of a global or a limited range of knowledge), cultural perspective (authoritative, ideological, didactic, utilitarian), authorship (qualifications, style), readership (education level, background, interests, capabilities), and the technologies available for their production and distribution (hand-written manuscripts, small or large print runs, Internet). As a valued source of reliable information compiled by experts, printed versions found a prominent place in libraries, schools and other educational institutions.
The appearance of digital and open-source versions in the 21st century, such as Wikipedia, has vastly expanded the accessibility, authorship, readership, and variety of encyclopedia entries.[10]
Indeed, the purpose of an encyclopedia is to collect knowledge disseminated around the globe; to set forth its general system to the men with whom we live, and transmit it to those who will come after us, so that the work of preceding centuries will not become useless to the centuries to come; and so that our offspring, becoming better instructed, will at the same time become more virtuous and happy, and that we should not die without having rendered a service to the human race in the future years to come.
The modern encyclopedia was developed from the dictionary in the 18th century. Historically, both encyclopedias and dictionaries have been researched and written by well-educated, well-informed content experts, but they are significantly different in structure. A dictionary is a linguistic work which primarily focuses on alphabetical listing of words and their definitions. Synonymous words and those related by the subject matter are to be found scattered around the dictionary, giving no obvious place for in-depth treatment. Thus, a dictionary typically provides limited information, analysis or background for the word defined. While it may offer a definition, it may leave the reader lacking in understanding the meaning, significance or limitations of a term, and how the term relates to a broader field of knowledge.
To address those needs, an encyclopedia article is typically not limited to simple definitions, and is not limited to defining an individual word, but provides a more extensive meaning for a subject or discipline. In addition to defining and listing synonymous terms for the topic, the article is able to treat the topic's more extensive meaning in more depth and convey the most relevant accumulated knowledge on that subject. An encyclopedia article also often includes many maps and illustrations, as well as bibliography and statistics.[5] An encyclopedia is, theoretically, not written in order to convince, although one of its goals is indeed to convince its reader of its own veracity.
There are four major elements that define an encyclopedia: its subject matter, its scope, its method of organization, and its method of production:
Some works entitled "dictionaries" are actually similar to encyclopedias, especially those concerned with a particular field (such as the Dictionary of the Middle Ages, the Dictionary of American Naval Fighting Ships, and Black's Law Dictionary). The Macquarie Dictionary, Australia's national dictionary, became an encyclopedic dictionary after its first edition in recognition of the use of proper nouns in common communication, and the words derived from such proper nouns.
There are some broad differences between encyclopedias and dictionaries. Most noticeably, encyclopedia articles are longer, fuller and more thorough than entries in most general-purpose dictionaries.[3][18] There are differences in content as well. Generally speaking, dictionaries provide linguistic information about words themselves, while encyclopedias focus more on the things for which those words stand.[6][7][8][9] Thus, while dictionary entries are inextricably fixed to the word described, encyclopedia articles can be given a different entry name. As such, dictionary entries are not fully translatable into other languages, but encyclopedia articles can be.[6]
In practice, however, the distinction is not concrete, as there is no clear-cut difference between factual, "encyclopedic" information and linguistic information such as appears in dictionaries.[8][18][19] Thus encyclopedias may contain material that is also found in dictionaries, and vice versa.[19] In particular, dictionary entries often contain factual information about the thing named by the word.[18][19]
The earliest encyclopedic work to have survived to modern times is the Naturalis Historia of Pliny the Elder, a Roman statesman living in the 1st century AD.[5][20][21][22] He compiled a work of 37 chapters covering natural history, architecture, medicine, geography, geology, and all aspects of the world around him.[22] This work became very popular in Antiquity, was one of the first classical manuscripts to be printed in 1470, and has remained popular ever since as a source of information on the Roman world, and especially Roman art, Roman technology and Roman engineering.
Another Christian encyclopedia was the Institutiones divinarum et saecularium litterarum of Cassiodorus (543-560) dedicated to the Christian divinity and to the seven liberal arts.[21][5] The encyclopedia of Suda, a massive 10th-century Byzantine encyclopedia, had 30,000 entries, many drawing from ancient sources that have since been lost, and often derived from medieval Christian compilers. The text was arranged alphabetically with some slight deviations from common vowel order and place in the Greek alphabet.[21]
Before the advent of the printing press, encyclopedic works were all hand copied and thus rarely available, beyond wealthy patrons or monastic men of learning: they were expensive, and usually written for those extending knowledge rather than those using it.
During the Renaissance, the creation of printing allowed a wider diffusion of encyclopedias and every scholar could have his or her own copy. The De expetendis et fugiendis rebus by Giorgio Valla was posthumously printed in 1501 by Aldo Manuzio in Venice. This work followed the traditional scheme of liberal arts. However, Valla added the translation of ancient Greek works on mathematics (firstly by Archimedes), newly discovered and translated. The Margarita Philosophica by Gregor Reisch, printed in 1503, was a complete encyclopedia explaining the seven liberal arts.
In the United States, the 1950s and 1960s saw the introduction of several large popular encyclopedias, often sold on installment plans. The best known of these were World Book and Funk and Wagnalls. As many as 90% were sold door to door.[20] Jack Lynch says in his book You Could Look It Up that encyclopedia salespeople were so common that they became the butt of jokes. He describes their sales pitch saying, "They were selling not books but a lifestyle, a future, a promise of social mobility." A 1961 World Book ad said, "You are holding your family's future in your hands right now," while showing a feminine hand holding an order form.[36]
By the late 20th century, encyclopedias were being published on CD-ROMs for use with personal computers. This was the usual way computer users accessed encyclopedic knowledge from the 1980s and 1990s. Later DVD discs replaced CD-ROMs and from mid-2000s internet encyclopedias became dominant and replaced disc-based software encyclopedias.[5]
CD-ROM encyclopedias were usually a macOS or Microsoft Windows (3.0, 3.1 or 95/98) application on a CD-ROM disc. The user would execute the encyclopedia's software program to see a menu that allowed them to start browsing the encyclopedia's articles, and most encyclopedias also supported a way to search the contents of the encyclopedia. The article text was usually hyperlinked and also included photographs, audio clips (for example in articles about historical speeches or musical instruments), and video clips. In the CD-ROM age the video clips had usually a low resolution, often 160x120 or 320x240 pixels. Such encyclopedias which made use of photos, audio and video were also called multimedia encyclopedias. However, because of the online encyclopedia, CD-ROM encyclopedias have been declared obsolete.[by whom?]
Microsoft's Encarta, launched in 1993, was a landmark example as it had no printed equivalent. Articles were supplemented with video and audio files as well as numerous high-quality images. After sixteen years, Microsoft discontinued the Encarta line of products in 2009.[37] Other examples of CD-ROM encyclopedia are Grolier Multimedia Encyclopedia and Britannica.
Digital encyclopedias enable "Encyclopedia Services" (such as Wikimedia Enterprise) to facilitate programatic access to the content.[38]
The concept of a free encyclopedia began with the Interpedia proposal on Usenet in 1993, which outlined an Internet-based online encyclopedia to which anyone could submit content and that would be freely accessible. Early projects in this vein included Everything2 and Open Site. In 1999, Richard Stallman proposed the GNUPedia, an online encyclopedia which, similar to the GNU operating system, would be a "generic" resource. The concept was very similar to Interpedia, but more in line with Stallman's GNU philosophy.
It was not until Nupedia and later Wikipedia that a stable free encyclopedia project was able to be established on the Internet.
The English Wikipedia, which was started in 2001, became the world's largest encyclopedia in 2004 at the 300,000 article stage.[39] By late 2005, Wikipedia had produced over two million articles in more than 80 languages with content licensed under the copyleft GNU Free Documentation License. As of August 2009, Wikipedia had over 3 million articles in English and well over 10 million combined in over 250 languages. Wikipedia currently has 6,623,776 articles in English.
Since 2003, other free encyclopedias like the Chinese-language Baidu Baike and Hudong, as well as English language encyclopedias such as Citizendium and Knol have appeared, the latter of which has been discontinued.
You're welcome to edit anonymously, but there are many benefits of registering an account. It's quick and free.
You can test out how editing feels by editing one of the "sandbox" test pages below:
Edit page visually or Edit using wiki markup
Edit page visually or Edit using wiki markup
Just type some text and click Publish page when you're happy with the way it looks. Don't worry about breaking anything; these pages are open areas for experimentation.
Wikipedia has many community pages in addition to its articles.
See also Wikipedia:Statistics and Category:Wikipedia statistics for a fuller list of pages that provide, analyze or discuss Wikipedia statistics.
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
English is a West Germanic language in the Indo-European language family, with its earliest forms spoken by the inhabitants of early medieval England.[3][4][5] It is named after the Angles, one of the ancient Germanic peoples that migrated to the island of Great Britain. Existing on a dialect continuum with Scots and then most closely related to the Low German and Frisian languages, English is genealogically Germanic. However, its vocabulary also shows major influences from French (about 28% of Modern English words) and Latin (also about 28%),[6] plus some grammar and a small amount of core vocabulary influenced by Old Norse (a North Germanic language).[7][8][9] Speakers of English are called Anglophones.
The earliest forms of English, collectively known as "Old English", evolved from a group of North Sea Germanic dialects brought to Great Britain by Anglo-Saxon settlers in the 5th century and further mutated by Norse-speaking Viking settlers starting in the 8th and 9th centuries. Middle English began in the late 11th century after the Norman Conquest of England, when considerable Old French (especially Old Norman French) and Latin-derived vocabulary was incorporated into English over some three hundred years.[10][11] Early Modern English began in the late 15th century with the start of the Great Vowel Shift and the Renaissance trend of borrowing further Latin and Greek words and roots into English, concurrent with the introduction of the printing press to London. This era notably culminated in the King James Bible and the works of William Shakespeare.[12][13]
English is an Indo-European language and belongs to the West Germanic group of the Germanic languages.[19] Old English originated from a Germanic tribal and linguistic continuum along the Frisian North Sea coast, whose languages gradually evolved into the Anglic languages in the British Isles, and into the Frisian languages and Low German/Low Saxon on the continent. The Frisian languages, which together with the Anglic languages form the Anglo-Frisian languages, are the closest living relatives of English. Low German/Low Saxon is also closely related, and sometimes English, the Frisian languages, and Low German are grouped together as the Ingvaeonic (North Sea Germanic) languages, though this grouping remains debated.[8] Old English evolved into Middle English, which in turn evolved into Modern English.[20] Particular dialects of Old and Middle English also developed into a number of other Anglic languages, including Scots[21] and the extinct Fingallian and Forth and Bargy (Yola) dialects of Ireland.[22]
Like Icelandic and Faroese, the development of English in the British Isles isolated it from the continental Germanic languages and influences, and it has since diverged considerably. English is not mutually intelligible with any continental Germanic language, differing in vocabulary, syntax, and phonology, although some of these, such as Dutch or Frisian, do show strong affinities with English, especially with its earlier stages.[23]
Old English is essentially a distinct language from Modern English and is virtually impossible for 21st-century unstudied English-speakers to understand. Its grammar was similar to that of modern German: nouns, adjectives, pronouns, and verbs had many more inflectional endings and forms, and word order was much freer than in Modern English. Modern English has case forms in pronouns (he, him, his) and has a few verb inflections (speak, speaks, speaking, spoke, spoken), but Old English had case endings in nouns as well, and verbs had more person and number endings.[39][40][41] Its closest relative is Old Frisian, but even some centuries after the Anglo-Saxon migration, Old English retained considerable mutual intelligibility with other Germanic varieties. Even in the 9th and 10th centuries, amidst the Danelaw and other Viking invasions, there is historical evidence that Old Norse and Old English retained considerable mutual intelligibility.[42] Theoretically, as late as the 900s AD, a commoner from England could hold a conversation with a commoner from Scandinavia. Research continues into the details of the myriad tribes in peoples in England and Scandinavia and the mutual contacts between them.[42]
The translation of Matthew 8:20 from 1000 shows examples of case endings (nominative plural, accusative plural, genitive singular) and a verb ending (present plural):
From the 8th to the 12th century, Old English gradually transformed through language contact into Middle English. Middle English is often arbitrarily defined as beginning with the conquest of England by William the Conqueror in 1066, but it developed further in the period from 1200 to 1450.
First, the waves of Norse colonisation of northern parts of the British Isles in the 8th and 9th centuries put Old English into intense contact with Old Norse, a North Germanic language. Norse influence was strongest in the north-eastern varieties of Old English spoken in the Danelaw area around York, which was the centre of Norse colonisation; today these features are still particularly present in Scots and Northern English. However, the centre of norsified English seems to have been in the Midlands around Lindsey, and after 920 CE when Lindsey was reincorporated into the Anglo-Saxon polity, Norse features spread from there into English varieties that had not been in direct contact with Norse speakers. An element of Norse influence that persists in all English varieties today is the group of pronouns beginning with th- (they, them, their) which replaced the Anglo-Saxon pronouns with h- (hie, him, hera).[45]
With the Norman Conquest of England in 1066, the now norsified Old English language was subject to contact with Old French, in particular with the Old Norman dialect. The Norman language in England eventually developed into Anglo-Norman.[10] Because Norman was spoken primarily by the elites and nobles, while the lower classes continued speaking Anglo-Saxon (English), the main influence of Norman was the introduction of a wide range of loanwords related to politics, legislation and prestigious social domains.[9] Middle English also greatly simplified the inflectional system, probably in order to reconcile Old Norse and Old English, which were inflectionally different but morphologically similar. The distinction between nominative and accusative cases was lost except in personal pronouns, the instrumental case was dropped, and the use of the genitive case was limited to indicating possession. The inflectional system regularised many irregular inflectional forms,[46] and gradually simplified the system of agreement, making word order less flexible.[47] In Wycliff'e Bible of the 1380s, the verse Matthew 8:20 was written: Foxis han dennes, and briddis of heuene han nestis.[48] Here the plural suffix -n on the verb have is still retained, but none of the case endings on the nouns are present. By the 12th century Middle English was fully developed, integrating both Norse and French features; it continued to be spoken until the transition to early Modern English around 1500. Middle English literature includes Geoffrey Chaucer's The Canterbury Tales, and Thomas Malory's Le Morte d'Arthur. In the Middle English period, the use of regional dialects in writing proliferated, and dialect traits were even used for effect by authors such as Chaucer.[49]
The Great Vowel Shift affected the stressed long vowels of Middle English. It was a chain shift, meaning that each shift triggered a subsequent shift in the vowel system. Mid and open vowels were raised, and close vowels were broken into diphthongs. For example, the word bite was originally pronounced as the word beet is today, and the second vowel in the word about was pronounced as the word boot is today. The Great Vowel Shift explains many irregularities in spelling since English retains many spellings from Middle English, and it also explains why English vowel letters have very different pronunciations from the same letters in other languages.[50][51]
By the late 18th century, the British Empire had spread English through its colonies and geopolitical dominance. Commerce, science and technology, diplomacy, art, and formal education all contributed to English becoming the first truly global language. English also facilitated worldwide international communication.[54][3] England continued to form new colonies, and these later developed their own norms for speech and writing. English was adopted in parts of North America, parts of Africa, Australasia, and many other regions. When they obtained political independence, some of the newly independent nations that had multiple indigenous languages opted to continue using English as the official language to avoid the political and other difficulties inherent in promoting any one indigenous language above the others.[55][56][57] In the 20th century the growing economic and cultural influence of the United States and its status as a superpower following the Second World War has, along with worldwide broadcasting in English by the BBC[58] and other broadcasters, caused the language to spread across the planet much faster.[59][60] In the 21st century, English is more widely spoken and written than any language has ever been.[61]
As Modern English developed, explicit norms for standard usage were published, and spread through official media such as public education and state-sponsored publications. In 1755 Samuel Johnson published his A Dictionary of the English Language, which introduced standard spellings of words and usage norms. In 1828, Noah Webster published the American Dictionary of the English language to try to establish a norm for speaking and writing American English that was independent of the British standard. Within Britain, non-standard or lower class dialect features were increasingly stigmatised, leading to the quick spread of the prestige varieties among the middle classes.[62]
In modern English, the loss of grammatical case is almost complete (it is now only found in pronouns, such as he and him, she and her, who and whom), and SVO word order is mostly fixed.[62] Some changes, such as the use of do-support, have become universalised. (Earlier English did not use the word "do" as a general auxiliary as Modern English does; at first it was only used in question constructions, and even then was not obligatory.[63] Now, do-support with the verb have is becoming increasingly standardised.) The use of progressive forms in -ing, appears to be spreading to new constructions, and forms such as had been being built are becoming more common. Regularisation of irregular forms also slowly continues (e.g. dreamed instead of dreamt), and analytical alternatives to inflectional forms are becoming more common (e.g. more polite instead of politer). British English is also undergoing change under the influence of American English, fuelled by the strong presence of American English in the media and the prestige associated with the US as a world power.[64][65][66]
The countries where English is spoken can be grouped into different categories according to how English is used in each country. The "inner circle"[69] countries with many native speakers of English share an international standard of written English and jointly influence speech norms for English around the world. English does not belong to just one country, and it does not belong solely to descendants of English settlers. English is an official language of countries populated by few descendants of native speakers of English. It has also become by far the most important language of international communication when people who share no native language meet anywhere in the world.
The Indian linguist Braj Kachru distinguished countries where English is spoken with a three circles model.[69] In his model,
Kachru based his model on the history of how English spread in different countries, how users acquire English, and the range of uses English has in each country. The three circles change membership over time.[70]
Those countries have millions of native speakers of dialect continua ranging from an English-based creole to a more standard version of English. They have many more speakers of English who acquire English as they grow up through day-to-day use and listening to broadcasting, especially if they attend schools where English is the medium of instruction. Varieties of English learned by non-native speakers born to English-speaking parents may be influenced, especially in their grammar, by the other languages spoken by those learners.[79] Most of those varieties of English include words little used by native speakers of English in the inner-circle countries,[79] and they may show grammatical and phonological differences from inner-circle varieties as well. The standard English of the inner-circle countries is often taken as a norm for use of English in the outer-circle countries.[79]
In the three-circles model, countries such as Poland, China, Brazil, Germany, Japan, Indonesia, Egypt, and other countries where English is taught as a foreign language, make up the "expanding circle".[87] The distinctions between English as a first language, as a second language, and as a foreign language are often debatable and may change in particular countries over time.[86] For example, in the Netherlands and some other countries of Europe, knowledge of English as a second language is nearly universal, with over 80 percent of the population able to use it,[88] and thus English is routinely used to communicate with foreigners and often in higher education. In these countries, although English is not used for government business, its widespread use puts them at the boundary between the "outer circle" and "expanding circle". English is unusual among world languages in how many of its users are not native speakers but speakers of English as a second or foreign language.[89]
Many users of English in the expanding circle use it to communicate with other people from the expanding circle, so that interaction with native speakers of English plays no part in their decision to use the language.[90] Non-native varieties of English are widely used for international communication, and speakers of one such variety often encounter features of other varieties.[91] Very often today a conversation in English anywhere in the world may include no native speakers of English at all, even while including speakers from several different countries. This is particularly true of the shared vocabulary of mathematics and the sciences.[92]
Pie chart showing the percentage of native English speakers living in "inner circle" English-speaking countries. Native speakers are now substantially outnumbered worldwide by second-language speakers of English (not counted in this chart).
English is a pluricentric language, which means that no one national authority sets the standard for use of the language.[93][94][95][96] Spoken English, for example English used in broadcasting, generally follows national pronunciation standards that are established by custom rather than by regulation. International broadcasters are usually identifiable as coming from one country rather than another through their accents,[97] but newsreader scripts are also composed largely in international standard written English. The norms of standard written English are maintained purely by the consensus of educated English-speakers around the world, without any oversight by any government or international organisation.[98]
American listeners generally readily understand most British broadcasting, and British listeners readily understand most American broadcasting. Most English speakers around the world can understand radio programmes, television programmes, and films from many parts of the English-speaking world.[99] Both standard and non-standard varieties of English can include both formal or informal styles, distinguished by word choice and syntax and use both technical and non-technical registers.[100]
The settlement history of the English-speaking inner circle countries outside Britain helped level dialect distinctions and produce koineised forms of English in South Africa, Australia, and New Zealand.[101] The majority of immigrants to the United States without British ancestry rapidly adopted English after arrival. Now the majority of the United States population are monolingual English speakers,[71][102] and English has been given official or co-official status by 30 of the 50 state governments, as well as all five territorial governments of the US, though there has never been an official language at the federal level.[103][104]
English has ceased to be an "English language" in the sense of belonging only to people who are ethnically English.[105][106] Use of English is growing country-by-country internally and for international communication. Most people learn English for practical rather than ideological reasons.[107] Many speakers of English in Africa have become part of an "Afro-Saxon" language community that unites Africans from different countries.[108]
As decolonisation proceeded throughout the British Empire in the 1950s and 1960s, former colonies often did not reject English but rather continued to use it as independent countries setting their own language policies.[56][57][109] For example, the view of the English language among many Indians has gone from associating it with colonialism to associating it with economic progress, and English continues to be an official language of India.[110] English is also widely used in media and literature, and the number of English language books published annually in India is the third largest in the world after the US and UK.[111] However English is rarely spoken as a first language, numbering only around a couple hundred-thousand people, and less than 5% of the population speak fluent English in India.[112][113] David Crystal claimed in 2004 that, combining native and non-native speakers, India now has more people who speak or understand English than any other country in the world,[114] but the number of English speakers in India is uncertain, with most scholars concluding that the United States still has more speakers of English than India.[115]
Modern English, sometimes described as the first global lingua franca,[59][116] is also regarded as the first world language.[117][118] English is the world's most widely used language in newspaper publishing, book publishing, international telecommunications, scientific publishing, international trade, mass entertainment, and diplomacy.[118] English is, by international treaty, the basis for the required controlled natural languages[119] Seaspeak and Airspeak, used as international languages of seafaring[120] and aviation.[121] English used to have parity with French and German in scientific research, but now it dominates that field.[122] It achieved parity with French as a language of diplomacy at the Treaty of Versailles negotiations in 1919.[123] By the time of the foundation of the United Nations at the end of World War II, English had become pre-eminent[124] and is now the main worldwide language of diplomacy and international relations.[125] It is one of six official languages of the United Nations.[126] Many other worldwide international organisations, including the International Olympic Committee, specify English as a working language or official language of the organisation.
Many regional international organisations such as the European Free Trade Association, Association of Southeast Asian Nations (ASEAN),[60] and Asia-Pacific Economic Cooperation (APEC) set English as their organisation's sole working language even though most members are not countries with a majority of native English speakers. While the European Union (EU) allows member states to designate any of the national languages as an official language of the Union, in practice English is the main working language of EU organisations.[127]
Although in most countries English is not an official language, it is currently the language most often taught as a foreign language.[59][60] In the countries of the EU, English is the most widely spoken foreign language in nineteen of the twenty-five member states where it is not an official language (that is, the countries other than Ireland and Malta). In a 2012 official Eurobarometer poll (conducted when the UK was still a member of the EU), 38 percent of the EU respondents outside the countries where English is an official language said they could speak English well enough to have a conversation in that language. The next most commonly mentioned foreign language, French (which is the most widely known foreign language in the UK and Ireland), could be used in conversation by 12 percent of respondents.[128]
A working knowledge of English has become a requirement in a number of occupations and professions such as medicine[130] and computing. English has become so important in scientific publishing that more than 80 percent of all scientific journal articles indexed by Chemical Abstracts in 1998 were written in English, as were 90 percent of all articles in natural science publications by 1996 and 82 percent of articles in humanities publications by 1995.[131]
International communities such as international business people may use English as an auxiliary language, with an emphasis on vocabulary suitable for their domain of interest. This has led some scholars to develop the study of English as an auxiliary language. The trademarked Globish uses a relatively small subset of English vocabulary (about 1500 words, designed to represent the highest use in international business English) in combination with the standard English grammar.[132] Other examples include Simple English.
The increased use of the English language globally has had an effect on other languages, leading to some English words being assimilated into the vocabularies of other languages. This influence of English has led to concerns about language death,[133] and to claims of linguistic imperialism,[134] and has provoked resistance to the spread of English; however the number of speakers continues to increase because many people around the world think that English provides them with opportunities for better employment and improved lives.[135]
Although some scholars[who?] mention a possibility of future divergence of English dialects into mutually unintelligible languages, most think a more likely outcome is that English will continue to function as a koineised language in which the standard form unifies speakers from around the world.[136] English is used as the language for wider communication in countries around the world.[137] Thus English has grown in worldwide use much more than any constructed language proposed as an international auxiliary language, including Esperanto.[138][139]
The phonetic symbols used below are from the International Phonetic Alphabet (IPA).[141][142][143]
The pronunciation of vowels varies a great deal between dialects and is one of the most detectable aspects of a speaker's accent. The table below lists the vowel phonemes in Received Pronunciation (RP) and General American (GA), with examples of words in which they occur from lexical sets compiled by linguists. The vowels are represented with symbols from the International Phonetic Alphabet; those given for RP are standard in British dictionaries and other publications.[150]
Stress plays an important role in English. Certain syllables are stressed, while others are unstressed. Stress is a combination of duration, intensity, vowel quality, and sometimes changes in pitch. Stressed syllables are pronounced longer and louder than unstressed syllables, and vowels in unstressed syllables are frequently reduced while vowels in stressed syllables are not.[158] Some words, primarily short function words but also some modal verbs such as can, have weak and strong forms depending on whether they occur in stressed or non-stressed position within a sentence.
In terms of rhythm, English is generally described as a stress-timed language, meaning that the amount of time between stressed syllables tends to be equal.[163] Stressed syllables are pronounced longer, but unstressed syllables (syllables between stresses) are shortened. Vowels in unstressed syllables are shortened as well, and vowel shortening causes changes in vowel quality: vowel reduction.[164]
Varieties of English vary the most in pronunciation of vowels. The best known national varieties used as standards for education in non-English-speaking countries are British (BrE) and American (AmE). Countries such as Canada, Australia, Ireland, New Zealand and South Africa have their own standard varieties which are less often used as standards for education internationally. Some differences between the various dialects are shown in the table "Varieties of Standard English and their features".[165]
English has undergone many historical sound changes, some of them affecting all varieties, and others affecting only a few. Most standard varieties are affected by the Great Vowel Shift, which changed the pronunciation of long vowels, but a few dialects have slightly different results. In North America, a number of chain shifts such as the Northern Cities Vowel Shift and Canadian Shift have produced very different vowel landscapes in some regional accents.[166]
General American and Received Pronunciation vary in their pronunciation of historical /r/ after a vowel at the end of a syllable (in the syllable coda). GA is a rhotic dialect, meaning that it pronounces /r/ at the end of a syllable, but RP is non-rhotic, meaning that it loses /r/ in that position. English dialects are classified as rhotic or non-rhotic depending on whether they elide /r/ like RP or keep it like GA.[170]
As is typical of an Indo-European language, English follows accusative morphosyntactic alignment. Unlike other Indo-European languages though, English has largely abandoned the inflectional case system in favour of analytic constructions. Only the personal pronouns retain morphological case more strongly than any other word class. English distinguishes at least seven major word classes: verbs, nouns, adjectives, adverbs, determiners (including articles), prepositions, and conjunctions. Some analyses add pronouns as a class separate from nouns, and subdivide conjunctions into subordinators and coordinators, and add the class of interjections.[173] English also has a rich set of auxiliary verbs, such as have and do, expressing the categories of mood and aspect. Questions are marked by do-support, wh-movement (fronting of question words beginning with wh-) and word order inversion with some verbs.[174]
Some traits typical of Germanic languages persist in English, such as the distinction between irregularly inflected strong stems inflected through ablaut (i.e. changing the vowel of the stem, as in the pairs speak/spoke and foot/feet) and weak stems inflected through affixation (such as love/loved, hand/hands).[175] Vestiges of the case and gender system are found in the pronoun system (he/him, who/whom) and in the inflection of the copula verb to be.[175]
English nouns are only inflected for number and possession. New nouns can be formed through derivation or compounding. They are semantically divided into proper nouns (names) and common nouns. Common nouns are in turn divided into concrete and abstract nouns, and grammatically into count nouns and mass nouns.[177]
Most count nouns are inflected for plural number through the use of the plural suffix -s, but a few nouns have irregular plural forms. Mass nouns can only be pluralised through the use of a count noun classifier, e.g. one loaf of bread, two loaves of bread.[178]
Possession can be expressed either by the possessive enclitic -s (also traditionally called a genitive suffix), or by the preposition of. Historically the -s possessive has been used for animate nouns, whereas the of possessive has been reserved for inanimate nouns. Today this distinction is less clear, and many speakers use -s also with inanimates. Orthographically the possessive -s is separated from a singular noun with an apostrophe. If the noun is plural formed with -s the apostrophe follows the -s.[174]
Nouns can form noun phrases (NPs) where they are the syntactic head of the words that depend on them such as determiners, quantifiers, conjunctions or adjectives.[179] Noun phrases can be short, such as the man, composed only of a determiner and a noun. They can also include modifiers such as adjectives (e.g. red, tall, all) and specifiers such as determiners (e.g. the, that). But they can also tie together several nouns into a single long NP, using conjunctions such as and, or prepositions such as with, e.g. the tall man with the long red trousers and his skinny wife with the spectacles (this NP uses conjunctions, prepositions, specifiers, and modifiers). Regardless of length, an NP functions as a syntactic unit.[174] For example, the possessive enclitic can, in cases which do not lead to ambiguity, follow the entire noun phrase, as in The President of India's wife, where the enclitic follows India and not President.
The class of determiners is used to specify the noun they precede in terms of definiteness, where the marks a definite noun and a or an an indefinite one. A definite noun is assumed by the speaker to be already known by the interlocutor, whereas an indefinite noun is not specified as being previously known. Quantifiers, which include one, many, some and all, are used to specify the noun in terms of quantity or number. The noun must agree with the number of the determiner, e.g. one man (sg.) but all men (pl.). Determiners are the first constituents in a noun phrase.[180]
English adjectives are words such as good, big, interesting, and Canadian that most typically modify nouns, denoting characteristics of their referents (e.g., a red car). As modifiers, they come before the nouns they modify and after determiners.[181] English adjectives also function as predicative complements (e.g., the child is happy).
In Modern English, adjectives are not inflected so as to agree in form with the noun they modify, as adjectives in most other Indo-European languages do. For example, in the phrases the slender boy, and many slender girls, the adjective slender does not change form to agree with either the number or gender of the noun.
Some adjectives are inflected for degree of comparison, with the positive degree unmarked, the suffix -er marking the comparative, and -est marking the superlative: a small boy, the boy is smaller than the girl, that boy is the smallest. Some adjectives have irregular suppletive comparative and superlative forms, such as good, better, and best. Other adjectives have comparatives formed by periphrastic constructions, with the adverb more marking the comparative, and most marking the superlative: happier or more happy, the happiest or most happy.[182] There is some variation among speakers regarding which adjectives use inflected or periphrastic comparison, and some studies have shown a tendency for the periphrastic forms to become more common at the expense of the inflected form.[183]
English determiners are words such as the, each, many, some, and which, occurring most typically in noun phrases before the head nouns and any modifiers and marking the noun phrase as definite or indefinite.[184] They often agree with the noun in number. They do not typically inflect for degree of comparison.
English pronouns conserve many traits of case and gender inflection. The personal pronouns retain a difference between subjective and objective case in most persons (I/me, he/him, she/her, we/us, they/them) as well as an animateness distinction in the third person singular (distinguishing it from the three sets of animate third person singular pronouns) and an optional gender distinction in the animate third person singular (distinguishing between she/her [feminine], they/them [epicene], and he/him [masculine]).[185][186] The subjective case corresponds to the Old English nominative case, and the objective case is used in the sense both of the previous accusative case (for a patient, or direct object of a transitive verb), and of the Old English dative case (for a recipient or indirect object of a transitive verb).[187][188] The subjective is used when the pronoun is the subject of a finite clause, otherwise the objective is used.[189] While grammarians such as Henry Sweet[190] and Otto Jespersen[191] noted that the English cases did not correspond to the traditional Latin-based system, some contemporary grammars, for example Huddleston & Pullum (2002), retain traditional labels for the cases, calling them nominative and accusative cases respectively.
Possessive pronouns exist in dependent and independent forms; the dependent form functions as a determiner specifying a noun (as in my chair), while the independent form can stand alone as if it were a noun (e.g. the chair is mine).[192] The English system of grammatical person no longer has a distinction between formal and informal pronouns of address (the old second person singular familiar pronoun thou acquired a pejorative or inferior tinge of meaning and was abandoned).
Both the second and third persons share pronouns between the plural and singular:
English verbs are inflected for tense and aspect and marked for agreement with present-tense third-person singular subject. Only the copula verb to be is still inflected for agreement with the plural and first and second person subjects.[182] Auxiliary verbs such as have and be are paired with verbs in the infinitive, past, or progressive forms. They form complex tenses, aspects, and moods. Auxiliary verbs differ from other verbs in that they can be followed by the negation, and in that they can occur as the first constituent in a question sentence.[197][198]
Most verbs have six inflectional forms. The primary forms are a plain present, a third-person singular present, and a preterite (past) form. The secondary forms are a plain form used for the infinitive, a gerund-participle and a past participle.[199] The copula verb to be is the only verb to retain some of its original conjugation, and takes different inflectional forms depending on the subject. The first-person present-tense form is am, the third person singular form is is, and the form are is used in the second-person singular and all three plurals. The only verb past participle is been and its gerund-participle is being.
English has two primary tenses, past (preterite) and non-past. The preterite is inflected by using the preterite form of the verb, which for the regular verbs includes the suffix -ed, and for the strong verbs either the suffix -t or a change in the stem vowel. The non-past form is unmarked except in the third person singular, which takes the suffix -s.[197]
English does not have future verb forms.[200] The future tense is expressed periphrastically with one of the auxiliary verbs will or shall.[201] Many varieties also use a near future constructed with the phrasal verb be going to ("going-to future").[202]
Further aspectual distinctions are shown by auxiliary verbs, primarily have and be, which show the contrast between a perfect and non-perfect past tense (I have run vs. I was running), and compound tenses such as preterite perfect (I had been running) and present perfect (I have been running).[203]
For the expression of mood, English uses a number of modal auxiliaries, such as can, may, will, shall and the past tense forms could, might, would, should. There are also subjunctive and imperative moods, both based on the plain form of the verb (i.e. without the third person singular -s), for use in subordinate clauses (e.g. subjunctive: It is important that he run every day; imperative Run!).[201]
An infinitive form, that uses the plain form of the verb and the preposition to, is used for verbal clauses that are syntactically subordinate to a finite verbal clause. Finite verbal clauses are those that are formed around a verb in the present or preterite form. In clauses with auxiliary verbs, they are the finite verbs and the main verb is treated as a subordinate clause.[204] For example, he has to go where only the auxiliary verb have is inflected for time and the main verb to go is in the infinitive, or in a complement clause such as I saw him leave, where the main verb is see, which is in a preterite form, and leave is in the infinitive.
English also makes frequent use of constructions traditionally called phrasal verbs, verb phrases that are made up of a verb root and a preposition or particle that follows the verb. The phrase then functions as a single predicate. In terms of intonation the preposition is fused to the verb, but in writing it is written as a separate word. Examples of phrasal verbs are to get up, to ask out, to back up, to give up, to get together, to hang out, to put up with, etc. The phrasal verb frequently has a highly idiomatic meaning that is more specialised and restricted than what can be simply extrapolated from the combination of verb and preposition complement (e.g. lay off meaning terminate someone's employment).[205] In spite of the idiomatic meaning, some grammarians, including Huddleston & Pullum (2002:274), do not consider this type of construction to form a syntactic constituent and hence refrain from using the term "phrasal verb". Instead, they consider the construction simply to be a verb with a prepositional phrase as its syntactic complement, i.e. he woke up in the morning and he ran up in the mountains are syntactically equivalent.
The function of adverbs is to modify the action or event described by the verb by providing additional information about the manner in which it occurs.[174] Many adverbs are derived from adjectives by appending the suffix -ly. For example, in the phrase the woman walked quickly, the adverb quickly is derived in this way from the adjective quick. Some commonly used adjectives have irregular adverbial forms, such as good, which has the adverbial form well.
Modern English syntax language is moderately analytic.[206] It has developed features such as modal verbs and word order as resources for conveying meaning. Auxiliary verbs mark constructions such as questions, negative polarity, the passive voice and progressive aspect.
In most sentences, English only marks grammatical relations through word order.[208] The subject constituent precedes the verb and the object constituent follows it. The example below demonstrates how the grammatical roles of each constituent are marked only by the position relative to the verb:
An exception is found in sentences where one of the constituents is a pronoun, in which case it is doubly marked, both by word order and by case inflection, where the subject pronoun precedes the verb and takes the subjective case form, and the object pronoun follows the verb and takes the objective case form.[209] The example below demonstrates this double marking in a sentence where both object and subject are represented with a third person singular masculine pronoun:
Indirect objects (IO) of ditransitive verbs can be placed either as the first object in a double object construction (S V IO O), such as I gave Jane the book or in a prepositional phrase, such as I gave the book to Jane.[210]
In English a sentence may be composed of one or more clauses, that may, in turn, be composed of one or more phrases (e.g. Noun Phrases, Verb Phrases, and Prepositional Phrases). A clause is built around a verb and includes its constituents, such as any NPs and PPs. Within a sentence, there is always at least one main clause (or matrix clause) whereas other clauses are subordinate to a main clause. Subordinate clauses may function as arguments of the verb in the main clause. For example, in the phrase I think (that) you are lying, the main clause is headed by the verb think, the subject is I, but the object of the phrase is the subordinate clause (that) you are lying. The subordinating conjunction that shows that the clause that follows is a subordinate clause, but it is often omitted.[211] Relative clauses are clauses that function as a modifier or specifier to some constituent in the main clause: For example, in the sentence I saw the letter that you received today, the relative clause that you received today specifies the meaning of the word letter, the object of the main clause. Relative clauses can be introduced by the pronouns who, whose, whom and which as well as by that (which can also be omitted.)[212] In contrast to many other Germanic languages there are no major differences between word order in main and subordinate clauses.[213]
Negation is done with the adverb not, which precedes the main verb and follows an auxiliary verb. A contracted form of not -n't can be used as an enclitic attaching to auxiliary verbs and to the copula verb to be. Just as with questions, many negative constructions require the negation to occur with do-support, thus in Modern English I don't know him is the correct answer to the question Do you know him?, but not *I know him not, although this construction may be found in older English.[215]
Passive constructions also use auxiliary verbs. A passive construction rephrases an active construction in such a way that the object of the active phrase becomes the subject of the passive phrase, and the subject of the active phrase is either omitted or demoted to a role as an oblique argument introduced in a prepositional phrase. They are formed by using the past participle either with the auxiliary verb to be or to get, although not all varieties of English allow the use of passives with get. For example, putting the sentence she sees him into the passive becomes he is seen (by her), or he gets seen (by her).[216]
While English is a subject-prominent language, at the discourse level it tends to use a topic-comment structure, where the known information (topic) precedes the new information (comment). Because of the strict SVO syntax, the topic of a sentence generally has to be the grammatical subject of the sentence. In cases where the topic is not the grammatical subject of the sentence, it is often promoted to subject position through syntactic means. One way of doing this is through a passive construction, the girl was stung by the bee. Another way is through a cleft sentence where the main clause is demoted to be a complement clause of a copula sentence with a dummy subject such as it or there, e.g. it was the girl that the bee stung, there was a girl who was stung by a bee.[218] Dummy subjects are also used in constructions where there is no grammatical subject such as with impersonal verbs (e.g., it is raining) or in existential clauses (there are many cars on the street). Through the use of these complex sentence constructions with informationally vacuous subjects, English is able to maintain both a topic-comment sentence structure and a SVO syntax.
Focus constructions emphasise a particular piece of new or salient information within a sentence, generally through allocating the main sentence level stress on the focal constituent. For example, the girl was stung by a bee (emphasising it was a bee and not, for example, a wasp that stung her), or The girl was stung by a bee (contrasting with another possibility, for example that it was the boy).[219] Topic and focus can also be established through syntactic dislocation, either preposing or postposing the item to be focused on relative to the main clause. For example, That girl over there, she was stung by a bee, emphasises the girl by preposition, but a similar effect could be achieved by postposition, she was stung by a bee, that girl over there, where reference to the girl is established as an "afterthought".[220]
Cohesion between sentences is achieved through the use of deictic pronouns as anaphora (e.g. that is exactly what I mean where that refers to some fact known to both interlocutors, or then used to locate the time of a narrated event relative to the time of a previously narrated event).[221] Discourse markers such as oh, so or well, also signal the progression of ideas between sentences and help to create cohesion. Discourse markers are often the first constituents in sentences. Discourse markers are also used for stance taking in which speakers position themselves in a specific attitude towards what is being said, for example, no way is that true! (the idiomatic marker no way! expressing disbelief), or boy! I'm hungry (the marker boy expressing emphasis). While discourse markers are particularly characteristic of informal and spoken registers of English, they are also used in written and formal registers.[222]
Due to its status as an international language, English adopts foreign words quickly, and borrows vocabulary from many other sources. Early studies of English vocabulary by lexicographers, the scholars who formally study vocabulary, compile dictionaries, or both, were impeded by a lack of comprehensive data on actual vocabulary in use from good-quality linguistic corpora,[225] collections of actual written texts and spoken passages. Many statements published before the end of the 20th century about the growth of English vocabulary over time, the dates of first use of various words in English, and the sources of English vocabulary will have to be corrected as new computerised analysis of linguistic corpus data becomes available.[224][226]
English forms new words from existing words or roots in its vocabulary through a variety of processes. One of the most productive processes in English is conversion,[227] using a word with a different grammatical role, for example using a noun as a verb or a verb as a noun. Another productive word-formation process is nominal compounding,[224][226] producing compound words such as babysitter or ice cream or homesick.[227] A process more common in Old English than in Modern English, but still productive in Modern English, is the use of derivational suffixes (-hood, -ness, -ing, -ility) to derive new words from existing words (especially those of Germanic origin) or stems (especially for words of Latin or Greek origin).
Formation of new words, called neologisms, based on Greek and/or Latin roots (for example television or optometry) is a highly productive process in English and in most modern European languages, so much so that it is often difficult to determine in which language a neologism originated. For this reason, American lexicographer Philip Gove attributed many such words to the "international scientific vocabulary" (ISV) when compiling Webster's Third New International Dictionary (1961). Another active word-formation process in English are acronyms,[228] words formed by pronouncing as a single word abbreviations of longer phrases, e.g. NATO, laser.
English, besides forming new words from existing words and their roots, also borrows words from other languages. This adoption of words from other languages is commonplace in many world languages, but English has been especially open to borrowing of foreign words throughout the last 1,000 years.[230] The most commonly used words in English are West Germanic.[231] The words in English learned first by children as they learn to speak, particularly the grammatical words that dominate the word count of both spoken and written texts, are mainly the Germanic words inherited from the earliest periods of the development of Old English.[224]
But one of the consequences of long language contact between French and English in all stages of their development is that the vocabulary of English has a very high percentage of "Latinate" words (derived from French, especially, and also from other Romance languages and Latin). French words from various periods of the development of French now make up one-third of the vocabulary of English.[232] Linguist Anthony Lacoudre estimated that over 40,000 English words are of French origin and may be understood without orthographical change by French speakers.[233] Words of Old Norse origin have entered the English language primarily from the contact between Old Norse and Old English during colonisation of eastern and northern England. Many of these words are part of English core vocabulary, such as egg and knife.[234]
English has also borrowed many words directly from Latin, the ancestor of the Romance languages, during all stages of its development.[226][224] Many of these words had earlier been borrowed into Latin from Greek. Latin or Greek are still highly productive sources of stems used to form vocabulary of subjects learned in higher education such as the sciences, philosophy, and mathematics.[235] English continues to gain new loanwords and calques ("loan translations") from languages all over the world, and words from languages other than the ancestral Anglo-Saxon language make up about 60% of the vocabulary of English.[236]
English has formal and informal speech registers; informal registers, including child-directed speech, tend to be made up predominantly of words of Anglo-Saxon origin, while the percentage of vocabulary that is of Latinate origin is higher in legal, scientific, and academic texts.[237][238]
English has had a strong influence on the vocabulary of other languages.[232][239] The influence of English comes from such factors as opinion leaders in other countries knowing the English language, the role of English as a world lingua franca, and the large number of books and films that are translated from English into other languages.[240] That pervasive use of English leads to a conclusion in many places that English is an especially suitable language for expressing new ideas or describing new technologies. Among varieties of English, it is especially American English that influences other languages.[241] Some languages, such as Chinese, write words borrowed from English mostly as calques, while others, such as Japanese, readily take in English loanwords written in sound-indicating script.[242] Dubbed films and television programmes are an especially fruitful source of English influence on languages in Europe.[242]
Since the ninth century, English has been written in a Latin alphabet (also called Roman alphabet). Earlier Old English texts in Anglo-Saxon runes are only short inscriptions. The great majority of literary works in Old English that survive to today are written in the Roman alphabet.[37] The modern English alphabet contains 26 letters of the Latin script: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z (which also have capital forms: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z).
The spelling system, or orthography, of English is multi-layered and complex, with elements of French, Latin, and Greek spelling on top of the native Germanic system.[243] Further complications have arisen through sound changes with which the orthography has not kept pace.[50] Compared to European languages for which official organisations have promoted spelling reforms, English has spelling that is a less consistent indicator of pronunciation, and standard spellings of words that are more difficult to guess from knowing how a word is pronounced.[244] There are also systematic spelling differences between British and American English. These situations have prompted proposals for spelling reform in English.[245]
Although letters and speech sounds do not have a one-to-one correspondence in standard English spelling, spelling rules that take into account syllable structure, phonetic changes in derived words, and word accent are reliable for most English words.[246] Moreover, standard English spelling shows etymological relationships between related words that would be obscured by a closer correspondence between pronunciation and spelling, for example the words photograph, photography, and photographic,[246] or the words electricity and electrical. While few scholars agree with Chomsky and Halle (1968) that conventional English orthography is "near-optimal",[243] there is a rationale for current English spelling patterns.[247] The standard orthography of English is the most widely used writing system in the world.[248] Standard English spelling is based on a graphomorphemic segmentation of words into written clues of what meaningful units make up each word.[249]
For the vowel sounds of the English language, however, correspondences between spelling and pronunciation are more irregular. There are many more vowel phonemes in English than there are single vowel letters (a, e, i, o, u, w, y). As a result, some "long vowels" are often indicated by combinations of letters (like the oa in boat, the ow in how, and the ay in stay), or the historically based silent e (as in note and cake).[247]
The consequence of this complex orthographic history is that learning to read and write can be challenging in English. It can take longer for school pupils to become independently fluent readers of English than of many other languages, including Italian, Spanish, and German.[251] Nonetheless, there is an advantage for learners of English reading in learning the specific sound-symbol regularities that occur in the standard English spellings of commonly used words.[246] Such instruction greatly reduces the risk of children experiencing reading difficulties in English.[252][253] Making primary school teachers more aware of the primacy of morpheme representation in English may help learners learn more efficiently to read and write English.[254]
English writing also includes a system of punctuation marks that is similar to those used in most alphabetic languages around the world. The purpose of punctuation is to mark meaningful grammatical relationships in sentences to aid readers in understanding a text and to indicate features important for reading a text aloud.[255]
Dialectologists identify many English dialects, which usually refer to regional varieties that differ from each other in terms of patterns of grammar, vocabulary, and pronunciation. The pronunciation of particular areas distinguishes dialects as separate regional accents. The major native dialects of English are often divided by linguists into the two extremely general categories of British English (BrE) and North American English (NAE).[256] There also exists a third common major grouping of English varieties: Southern Hemisphere English, the most prominent being Australian and New Zealand English.
Since the English language first evolved in Britain and Ireland, the archipelago is home to the most diverse dialects, particularly in England. Within the United Kingdom, the Received Pronunciation (RP), an educated dialect of South East England, is traditionally used as the broadcast standard and is considered the most prestigious of the British dialects. The spread of RP (also known as BBC English) through the media has caused many traditional dialects of rural England to recede, as youths adopt the traits of the prestige variety instead of traits from local dialects. At the time of the Survey of English Dialects, grammar and vocabulary differed across the country, but a process of lexical attrition has led most of this variation to disappear.[257]
English in England can be divided into four major dialect regions, Southwest English, South East English, Midlands English, and Northern English. Within each of these regions several local subdialects exist: Within the Northern region, there is a division between the Yorkshire dialects and the Geordie dialect spoken in Northumbria around Newcastle, and the Lancashire dialects with local urban dialects in Liverpool (Scouse) and Manchester (Mancunian). Having been the centre of Danish occupation during the Viking Invasions, Northern English dialects, particularly the Yorkshire dialect, retain Norse features not found in other English varieties.[261]
Scots is today considered a separate language from English, but it has its origins in early Northern Middle English[266] and developed and changed during its history with influence from other sources, particularly Scots Gaelic and Old Norse. Scots itself has a number of regional dialects. And in addition to Scots, Scottish English comprises the varieties of Standard English spoken in Scotland; most varieties are Northern English accents, with some influence from Scots.[267]
In Ireland, various forms of English have been spoken since the Norman invasions of the 11th century. In County Wexford, in the area surrounding Dublin, two extinct dialects known as Forth and Bargy and Fingallian developed as offshoots from Early Middle English, and were spoken until the 19th century. Modern Irish English, however, has its roots in English colonisation in the 17th century. Today Irish English is divided into Ulster English, the Northern Ireland dialect with strong influence from Scots, and various dialects of the Republic of Ireland. Like Scottish and most North American accents, almost all Irish accents preserve the rhoticity which has been lost in the dialects influenced by RP.[22][268]
North American English has been regarded as fairly homogeneous compared to British English but this has been disputed.[269] Today, American accent variation is often increasing at the regional level and decreasing at the very local level,[270] though most Americans still speak within a phonological continuum of similar accents,[271] known collectively as General American (GA), with differences hardly noticed even among Americans themselves (such as Midland and Western American English).[272][273][274] In most American and Canadian English dialects, rhoticity (or r-fulness) is dominant, with non-rhoticity (r-dropping) becoming associated with lower prestige and social class especially after World War II; this contrasts with the situation in England, where non-rhoticity has become the standard.[275]
Today spoken primarily by working- and middle-class African Americans, African-American Vernacular English (AAVE) is also largely non-rhotic and likely originated among enslaved Africans and African Americans influenced primarily by the non-rhotic, non-standard older Southern dialects. A minority of linguists,[284] contrarily, propose that AAVE mostly traces back to African languages spoken by the slaves who had to develop a pidgin or Creole English to communicate with slaves of other ethnic and linguistic origins.[285] AAVE's important commonalities with Southern accents suggests it developed into a highly coherent and homogeneous variety in the 19th or early 20th century. AAVE is commonly stigmatised in North America as a form of "broken" or "uneducated" English, as are white Southern accents, but linguists today recognise both as fully developed varieties of English with their own norms shared by a large speech community.[286][287]
Since 1788, English has been spoken in Oceania, and Australian English has developed as a first language of the vast majority of the inhabitants of the Australian continent, its standard accent being General Australian. The English of neighbouring New Zealand has to a lesser degree become an influential standard variety of the language.[288] Australian and New Zealand English are each other's closest relatives with few differentiating characteristics, followed by South African English and the English of southeastern England, all of which have similarly non-rhotic accents, aside from some accents in the South Island of New Zealand. Australian and New Zealand English stand out for their innovative vowels: many short vowels are fronted or raised, whereas many long vowels have diphthongised. Australian English also has a contrast between long and short vowels, not found in most other varieties. Australian English grammar aligns closely to British and American English; like American English, collective plural subjects take on a singular verb (as in the government is rather than are).[289][290] New Zealand English uses front vowels that are often even higher than in Australian English.[291][292][293]
The first significant exposure of the Philippines to the English language occurred in 1762 when the British occupied Manila during the Seven Years' War, but this was a brief episode that had no lasting influence. English later became more important and widespread during American rule between 1898 and 1946, and remains an official language of the Philippines. Today, the use of English is ubiquitous in the Philippines, from street signs and marquees, government documents and forms, courtrooms, the media and entertainment industries, the business sector, and other aspects of daily life. One such usage that is also prominent in the country is in speech, where most Filipinos from Manila would use or have been exposed to Taglish, a form of code-switching between Tagalog and English. A similar code-switching method is used by urban native speakers of Bisayan languages called Bislish.
Several varieties of English are also spoken in the Caribbean islands that were colonial possessions of Britain, including Jamaica, and the Leeward and Windward Islands and Trinidad and Tobago, Barbados, the Cayman Islands, and Belize. Each of these areas is home both to a local variety of English and a local English-based creole, combining English and African languages. The most prominent varieties are Jamaican English and Jamaican Creole. In Central America, English-based creoles are spoken in on the Caribbean coasts of Nicaragua and Panama.[299] Locals are often fluent both in the local English variety and the local creole languages and code-switching between them is frequent, indeed another way to conceptualise the relationship between Creole and Standard varieties is to see a spectrum of social registers with the Creole forms serving as "basilect" and the more RP-like forms serving as the "acrolect", the most formal register.[300]
Article 1 of the Universal Declaration of Human Rights in English:[305]
Click on a date/time to view the file as it appeared at that time.
More than 100 pages use this file.
The following list shows the first 100 pages that use this file only.
A full list is available.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
The species is found on many Japanese islands, including Honshu, Shikoku, and Kyushu. Their habitats include both natural and artificial bodies of water, as well as forests and grasslands. They breed from spring to the beginning of summer, both sexes producing pheromones when ready to mate. Eggs are laid separately, hatching after about three weeks. They grow from larval to juvenile form in between five and six months. Juveniles eat soil-dwelling prey, and adults eat a wide variety of insects, tadpoles, and the eggs of their own species. They have several adaptations to avoid predators, although which they use depends on where they live. Several aspects of their biology have been studied, including their ability to regrow missing body parts.
The Japanese fire-bellied newt first diverged from its closest relative in the Middle Miocene, before splitting into four distinct varieties, each with a mostly separate range, although all four are formally recognized as composing a single species. Currently, their population is declining, and they face threats from disease and the pet trade. They can be successfully kept in captivity.
The Integrated Taxonomic Information System lists sixteen synonyms for Cynops pyrrhogaster.[9] Common names of the species include Japanese fire-bellied newt,[1] red-bellied newt,[10] and Japanese fire-bellied salamander.[11] Studies examining morphological and geographic variation had formerly recognized six races: Tohoku, Kanto, Atsumi, intermediate, Sasayama, and Hiroshima,[12] one of which, the Sasayama, was described as a subspecies in 1969 by Robert Mertens as Triturus pyrrhogaster sasayamae, which is now considered a synonym of C. pyrrhogaster.[2] Modern molecular analysis supports the division of C. pyrrhogaster into four clades instead.[12] In particular, the validity of the Sasayama and intermediate races has never been proven, with one study finding no behavioral differences between the two supposed forms.[13]
On the newt's upper body, the skin is dark brown, approaching black, and covered in wartlike bumps. The underbelly and the underside of its tail are bright red, with black spots.[4] Younger juveniles have creamy coloration instead of red, although most larger juveniles have some red present.[15] Adults from smaller islands tend to have more red on their ventral (belly) regions than those from larger islands, sometimes with extremely small spots or none at all. In general males tend to have more red than females.[16] Males can also be distinguished from females by their flat, wide tails and swelling around the ventral region.[17] An entirely red variant exists: that coloration is believed to be inherited and recessive. This variant is not confined to any single population, but is more common in the western half of Japan overall.[18]
Of the four clades, the northern is found in the districts of Tohoku and Kanto. This does not overlap with the range of the central clade, which is found in Chubu, northern Kansai, and eastern Chugoku. The central's range has a small amount of overlap with the western, which is found in southern Kinki, western Chugoku, Shikoku, and central Kyushu. The western also has some overlap with the southern clade, which is found in western and southern Kyushu.[12]
Courtship begins when the male approaches the female, sniffing its sides or cloaca. The male then brings its tail to the female and rapidly vibrates it. The female responds by pushing the male's neck with its snout. At this point, the male slowly moves away, undulating its tail, and the female follows, touching the tail with its snout when close enough. The male then deposits two to four spermatophores, one at a time, moving several centimeters after each, which the female attempts to pick up with its cloaca, sometimes unsuccessfully.[24] Females lay eggs separately on underwater objects, such as leaves and submerged grass roots, fertilized one by one from the spermatophores they carry. They can lay up to 40 eggs in one session, and 100 to 400 eggs in a breeding season.[24]
Newts in Mainland Japan have different antipredator behavior than newts on smaller islands. Individuals on smaller islands (for instance, Fukue Island) generally use a maneuver called the unken reflex, where they expose their bright red underbelly to attackers. As their main predators are birds, which are capable of distinguishing the color red, this technique is effective. In Mainland Japan the newts must also avoid mammalian predators, which cannot distinguish colors as well as avian hunters. This leads these populations to use the maneuver less, as it can result in death if attempted.[16]
Against snakes, newts from Fukue Island tend to perform tail-wagging displays, designed to bring a predator's attention to their replaceable tail rather than their more valuable head; those from Nagasaki Prefecture in Mainland Japan tend to simply flee. Snakes are present in both areas. This is likely because those from the mainland are adapted to escape from mammalian hunters, which are less likely to be repelled by such a display.[28]
Wild Japanese fire-bellied newts contain high levels of the neurotoxin tetrodotoxin (TTX).[29] This toxin inhibits the activity of sodium channels in most vertebrates, discouraging predation by both birds and mammals.[28] Experiments have shown the toxin is almost entirely derived from the newt's diet. When raised in captivity with no source of TTX, 36- to 70-week-old juveniles did not contain detectable levels, but wild specimens from the same original habitat had high toxicity. In younger captive-reared newts some TTX was still detected, which was inferred to have been transferred by adult females to their eggs.[29] In a follow-up experiment by the same team captive-reared newts were given food containing the neurotoxin. They readily consumed TTX-laced bloodworms when offered, not showing any symptoms after ingesting the poison. It was detectable in their bodies afterward, further indicating food to be the source of the toxin. No TTX-producing organisms are known from their habitat, but their existence is likely, and would explain the origin of TTX in wild newts.[30]
The International Union for the Conservation of Nature (IUCN) has ranked it as near-threatened. This assessment was made in 2020,[1] a shift from 2004 when it was rated least-concern.[31] It successfully reproduces in Australian zoos.[1] One major threat that C. pyrrhogaster faces is collection for the pet trade. The IUCN states that this trade needs to be ended immediately. Their population is decreasing, particularly near areas of human habitation.[1]
Japanese fire-bellied newts with mysterious skin lesions at Lake Biwa in Japan's Shiga Prefecture were found to be suffering from infections caused by a single-celled eukaryote in the order Dermocystida. The lesions contained cysts, which were filled with spores. Nearly all the lesions were external, although one was found on the liver. Globally, diseases are one of the causes for declining amphibian populations. There is concern that this affliction could spread to other nearby species, including Zhangixalus arboreus and Hynobius vandenburghi.[32]
A variety, believed to be found exclusively on the Atsumi Peninsula, was thought to have become extinct in the 1960s. Then, in 2016, a trio of researchers discovered that newts on the Chita Peninsula were very likely the same variant due to their similar morphological traits. Both groups share a preference for cooler temperature and have smooth and soft bodies, pale dorsal regions, and yellowish undersides. Even if still alive, this form is highly threatened and will soon be wiped out without immediate protection.[33]
Japanese fire-bellied newts serve as a highly useful model organism in laboratory settings, but they become more difficult to care for after metamorphosis. An experiment supported by the Japan Society for the Promotion of Science found that thiourea (TU) can prevent this process from occurring, allowing the animals to stay in their pre-metamorphosis form for as long as two years, while still capable of metamorphosizing when removed from the TU solution. This did not have any impact on their regeneration capabilities.[25]
Japanese fire-bellied newts produce motilin, a peptide that stimulates gastrointestinal contractions, identified in many vertebrates. It is created in the upper small intestine and pancreas. The discovery of the latter was the first time pancreatic motilin had been observed. The organ also produces insulin. These results represented the first discovery of motilin in amphibians, suggesting that it has a similar role for them as it does for birds and mammals. The existence of pancreatic motilin also indicated another, unknown function.[34]
This species, as well as other Urodele amphibians, is capable of regrowing missing body parts, including limbs with functional joints and the lower jaw.[35][36] When this process occurs, the regenerated tissue tends to mirror intact tissue in form.[35] It is also able to regrow missing lenses, taking thirty days to do so as a larva and eighty days as an adult. The difference in time is purely due to the size of the eye, and regenerative ability does not change; the discovery of this fact contradicted a popular claim that juvenile animals are quicker to regenerate than adults.[37]
Doctor of Veterinary Medicine Lianne McLeod described them as "low-maintenance", noting that captive newts enjoy bloodworms, brine shrimp, glass shrimp, Daphnia, and, for larger individuals, guppies.
In biology, a species is the basic unit of classification and a taxonomic rank of an organism, as well as a unit of biodiversity. A species is often defined as the largest group of organisms in which any two individuals of the appropriate sexes or mating types can produce fertile offspring, typically by sexual reproduction. Other ways of defining species include their karyotype, DNA sequence, morphology, behaviour or ecological niche. In addition, paleontologists use the concept of the chronospecies since fossil reproduction cannot be examined.
The most recent rigorous estimate for the total number of species of eukaryotes is between 8 and 8.7 million.[1][2][3] However, only about 14% of these had been described by 2011.[3]
All species (except viruses) are given a two-part name, a "binomial". The first part of a binomial is the genus to which the species belongs. The second part is called the specific name or the specific epithet (in botanical nomenclature, also sometimes in zoological nomenclature). For example, Boa constrictor is one of the species of the genus Boa, with constrictor being the species' epithet.
While the definitions given above may seem adequate at first glance, when looked at more closely they represent problematic species concepts. For example, the boundaries between closely related species become unclear with hybridisation, in a species complex of hundreds of similar microspecies, and in a ring species. Also, among organisms that reproduce only asexually, the concept of a reproductive species breaks down, and each clone is potentially a microspecies. Although none of these are entirely satisfactory definitions, and while the concept of species may not be a perfect model of life, it is still an incredibly useful tool to scientists and conservationists for studying life on Earth, regardless of the theoretical difficulties. If species were fixed and clearly distinct from one another, there would be no problem, but evolutionary processes cause species to change. This obliges taxonomists to decide, for example, when enough change has occurred to declare that a lineage should be divided into multiple chronospecies, or when populations have diverged to have enough distinct character states to be described as cladistic species.
Species were seen from the time of Aristotle until the 18th century as fixed categories that could be arranged in a hierarchy, the great chain of being. In the 19th century, biologists grasped that species could evolve given sufficient time. Charles Darwin's 1859 book On the Origin of Species explained how species could arise by natural selection. That understanding was greatly extended in the 20th century through genetics and population ecology. Genetic variability arises from mutations and recombination, while organisms themselves are mobile, leading to geographical isolation and genetic drift with varying selection pressures. Genes can sometimes be exchanged between species by horizontal gene transfer; new species can arise rapidly through hybridisation and polyploidy; and species may become extinct for a variety of reasons. Viruses are a special case, driven by a balance of mutation and selection, and can be treated as quasispecies.
Biologists and taxonomists have made many attempts to define species, beginning from morphology and moving towards genetics. Early taxonomists such as Linnaeus had no option but to describe what they saw: this was later formalised as the typological or morphological species concept. Ernst Mayr emphasised reproductive isolation, but this, like other species concepts, is hard or even impossible to test.[4][5] Later biologists have tried to refine Mayr's definition with the recognition and cohesion concepts, among others.[6] Many of the concepts are quite similar or overlap, so they are not easy to count: the biologist R. L. Mayden recorded about 24 concepts,[7] and the philosopher of science John Wilkins counted 26.[4] Wilkins further grouped the species concepts into seven basic kinds of concepts: (1) agamospecies for asexual organisms (2) biospecies for reproductively isolated sexual organisms (3) ecospecies based on ecological niches (4) evolutionary species based on lineage (5) genetic species based on gene pool (6) morphospecies based on form or phenotype and (7) taxonomic species, a species as determined by a taxonomist.[8]
A typological species is a group of organisms in which individuals conform to certain fixed properties (a type), so that even pre-literate people often recognise the same taxon as do modern taxonomists.[10][11] The clusters of variations or phenotypes within specimens (such as longer or shorter tails) would differentiate the species. This method was used as a "classical" method of determining species, such as with Linnaeus, early in evolutionary theory. However, different phenotypes are not necessarily different species (e.g. a four-winged Drosophila born to a two-winged mother is not a different species). Species named in this manner are called morphospecies.[12][13]
In the 1970s, Robert R. Sokal, Theodore J. Crovello and Peter Sneath proposed a variation on the morphological species concept, a phenetic species, defined as a set of organisms with a similar phenotype to each other, but a different phenotype from other sets of organisms.[14] It differs from the morphological species concept in including a numerical measure of distance or similarity to cluster entities based on multivariate comparisons of a reasonably large number of phenotypic traits.[15]
A mate-recognition species is a group of sexually reproducing organisms that recognise one another as potential mates.[16][17] Expanding on this to allow for post-mating isolation, a cohesion species is the most inclusive population of individuals having the potential for phenotypic cohesion through intrinsic cohesion mechanisms; no matter whether populations can hybridise successfully, they are still distinct cohesion species if the amount of hybridisation is insufficient to completely mix their respective gene pools.[18] A further development of the recognition concept is provided by the biosemiotic concept of species.[19]
The average nucleotide identity method quantifies genetic distance between entire genomes, using regions of about 10,000 base pairs. With enough data from genomes of one genus, algorithms can be used to categorize species, as for Pseudomonas avellanae in 2013,[22] and for all sequenced bacteria and archaea since 2020.[23]
DNA barcoding has been proposed as a way to distinguish species suitable even for non-specialists to use.[24] One of the barcodes is a region of mitochondrial DNA within the gene for cytochrome c oxidase. A database, Barcode of Life Data System, contains DNA barcode sequences from over 190,000 species.[25][26] However, scientists such as Rob DeSalle have expressed concern that classical taxonomy and DNA barcoding, which they consider a misnomer, need to be reconciled, as they delimit species differently.[27] Genetic introgression mediated by endosymbionts and other vectors can further make barcodes ineffective in the identification of species.[28]
An evolutionary species, suggested by George Gaylord Simpson in 1951, is "an entity composed of organisms which maintains its identity from other such entities through time and over space, and which has its own independent evolutionary fate and historical tendencies".[7][41] This differs from the biological species concept in embodying persistence over time. Wiley and Mayden stated that they see the evolutionary species concept as "identical" to Willi Hennig's species-as-lineages concept, and asserted that the biological species concept, "the several versions" of the phylogenetic species concept, and the idea that species are of the same kind as higher taxa are not suitable for biodiversity studies (with the intention of estimating the number of species accurately). They further suggested that the concept works for both asexual and sexually-reproducing species.[42] A version of the concept is Kevin de Queiroz's "General Lineage Concept of Species".[43]
An ecological species is a set of organisms adapted to a particular set of resources, called a niche, in the environment. According to this concept, populations form the discrete phenetic clusters that we recognise as species because the ecological and evolutionary processes controlling how resources are divided up tend to produce those clusters.[44]
A genetic species as defined by Robert Baker and Robert Bradley is a set of genetically isolated interbreeding populations. This is similar to Mayr's Biological Species Concept, but stresses genetic rather than reproductive isolation.[45] In the 21st century, a genetic species can be established by comparing DNA sequences, but other methods were available earlier, such as comparing karyotypes (sets of chromosomes) and allozymes (enzyme variants).[46]
An evolutionarily significant unit (ESU) or "wildlife species"[47] is a population of organisms considered distinct for purposes of conservation.[48]
In palaeontology, with only comparative anatomy (morphology) from fossils as evidence, the concept of a chronospecies can be applied. During anagenesis (evolution, not necessarily involving branching), palaeontologists seek to identify a sequence of species, each one derived from the phyletically extinct one before through continuous, slow and more or less uniform change. In such a time sequence, palaeontologists assess how much change is required for a morphologically distinct form to be considered a different species from its ancestors.[49][50][51][52]
Most modern textbooks make use of Ernst Mayr's 1942 definition,[60][61] known as the Biological Species Concept as a basis for further discussion on the definition of species. It is also called a reproductive or isolation concept. This defines a species as[62]
groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.[62]
It has been argued that this definition is a natural consequence of the effect of sexual reproduction on the dynamics of natural selection.[63][64][65][66] Mayr's use of the adjective "potentially" has been a point of debate; some interpretations exclude unusual or artificial matings that occur only in captivity, or that involve animals capable of mating but that do not normally do so in the wild.[62]
It is difficult to define a species in a way that applies to all organisms.[67] The debate about species concepts is called the species problem.[62][68][69][70] The problem was recognised even in 1859, when Darwin wrote in On the Origin of Species:
No one definition has satisfied all naturalists; yet every naturalist knows vaguely what he means when he speaks of a species. Generally the term includes the unknown element of a distinct act of creation.[71]
A simple textbook definition, following Mayr's concept, works well for most multi-celled organisms, but breaks down in several situations:
Species identification is made difficult by discordance between molecular and morphological investigations; these can be categorised as two types: (i) one morphology, multiple lineages (e.g. morphological convergence, cryptic species) and (ii) one lineage, multiple morphologies (e.g. phenotypic plasticity, multiple life-cycle stages).[81] In addition, horizontal gene transfer (HGT) makes it difficult to define a species.[82] All species definitions assume that an organism acquires its genes from one or two parents very like the "daughter" organism, but that is not what happens in HGT.[83] There is strong evidence of HGT between very dissimilar groups of prokaryotes, and at least occasionally between dissimilar groups of eukaryotes,[82] including some crustaceans and echinoderms.[84]
there is no easy way to tell whether related geographic or temporal forms belong to the same or different species. Species gaps can be verified only locally and at a point of time. One is forced to admit that Darwin's insight is correct: any local reality or integrity of species is greatly reduced over large geographic ranges and time periods.[18]
Wilkins writes that biologists such as the botanist Brent Mishler[85] have argued that the species concept is not valid, and that "if we were being true to evolution and the consequent phylogenetic approach to taxa, we should replace it with a 'smallest clade' idea" (a phylogenetic species concept).[86] Wilkins states that he concurs[87] with this approach, while noting the difficulties it would cause to taxonomy. He cites the ichthyologist Charles Tate Regan's early 20th century remark that "a species is whatever a suitably qualified biologist chooses to call a species".[86] Wilkins notes that the philosopher Philip Kitcher called this the "cynical species concept",[88] and arguing that far from being cynical, it usefully leads to an empirical taxonomy for any given group, based on taxonomists' experience.[86]
Blackberries belong to any of hundreds of microspecies of the Rubus fruticosus species aggregate.
Natural hybridisation presents a challenge to the concept of a reproductively isolated species, as fertile hybrids permit gene flow between two populations. For example, the carrion crow Corvus corone and the hooded crow Corvus cornix appear and are classified as separate species, yet they can hybridise where their geographical ranges overlap.[97]
Seven "species" of Larus gulls interbreed in a ring around the Arctic.
Opposite ends of the ring: a herring gull (Larus argentatus) (front) and a lesser black-backed gull (Larus fuscus) in Norway
Presumed evolution of five "species"  of greenish warblers around the Himalayas
The commonly used names for kinds of organisms are often ambiguous: "cat" could mean the domestic cat, Felis catus, or the cat family, Felidae. Another problem with common names is that they often vary from place to place, so that puma, cougar, catamount, panther, painter and mountain lion all mean Puma concolor in various parts of America, while "panther" may also mean the jaguar (Panthera onca) of Latin America or the leopard (Panthera pardus) of Africa and Asia. In contrast, the scientific names of species are chosen to be unique and universal; they are in two parts used together: the genus as in Puma, and the specific epithet as in concolor.[106][107]
A species is given a taxonomic name when a type specimen is described formally, in a publication that assigns it a unique scientific name. The description typically provides means for identifying the new species, which may not be based solely on morphology[108] (see cryptic species), differentiating it from other previously described and related or confusable species and provides a validly published name (in botany) or an available name (in zoology) when the paper is accepted for publication. The type material is usually held in a permanent repository, often the research collection of a major museum or university, that allows independent verification and the means to compare specimens.[109][110][111] Describers of new species are asked to choose names that, in the words of the International Code of Zoological Nomenclature, are "appropriate, compact, euphonious, memorable, and do not cause offence".[112]
Books and articles sometimes intentionally do not identify species fully, using the abbreviation "sp." in the singular or "spp." (standing for species pluralis, Latin for "multiple species") in the plural in place of the specific name or epithet (e.g. Canis sp.). This commonly occurs when authors are confident that some individuals belong to a particular genus but are not sure to which exact species they belong, as is common in paleontology.[113]
Authors may also use "spp." as a short way of saying that something applies to many species within a genus, but not to all. If scientists mean that something applies to all species within a genus, they use the genus name without the specific name or epithet. The names of genera and species are usually printed in italics. However, abbreviations such as "sp." should not be italicised.[113]
When a species's identity is not clear, a specialist may use "cf." before the epithet to indicate that confirmation is required. The abbreviations "nr." (near) or "aff." (affine) may be used when the identity is unclear but when the species appears to be similar to the species mentioned after.[113]
With the rise of online databases, codes have been devised to provide identifiers for species that are already defined, including:
The naming of a particular species, including which genus (and higher taxa) it is placed in, is a hypothesis about the evolutionary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be corroborated or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual organisms later identified as the same species. When two species names are discovered to apply to the same species, the older species name is given priority and usually retained, and the newer name considered as a junior synonym, a process called synonymy. Dividing a taxon into multiple, often new, taxa is called splitting. Taxonomists are often referred to as "lumpers" or "splitters" by their colleagues, depending on their personal approach to recognising differences or commonalities between organisms.[118][119][113] The circumscription of taxa, considered a taxonomic decision at the discretion of cognizant specialists, is not governed by the Codes of Zoological or Botanical Nomenclature.
The nomenclatural codes that guide the naming of species, including the ICZN for animals and the ICN for plants, do not make rules for defining the boundaries of the species. Research can change the boundaries, also known as circumscription, based on new evidence. Species may then need to be distinguished by the boundary definitions used, and in such cases the names may be qualified with sensu stricto ("in the narrow sense") to denote usage in the exact meaning given by an author such as the person who named the species, while the antonym sensu lato ("in the broad sense") denotes a wider usage, for instance including other subspecies. Other abbreviations such as "auct." ("author"), and qualifiers such as "non" ("not") may be used to further clarify the sense in which the specified authors delineated or described the species.[113][120][121]
Species are subject to change, whether by evolving into new species,[122] exchanging genes with other species,[123] merging with other species or by becoming extinct.[124]
Horizontal gene transfer between organisms of different species, either through hybridisation, antigenic shift, or reassortment, is sometimes an important source of genetic variation. Viruses can transfer genes between species. Bacteria can exchange plasmids with bacteria of other species, including some apparently distantly related ones in different phylogenetic domains, making analysis of their relationships difficult, and weakening the concept of a bacterial species.[129][82][130][123]
Louis-Marie Bobay and Howard Ochman suggest, based on analysis of the genomes of many types of bacteria, that they can often be grouped "into communities that regularly swap genes", in much the same way that plants and animals can be grouped into reproductively isolated breeding populations. Bacteria may thus form species, analogous to Mayr's biological species concept, consisting of asexually reproducing populations that exchange genes by homologous recombination.[131][132]
A species is extinct when the last individual of that species dies, but it may be functionally extinct well before that moment. It is estimated that over 99 percent of all species that ever lived on Earth, some five billion species, are now extinct. Some of these were in mass extinctions such as those at the ends of the Ordovician, Devonian, Permian, Triassic and Cretaceous periods. Mass extinctions had a variety of causes including volcanic activity, climate change, and changes in oceanic and atmospheric chemistry, and they in turn had major effects on Earth's ecology, atmosphere, land surface and waters.[133][134] Another form of extinction is through the assimilation of one species by another through hybridization. The resulting single species has been termed as a "compilospecies".[135]
Biologists and conservationists need to categorise and identify organisms in the course of their work. Difficulty assigning organisms reliably to a species constitutes a threat to the validity of research results, for example making measurements of how abundant a species is in an ecosystem moot. Surveys using a phylogenetic species concept reported 48% more species and accordingly smaller populations and ranges than those using nonphylogenetic concepts; this was termed "taxonomic inflation",[136] which could cause a false appearance of change to the number of endangered species and consequent political and practical difficulties.[137][138] Some observers claim that there is an inherent conflict between the desire to understand the processes of speciation and the need to identify and to categorise.[138]
Conservation laws in many countries make special provisions to prevent species from going extinct. Hybridization zones between two species, one that is protected and one that is not, have sometimes led to conflicts between lawmakers, land owners and conservationists. One of the classic cases in North America is that of the protected northern spotted owl which hybridises with the unprotected California spotted owl and the barred owl; this has led to legal debates.[139] It has been argued that the species problem is created by the varied uses of the concept of species, and that the solution is to abandon it and all other taxonomic ranks, and use unranked monophyletic groups instead. It has been argued, too, that since species are not comparable, counting them is not a valid measure of biodiversity; alternative measures of phylogenetic biodiversity have been proposed.[140][141]
When observers in the Early Modern period began to develop systems of organization for living things, they placed each kind of animal or plant into a context. Many of these early delineation schemes would now be considered whimsical: schemes included consanguinity based on colour (all plants with yellow flowers) or behaviour (snakes, scorpions and certain biting ants). John Ray, an English naturalist, was the first to attempt a biological definition of species in 1686, as follows:
In the 18th century, the Swedish scientist Carl Linnaeus classified organisms according to shared physical characteristics, and not simply based upon differences.[144] He established the idea of a taxonomic hierarchy of classification based upon observable characteristics and intended to reflect natural relationships.[145][146] At the time, however, it was still widely believed that there was no organic connection between species, no matter how similar they appeared. This view was influenced by European scholarly and religious education, which held that the categories of life are dictated by God, forming an Aristotelian hierarchy, the scala naturae or great chain of being. However, whether or not it was supposed to be fixed, the scala (a ladder) inherently implied the possibility of climbing.[147]
In viewing evidence of hybridisation, Linnaeus recognised that species were not fixed and could change; he did not consider that new species could emerge and maintained a view of divinely fixed species that may alter through processes of hybridisation or acclimatisation.[148] By the 19th century, naturalists understood that species could change form over time, and that the history of the planet provided enough time for major changes. Jean-Baptiste Lamarck, in his 1809 Zoological Philosophy, described the transmutation of species, proposing that a species could change over time, in a radical departure from Aristotelian thinking.[149]
In 1859, Charles Darwin and Alfred Russel Wallace provided a compelling account of evolution and the formation of new species. Darwin argued that it was populations that evolved, not individuals, by natural selection from naturally occurring variation among individuals.[150] This required a new definition of species. Darwin concluded that species are what they appear to be: ideas, provisionally useful for naming groups of interacting individuals, writing:
The term "mainland Japan" is used to distinguish the large islands of the Japanese archipelago from the remote, smaller islands; it refers to the main islands of Hokkaido, Honshu, Kyushu and Shikoku.[6] From 1943 until the end of the Pacific War, Karafuto Prefecture was designated part of the mainland.
The term "home islands" was used at the end of World War II to define the area where Japanese sovereignty and constitutional rule of its emperor would be restricted.[citation needed] The term is also commonly used today to distinguish the archipelago from Japan's colonies and other territories.[7]
Japanese archipelago, Sea of Japan and surrounding part of continental East Asia in Middle Pliocene to Late Pliocene (3.5-2 Ma)
The archipelago consists of 6,852 islands[8] (here defined as land more than 100 m in circumference), of which 430 are inhabited.[9] The five main islands, from north to south, are Hokkaido, Honshu, Shikoku, Kyushu, and Okinawa.[6] Honshu is the largest and referred to as the Japanese mainland.[10]
The Ryukyu Islands, which stretch towards Taiwan, are administered by Kagoshima Prefecture and Okinawa Prefecture
Seabed relief map, showing surface and underwater terrain and islands such as Minami-Tori-Shima, Benten-jima, Okinotorishima, and Yonaguni.
In ecology, the term habitat summarises the array of resources, physical and biotic factors that are present in an area, such as to support the survival and reproduction of a particular species. A species habitat can be seen as the physical manifestation of its ecological niche. Thus "habitat" is a species-specific term, fundamentally different from concepts such as environment or vegetation assemblages, for which the term "habitat-type" is more appropriate.[2]
The physical factors may include (for example): soil, moisture, range of temperature, and light intensity. Biotic factors will include the availability of food and the presence or absence of predators. Every species has particular habitat requirements, with habitat generalist species able to thrive in a wide array of environmental conditions while habitat specialist species requiring a very limited set of factors to survive. The habitat of a species is not necessarily found in a geographical area, it can be the interior of a stem, a rotten log, a rock or a clump of moss; a parasitic organism has as its habitat the body of its host, part of the host's body (such as the digestive tract), or a single cell within the host's body.[3]
The chief environmental factors affecting the distribution of living organisms are temperature, humidity, climate, soil and light intensity, and the presence or absence of all the requirements that the organism needs to sustain it. Generally speaking, animal communities are reliant on specific types of plant communities.[7]
Some plants and animals have habitat requirements which are met in a wide range of locations. The small white butterfly Pieris rapae for example is found on all the continents of the world apart from Antarctica. Its larvae feed on a wide range of Brassicas and various other plant species, and it thrives in any open location with diverse plant associations.[8] The large blue butterfly Phengaris arion is much more specific in its requirements; it is found only in chalk grassland areas, its larvae feed on Thymus species and because of complex lifecycle requirements it inhabits only areas in which Myrmica ants live.[9]
Disturbance is important in the creation of biodiverse habitat types. In the absence of disturbance, a climax vegetation cover develops that prevents the establishment of other species. Wildflower meadows are sometimes created by conservationists but most of the flowering plants used are either annuals or biennials and disappear after a few years in the absence of patches of bare ground on which their seedlings can grow.[10] Lightning strikes and toppled trees in tropical forests allow species richness to be maintained as pioneering species move in to fill the gaps created.[11] Similarly coastal habitat types can become dominated by kelp until the seabed is disturbed by a storm and the algae swept away, or shifting sediment exposes new areas for colonisation. Another cause of disturbance is when an area may be overwhelmed by an invasive introduced species which is not kept under control by natural enemies in its new habitat.[12]
Terrestrial habitat types include forests, grasslands, wetlands and deserts. Within these broad biomes are more specific habitat types with varying climate types, temperature regimes, soils, altitudes and vegetation. Many of these habitat types grade into each other and each one has its own typical communities of plants and animals. A habitat-type may suit a particular species well, but its presence or absence at any particular location depends to some extent on chance, on its dispersal abilities and its efficiency as a colonizer.[13]
Arid habitats are those where there is little available water. The most extreme arid habitats are deserts. Desert animals have a variety of adaptations to survive the dry conditions. Some frogs live in deserts, creating moist habitat types underground and hibernating while conditions are adverse. Couch's spadefoot toad (Scaphiopus couchii) emerges from its burrow when a downpour occurs and lays its eggs in the transient pools that form; the tadpoles develop with great rapidity, sometimes in as little as nine days, undergo metamorphosis, and feed voraciously before digging a burrow of their own.[14]
Other organisms cope with the drying up of their aqueous habitat in other ways. Vernal pools are ephemeral ponds that form in the rainy season and dry up afterwards. They have their specially-adapted characteristic flora, mainly consisting of annuals, the seeds of which survive the drought, but also some uniquely adapted perennials.[15] Animals adapted to these extreme habitat types also exist; fairy shrimps can lay "winter eggs" which are resistant to desiccation, sometimes being blown about with the dust, ending up in new depressions in the ground. These can survive in a dormant state for as long as fifteen years.[16] Some killifish behave in a similar way; their eggs hatch and the juvenile fish grow with great rapidity when the conditions are right, but the whole population of fish may end up as eggs in diapause in the dried up mud that was once a pond.[17]
Freshwater habitat types include rivers, streams, lakes, ponds, marshes and bogs.[18] Although some organisms are found across most of these habitat types, the majority have more specific requirements. The water velocity, its temperature and oxygen saturation are important factors, but in river systems, there are fast and slow sections, pools, bayous and backwaters which provide a range of habitat types. Similarly, aquatic plants can be floating, semi-submerged, submerged or grow in permanently or temporarily saturated soils besides bodies of water. Marginal plants provide important habitat for both invertebrates and vertebrates, and submerged plants provide oxygenation of the water, absorb nutrients and play a part in the reduction of pollution.[19]
Marine habitats include brackish water, estuaries, bays, the open sea, the intertidal zone, the sea bed, reefs and deep / shallow water zones.[18] Further variations include rock pools, sand banks, mudflats, brackish lagoons, sandy and pebbly beaches, and seagrass beds, all supporting their own flora and fauna. The benthic zone or seabed provides a home for both static organisms, anchored to the substrate, and for a large range of organisms crawling on or burrowing into the surface. Some creatures float among the waves on the surface of the water, or raft on floating debris, others swim at a range of depths, including organisms in the demersal zone close to the seabed, and myriads of organisms drift with the currents and form the plankton.[20]
Many animals and plants have taken up residence in urban environments. They tend to be adaptable generalists and use the town's features to make their homes. Rats and mice have followed man around the globe, pigeons, peregrines, sparrows, swallows and house martins use the buildings for nesting, bats use roof space for roosting, foxes visit the garbage bins and squirrels, coyotes, raccoons and skunks roam the streets. About 2,000 coyotes are thought to live in and around Chicago.[21] A survey of dwelling houses in northern European cities in the twentieth century found about 175 species of invertebrate inside them, including 53 species of beetle, 21 flies, 13 butterflies and moths, 13 mites, 9 lice, 7 bees, 5 wasps, 5 cockroaches, 5 spiders, 4 ants and a number of other groups.[22] In warmer climates, termites are serious pests in the urban habitat; 183 species are known to affect buildings and 83 species cause serious structural damage.[23]
A microhabitat is the small-scale physical requirements of a particular organism or population. Every habitat includes large numbers of microhabitat types with subtly different exposure to light, humidity, temperature, air movement, and other factors. The lichens that grow on the north face of a boulder are different from those that grow on the south face, from those on the level top, and those that grow on the ground nearby; the lichens growing in the grooves and on the raised surfaces are different from those growing on the veins of quartz. Lurking among these miniature "forests" are the microfauna, species of invertebrate, each with its own specific habitat requirements.[24]
There are numerous different microhabitat types in a wood; coniferous forest, broad-leafed forest, open woodland, scattered trees, woodland verges, clearings, and glades; tree trunk, branch, twig, bud, leaf, flower, and fruit; rough bark, smooth bark, damaged bark, rotten wood, hollow, groove, and hole; canopy, shrub layer, plant layer, leaf litter, and soil; buttress root, stump, fallen log, stem base, grass tussock, fungus, fern, and moss.[25] The greater the structural diversity in the wood, the greater the number of microhabitat types that will be present. A range of tree species with individual specimens of varying sizes and ages, and a range of features such as streams, level areas, slopes, tracks, clearings, and felled areas will provide suitable conditions for an enormous number of biodiverse plants and animals. For example, in Britain it has been estimated that various types of rotting wood are home to over 1700 species of invertebrate.[25]
For a parasitic organism, its habitat is the particular part of the outside or inside of its host on or in which it is adapted to live. The life cycle of some parasites involves several different host species, as well as free-living life stages, sometimes within vastly different microhabitat types.[26] One such organism is the trematode (flatworm) Microphallus turgidus, present in brackish water marshes in the southeastern United States. Its first intermediate host is a snail and the second, a glass shrimp. The final host is the waterfowl or mammal that consumes the shrimp.[27]
Although the vast majority of life on Earth lives in mesophyllic (moderate) environments, a few organisms, most of them microbes, have managed to colonise extreme environments that are unsuitable for more complex life forms. There are bacteria, for example, living in Lake Whillans, half a mile below the ice of Antarctica; in the absence of sunlight, they must rely on organic material from elsewhere, perhaps decaying matter from glacier melt water or minerals from the underlying rock.[28] Other bacteria can be found in abundance in the Mariana Trench, the deepest place in the ocean and on Earth; marine snow drifts down from the surface layers of the sea and accumulates in this undersea valley, providing nourishment for an extensive community of bacteria.[29]
Besides providing locomotion opportunities for winged animals and a conduit for the dispersal of pollen grains, spores and seeds, the atmosphere can be considered to be a habitat-type in its own right. There are metabolically active microbes present that actively reproduce and spend their whole existence airborne, with hundreds of thousands of individual organisms estimated to be present in a cubic meter of air. The airborne microbial community may be as diverse as that found in soil or other terrestrial environments, however these organisms are not evenly distributed, their densities varying spatially with altitude and environmental conditions. Aerobiology has not been studied much, but there is evidence of nitrogen fixation in clouds, and less clear evidence of carbon cycling, both facilitated by microbial activity.[39]
Whether from natural processes or the activities of man, landscapes and their associated habitat types change over time. There are the slow geomorphological changes associated with the geologic processes that cause tectonic uplift and subsidence, and the more rapid changes associated with earthquakes, landslides, storms, flooding, wildfires, coastal erosion, deforestation and changes in land use.[48] Then there are the changes in habitat types brought on by alterations in farming practices, tourism, pollution, fragmentation and climate change.[49]
Loss of habitat is the single greatest threat to any species. If an island on which an endemic organism lives becomes uninhabitable for some reason, the species will become extinct. Any type of habitat surrounded by a different habitat is in a similar situation to an island. If a forest is divided into parts by logging, with strips of cleared land separating woodland blocks, and the distances between the remaining fragments exceeds the distance an individual animal is able to travel, that species becomes especially vulnerable. Small populations generally lack genetic diversity and may be threatened by increased predation, increased competition, disease and unexpected catastrophe.[49] At the edge of each forest fragment, increased light encourages secondary growth of fast-growing species and old growth trees are more vulnerable to logging as access is improved. The birds that nest in their crevices, the epiphytes that hang from their branches and the invertebrates in the leaf litter are all adversely affected and biodiversity is reduced.[49] Habitat fragmentation can be ameliorated to some extent by the provision of wildlife corridors connecting the fragments. These can be a river, ditch, strip of trees, hedgerow or even an underpass to a highway. Without the corridors, seeds cannot disperse and animals, especially small ones, cannot travel through the hostile territory, putting populations at greater risk of local extinction.[50]
Habitat disturbance can have long-lasting effects on the environment. Bromus tectorum is a vigorous grass from Europe which has been introduced to the United States where it has become invasive. It is highly adapted to fire, producing large amounts of flammable detritus and increasing the frequency and intensity of wildfires. In areas where it has become established, it has altered the local fire regimen to such an extant that native plants cannot survive the frequent fires, allowing it to become even more dominant.[51] A marine example is when sea urchin populations "explode" in coastal waters and destroy all the macroalgae present. What was previously a kelp forest becomes an urchin barren that may last for years and this can have a profound effect on the food chain. Removal of the sea urchins, by disease for example, can result in the seaweed returning, with an over-abundance of fast-growing kelp.[52]
Habitat destruction (also termed habitat loss and habitat reduction) is the process by which a natural habitat becomes incapable of supporting its native species. The organisms that previously inhabited the site are displaced or dead, thereby reducing biodiversity and species abundance.[57][58] Habitat destruction is the leading cause of biodiversity loss.[59] Fragmentation and loss of habitat have become one of the most important topics of research in ecology as they are major threats to the survival of endangered species.[60]
Activities such as harvesting natural resources, industrial production and urbanization are human contributions to habitat destruction. Pressure from agriculture is the principal human cause. Some others include mining, logging, trawling, and urban sprawl. Habitat destruction is currently considered the primary cause of species extinction worldwide.[61] Environmental factors can contribute to habitat destruction more indirectly. Geological processes, climate change,[58] introduction of invasive species, ecosystem nutrient depletion, water and noise pollution are some examples. Loss of habitat can be preceded by an initial habitat fragmentation.
The protection of habitat types is a necessary step in the maintenance of biodiversity because if habitat destruction occurs, the animals and plants reliant on that habitat suffer. Many countries have enacted legislation to protect their wildlife. This may take the form of the setting up of national parks, forest reserves and wildlife reserves, or it may restrict the activities of humans with the objective of benefiting wildlife. The laws may be designed to protect a particular species or group of species, or the legislation may prohibit such activities as the collecting of bird eggs, the hunting of animals or the removal of plants. A general law on the protection of habitat types may be more difficult to implement than a site specific requirement. A concept introduced in the United States in 1973 involves protecting the critical habitat of endangered species, and a similar concept has been incorporated into some Australian legislation.[63]
International treaties may be necessary for such objectives as the setting up of marine reserves. Another international agreement, the Convention on the Conservation of Migratory Species of Wild Animals, protects animals that migrate across the globe and need protection in more than one country.[64] Even where legislation protects the environment, a lack of enforcement often prevents effective protection. However, the protection of habitat types needs to take into account the needs of the local residents for food, fuel and other resources. Faced with hunger and destitution, a farmer is likely to plough up a level patch of ground despite it being the last suitable habitat for an endangered species such as the San Quintin kangaroo rat, and even kill the animal as a pest.[65] In the interests of ecotourism it is desirable that local communities are educated on the uniqueness of their flora and fauna.[66]
The larva's appearance is generally very different from the adult form (e.g. caterpillars and butterflies) including different unique structures and organs that do not occur in the adult form. Their diet may also be considerably different.
Larvae are frequently adapted to different environments than adults. For example, some larvae such as tadpoles live almost exclusively in aquatic environments, but can live outside water as adult frogs. By living in a distinct environment, larvae may be given shelter from predators and reduce competition for resources with the adult population.
Animals in the larval stage will consume food to fuel their transition into the adult form. In some organisms like polychaetes and barnacles, adults are immobile but their larvae are mobile, and use their mobile larval form to distribute themselves.[1][2] These larvae used for dispersal are either planktotrophic (feeding) or lecithotrophic (non-feeding).
Some larvae are dependent on adults to feed them. In many eusocial Hymenoptera species, the larvae are fed by female workers. In Ropalidia marginata (a paper wasp) the males are also capable of feeding larvae but they are much less efficient, spending more time and getting less food to the larvae.[3]
The larvae of some organisms (for example, some newts) can become pubescent and do not develop further into the adult form. This is a type of neoteny.[4]
It is a misunderstanding that the larval form always reflects the group's evolutionary history. This could be the case, but often the larval stage has evolved secondarily, as in insects.[5][6] In these cases the larval form may differ more than the adult form from the group's common origin.[7]
Within Insects, only Endopterygotes show complete metamorphosis, including a distinct larval stage.[9][10] Several classifications have been suggested by many entomologists,[11][12] and following classification is based on Antonio Berlese classification in 1913. There are four main types of endopterygote larvae types:[13][14]
A juvenile is an individual organism (especially an animal) that has not yet reached its adult form, sexual maturity or size. Juveniles can look very different from the adult form, particularly in colour, and may not fill the same niche as the adult form.[1] In many organisms the juvenile has a different name from the adult (see List of animal names).
Many invertebrates, on reaching the adult stage, are fully mature and their development and growth stops. Their juveniles are larvae or nymphs.
In vertebrates and some invertebrates (e.g. spiders), larval forms (e.g. tadpoles) are usually considered a development stage of their own, and "juvenile" refers to a post-larval stage that is not fully grown and not sexually mature. In amniotes, the embryo represents the larval stage. Here, a "juvenile" is an individual in the time between hatching/birth/germination and reaching maturity.
This developmental biology article is a stub. You can help Wikipedia by expanding it.
Insects (from Latin insectum) are pancrustacean hexapod invertebrates of the class Insecta. They are the largest group within the arthropod phylum. Insects have a chitinous exoskeleton, a three-part body (head, thorax and abdomen), three pairs of jointed legs, compound eyes and one pair of antennae. Their blood is not totally contained in vessels; some circulates in an open cavity known as the haemocoel. Insects are the most diverse group of animals; they include more than a million described species and represent more than half of all known living organisms.[1][2] The total number of extant species is estimated at between six and ten million;[1][3][4] potentially over 90% of the animal life forms on Earth are insects.[4][5] Insects may be found in nearly all environments, although only a small number of species reside in the oceans, which are dominated by another arthropod group, crustaceans, which recent research has indicated insects are nested within.
Adult insects typically move about by walking, flying, or sometimes swimming. As it allows for rapid yet stable movement, many insects adopt a tripedal gait in which they walk with their legs touching the ground in alternating triangles, composed of the front and rear on one side with the middle on the other side. Insects are the only invertebrate group with members able to achieve sustained powered flight, and all flying insects derive from one common ancestor. Many insects spend at least part of their lives under water, with larval adaptations that include gills, and some adult insects are aquatic and have adaptations for swimming. Some species, such as water striders, are capable of walking on the surface of water. Insects are mostly solitary, but some, such as certain bees, ants and termites, are social and live in large, well-organized colonies. Some insects, such as earwigs, show maternal care, guarding their eggs and young. Insects can communicate with each other in a variety of ways. Male moths can sense the pheromones of female moths over great distances. Other species communicate with sounds: crickets stridulate, or rub their wings together, to attract a mate and repel other males. Lampyrid beetles communicate with light.
Humans regard certain insects as pests, and attempt to control them using insecticides, and a host of other techniques. Some insects damage crops by feeding on sap, leaves, fruits, or wood. Some species are parasitic, and may vector diseases. Some insects perform complex ecological roles; blow-flies, for example, help consume carrion but also spread diseases. Insect pollinators are essential to the life cycle of many flowering plant species on which most organisms, including humans, are at least partly dependent; without them, the terrestrial portion of the biosphere would be devastated.[7] Many insects are considered ecologically beneficial as predators and a few provide direct economic benefit. Silkworms produce silk and honey bees produce honey, and both have been domesticated by humans. Insects are consumed as food in 80% of the world's nations, by people in roughly 3000 ethnic groups.[8][9] Human activities also have effects on insect biodiversity.
In common parlance, insects are also called bugs, though this term usually includes all terrestrial arthropods.[a] The term is also occasionally extended to colloquial names for freshwater or marine crustaceans (e.g. Balmain bug, Moreton Bay bug, mudbug) and used by physicians and bacteriologists for disease-causing germs (e.g. superbugs), but entomologists to some extent reserve this term for a narrow category of "true bugs", insects of the order Hemiptera, such as cicadas and shield bugs.[16]
The precise definition of the taxon Insecta and the equivalent English name "insect" varies; three alternative definitions are shown in the table.
Although traditionally grouped with millipedes and centipedes,[22] more recent analysis indicates closer evolutionary ties with crustaceans. In the Pancrustacea theory, insects, together with Entognatha, Remipedia, and Cephalocarida, form a clade, the Pancrustacea.[23] Insects form a single clade, closely related to crustaceans and myriapods.[24]
Other terrestrial arthropods, such as centipedes, millipedes, scorpions, spiders, woodlice, mites, and ticks are sometimes confused with insects since their body plans can appear similar, sharing (as do all arthropods) a jointed exoskeleton. However, upon closer examination, their features differ significantly; most noticeably, they do not have the six-legged characteristic of adult insects.[25]
A phylogenetic tree of the arthropods places the insects in the context of other hexapods and the crustaceans, and the more distantly-related myriapods and chelicerates.[26]
Four large-scale radiations of insects have occurred: beetles (from about 300 million years ago), flies (from about 250 million years ago), moths and wasps (both from about 150 million years ago).[27] These four groups account for the majority of described species. 
The origins of insect flight remain obscure, since the earliest winged insects currently known appear to have been capable fliers. Some extinct insects had an additional pair of winglets attaching to the first segment of the thorax, for a total of three pairs. As of 2009, no evidence suggests the insects were a particularly successful group of animals before they evolved to have wings.[28]
The remarkably successful Hymenoptera (wasps, bees, and ants) appeared as long as 200 million years ago in the Triassic period, but achieved their wide diversity more recently in the Cenozoic era, which began 66 million years ago. Some highly successful insect groups evolved in conjunction with flowering plants, a powerful illustration of coevolution.[34]
The internal phylogeny is based on the works of Sroka, Staniczek & Bechly 2014,[35] Prokop et al. 2017[36] and Wipfler et al. 2019.[37]
Insects can be divided into two groups historically treated as subclasses: wingless insects, known as Apterygota, and winged insects, known as Pterygota. The Apterygota consisted of the primitively wingless orders Archaeognatha (jumping bristletails) and Zygentoma (silverfish). However, Apterygota is not a monophyletic group, as Archaeognatha are the sister group to all other insects, based on the arrangement of their mandibles, while Zygentoma and Pterygota are grouped together as Dicondylia. It was originally believed that Archaeognatha possessed a single phylogenetically primitive condyle each (thus the name "Monocondylia"), where all more derived insects have two, but this has since been shown to be incorrect; all insects, including Archaeognatha, have dicondylic mandibles, but archaeognaths possess two articulations that are homologous to those in other insects, though slightly different.[40] The Zygentoma themselves possibly are not monophyletic, with the family Lepidotrichidae being a sister group to the Dicondylia (Pterygota and the remaining Zygentoma).[41][42][clarification needed]
Paleoptera and Neoptera are the winged orders of insects differentiated by the presence of hardened body parts called sclerites, and in the Neoptera, muscles that allow their wings to fold flatly over the abdomen. Neoptera can further be divided into incomplete metamorphosis-based (Polyneoptera and Paraneoptera) and complete metamorphosis-based groups. It has proved difficult to clarify the relationships between the orders in Polyneoptera because of constant new findings calling for revision of the taxa. For example, the Paraneoptera have turned out to be more closely related to the Endopterygota than to the rest of the Exopterygota. The recent molecular finding that the traditional louse orders Mallophaga and Anoplura are derived from within Psocoptera has led to the new taxon Psocodea.[43] Phasmatodea and Embiidina have been suggested to form the Eukinolabia.[44] Mantodea, Blattodea, and Isoptera are thought to form a monophyletic group termed Dictyoptera.[45]
The Exopterygota likely are paraphyletic in regard to the Endopterygota. The Neuropterida are often lumped or split on the whims of the taxonomist. Fleas are now thought to be closely related to boreid mecopterans.[46] Many questions remain in the basal relationships among endopterygote orders, particularly the Hymenoptera.
Insects are prey for a variety of organisms, including terrestrial vertebrates. The earliest vertebrates on land existed 400 million years ago and were large amphibious piscivores. Through gradual evolutionary change, insectivory was the next diet type to evolve.[47]
Insects were among the earliest terrestrial herbivores and acted as major selection agents on plants.[34] Plants evolved chemical defenses against this herbivory and the insects, in turn, evolved mechanisms to deal with plant toxins. Many insects make use of these toxins to protect themselves from their predators. Such insects often advertise their toxicity using warning colors.[48] This successful evolutionary pattern has also been used by mimics. Over time, this has led to complex groups of coevolved species. Conversely, some interactions between plants and insects, like pollination, are beneficial to both organisms. Coevolution has led to the development of very specific mutualisms in such systems.
Estimates of the total number of insect species, or those within specific orders, often vary considerably. Globally, averages of these estimates suggest there are around 1.5 million beetle species and 5.5 million insect species, with about 1 million insect species currently found and described.[49] E. O. Wilson has estimated that the number of insects living at any one time are around 10 quintillion (10 billion billion).[50]
Between 950,000 and 1,000,000 of all described species are insects, so over 50% of all described eukaryotes (1.8 million) are insects (see illustration). With only 950,000 known non-insects, if the actual number of insects is 5.5 million, they may represent over 80% of the total. As only about 20,000 new species of all organisms are described each year, most insect species may remain undescribed, unless the rate of species descriptions greatly increases. Of the 24 orders of insects, four dominate in terms of numbers of described species; at least 670,000 identified species belong to Coleoptera, Diptera, Hymenoptera or Lepidoptera.
As of 2017, at least 66 insect species extinctions had been recorded in the previous 500 years, generally on oceanic islands.[52] Declines in insect abundance have been attributed to artificial lighting,[53] land use changes such as urbanization or agricultural use,[54][55] pesticide use,[56] and invasive species.[57] Studies summarized in a 2019 review suggested that a large proportion of insect species is threatened with extinction in the 21st century.[58] The ecologist Manu Sanders notes that the 2019 review was biased by mostly excluding data showing increases or stability in insect population, with the studies limited to specific geographic areas and specific groups of species.[59] A larger 2020 meta-study, analyzing data from 166 long-term surveys, suggested that populations of terrestrial insects are decreasing rapidly, by about 9% per decade.[60][61] Claims of pending mass insect extinctions or "insect apocalypse" based on a subset of these studies have been popularized in news reports, but often extrapolate beyond the study data or hyperbolize study findings.[62] Other areas have shown increases in some insect species, although trends in most regions are currently unknown. It is difficult to assess long-term trends in insect abundance or diversity because historical measurements are generally not known for many species. Robust data to assess at-risk areas or species is especially lacking for arctic and tropical regions and a majority of the southern hemisphere.[62]
The thorax is a tagma composed of three sections, the prothorax, mesothorax and the metathorax. The anterior segment, closest to the head, is the prothorax, with the major features being the first pair of legs and the pronotum. The middle segment is the mesothorax, with the major features being the second pair of legs and the anterior wings. The third and most posterior segment, abutting the abdomen, is the metathorax, which features the third pair of legs and the posterior wings. Each segment is delineated by an intersegmental suture. Each segment has four basic regions. The dorsal surface is called the tergum (or notum) to distinguish it from the abdominal terga.[38] The two lateral regions are called the pleura (singular: pleuron) and the ventral aspect is called the sternum. In turn, the notum of the prothorax is called the pronotum, the notum for the mesothorax is called the mesonotum and the notum for the metathorax is called the metanotum. Continuing with this logic, the mesopleura and metapleura, as well as the mesosternum and metasternum, are used.[64]
During growth insects goes through a various number of instars where the old exoskeleton is shed, but once they reach sexual maturity, they stop molting. The exceptions are apterygote (ancestrally wingless) insects. Mayflies are the only insects with a sexually immature instar with functional wings, called subimago.[65]
Having their muscles attached to their exoskeletons is efficient and allows more muscle connections.
The thoracic segments have one ganglion on each side, which are connected into a pair, one pair per segment. This arrangement is also seen in the abdomen but only in the first eight segments. Many species of insects have reduced numbers of ganglia due to fusion or reduction.[66] Some cockroaches have just six ganglia in the abdomen, whereas the wasp Vespa crabro has only two in the thorax and three in the abdomen. Some insects, like the house fly Musca domestica, have all the body ganglia fused into a single large thoracic ganglion. [67]
At least some insects have nociceptors, cells that detect and transmit signals responsible for the sensation of pain.[68][failed verification][69] This was discovered in 2003 by studying the variation in reactions of larvae of the common fruit-fly Drosophila to the touch of a heated probe and an unheated one. The larvae reacted to the touch of the heated probe with a stereotypical rolling behavior that was not exhibited when the larvae were touched by the unheated probe.[70] Although nociception has been demonstrated in insects, there is no consensus that insects feel pain consciously[71]
An insect uses its digestive system to extract nutrients and other substances from the food it consumes.[73] Most of this food is ingested in the form of macromolecules and other complex substances like proteins, polysaccharides, fats and nucleic acids. These macromolecules must be broken down by catabolic reactions into smaller molecules like amino acids and simple sugars before being used by cells of the body for energy, growth, or reproduction. This break-down process is known as digestion.
There is extensive variation among different orders, life stages, and even castes in the digestive system of insects.[74] This is the result of extreme adaptations to various lifestyles. The present description focuses on a generalized composition of the digestive system of an adult orthopteroid insect, which is considered basal to interpreting particularities of other groups.
Digestion starts in buccal cavity (mouth) as partially chewed food is broken down by saliva from the salivary glands. As the salivary glands produce fluid and carbohydrate-digesting enzymes (mostly amylases), strong muscles in the pharynx pump fluid into the buccal cavity, lubricating the food like the salivarium does, and helping blood feeders, and xylem and phloem feeders.
In the hindgut (element 16 in numbered diagram), or proctodaeum, undigested food particles are joined by uric acid to form fecal pellets. The rectum absorbs 90% of the water in these fecal pellets, and the dry pellet is then eliminated through the anus (element 17), completing the process of digestion. Envaginations at the anterior end of the hindgut form the Malpighian tubules, which form the main excretory system of insects.
Insect respiration is accomplished without lungs. Instead, the insect respiratory system uses a system of internal tubes and sacs through which gases either diffuse or are actively pumped, delivering oxygen directly to tissues that need it via their trachea (element 8 in numbered diagram). In most insects, air is taken in through openings on the sides of the abdomen and thorax called spiracles.
Some insects use parthenogenesis, a process in which the female can reproduce and give birth without having the eggs fertilized by a male. Many aphids undergo a form of parthenogenesis, called cyclical parthenogenesis, in which they alternate between one or many generations of asexual and sexual reproduction.[90][91] In summer, aphids are generally female and parthenogenetic; in the autumn, males may be produced for sexual reproduction. Other insects produced by parthenogenesis are bees, wasps and ants, in which they spawn males. However, overall, most individuals are female, which are produced by fertilization. The males are haploid and the females are diploid.[6]
Insect life-histories show adaptations to withstand cold and dry conditions. Some temperate region insects are capable of activity during winter, while some others migrate to a warmer climate or go into a state of torpor.[92] Still other insects have evolved mechanisms of diapause that allow eggs or pupae to survive these conditions.[93]
Metamorphosis in insects is the biological process of development all insects must undergo. There are two forms of metamorphosis: incomplete metamorphosis and complete metamorphosis.
Immature insects that go through incomplete metamorphosis are called nymphs or in the case of dragonflies and damselflies, also naiads. Nymphs are similar in form to the adult except for the presence of wings, which are not developed until adulthood. With each molt, nymphs grow larger and become more similar in appearance to adult insects.
Some insects display a rudimentary sense of numbers,[98] such as the solitary wasps that prey upon a single species. The mother wasp lays her eggs in individual cells and provides each egg with a number of live caterpillars on which the young feed when hatched. Some species of wasp always provide five, others twelve, and others as high as twenty-four caterpillars per cell. The number of caterpillars is different among species, but always the same for each sex of larva. The male solitary wasp in the genus Eumenes is smaller than the female, so the mother of one species supplies him with only five caterpillars; the larger female receives ten caterpillars in her cell.
A few insects, such as members of the families Poduridae and Onychiuridae (Collembola), Mycetophilidae (Diptera) and the beetle families Lampyridae, Phengodidae, Elateridae and Staphylinidae are bioluminescent. The most familiar group are the fireflies, beetles of the family Lampyridae. Some species are able to control this light generation to produce flashes. The function varies with some species using them to attract mates, while others use them to lure prey. Cave dwelling larvae of Arachnocampa (Mycetophilidae, fungus gnats) glow to lure small flying insects into sticky strands of silk.[99] Some fireflies of the genus Photuris mimic the flashing of female Photinus species to attract males of that species, which are then captured and devoured.[100] The colors of emitted light vary from dull blue (Orfelia fultoni, Mycetophilidae) to the familiar greens and the rare reds (Phrixothrix tiemanni, Phengodidae).[101]
Most insects, except some species of cave crickets, are able to perceive light and dark. Many species have acute vision capable of detecting minute movements. The eyes may include simple eyes or ocelli as well as compound eyes of varying sizes. Many species are able to detect light in the infrared, ultraviolet and visible light wavelengths. Color vision has been demonstrated in many species and phylogenetic analysis suggests that UV-green-blue trichromacy existed from at least the Devonian period between 416 and 359 million years ago.[102]
The individual lenses in compound eyes are immobile, and it was therefore presumed that insects were not able to focus. But research on fruit flies, which is the only insects studied so far, has shown that photoreceptor cells underneath each lens move rapidly in and out of focus in a series of movements called photoreceptor microsaccades. This gives them a much clearer image of the world than previously assumed.[103]
Very low sounds are also produced in various species of Coleoptera, Hymenoptera, Lepidoptera, Mantodea and Neuroptera. These low sounds are simply the sounds made by the insect's movement. Through microscopic stridulatory structures located on the insect's muscles and joints, the normal sounds of the insect moving are amplified and can be used to warn or communicate with other insects. Most sound-making insects also have tympanal organs that can perceive airborne sounds. Some species in Hemiptera, such as the corixids (water boatmen), are known to communicate via underwater sounds.[111] Most insects are also able to sense vibrations transmitted through surfaces.
Communication using surface-borne vibrational signals is more widespread among insects because of size constraints in producing air-borne sounds.[112] Insects cannot effectively produce low-frequency sounds, and high-frequency sounds tend to disperse more in a dense environment (such as foliage), so insects living in such environments communicate primarily using substrate-borne vibrations.[113] The mechanisms of production of vibrational signals are just as diverse as those for producing sound in insects.
Some species use vibrations for communicating within members of the same species, such as to attract mates as in the songs of the shield bug Nezara viridula.[114] Vibrations can also be used to communicate between entirely different species; lycaenid (gossamer-winged butterfly) caterpillars, which are myrmecophilous (living in a mutualistic association with ants) communicate with ants in this way.[115] The Madagascar hissing cockroach has the ability to press air through its spiracles to make a hissing noise as a sign of aggression;[116] the death's-head hawkmoth makes a squeaking noise by forcing air out of their pharynx when agitated, which may also reduce aggressive worker honey bee behavior when the two are close.[117]
Chemical communications in animals rely on a variety of aspects including taste and smell. Chemoreception is the physiological response of a sense organ (i.e. taste or smell) to a chemical stimulus where the chemicals act as signals to regulate the state or activity of a cell. A semiochemical is a message-carrying chemical that is meant to attract, repel, and convey information. Types of semiochemicals include pheromones and kairomones. One example is the butterfly Phengaris arion which uses chemical signals as a form of mimicry to aid in predation.[118]
The eusocial insects build nests, guard eggs, and provide food for offspring full-time.
Most insects, however, lead short lives as adults, and rarely interact with one another except to mate or compete for mates. A small number exhibit some form of parental care, where they will at least guard their eggs, and sometimes continue guarding their offspring until adulthood, and possibly even feeding them. Another simple form of parental care is to construct a nest (a burrow or an actual construction, either of which may be simple or complex), store provisions in it, and lay an egg upon those provisions. The adult does not contact the growing offspring, but it nonetheless does provide food. This sort of care is typical for most species of bees and various types of wasps.[123]
Insect flight has been a topic of great interest in aerodynamics due partly to the inability of steady-state theories to explain the lift generated by the tiny wings of insects. But insect wings are in motion, with flapping and vibrations, resulting in churning and eddies, and the misconception that physics says "bumblebees can't fly" persisted throughout most of the twentieth century.
Unlike birds, many small insects are swept along by the prevailing winds[127] although many of the larger insects are known to make migrations. Aphids are known to be transported long distances by low-level jet streams.[128] As such, fine line patterns associated with converging winds within weather radar imagery, like the WSR-88D radar network, often represent large groups of insects.[129] Radar can also be deliberately used to monitor insects.[130]
Many adult insects use six legs for walking, with an alternating tripod gait. This allows for rapid walking while always having a stable stance; it has been studied extensively in cockroaches and ants. For the first step, the middle right leg and the front and rear left legs are in contact with the ground and move the insect forward, while the front and rear right leg and the middle left leg are lifted and moved forward to a new position. When they touch the ground to form a new stable triangle the other legs can be lifted and brought forward in turn and so on.[131] The purest form of the tripedal gait is seen in insects moving at high speeds. However, this type of locomotion is not rigid and insects can adapt a variety of gaits. For example, when moving slowly, turning, avoiding obstacles, climbing or slippery surfaces, four (tetrapod) or more feet (wave-gait[132]) may be touching the ground. Insects can also adapt their gait to cope with the loss of one or more limbs.
Cockroaches are among the fastest insect runners and, at full speed, adopt a bipedal run to reach a high velocity in proportion to their body size. As cockroaches move very quickly, they need to be video recorded at several hundred frames per second to reveal their gait. More sedate locomotion is seen in the stick insects or walking sticks (Phasmatodea). A few insects have evolved to walk on the surface of the water, especially members of the Gerridae family, commonly known as water striders. A few species of ocean-skaters in the genus Halobates even live on the surface of open oceans, a habitat that has few insect species.[133]
Insect walking is of particular interest as practical form of robot locomotion. The study of insects and bipeds has a significant impact on possible robotic methods of transport. This may allow new hexapod robots to be designed that can traverse terrain that robots with wheels may be unable to handle.[131]
A large number of insects live either part or the whole of their lives underwater. In many of the more primitive orders of insect, the immature stages are spent in an aquatic environment. Some groups of insects, like certain water beetles, have aquatic adults as well.[80]
Many of these species have adaptations to help in under-water locomotion. Water beetles and water bugs have legs adapted into paddle-like structures. Dragonfly naiads use jet propulsion, forcibly expelling water out of their rectal chamber.[134] Some species like the water striders are capable of walking on the surface of water. They can do this because their claws are not at the tips of the legs as in most insects, but recessed in a special groove further up the leg; this prevents the claws from piercing the water's surface film.[80] Other insects such as the Rove beetle Stenus are known to emit pygidial gland secretions that reduce surface tension making it possible for them to move on the surface of water by Marangoni propulsion (also known by the German term Entspannungsschwimmen).[135][136]
Insects are mostly soft bodied, fragile and almost defenseless compared to other, larger lifeforms. The immature stages are small, move slowly or are immobile, and so all stages are exposed to predation and parasitism. Insects then have a variety of defense strategies to avoid being attacked by predators or parasitoids. These include camouflage, mimicry, toxicity and active defense.[140]
Camouflage is an important defense strategy, which involves the use of coloration or shape to blend into the surrounding environment.[141] This sort of protective coloration is common and widespread among beetle families, especially those that feed on wood or vegetation, such as many of the leaf beetles (family Chrysomelidae) or weevils. In some of these species, sculpturing or various colored scales or hairs cause the beetle to resemble bird dung or other inedible objects. Many of those that live in sandy environments blend in with the coloration of the substrate.[140] Most phasmids are known for effectively replicating the forms of sticks and leaves, and the bodies of some species (such as O. macklotti and Palophus centaurus) are covered in mossy or lichenous outgrowths that supplement their disguise. Very rarely, a species may have the ability to change color as their surroundings shift (Bostra scabrinota). In a further behavioral adaptation to supplement crypsis, a number of species have been noted to perform a rocking motion where the body is swayed from side to side that is thought to reflect the movement of leaves or twigs swaying in the breeze. Another method by which stick insects avoid predation and resemble twigs is by feigning death (catalepsy), where the insect enters a motionless state that can be maintained for a long period. The nocturnal feeding habits of adults also aids Phasmatodea in remaining concealed from predators.[142]
Pollination is the process by which pollen is transferred in the reproduction of plants, thereby enabling fertilisation and sexual reproduction. Most flowering plants require an animal to do the transportation. While other animals are included as pollinators, the majority of pollination is done by insects.[145] Because insects usually receive benefit for the pollination in the form of energy rich nectar it is a grand example of mutualism. The various flower traits (and combinations thereof) that differentially attract one type of pollinator or another are known as pollination syndromes. These arose through complex plant-animal adaptations. Pollinators find flowers through bright colorations, including ultraviolet, and attractant pheromones. The study of pollination by insects is known as anthecology.
Many insects are parasites of other insects such as the parasitoid wasps. These insects are known as entomophagous parasites. They can be beneficial due to their devastation of pests that can destroy crops and other resources. Many insects have a parasitic relationship with humans such as the mosquito. These insects are known to spread diseases such as malaria and yellow fever and because of such, mosquitoes indirectly cause more deaths of humans than any other animal.
Despite the large amount of effort focused at controlling insects, human attempts to kill pests with insecticides can backfire. If used carelessly, the poison can kill all kinds of organisms in the area, including insects' natural predators, such as birds, mice and other insectivores. The effects of DDT's use exemplifies how some insecticides can threaten wildlife beyond intended populations of pest insects.[148][149]
The economic value of pollination by insects has been estimated to be about $34 billion in the US alone.[153]
Products made by insects. Insects also produce useful substances such as honey, wax, lacquer and silk. Honey bees have been cultured by humans for thousands of years for honey, although contracting for crop pollination is becoming more significant for beekeepers. The silkworm has greatly affected human history, as silk-driven trade established relationships between China and the rest of the world.
Medical uses. Insects are also used in medicine, for example fly larvae (maggots) were formerly used to treat wounds to prevent or stop gangrene, as they would only consume dead flesh. This treatment is finding modern usage in some hospitals. Recently insects have also gained attention as potential sources of drugs and other medicinal substances.[157] Adult insects, such as crickets and insect larvae of various kinds, are also commonly used as fishing bait.[158]
Insects play important roles in biological research. For example, because of its small size, short generation time and high fecundity, the common fruit fly Drosophila melanogaster is a model organism for studies in the genetics of eukaryotes. D. melanogaster has been an essential part of studies into principles like genetic linkage, interactions between genes, chromosomal genetics, development, behavior and evolution. Because genetic systems are well conserved among eukaryotes, understanding basic cellular processes like DNA replication or transcription in fruit flies can help to understand those processes in other eukaryotes, including humans.[159] The genome of D. melanogaster was sequenced in 2000, reflecting the organism's important role in biological research. It was found that 70% of the fly genome is similar to the human genome, supporting the evolution theory.[160]
Because of the abundance of insects and a worldwide concern of food shortages, the Food and Agriculture Organization of the United Nations considers that the world may have to, in the future, regard the prospects of eating insects as a food staple. Insects are noted for their nutrients, having a high content of protein, minerals and fats and are eaten by one-third of the global population.[164]
Several insect species such as the black soldier fly or the housefly in their maggot forms, as well as beetle larvae such as mealworms can be processed and used as feed for farmed animals such as chicken, fish and pigs.[165]
Black soldier fly larvae can provide protein, fats for use in cosmetics,[166] and chitin.
Also, insect cooking oil, insect butter and fatty alcohols can be made from such insects as the superworm (Zophobas morio).[167][168]
Many species of insects are sold and kept as pets. There are special hobbyist magazines such as "Bugs" (now discontinued).[169]
A tadpole is the larval stage in the biological life cycle of an amphibian. Most tadpoles are fully aquatic, though some species of amphibians have tadpoles that are terrestrial. Tadpoles have some fish-like features that may not be found in adult amphibians such as a lateral line, gills and swimming tails. As they undergo metamorphosis, they start to develop functional lungs for breathing air, and the diet of tadpoles changes drastically.
Tadpoles are eaten as human food in some parts of the world and are mentioned in various folk tales from around the world.
The name tadpole is from Middle English taddepol, made up of the elements tadde, 'toad', and pol, 'head' (modern English poll). Similarly, pollywog / polliwog is from Middle English polwygle, made up of the same pol, 'head', and wiglen, 'to wiggle'.[1]
The life cycle of all amphibians involves a larval stage that is intermediate between embryo and adult. In most cases this larval stage is a limbless free-living organism that has a tail and is referred to as a tadpole, although in a few cases (e.g., in the Breviceps and Probreviceps genera of frogs) direct development occurs in which the larval stage is confined within the egg. Tadpoles of frogs are mostly herbivorous, while tadpoles of salamanders and caecilians are carnivorous.
Tadpoles of frogs and toads are usually globular, with a laterally compressed tail with which they swim by lateral undulation. When first hatched, anuran tadpoles have external gills that are eventually covered by skin, forming an opercular chamber with internal gills vented by spiracles. Depending on the species, there can be two spiracles on both sides of the body, a single spiracle on the underside near the vent, or a single spiracle on the left side of the body.[2] Newly hatched tadpoles are also equipped with a cement gland which allows them to attach to objects. The tadpoles have a cartilaginous skeleton and a notochord which eventually develops into a proper spinal cord. 
Anuran tadpoles are usually herbivorous, feeding on soft decaying plant matter. The gut of most tadpoles is long and spiral-shaped to efficiently digest organic matter and can be seen through the bellies of many species. Though many tadpoles will feed on dead animals if available to them, only a few species of frog have strictly carnivorous tadpoles, an example being the frogs of the family Ceratophryidae, their cannibalistic tadpoles having wide gaping mouths with which they devour other organisms, including other tadpoles. Another example is the tadpoles of the New Mexico spadefoot toad (Spea multiplicata) which will develop a carnivorous diet along with a broader head, larger jaw muscles, and a shorter gut if food is scarce, allowing them to consume fairy shrimp and their smaller herbivorous siblings.[3] A few genera such as Pipidae and Microhylidae have species whose tadpoles are filter feeders that swim through the water column feeding on plankton. Megophrys tadpoles feed at the water surface using unusual funnel-shaped mouths.[4]
As a frog tadpole matures it gradually develops its limbs, with the back legs growing first and the front legs second. The tail is absorbed into the body using apoptosis. Lungs develop around the time as the legs start growing, and tadpoles at this stage will often swim to the surface and gulp air. During the final stages of metamorphosis, the tadpole's mouth changes from a small, enclosed mouth at the front of the head to a large mouth the same width as the head. The intestines shorten as they transition from a herbivorous diet to the carnivorous diet of adult frogs.
While most anuran tadpoles inhabit wetlands, ponds, vernal pools, and other small bodies of water with slow moving water, a few species are adapted to different environments. Some frogs have terrestrial tadpoles, such as the family Ranixalidae, whose tadpoles are found in wet crevices near streams. The tadpoles of Micrixalus herrei are adapted to a fossorial lifestyle, with a muscular body and tail, eyes covered by a layer of skin, and reduced pigment.[8] Several frogs have stream dwelling tadpoles equipped with a strong oral sucker that allows them to hold onto rocks in fast flowing water, two examples being the Indian purple frog (Nasikabatrachus sahyadrensis) and the tailed frogs (Ascaphus) of Western North America. Although there are no marine tadpoles, the tadpoles of the crab-eating frog can cope with brackish water.[9]
Some anurans will provide parental care towards their tadpoles. Frogs of the genus Afrixalus will lay their eggs on leaves above water, folding the leaves around the eggs for protection. Female Pipa frogs will embed the eggs into their backs where they get covered by a thin layer of skin. The eggs will hatch underneath her skin and grow, eventually leaving as either large tadpoles (such as in Pipa parva) or as fully formed froglets (Pipa pipa). Female marsupial frogs (Hemiphractidae) will carry eggs on her back for various amounts of time, with it going as far as letting the tadpoles develop into tiny froglets in a pouch. Male African bullfrogs (Pyxicephalus adspersus) will keep watch over their tadpoles, attacking anything that might be a potential threat, even though he may eat some of the tadpoles himself.[10]
Males of the Emei mustache toads (Leptobrachium boringii) will construct nests along riverbanks where they breed with females and keep watch over the eggs, losing as much as 7.3% of their body mass in the time they spend protecting the nest.[11] Male midwife toads (Alytes) will carry eggs between their legs to protect them from predators, eventually releasing them into a body of water when they are ready to hatch. Poison dart frogs (Dendrobatidae) will carry their tadpoles to various locations, usually phytotelma, where they remain until metamorphosis. Some female dart frogs such as the strawberry poison dart frog (Oophaga pumilio) will regularly lay unfertilized eggs for the developing tadpoles to feed on.[12]
According to Sir George Scott, in the origin myths of the Wa people in China and Myanmar, the first Wa originated from two female ancestors Ya Htawm and Ya Htai, who spent their early phase as tadpoles ("rairoh") in a lake in the Wa country known as Nawng Hkaeo.[18]
In the Ancient Egyptian numerals, a hieroglyphic representing a tadpole was used to denote the value of 100,000.
Collective intelligence
Collective action
Self-organized criticality
Herd mentality
Phase transition
Agent-based modelling
Synchronization
Ant colony optimization
Particle swarm optimization
Swarm behaviour
Evolutionary computation
Genetic algorithms
Genetic programming
Artificial life
Machine learning
Evolutionary developmental biology
Artificial intelligence
Evolutionary robotics
In biology, adaptation has three related meanings. Firstly, it is the dynamic evolutionary process of natural selection that fits organisms to their environment, enhancing their evolutionary fitness. Secondly, it is a state reached by the population during that process. Thirdly, it is a phenotypic trait or adaptive trait, with a functional role in each individual organism, that is maintained and has evolved through natural selection. 
Historically, adaptation has been described from the time of the ancient Greek philosophers such as Empedocles and Aristotle. In 18th and 19th century natural theology, adaptation was taken as evidence for the existence of a deity. Charles Darwin proposed instead that it was explained by natural selection.
Adaptation is a major topic in the philosophy of biology, as it concerns function and purpose (teleology). Some biologists try to avoid terms which imply purpose in adaptation, not least because it suggests a deity's intentions, but others note that adaptation is necessarily purposeful.
Adaptation is an observable fact of life accepted by philosophers and natural historians from ancient times, independently of their views on evolution, but their explanations differed. Empedocles did not believe that adaptation required a final cause (a purpose), but thought that it "came about naturally, since such things survived." Aristotle did believe in final causes, but assumed that species were fixed.[1]
In natural theology, adaptation was interpreted as the work of a deity and as evidence for the existence of God.[2] William Paley believed that organisms were perfectly adapted to the lives they led, an argument that shadowed Gottfried Wilhelm Leibniz, who had argued that God had brought about "the best of all possible worlds." Voltaire's satire Dr. Pangloss[3] is a parody of this optimistic idea, and David Hume also argued against design.[4] Charles Darwin broke with the tradition by emphasising the flaws and limitations which occurred in the animal and plant worlds.[5]
Jean-Baptiste Lamarck proposed a tendency for organisms to become more complex, moving up a ladder of progress, plus "the influence of circumstances," usually expressed as use and disuse.[6] This second, subsidiary element of his theory is what is now called Lamarckism, a proto-evolutionary hypothesis of the inheritance of acquired characteristics, intended to explain adaptations by natural means.[7]
Other natural historians, such as Buffon, accepted adaptation, and some also accepted evolution, without voicing their opinions as to the mechanism. This illustrates the real merit of Darwin and Alfred Russel Wallace, and secondary figures such as Henry Walter Bates, for putting forward a mechanism whose significance had only been glimpsed previously. A century later, experimental field studies and breeding experiments by people such as E. B. Ford and Theodosius Dobzhansky produced evidence that natural selection was not only the 'engine' behind adaptation, but was a much stronger force than had previously been thought.[8][9][10]
The significance of an adaptation can only be understood in relation to the total biology of the species.
Adaptation is primarily a process rather than a physical form or part of a body.[12] An internal parasite (such as a liver fluke) can illustrate the distinction: such a parasite may have a very simple bodily structure, but nevertheless the organism is highly adapted to its specific environment. From this we see that adaptation is not just a matter of visible traits: in such parasites critical adaptations take place in the life cycle, which is often quite complex.[13] However, as a practical term, "adaptation" often refers to a product: those features of a species which result from the process. Many aspects of an animal or plant can be correctly called adaptations, though there are always some features whose function remains in doubt. By using the term adaptation for the evolutionary process, and adaptive trait for the bodily part or function (the product), one may distinguish the two different senses of the word.[14][15][16][17]
Adaptation is one of the two main processes that explain the observed diversity of species, such as the different species of Darwin's finches. The other process is speciation, in which new species arise, typically through reproductive isolation.[18][19] An example widely used today to study the interplay of adaptation and speciation is the evolution of cichlid fish in African lakes, where the question of reproductive isolation is complex.[20][21]
Adaptation is not always a simple matter where the ideal phenotype evolves for a given environment. An organism must be viable at all stages of its development and at all stages of its evolution. This places constraints on the evolution of development, behaviour, and structure of organisms. The main constraint, over which there has been much debate, is the requirement that each genetic and phenotypic change during evolution should be relatively small, because developmental systems are so complex and interlinked. However, it is not clear what "relatively small" should mean, for example polyploidy in plants is a reasonably common large genetic change.[22] The origin of eukaryotic endosymbiosis is a more dramatic example.[23]
All adaptations help organisms survive in their ecological niches. The adaptive traits may be structural, behavioural or physiological. Structural adaptations are physical features of an organism, such as shape, body covering, armament, and internal organization. Behavioural adaptations are inherited systems of behaviour, whether inherited in detail as instincts, or as a neuropsychological capacity for learning. Examples include searching for food, mating, and vocalizations. Physiological adaptations permit the organism to perform special functions such as making venom, secreting slime, and phototropism, but also involve more general functions such as growth and development, temperature regulation, ionic balance and other aspects of homeostasis. Adaptation affects all aspects of the life of an organism.[24]
The following definitions are given by the evolutionary biologist Theodosius Dobzhansky:
Adaptation differs from flexibility, acclimatization, and learning, all of which are changes during life which are not inherited. Flexibility deals with the relative capacity of an organism to maintain itself in different habitats: its degree of specialization. Acclimatization describes automatic physiological adjustments during life;[30] learning means improvement in behavioural performance during life.[31]
If humans move to a higher altitude, respiration and physical exertion become a problem, but after spending time in high altitude conditions they acclimatize to the reduced partial pressure of oxygen, such as by producing more red blood cells. The ability to acclimatize is an adaptation, but the acclimatization itself is not. The reproductive rate declines, but deaths from some tropical diseases also go down. Over a longer period of time, some people are better able to reproduce at high altitudes than others. They contribute more heavily to later generations, and gradually by natural selection the whole population becomes adapted to the new conditions. This has demonstrably occurred, as the observed performance of long-term communities at higher altitude is significantly better than the performance of new arrivals, even when the new arrivals have had time to acclimatize.[35]
There is a relationship between adaptedness and the concept of fitness used in population genetics. Differences in fitness between genotypes predict the rate of evolution by natural selection. Natural selection changes the relative frequencies of alternative phenotypes, insofar as they are heritable.[36] However, a phenotype with high adaptedness may not have high fitness. Dobzhansky mentioned the example of the Californian redwood, which is highly adapted, but a relict species in danger of extinction.[25] Elliott Sober commented that adaptation was a retrospective concept since it implied something about the history of a trait, whereas fitness predicts a trait's future.[37]
Sewall Wright proposed that populations occupy adaptive peaks on a fitness landscape. To evolve to another, higher peak, a population would first have to pass through a valley of maladaptive intermediate stages, and might be "trapped" on a peak that is not optimally adapted.[41]
Before Darwin, adaptation was seen as a fixed relationship between an organism and its habitat. It was not appreciated that as the climate changed, so did the habitat; and as the habitat changed, so did the biota. Also, habitats are subject to changes in their biota: for example, invasions of species from other areas. The relative numbers of species in a given habitat are always changing. Change is the rule, though much depends on the speed and degree of the change.
When the habitat changes, three main things may happen to a resident population: habitat tracking, genetic change or extinction. In fact, all three things may occur in sequence. Of these three effects only genetic change brings about adaptation.
When a habitat changes, the resident population typically moves to more suitable places; this is the typical response of flying insects or oceanic organisms, which have wide (though not unlimited) opportunity for movement.[43] This common response is called habitat tracking. It is one explanation put forward for the periods of apparent stasis in the fossil record (the punctuated equilibrium theory).[44]
Without mutation, the ultimate source of all genetic variation, there would be no genetic changes and no subsequent adaptation through evolution by natural selection. Genetic change occurs in a population when mutation increases or decreases in its initial frequency followed by random genetic drift, migration, recombination or natural selection act on this genetic variation.[45] One example is that the first pathways of enzyme-based metabolism at the very origin of life on Earth may have been co-opted components of the already-existing purine nucleotide metabolism, a metabolic pathway that evolved in an ancient RNA world. The co-option requires new mutations and through natural selection, the population then adapts genetically to its present circumstances.[10] Genetic changes may result in entirely new or gradual change to visible structures, or they may adjust physiological activity in a way that suits the habitat. The varying shapes of the beaks of Darwin's finches, for example, are driven by adaptive mutations in the ALX1 gene.[46] The coat color of different wild mouse species matches their environments, whether black lava or light sand, owing to adaptive mutations in the melanocortin 1 receptor and other melanin pathway genes.[47][48] Physiological resistance to the heart poisons (cardiac glycosides) that monarch butterflies store in their bodies to protect themselves from predators[49][50] are driven by adaptive mutations in the target of the poison, the sodium pump, resulting in target site insensitivity.[51][52][53] These same adaptive mutations and similar changes at the same amino acid sites were found to evolve in a parallel manner in distantly related insects that feed on the same plants, and even in a bird that feeds on monarchs through convergent evolution, a hallmark of adaptation.[54][55] Convergence at the gene-level across distantly related species can arise because of evolutionary constraint.[56]
Habitats and biota do frequently change over time and space. Therefore, it follows that the process of adaptation is never fully complete.[57] Over time, it may happen that the environment changes little, and the species comes to fit its surroundings better and better, resulting in stabilizing selection. On the other hand, it may happen that changes in the environment occur suddenly, and then the species becomes less and less well adapted. The only way for it to climb back up that fitness peak is via the introduction of new genetic variation for natural selection to act upon. Seen like this, adaptation is a genetic tracking process, which goes on all the time to some extent, but especially when the population cannot or does not move to another, less hostile area. Given enough genetic change, as well as specific demographic conditions, an adaptation may be enough to bring a population back from the brink of extinction in a process called evolutionary rescue. Adaptation does affect, to some extent, every species in a particular ecosystem.[58][59]
Leigh Van Valen thought that even in a stable environment, because of antagonistic species interactions and limited resources, a species must constantly had to adapt to maintain its relative standing. This became known as the Red Queen hypothesis, as seen in host-parasite interactions.[60]
Existing genetic variation and mutation were the traditional sources of material on which natural selection could act. In addition, horizontal gene transfer is possible between organisms in different species, using mechanisms as varied as gene cassettes, plasmids, transposons and viruses such as bacteriophages.[61][62][63]
In coevolution, where the existence of one species is tightly bound up with the life of another species, new or 'improved' adaptations which occur in one species are often followed by the appearance and spread of corresponding features in the other species. In other words, each species triggers reciprocal natural selection in the other. These co-adaptational relationships are intrinsically dynamic, and may continue on a trajectory for millions of years, as has occurred in the relationship between flowering plants and pollinating insects.[64][65]
It is a profound truth that Nature does not know best; that genetical evolution... is a story of waste, makeshift, compromise and blunder.
All adaptations have a downside: horse legs are great for running on grass, but they can't scratch their backs; mammals' hair helps temperature, but offers a niche for ectoparasites; the only flying penguins do is under water. Adaptations serving different functions may be mutually destructive. Compromise and makeshift occur widely, not perfection. Selection pressures pull in different directions, and the adaptation that results is some kind of compromise.[72]
Since the phenotype as a whole is the target of selection, it is impossible to improve simultaneously all aspects of the phenotype to the same degree.
Consider the antlers of the Irish elk, (often supposed to be far too large; in deer antler size has an allometric relationship to body size). Obviously, antlers serve positively for defence against predators, and to score victories in the annual rut. But they are costly in terms of resources. Their size during the last glacial period presumably depended on the relative gain and loss of reproductive capacity in the population of elks during that time.[74] As another example, camouflage to avoid detection is destroyed when vivid coloration is displayed at mating time. Here the risk to life is counterbalanced by the necessity for reproduction.[75]
Stream-dwelling salamanders, such as Caucasian salamander or Gold-striped salamander have very slender, long bodies, perfectly adapted to life at the banks of fast small rivers and mountain brooks. Elongated body protects their larvae from being washed out by current. However, elongated body increases risk of desiccation and decreases dispersal ability of the salamanders; it also negatively affects their fecundity. As a result, fire salamander, less perfectly adapted to the mountain brook habitats, is in general more successful, have a higher fecundity and broader geographic range.[76]
The peacock's ornamental train (grown anew in time for each mating season) is a famous adaptation. It must reduce his maneuverability and flight, and is hugely conspicuous; also, its growth costs food resources. Darwin's explanation of its advantage was in terms of sexual selection: "This depends on the advantage which certain individuals have over other individuals of the same sex and species, in exclusive relation to reproduction."[77] The kind of sexual selection represented by the peacock is called 'mate choice,' with an implication that the process selects the more fit over the less fit, and so has survival value.[78] The recognition of sexual selection was for a long time in abeyance, but has been rehabilitated.[79]
Pre-adaptation occurs when a population has characteristics which by chance are suited for a set of conditions not previously experienced. For example, the polyploid cordgrass Spartina townsendii is better adapted than either of its parent species to their own habitat of saline marsh and mud-flats.[86] Among domestic animals, the White Leghorn chicken is markedly more resistant to vitamin B1 deficiency than other breeds; on a plentiful diet this makes no difference, but on a restricted diet this preadaptation could be decisive.[87]
Features that now appear as adaptations sometimes arose by co-option of existing traits, evolved for some other purpose. The classic example is the ear ossicles of mammals, which we know from paleontological and embryological evidence originated in the upper and lower jaws and the hyoid bone of their synapsid ancestors, and further back still were part of the gill arches of early fish.[91][92] The word exaptation was coined to cover these common evolutionary shifts in function.[93] The flight feathers of birds evolved from the much earlier feathers of dinosaurs,[94] which might have been used for insulation or for display.[95][96]
Animals including earthworms, beavers and humans use some of their adaptations to modify their surroundings, so as to maximize their chances of surviving and reproducing. Beavers create dams and lodges, changing the ecosystems of the valleys around them. Earthworms, as Darwin noted, improve the topsoil in which they live by incorporating organic matter. Humans have constructed extensive civilizations with cities in environments as varied as the Arctic and hot deserts.
In all three cases, the construction and maintenance of ecological niches helps drive the continued selection of the genes of these animals, in an environment that the animals have modified.[97]
Some traits do not appear to be adaptive as they have a neutral or deleterious effect on fitness in the current environment. Because genes often have pleiotropic effects, not all traits may be functional: they may be what Stephen Jay Gould and Richard Lewontin called spandrels, features brought about by neighbouring adaptations, on the analogy with the often highly decorated triangular areas between pairs of arches in architecture, which began as functionless features.[98]
Another possibility is that a trait may have been adaptive at some point in an organism's evolutionary history, but a change in habitats caused what used to be an adaptation to become unnecessary or even maladapted. Such adaptations are termed vestigial. Many organisms have vestigial organs, which are the remnants of fully functional structures in their ancestors. As a result of changes in lifestyle the organs became redundant, and are either not functional or reduced in functionality. Since any structure represents some kind of cost to the general economy of the body, an advantage may accrue from their elimination once they are not functional. Examples: wisdom teeth in humans; the loss of pigment and functional eyes in cave fauna; the loss of structure in endoparasites.[99]
If a population cannot move or change sufficiently to preserve its long-term viability, then obviously, it will become extinct, at least in that locale. The species may or may not survive in other locales. Species extinction occurs when the death rate over the entire species exceeds the birth rate for a long enough period for the species to disappear. It was an observation of Van Valen that groups of species tend to have a characteristic and fairly regular rate of extinction.[100]
Just as there is co-adaptation, there is also coextinction, the loss of a species due to the extinction of another with which it is coadapted, as with the extinction of a parasitic insect following the loss of its host, or when a flowering plant loses its pollinator, or when a food chain is disrupted.[101][102]
The first stage in the evolution of life on earth is often hypothesized to be the RNA world in which short self-replicating RNA molecules proliferated before the evolution of DNA and proteins. By this hypothesis, life started when RNA chains began to self-replicate, initiating the three mechanisms of Darwinian selection: heritability, variation of type, and competition for resources. The fitness of an RNA replicator (its per capita rate of increase) would likely have been a function of its intrinsic adaptive capacities, determined by its nucleotide sequence, and the availability of resources.[103][104] The three primary adaptive capacities may have been: (1) replication with moderate fidelity, giving rise to heritability while allowing variation of type, (2) resistance to decay, and (3) acquisition of resources.[103][104] These adaptive capacities would have been determined by the folded configurations of the RNA replicators resulting from their nucleotide sequences.
Tetrodotoxin (TTX) is a potent neurotoxin. Its name derives from Tetraodontiformes, an order that includes pufferfish, porcupinefish, ocean sunfish, and triggerfish; several of these species carry the toxin. Although tetrodotoxin was discovered in these fish and found in several other animals (e.g., in blue-ringed octopuses, rough-skinned newts, and moon snails), it is actually produced by certain infecting or symbiotic bacteria like Pseudoalteromonas, Pseudomonas, and Vibrio as well as other species found in animals.[1][2]
Tetrodotoxin is a sodium channel blocker. It inhibits the firing of action potentials in neurons by binding to the voltage-gated sodium channels in nerve cell membranes and blocking the passage of sodium ions (responsible for the rising phase of an action potential) into the neuron. This prevents the nervous system from carrying messages and thus muscles from contracting in response to nervous stimulation.[3]
Its mechanism of action, selective blocking of the sodium channel, was shown definitively in 1964 by Toshio Narahashi and John W. Moore at Duke University, using the sucrose gap voltage clamp technique.[4]
Apart from their bacterial species of most likely ultimate biosynthetic origin (see below), tetrodotoxin has been isolated from widely differing animal species, including:[1]
Tarichatoxin was shown to be identical to TTX in 1964 by Mosher et al.,[10][11] and the identity of maculotoxin and TTX was reported in Science in 1978,[12] and the synonymity of these two toxins is supported in modern reports (e.g., at Pubchem[13] and in modern toxicology textbooks[14]) though historic monographs questioning this continue in reprint.[15]
The toxin is variously used by metazoans as a defensive biotoxin to ward off predation, or as both a defensive and predatory venom (e.g., in octopuses, chaetognaths, and ribbon worms).[16] Even though the toxin acts as a defense mechanism, some predators such as the common garter snake have developed insensitivity to TTX, which allows them to prey upon toxic newts.[17]
The association of TTX with consumed, infecting, or symbiotic bacterial populations within the metazoan species from which it is isolated is relatively clear;[1] presence of TTX-producing bacteria within a metazoan's microbiome is determined by culture methods, the presence of the toxin by chemical analysis, and the association of the bacteria with TTX production by toxicity assay of media in which suspected bacteria are grown.[2] As Lago et al. note, "there is good evidence that uptake of bacteria producing TTX is an important element of TTX toxicity in marine metazoans that present this toxin."[2] TTX-producing bacteria include Actinomyces, Aeromonas, Alteromonas, Bacillus, Pseudomonas, and Vibrio species;[2] in the following animals, specific bacterial species have been implicated:[1]
Tetrodotoxin binds to what is known as site 1 of the fast voltage-gated sodium channel.[24]  Site 1 is located at the extracellular pore opening of the ion channel. The binding of any molecules to this site will temporarily disable the function of the ion channel, thereby blocking the passage of sodium ions into the nerve cell (which is ultimately necessary for nerve conduction); neosaxitoxin and several of the conotoxins also bind the same site.
TTX and its analogs have historically been important agents for use as chemical tool compounds, for use in channel characterization and in fundamental studies of channel function.[27][28] The prevalence of TTX-s Na+ channels in the central nervous system makes tetrodotoxin a valuable agent for the silencing of neural activity within a cell culture.
The toxin can enter the body of a victim by ingestion, injection, or inhalation, or through abraded skin.[42]
Poisoning occurring as a consequence of consumption of fish from the order Tetraodontiformes is extremely serious. The organs (e.g. liver) of the pufferfish can contain levels of tetrodotoxin sufficient to produce the described paralysis of the diaphragm and corresponding death due to respiratory failure.[43] Toxicity varies between species and at different seasons and geographic localities, and the flesh of many pufferfish may not be dangerously toxic.[3]
The mechanism of toxicity is through the blockage of fast voltage-gated sodium channels, which are required for the normal transmission of signals between the body and brain.[44] As a result, TTX causes loss of sensation, and paralysis of voluntary muscles including the diaphragm and intercostal muscles, stopping breathing.[45]
The German physician Engelbert Kaempfer, in his "A History of Japan" (translated and published in English in 1727), described how well known the toxic effects of the fish were, to the extent that it would be used for suicide and that the Emperor specifically decreed that soldiers were not permitted to eat it.[47] There is also evidence from other sources that knowledge of such toxicity was widespread throughout southeast Asia and India.[27]
The first recorded cases of TTX poisoning affecting Westerners are from the logs of Captain James Cook from 7 September 1774.[43] On that date Cook recorded his crew eating some local tropic fish (pufferfish), then feeding the remains to the pigs kept on board. The crew experienced numbness and shortness of breath, while the pigs were all found dead the next morning. In hindsight, it is clear that the crew survived a mild dose of tetrodotoxin, while the pigs ate the pufferfish body parts that contain most of the toxin, thus being fatally poisoned.
The toxin was first isolated and named in 1909 by Japanese scientist Dr. Yoshizumi Tahara.[2][48][43]  It was one of the agents studied by Japan's Unit 731, which evaluated biological weapons on human subjects in the 1930s.[49]
The diagnosis of pufferfish poisoning is based on the observed symptomatology and recent dietary history.[50]
Symptoms typically develop within 30 minutes of ingestion, but may be delayed by up to four hours; however, if the dose is fatal, symptoms are usually present within 17 minutes of ingestion.[43] Paresthesia of the lips and tongue is followed by developing paresthesia in the extremities, hypersalivation, sweating, headache, weakness, lethargy, incoordination, tremor, paralysis, cyanosis, aphonia, dysphagia, and seizures. The gastrointestinal symptoms are often severe and include nausea, vomiting, diarrhea, and abdominal pain; death is usually secondary to respiratory failure.[45][50] There is increasing respiratory distress, speech is affected, and the victim usually exhibits dyspnea, mydriasis, and hypotension. Paralysis increases, and convulsions, mental impairment, and cardiac arrhythmia may occur. The victim, although completely paralyzed, may be conscious and in some cases completely lucid until shortly before death, which generally occurs within 4 to 6 hours (range ~20 minutes to ~8 hours). However, some victims enter a coma.[45][51]
If the patient survives 24 hours, recovery without any residual effects will usually occur over a few days.[50]
Therapy is supportive and based on symptoms, with aggressive early airway management.[43] If ingested, treatment can consist of emptying the stomach, feeding the victim activated charcoal to bind the toxin, and taking standard life-support measures to keep the victim alive until the effect of the poison has worn off.[43] Alpha adrenergic agonists are recommended in addition to intravenous fluids to combat hypotension; anticholinesterase agents "have been proposed as a treatment option but have not been tested adequately".[51]
No antidote has been developed and approved for human use, but a primary research report (preliminary result) indicates that a monoclonal antibody specific to tetrodotoxin is in development by USAMRIID that was effective, in the one study, for reducing toxin lethality in tests on mice.[52]
Poisonings from tetrodotoxin have been almost exclusively associated with the consumption of pufferfish from waters of the Indo-Pacific Ocean regions. Pufferfishes from other regions are much less commonly eaten. Several reported cases of poisonings, including fatalities, involved pufferfish from the Atlantic Ocean, Gulf of Mexico, and Gulf of California. There have been no confirmed cases of tetrodotoxicity from the Atlantic pufferfish, Sphoeroides maculatus, but in three studies, extracts from fish of this species were highly toxic in mice. Several recent intoxications from these fishes in Florida were due to saxitoxin, which causes paralytic shellfish poisoning with very similar symptoms and signs. The trumpet shell Charonia sauliae has been implicated in food poisonings, and evidence suggests it contains a tetrodotoxin derivative. There have been several reported poisonings from mislabelled pufferfish, and at least one report of a fatal episode in Oregon when an individual swallowed a rough-skinned newt Taricha granulosa.[53]
In 2009, a major scare in the Auckland Region of New Zealand was sparked after several dogs died eating Pleurobranchaea maculata (grey side-gilled seaslug) on beaches.[54] Children and pet owners were asked to avoid beaches, and recreational fishing was also interrupted for a time. After exhaustive analysis, it was found that the sea slugs must have ingested tetrodotoxin.[55]
Genetic background is not a factor in susceptibility to tetrodotoxin poisoning. This toxicosis may be avoided by not consuming animal species known to contain tetrodotoxin, principally pufferfish; other tetrodotoxic species are not usually consumed by humans.
Poisoning from tetrodotoxin is of particular public health concern in Japan, where "fugu" is a traditional delicacy. It is prepared and sold in special restaurants where trained and licensed chefs carefully remove the viscera to reduce the danger of poisoning.[61] There is potential for misidentification and mislabelling, particularly of prepared, frozen fish products.
The mouse bioassay developed for paralytic shellfish poisoning (PSP) can be used to monitor tetrodotoxin in pufferfish and is the current method of choice. An HPLC method with post-column reaction with alkali and fluorescence has been developed to determine tetrodotoxin and its associated toxins. The alkali degradation products can be confirmed as their trimethylsilyl derivatives by gas chromatography/mass spectrometry.[citation needed]
Tetrodotoxin may be quantified in serum, whole blood or urine to confirm a diagnosis of poisoning in hospitalized patients or to assist in the forensic investigation of a case of fatal overdosage. Most analytical techniques involve mass spectrometric detection following gas or liquid chromatographic separation.[62]
Tetrodotoxin has been investigated as a possible treatment for cancer-associated pain. Early clinical trials demonstrate significant pain relief in some patients.[63][64]
In addition to the cancer pain application mentioned, mutations in one particular TTX-sensitive Na+ channel are associated with some migraine headaches,[65] although it is unclear as to whether this has any therapeutic relevance for most people with migraine.[66]
Tetrodotoxin has been used clinically to relieve the headache associated with heroin withdrawal.[67]
Tetrodotoxin serves as a plot device for characters to fake death, as in the films Hello Again (1987), The Serpent and the Rainbow (1988), The A-Team (2010) and Captain America: The Winter Soldier (2014), War (2019), and in episodes of "Jane the Virgin", Miami Vice (1985),[70] Nikita, MacGyver Season 7, Episode 6, where the antidote is Datura stramonium leaf, CSI: NY (Season 4, episode 9 "Boo") and Chuck. In Law Abiding Citizen (2009) and Alex Cross (2012), its paralysis is presented as a method of assisting torture. The toxin was also referenced in "synthetic form" in the S1E2 of the series "FBI". The toxin is used as a weapon in both the second season of Archer, in Covert Affairs and in the Inside No. 9 episode "The Riddle of the Sphinx".[71][72]
Based on the presumption that tetrodotoxin is not always fatal, but at near-lethal doses can leave a person extremely unwell with the person remaining conscious,[50] tetrodotoxin has been alleged to result in zombieism, and has been suggested as an ingredient in Haitian Vodou preparations.[73] This idea first appeared in the 1938 non-fiction book Tell My Horse by Zora Neale Hurston in which there were multiple accounts of purported tetrodotoxin poisoning in Haiti by a voodoo sorcerer called the Bokor.[74] These stories were later popularized by Harvard-trained ethnobotanist Wade Davis[73] in his 1985 book and Wes Craven's 1988 film, both titled The Serpent and the Rainbow. James Ellroy includes "blowfish toxin" as an ingredient in Haitian Vodou preparations to produce zombieism and poisoning deaths in his dark, disturbing, violent novel Blood's a Rover.  But this theory has been questioned by the scientific community since the 1990s based on analytical chemistry-based tests of multiple preparations and review of earlier reports (see above).[58][59][60]
Neurotoxins are toxins that are destructive to nerve tissue (causing neurotoxicity).[3] Neurotoxins are an extensive class of exogenous chemical neurological insults[4] that can adversely affect function in both developing and mature nervous tissue.[5] The term can also be used to classify endogenous compounds, which, when abnormally contacted, can prove neurologically toxic.[4] Though neurotoxins are often neurologically destructive, their ability to specifically target neural components is important in the study of nervous systems.[6] Common examples of neurotoxins include lead,[7] ethanol (drinking alcohol),[8] glutamate,[9] nitric oxide,[10] botulinum toxin (e.g. Botox),[11] tetanus toxin,[12] and tetrodotoxin.[6] Some substances such as nitric oxide and glutamate are in fact essential for proper function of the body and only exert neurotoxic effects at excessive concentrations.
Neurotoxins inhibit neuron control over ion concentrations across the cell membrane,[6] or communication between neurons across a synapse.[13] Local pathology of neurotoxin exposure often includes neuron excitotoxicity or apoptosis[14] but can also include glial cell damage.[15] Macroscopic manifestations of neurotoxin exposure can include widespread central nervous system damage such as intellectual disability,[5] persistent memory impairments,[16] epilepsy, and dementia.[17] Additionally, neurotoxin-mediated peripheral nervous system damage such as neuropathy or myopathy is common. Support has been shown for a number of treatments aimed at attenuating neurotoxin-mediated injury, such as antioxidant[8] and antitoxin[18] administration.
Exposure to neurotoxins in society is not new,[19] as civilizations have been exposed to neurologically destructive compounds for thousands of years. One notable example is the possible significant lead exposure during the Roman Empire resulting from the development of extensive plumbing networks and the habit of boiling vinegared wine in lead pans to sweeten it, the process generating lead acetate, known as "sugar of lead".[20] In part, neurotoxins have been part of human history because of the fragile and susceptible nature of the nervous system, making it highly prone to disruption.
The nervous tissue found in the brain, spinal cord, and periphery comprises an extraordinarily complex biological system that largely defines many of the unique traits of individuals. As with any highly complex system, however, even small perturbations to its environment can lead to significant functional disruptions. Properties leading to the susceptibility of nervous tissue include a high surface area of neurons, a high lipid content which retains lipophilic toxins, high blood flow to the brain inducing increased effective toxin exposure, and the persistence of neurons through an individual's lifetime, leading to compounding of damages.[21] As a result, the nervous system has a number of mechanisms designed to protect it from internal and external assaults, including the blood brain barrier.
This barrier creates a tight hydrophobic layer around the capillaries in the brain, inhibiting the transport of large or hydrophilic compounds. In addition to the BBB, the choroid plexus provides a layer of protection against toxin absorption in the brain. The choroid plexuses are vascularized layers of tissue found in the third, fourth, and lateral ventricles of the brain, which through the function of their ependymal cells, are responsible for the synthesis of cerebrospinal fluid (CSF).[23] Importantly, through selective passage of ions and nutrients and trapping heavy metals such as lead, the choroid plexuses maintain a strictly regulated environment which contains the brain and spinal cord.[22][23]
By being hydrophobic and small, or inhibiting astrocyte function, some compounds including certain neurotoxins are able to penetrate into the brain and induce significant damage. In modern times, scientists and physicians have been presented with the challenge of identifying and treating neurotoxins, which has resulted in a growing interest in both neurotoxicology research and clinical studies.[24] Though clinical neurotoxicology is largely a burgeoning field, extensive inroads have been made in the identification of many environmental neurotoxins leading to the classification of 750 to 1000 known potentially neurotoxic compounds.[21] Due to the critical importance of finding neurotoxins in common environments, specific protocols have been developed by the United States Environmental Protection Agency (EPA) for testing and determining neurotoxic effects of compounds (USEPA 1998). Additionally, in vitro systems have increased in use as they provide significant improvements over the more common in vivo systems of the past. Examples of improvements include tractable, uniform environments, and the elimination of contaminating effects of systemic metabolism.[24] In vitro systems, however, have presented problems as it has been difficult to properly replicate the complexities of the nervous system, such as the interactions between supporting astrocytes and neurons in creating the BBB.[25] To even further complicate the process of determining neurotoxins when testing in-vitro, neurotoxicity and cytotoxicity may be difficult to distinguish as exposing neurons directly to compounds may not be possible in-vivo, as it is in-vitro. Additionally, the response of cells to chemicals may not accurately convey a distinction between neurotoxins and cytotoxins, as symptoms like oxidative stress or skeletal modifications may occur in response to either.[26]
In an effort to address this complication, neurite outgrowths (either axonal or dendritic) in response to applied compounds have recently been proposed as a more accurate distinction between true neurotoxins and cytotoxins in an in-vitro testing environment. Due to the significant inaccuracies associated with this process, however, it has been slow in gaining widespread support.[27] Additionally, biochemical mechanisms have become more widely used in neurotoxin testing, such that compounds can be screened for sufficiency to induce cell mechanism interference, like the inhibition of acetylcholinesterase capacity of organophosphates (includes DDT and sarin gas).[28] Though methods of determining neurotoxicity still require significant development, the identification of deleterious compounds and toxin exposure symptoms has undergone significant improvement.
Though diverse in chemical properties and functions, neurotoxins share the common property that they act by some mechanism leading to either the disruption or destruction of necessary components within the nervous system. Neurotoxins, however, by their very design can be very useful in the field of neuroscience. As the nervous system in most organisms is both highly complex and necessary for survival, it has naturally become a target for attack by both predators and prey. As venomous organisms often use their neurotoxins to subdue a predator or prey very rapidly, toxins have evolved to become highly specific to their target channels such that the toxin does not readily bind other targets[29] (see Ion Channel toxins). As such, neurotoxins provide an effective means by which certain elements of the nervous system may be accurately and efficiently targeted. An early example of neurotoxin based targeting used radiolabeled tetrodotoxin to assay sodium channels and obtain precise measurements about their concentration along nerve membranes.[29] Likewise through isolation of certain channel activities, neurotoxins have provided the ability to improve the original Hodgkin-Huxley model of the neuron in which it was theorized that single generic sodium and potassium channels could account for most nervous tissue function.[29] From this basic understanding, the use of common compounds such as tetrodotoxin, tetraethylammonium, and bungarotoxins have led to a much deeper understanding of the distinct ways in which individual neurons may behave.
As neurotoxins are compounds which adversely affect the nervous system, a number of mechanisms through which they function are through the inhibition of neuron cellular processes. These inhibited processes can range from membrane depolarization mechanisms to inter-neuron communication. By inhibiting the ability for neurons to perform their expected intracellular functions, or pass a signal to a neighboring cell, neurotoxins can induce systemic nervous system arrest as in the case of botulinum toxin,[13] or even nervous tissue death.[30] The time required for the onset of symptoms upon neurotoxin exposure can vary between different toxins, being on the order of hours for botulinum toxin[18] and years for lead.[31]
Tetrodotoxin (TTX) is a poison produced by organisms belonging to the Tetraodontiformes order, which includes the puffer fish, ocean sunfish, and porcupine fish.[55] Within the puffer fish, TTX is found in the liver, gonads, intestines, and skin.[6][56] TTX can be fatal if consumed, and has become a common form of poisoning in many countries. Common symptoms of TTX consumption include paraesthesia (often restricted to the mouth and limbs), muscle weakness, nausea, and vomiting[55] and often manifest within 30 minutes of ingestion.[57] The primary mechanism by which TTX is toxic is through the inhibition of sodium channel function, which reduces the functional capacity of neuron communication. This inhibition largely affects a susceptible subset of sodium channels known as TTX-sensitive (TTX-s), which also happens to be largely responsible for the sodium current that drives the depolarization phase of neuron action potentials.[6] 
 TTX-resistant (TTX-r) is another form of sodium channel which has limited sensitivity to TTX, and is largely found in small diameter axons such as those found in nociception neurons.[6] When a significant level of TTX is ingested, it will bind sodium channels on neurons and reduce their membrane permeability to sodium. This results in an increased effective threshold of required excitatory signals in order to induce an action potential in a postsynaptic neuron.[6] The effect of this increased signaling threshold is a reduced excitability of postsynaptic neurons, and subsequent loss of motor and sensory function which can result in paralysis and death. Though assisted ventilation may increase the chance of survival after TTX exposure, there is currently no antitoxin. The use of the acetylcholinesterase inhibitor Neostigmine or the muscarinic acetylcholine antagonist atropine (which will inhibit parasympathetic activity), however, can increase sympathetic nerve activity enough to improve the chance of survival after TTX exposure.[55]
Tetraethylammonium (TEA) is a compound that, like a number of neurotoxins, was first identified through its damaging effects to the nervous system and shown to have the capacity of inhibiting the function of motor nerves and thus the contraction of the musculature in a manner similar to that of curare.[58] Additionally, through chronic TEA administration, muscular atrophy would be induced.[58] It was later determined that TEA functions in-vivo primarily through its ability to inhibit both the potassium channels responsible for the delayed rectifier seen in an action potential and some population of calcium-dependent potassium channels.[32] It is this capability to inhibit potassium flux in neurons that has made TEA one of the most important tools in neuroscience. It has been hypothesized that the ability for TEA to inhibit potassium channels is derived from its similar space-filling structure to potassium ions.[58] What makes TEA very useful for neuroscientists is its specific ability to eliminate potassium channel activity, thereby allowing the study of neuron response contributions of other ion channels such as voltage gated sodium channels.[59] In addition to its many uses in neuroscience research, TEA has been shown to perform as an effective treatment of Parkinson's disease through its ability to limit the progression of the disease.[60]
Chlorotoxin (Cltx) is the active compound found in scorpion venom, and is primarily toxic because of its ability to inhibit the conductance of chloride channels.[33] Ingestion of lethal volumes of Cltx results in paralysis through this ion channel disruption. Similar to botulinum toxin, Cltx has been shown to possess significant therapeutic value. Evidence has shown that Cltx can inhibit the ability for gliomas to infiltrate healthy nervous tissue in the brain, significantly reducing the potential invasive harm caused by tumors.[61][62]
Tetanus neurotoxin (TeNT)  is a compound that functionally reduces inhibitory transmissions in the nervous system resulting in muscular tetany. TeNT is similar to BTX, and is in fact highly similar in structure and origin; both belonging to the same category of clostridial neurotoxins.[12] Like BTX, TeNT inhibits inter-neuron communication by means of vesicular neurotransmitter (NT) release.[36] One notable difference between the two compounds is that while BTX inhibits muscular contractions, TeNT induces them. Though both toxins inhibit vesicle release at neuron synapses, the reason for this different manifestation is that BTX functions mainly in the peripheral nervous system (PNS) while TeNT is largely active in the central nervous system (CNS).[68] This is a result of TeNT migration through motor neurons to the inhibitory neurons of the spinal cord after entering through endocytosis.[69] This results in a loss of function in inhibitory neurons within the CNS resulting in systemic muscular contractions. Similar to the prognosis of a lethal dose of BTX, TeNT leads to paralysis and subsequent suffocation.[69]
Neurotoxic behavior of Aluminium is known to occur upon entry into the circulatory system, where it can migrate to the brain and inhibit some of the crucial functions of the blood brain barrier (BBB).[37] A loss of function in the BBB can produce significant damage to the neurons in the CNS, as the barrier protecting the brain from other toxins found in the blood will no longer be capable of such action. Though the metal is known to be neurotoxic, effects are usually restricted to patients incapable of removing excess ions from the blood, such as those experiencing renal failure.[70] Patients experiencing aluminium toxicity can exhibit symptoms such as impaired learning and reduced motor coordination.[71] Additionally, systemic aluminium levels are known to increase with age, and have been shown to correlate with Alzheimer's Disease, implicating it as a neurotoxic causative compound of the disease.[72] Despite its known toxicity in its ionic form, studies are divided on the potential toxicity of using aluminium in packaging and cooking appliances.
Mercury is capable of inducing CNS damage by migrating into the brain by crossing the BBB.[38] Mercury exists in a number of different compounds, though methylmercury (MeHg+), dimethylmercury and diethylmercury are the only significantly neurotoxic forms. Diethylmercury and dimethylmercury are considered some of the most potent neurotoxins ever discovered.[38] MeHg+ is usually acquired through consumption of seafood, as it tends to concentrate in organisms high on the food chain.[73] It is known that the mercuric ion inhibits amino acid (AA) and glutamate (Glu) transport, potentially leading to excitotoxic effects.[74]
Investigations into anatoxin-a, also known as "Very Fast Death Factor", began in 1961 following the deaths of cows that drank from a lake containing an algal bloom in Saskatchewan, Canada.[41][42] It is a cyanotoxin produced by at least four different genera of cyanobacteria, and has been reported in North America, Europe, Africa, Asia, and New Zealand.[75]
Toxic effects from anatoxin-a progress very rapidly because it acts directly on the nerve cells (neurons). The progressive symptoms of anatoxin-a exposure are loss of coordination, twitching, convulsions and rapid death by respiratory paralysis. The nerve tissues which communicate with muscles contain a receptor called the nicotinic acetylcholine receptor. Stimulation of these receptors causes a muscular contraction. The anatoxin-a molecule is shaped so it fits this receptor, and in this way it mimics the natural neurotransmitter normally used by the receptor, acetylcholine. Once it has triggered a contraction, anatoxin-a does not allow the neurons to return to their resting state, because it is not degraded by cholinesterase which normally performs this function. As a result, the muscle cells contract permanently, the communication between the brain and the muscles is disrupted and breathing stops.[76][77]
When it was first discovered, the toxin was called the Very Fast Death Factor (VFDF) because when it was injected into the body cavity of mice it induced tremors, paralysis and death within a few minutes. In 1977, the structure of VFDF was determined as a secondary, bicyclic amine alkaloid, and it was renamed anatoxin-a.[78][79] Structurally, it is similar to cocaine.[80] There is continued interest in anatoxin-a because of the dangers it presents to recreational and drinking waters, and because it is a particularly useful molecule for investigating acetylcholine receptors in the nervous system.[81] The deadliness of the toxin means that it has a high military potential as a toxin weapon.[82]
Caramboxin (CBX) is a toxin found in star fruit (Averrhoa carambola). Individuals with some types of kidney disease are susceptible to adverse neurological effects including intoxication, seizures and even death after eating star fruit or drinking juice made of this fruit.  Caramboxin is a new nonpeptide amino acid toxin that stimulate the glutamate receptors in neurons. Caramboxin is an agonist of both NMDA and AMPA glutamatergic ionotropic receptors with potent excitatory, convulsant, and neurodegenerative properties.[43]
The term "curare" is ambiguous because it has been used to describe a number of poisons which at the time of naming were understood differently from present day understandings. In the past the characterization has meant poisons used by South American tribes on arrows or darts, though it has matured to specify a specific categorization of poisons which act on the neuromuscular junction to inhibit signaling and thus induce muscle relaxation.[86] The neurotoxin category contains a number of distinct poisons, though all were originally purified from plants originating in South America.[86] The effect with which injected curare poison is usually associated is muscle paralysis and resultant death.[87] Curare notably functions to inhibit nicotinic acetylcholine receptors at the neuromuscular junction.  Normally, these receptor channels allow sodium ions into muscle cells to initiate an action potential that leads to muscle contraction.  By blocking the receptors, the neurotoxin is capable of significantly reducing neuromuscular junction signaling, an effect which has resulted in its use by anesthesiologists to produce muscular relaxation.[88]
Ammonia toxicity is often seen through two routes of administration, either through consumption or through endogenous ailments such as liver failure.[89][90] One notable case in which ammonia toxicity is common is in response to cirrhosis of the liver which results in hepatic encephalopathy, and can result in cerebral edema (Haussinger 2006). This cerebral edema can be the result of nervous cell remodeling. As a consequence of increased concentrations, ammonia activity in-vivo has been shown to induce swelling of astrocytes in the brain through increased production of cGMP (Cyclic Guanosine Monophosphate) within the cells which leads to Protein Kinase G-mediated (PKG) cytoskeletal modifications.[46] The resultant effect of this toxicity can be reduced brain energy metabolism and function. Importantly, the toxic effects of ammonia on astrocyte remodeling can be reduced through administration of L-carnitine.[89] This astrocyte remodeling appears to be mediated through ammonia-induced mitochondrial permeability transition. This mitochondrial transition is a direct result of glutamine activity a compound which forms from ammonia in-vivo.[91] Administration of antioxidants or glutaminase inhibitor can reduce this mitochondrial transition, and potentially also astrocyte remodeling.[91]
Arsenic is a neurotoxin commonly found concentrated in areas exposed to agricultural runoff, mining, and smelting sites (Martinez-Finley 2011). One of the effects of arsenic ingestion during the development of the nervous system is the inhibition of neurite growth[92] which can occur both in PNS and the CNS.[93] This neurite growth inhibition can often lead to defects in neural migration, and significant morphological changes of neurons during development,[94]) often leading to neural tube defects in neonates.[95] As a metabolite of arsenic, arsenite is formed after ingestion of arsenic and has shown significant toxicity to neurons within about 24 hours of exposure. The mechanism of this cytotoxicity functions through arsenite-induced increases in intracellular calcium ion levels within neurons, which may subsequently reduce mitochondrial transmembrane potential which activates caspases, triggering cell death.[94] Another known function of arsenite is its destructive nature towards the cytoskeleton through inhibition of neurofilament transport.[47] This is particularly destructive as neurofilaments are used in basic cell structure and support. Lithium administration has shown promise, however, in restoring some of the lost neurofilament motility.[96] Additionally, similar to other neurotoxin treatments, the administration of certain antioxidants has shown some promise in reducing neurotoxicity of ingested arsenic.[94]
Lead is a potent neurotoxin whose toxicity has been recognized for at least thousands of years.[97] Though neurotoxic effects for lead are found in both adults and young children, the developing brain is particularly susceptible to lead-induced harm, effects which can include apoptosis and excitotoxicity.[97] An underlying mechanism by which lead is able to cause harm is its ability to be transported by calcium ATPase pumps across the BBB, allowing for direct contact with the fragile cells within the central nervous system.[98] Neurotoxicity results from lead's ability to act in a similar manner to calcium ions, as concentrated lead will lead to cellular uptake of calcium which disrupts cellular homeostasis and induces apoptosis.[48] It is this intracellular calcium increase that activates protein kinase C (PKC), which manifests as learning deficits in children as a result of early lead exposure.[48] In addition to inducing apoptosis, lead inhibits interneuron signaling through the disruption of calcium-mediated neurotransmitter release.[99]
As a neurotoxin, ethanol has been shown to induce nervous system damage and affect the body in a variety of ways. Among the known effects of ethanol exposure are both transient and lasting consequences. Some of the lasting effects include long-term reduced neurogenesis in the hippocampus,[100][101] widespread brain atrophy,[102] and induced inflammation in the brain.[103] Of note, chronic ethanol ingestion has additionally been shown to induce reorganization of cellular membrane constituents, leading to a lipid bilayer marked by increased membrane concentrations of cholesterol and saturated fat.[50] This is important as neurotransmitter transport can be impaired through vesicular transport inhibition, resulting in diminished neural network function. One significant example of reduced inter-neuron communication is the ability for ethanol to inhibit NMDA receptors in the hippocampus, resulting in reduced long-term potentiation (LTP) and memory acquisition.[49] NMDA has been shown to play an important role in LTP and consequently memory formation.[104] With chronic ethanol intake, however, the susceptibility of these NMDA receptors to induce LTP increases in the mesolimbic dopamine neurons in an inositol 1,4,5-triphosphate (IP3) dependent manner.[105] This reorganization may lead to neuronal cytotoxicity both through hyperactivation of postsynaptic neurons and through induced addiction to continuous ethanol consumption. It has, additionally, been shown that ethanol directly reduces intracellular calcium ion accumulation through inhibited NMDA receptor activity, and thus reduces the capacity for the occurrence of LTP.[106]
In addition to the neurotoxic effects of ethanol in mature organisms, chronic ingestion is capable of inducing severe developmental defects. Evidence was first shown in 1973 of a connection between chronic ethanol intake by mothers and defects in their offspring.[107] This work was responsible for creating the classification of fetal alcohol syndrome, a disease characterized by common morphogenesis aberrations such as defects in craniofacial formation, limb development, and cardiovascular formation. The magnitude of ethanol neurotoxicity in fetuses leading to fetal alcohol syndrome has been shown to be dependent on antioxidant levels in the brain such as vitamin E.[108] As the fetal brain is relatively fragile and susceptible to induced stresses, severe deleterious effects of alcohol exposure can be seen in important areas such as the hippocampus and cerebellum. The severity of these effects is directly dependent upon the amount and frequency of ethanol consumption by the mother, and the stage in development of the fetus.[109] It is known that ethanol exposure results in reduced antioxidant levels, mitochondrial dysfunction (Chu 2007),  and subsequent neuronal death, seemingly as a result of increased generation of reactive oxidative species (ROS).[30] This is a plausible mechanism, as there is a reduced presence in the fetal brain of antioxidant enzymes such as catalase and peroxidase.[110] In support of this mechanism, administration of high levels of dietary vitamin E results in reduced or eliminated ethanol-induced neurotoxic effects in fetuses.[8]
n-Hexane is a neurotoxin which has been responsible for the poisoning of several workers in Chinese electronics factories in recent years.[111][112][113][51]
Unlike most common sources of neurotoxins which are acquired by the body through ingestion, endogenous neurotoxins both originate from and exert their effects in-vivo. Additionally, though most venoms and exogenous neurotoxins will rarely possess useful in-vivo capabilities, endogenous neurotoxins are commonly used by the body in useful and healthy ways, such as nitric oxide which is used in cell communication.[114] It is often only when these endogenous compounds become highly concentrated that they lead to dangerous effects.[9]
Though nitric oxide (NO) is commonly used by the nervous system in inter-neuron communication and signaling, it can be active in mechanisms leading to ischemia in the cerebrum (Iadecola 1998). The neurotoxicity of NO is based on its importance in glutamate excitotoxicity, as NO is generated in a calcium-dependent manner in response to glutamate mediated NMDA activation, which occurs at an elevated rate in glutamate excitotoxicity.[52] Though NO facilitates increased blood flow to potentially ischemic regions of the brain, it is also capable of increasing oxidative stress,[115] inducing DNA damage and apoptosis.[116] Thus an increased presence of NO in an ischemic area of the CNS can produce significantly toxic effects.
Glutamate, like nitric oxide, is an endogenously produced compound used by neurons to perform normally, being present in small concentrations throughout the gray matter of the CNS.[9] One of the most notable uses of endogenous glutamate is its functionality as an excitatory neurotransmitter.[53] When concentrated, however, glutamate becomes toxic to surrounding neurons. This toxicity can be both a result of direct lethality of glutamate on neurons and a result of induced calcium flux into neurons leading to swelling and necrosis.[53] Support has been shown for these mechanisms playing significant roles in diseases and complications such as Huntington's disease, epilepsy, and stroke.[9]
Biology is the scientific study of life.[1][2][3] It is a natural science with a broad scope but has several unifying themes that tie it together as a single, coherent field.[1][2][3] For instance, all organisms are made up of cells that process hereditary information encoded in genes, which can be transmitted to future generations. Another major theme is evolution, which explains the unity and diversity of life.[1][2][3] Energy processing is also important to life as it allows organisms to move, grow, and reproduce.[1][2][3] Finally, all organisms are able to regulate their own internal environments.[1][2][3][4][5]
Biologists are able to study life at multiple levels of organization,[1] from the molecular biology of a cell to the anatomy and physiology of plants and animals, and evolution of populations.[1][6] Hence, there are multiple subdisciplines within biology, each defined by the nature of their research questions and the tools that they use.[7][8][9] Like other scientists, biologists use the scientific method to make observations, pose questions, generate hypotheses, perform experiments, and form conclusions about the world around them.[1]
Life on Earth, which emerged more than 3.7 billion years ago,[10] is immensely diverse. Biologists have sought to study and classify the various forms of life, from prokaryotic organisms such as archaea and bacteria to eukaryotic organisms such as protists, fungi, plants, and animals. These various organisms contribute to the biodiversity of an ecosystem, where they play specialized roles in the cycling of nutrients and energy through their biophysical environment.
Biology began to quickly develop with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop techniques of microscopic dissection and staining.[17] Advances in microscopy had a profound impact on biological thinking. In the early 19th century, biologists pointed to the central importance of the cell. In 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells, continuing to support spontaneous generation. However, Robert Remak and Rudolf Virchow were able to reify the third tenet, and by the 1860s most biologists accepted all three tenets which consolidated into cell theory.[18][19]
Serious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who presented a coherent theory of evolution.[23] The British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions.[24][25]
The basis for modern genetics began with the work of Gregor Mendel in 1865.[26] This outlined the principles of biological inheritance.[27] However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics.[28] In the 1940s and early 1950s, a series of experiments by Alfred Hershey and Martha Chase pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double-helical structure of DNA by James Watson and Francis Crick in 1953, marked the transition to the era of molecular genetics. From the 1950s onwards, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. The Human Genome Project was launched in 1990 to map the human genome.[29]
All organisms are made up of chemical elements;[30] oxygen, carbon, hydrogen, and nitrogen account for most (96%) of the mass of all organisms, with calcium, phosphorus, sulfur, sodium, chlorine, and magnesium constituting essentially all the remainder. Different elements can combine to form compounds such as water, which is fundamental to life.[30] Biochemistry is the study of chemical processes within and relating to living organisms. Molecular biology is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms, and interactions.
The simplest form of an organic molecule is the hydrocarbon, which is a large family of organic compounds that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other elements such as oxygen (O), hydrogen (H), phosphorus (P), and sulfur (S), which can change the chemical behavior of that compound.[31] Groups of atoms that contain these elements (O-, H-, P-, and S-) and are bonded to a central carbon atom or skeleton are called functional groups.[31] There are six prominent functional groups that can be found in organisms: amino group, carboxyl group, carbonyl group, hydroxyl group, phosphate group, and sulfhydryl group.[31]
In 1953, the Miller-Urey experiment showed that organic compounds could be synthesized abiotically within a closed system mimicking the conditions of early Earth, thus suggesting that complex organic molecules could have arisen spontaneously in early Earth (see abiogenesis).[33][31]
Macromolecules are large molecules made up of smaller subunits or monomers.[34] Monomers include  sugars, amino acids, and nucleotides.[35] Carbohydrates include monomers and polymers of sugars.[36] 
Lipids are the only class of macromolecules that are not made up of polymers. They include steroids, phospholipids, and fats,[35] largely nonpolar and hydrophobic (water-repelling) substances.[37] 
Proteins are the most diverse of the macromolecules. They include enzymes, transport proteins, large signaling molecules, antibodies, and structural proteins. The basic unit (or monomer) of a protein is an amino acid.[34] Twenty amino acids are used in proteins.[34] 
Nucleic acids are polymers of nucleotides.[38] Their function is to store, transmit, and express hereditary information.[35]
Every cell is enclosed within a cell membrane that separates its cytoplasm from the extracellular space.[41] A cell membrane consists of a lipid bilayer, including cholesterols that sit between phospholipids to maintain their fluidity at various temperatures. Cell membranes are semipermeable, allowing small molecules such as oxygen, carbon dioxide, and water to pass through while restricting the movement of larger molecules and charged particles such as ions.[42] Cell membranes also contains membrane proteins, including integral membrane proteins that go across the membrane serving as membrane transporters, and peripheral proteins that loosely attach to the outer side of the cell membrane, acting as enzymes shaping the cell.[43] Cell membranes are involved in various cellular processes such as cell adhesion, storing electrical energy, and cell signalling and serve as the attachment surface for several extracellular structures such as a cell wall, glycocalyx, and cytoskeleton.
Cellular respiration is a set of metabolic reactions and processes that take place in cells to convert chemical energy from nutrients into adenosine triphosphate (ATP), and then release waste products.[46] The reactions involved in respiration are catabolic reactions, which break large molecules into smaller ones, releasing energy. Respiration is one of the key ways a cell releases chemical energy to fuel cellular activity. The overall reaction occurs in a series of biochemical steps, some of which are redox reactions. Although cellular respiration is technically a combustion reaction, it clearly does not resemble one when it occurs in a cell because of the slow, controlled release of energy from the series of reactions.
Sugar in the form of glucose is the main nutrient used by animal and plant cells in respiration. Cellular respiration involving oxygen is called aerobic respiration, which has four stages: glycolysis, citric acid cycle (or Krebs cycle), electron transport chain, and oxidative phosphorylation.[47] Glycolysis is a metabolic process that occurs in the cytoplasm whereby glucose is converted into two pyruvates, with two net molecules of ATP being produced at the same time.[47] Each pyruvate is then oxidized into acetyl-CoA by the pyruvate dehydrogenase complex, which also generates NADH and carbon dioxide. Acetyl-Coa enters the citric acid cycle, which takes places inside the mitochondrial matrix. At the end of the cycle, the total yield from 1 glucose (or 2 pyruvates) is 6 NADH, 2 FADH2, and 2 ATP molecules. Finally, the next stage is oxidative phosphorylation, which in eukaryotes, occurs in the mitochondrial cristae. Oxidative phosphorylation comprises the electron transport chain, which is a series of four protein complexes that transfer electrons from one complex to another, thereby releasing energy from NADH and FADH2 that is coupled to the pumping of protons (hydrogen ions) across the inner mitochondrial membrane (chemiosmosis), which generates a proton motive force.[47] Energy from the proton motive force drives the enzyme ATP synthase to synthesize more ATPs by phosphorylating ADPs. The transfer of electrons terminates with molecular oxygen being the final electron acceptor.
If oxygen were not present, pyruvate would not be metabolized by cellular respiration but undergoes a process of fermentation. The pyruvate is not transported into the mitochondrion but remains in the cytoplasm, where it is converted to waste products that may be removed from the cell. This serves the purpose of oxidizing the electron carriers so that they can perform glycolysis again and removing the excess pyruvate. Fermentation oxidizes NADH to NAD+ so it can be re-used in glycolysis.  In the absence of oxygen, fermentation prevents the buildup of NADH in the cytoplasm and provides NAD+ for glycolysis.  This waste product varies depending on the organism. In skeletal muscles, the waste product is lactic acid. This type of fermentation is called lactic acid fermentation. In strenuous exercise, when energy demands exceed energy supply, the respiratory chain cannot process all of the hydrogen atoms joined by NADH. During anaerobic glycolysis, NAD+ regenerates when pairs of hydrogen combine with pyruvate to form lactate. Lactate formation is catalyzed by lactate dehydrogenase in a reversible reaction. Lactate can also be used as an indirect precursor for liver glycogen. During recovery, when oxygen becomes available, NAD+ attaches to hydrogen from lactate to form ATP. In yeast, the waste products are ethanol and carbon dioxide. This type of fermentation is known as alcoholic or ethanol fermentation. The ATP generated in this process is made by substrate-level phosphorylation, which does not require oxygen.
Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organism's metabolic activities via cellular respiration. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water.[48][49][50] In most cases, oxygen is released as a waste product. Most plants, algae, and cyanobacteria perform photosynthesis, which is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth.[51]
Photosynthesis has four stages: Light absorption, electron transport, ATP synthesis, and carbon fixation.[47] Light absorption is the initial step of photosynthesis whereby light energy is absorbed by chlorophyll pigments attached to proteins in the thylakoid membranes. The absorbed light energy is used to remove electrons from a donor (water) to a primary electron acceptor, a quinone designated as Q. In the second stage, electrons move from the quinone primary electron acceptor through a series of electron carriers until they reach a final electron acceptor, which is usually the oxidized form of NADP+, which is reduced to NADPH, a process that takes place in a protein complex called photosystem I (PSI). The transport of electrons is coupled to the movement of protons (or hydrogen) from the stroma to the thylakoid membrane, which forms a pH gradient across the membrane as hydrogen becomes more concentrated in the lumen than in the stroma. This is analogous to the proton-motive force generated across the inner mitochondrial membrane in aerobic respiration.[47]
During the third stage of photosynthesis, the movement of protons down their concentration gradients from the thylakoid lumen to the stroma through the ATP synthase is coupled to the synthesis of ATP by that same ATP synthase.[47] The NADPH and ATPs generated by the light-dependent reactions in the second and third stages, respectively, provide the energy and electrons to drive the synthesis of glucose by fixing atmospheric carbon dioxide into existing organic carbon compounds, such as ribulose bisphosphate (RuBP) in a sequence of light-independent (or dark) reactions called the Calvin cycle.[52]
Cell signaling (or communication) is the ability of cells to receive, process, and transmit signals with its environment and with itself.[53][54] Signals can be non-chemical such as light, electrical impulses, and heat, or chemical signals (or ligands) that interact with receptors, which can be found embedded in the cell membrane of another cell or located deep inside a cell.[55][54] There are generally four types of chemical signals: autocrine, paracrine, juxtacrine, and hormones.[55] In autocrine signaling, the ligand affects the same cell that releases it. Tumor cells, for example, can reproduce uncontrollably because they release signals that initiate their own self-division. In paracrine signaling, the ligand diffuses to nearby cells and affects them. For example, brain cells called neurons release ligands called neurotransmitters that diffuse across a synaptic cleft to bind with a receptor on an adjacent cell such as another neuron or muscle cell. In juxtacrine signaling, there is direct contact between the signaling and responding cells. Finally, hormones are ligands that travel through the circulatory systems of animals or vascular systems of plants to reach their target cells. Once a ligand binds with a receptor, it can influence the behavior of another cell, depending on the type of receptor. For instance, neurotransmitters that bind with an inotropic receptor can alter the excitability of a target cell. Other types of receptors include protein kinase receptors (e.g., receptor for the hormone insulin) and G protein-coupled receptors. Activation of G protein-coupled receptors can initiate second messenger cascades. The process by which a chemical or physical signal is transmitted through a cell as a series of molecular events is called signal transduction
Prokaryotes (i.e., archaea and bacteria) can also undergo cell division (or binary fission). Unlike the processes of mitosis and meiosis in eukaryotes, binary fission takes in prokaryotes takes place without the formation of a spindle apparatus on the cell. Before binary fission, DNA in the bacterium is tightly coiled. After it has uncoiled and duplicated, it is pulled to the separate poles of the bacterium as it increases the size to prepare for splitting. Growth of a new cell wall begins to separate the bacterium (triggered by FtsZ polymerization and "Z-ring" formation)[60] The new cell wall (septum) fully develops, resulting in the complete split of the bacterium. The new daughter cells have tightly coiled DNA rods, ribosomes, and plasmids.
Genetics is the scientific study of inheritance.[61][62][63] Mendelian inheritance, specifically, is the process by which genes and traits are passed on from parents to offspring.[27] It has several principles. The first is that genetic characteristics, alleles, are discrete and have alternate forms (e.g., purple vs. white or tall vs. dwarf), each inherited from one of two parents. Based on the law of dominance and uniformity, which states that some alleles are dominant while others are recessive; an organism with at least one dominant allele will display the phenotype of that dominant allele. During gamete formation, the alleles for each gene segregate, so that each gamete carries only one allele for each gene. Heterozygotic individuals produce gametes with an equal frequency of two alleles. Finally, the law of independent assortment, states that genes of different traits can segregate independently during the formation of gametes, i.e., genes are unlinked. An exception to this rule would include traits that are sex-linked. Test crosses can be performed to experimentally determine the underlying genotype of an organism with a dominant phenotype.[64] A Punnett square can be used to predict the results of a test cross. The chromosome theory of inheritance, which states that genes are found on chromosomes, was supported by Thomas Morgans's experiments with fruit flies, which established the sex linkage between eye color and sex in these insects.[65]
A gene is a unit of heredity that corresponds to a region of deoxyribonucleic acid (DNA) that carries genetic information that controls form or function of an organism. DNA is composed of two polynucleotide chains that coil around each other to form a double helix.[66] It is found as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell is collectively known as its genome. In eukaryotes, DNA is mainly in the cell nucleus.[67] In prokaryotes, the DNA is held within the nucleoid.[68] The genetic information is held within genes, and the complete assemblage in an organism is called its genotype.[69]
DNA replication is a semiconservative process whereby each strand serves as a template for a new strand of DNA.[66] Mutations are heritable changes in DNA.[66] They can arise spontaneously as a result of replication errors that were not corrected by proofreading or can be induced by an environmental mutagen such as a chemical (e.g., nitrous acid, benzopyrene) or radiation (e.g., x-ray, gamma ray, ultraviolet radiation, particles emitted by unstable isotopes).[66] Mutations can lead to phenotypic effects such as loss-of-function, gain-of-function, and conditional mutations.[66]
Some mutations are beneficial, as they are a source of genetic variation for evolution.[66] Others are harmful if they were to result in a loss of function of genes needed for survival.[66] Mutagens such as carcinogens are typically avoided as a matter of public health policy goals.[66]
Gene expression is the molecular process by which a genotype encoded in DNA gives rise to an observable phenotype in the proteins of an organism's body. This process is summarized by the central dogma of molecular biology, which was formulated by Francis Crick in 1958.[70][71][72] According to the Central Dogma, genetic information flows from DNA to RNA to protein. There are two gene expression processes: transcription (DNA to RNA) and translation (RNA to protein).[73]
The regulation of gene expression by environmental factors and during different stages of development can occur at each step of the process such as transcription, RNA splicing, translation, and post-translational modification of a protein.[74] Gene expression can be influenced by positive or negative regulation, depending on which of the two types of regulatory proteins called transcription factors bind to the DNA sequence close to or at a promoter.[74] A cluster of genes that share the same promoter is called an operon, found mainly in prokaryotes and some lower eukaryotes (e.g., Caenorhabditis elegans).[74][75] In positive regulation of gene expression, the activator is the transcription factor that stimulates transcription when it binds to the sequence near or at the promoter. Negative regulation occurs when another transcription factor called a repressor binds to a DNA sequence called an operator, which is part of an operon, to prevent transcription. Repressors can be inhibited by compounds called inducers (e.g., allolactose), thereby allowing transcription to occur.[74] Specific genes that can be activated by inducers are called inducible genes, in contrast to constitutive genes that are almost constantly active.[74] In contrast to both, structural genes encode proteins that are not involved in gene regulation.[74] In addition to regulatory events involving the promoter, gene expression can also be regulated by epigenetic changes to chromatin, which is a complex of DNA and protein found in eukaryotic cells.[74]
Development is the process by which a multicellular organism (plant or animal) goes through a series of changes, starting from a single cell, and taking on various forms that are characteristic of its life cycle.[76] There are four key processes that underlie development: Determination, differentiation, morphogenesis, and growth. Determination sets the developmental fate of a cell, which becomes more restrictive during development. Differentiation is the process by which specialized cells from less specialized cells such as stem cells.[77][78] Stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell.[79] Cellular differentiation dramatically changes a cell's size, shape, membrane potential, metabolic activity, and responsiveness to signals, which are largely due to highly controlled modifications in gene expression and epigenetics.  With a few exceptions, cellular differentiation almost never involves a change in the DNA sequence itself.[80] Thus, different cells can have very different physical characteristics despite having the same genome. Morphogenesis, or the development of body form, is the result of spatial differences in gene expression.[76] A small fraction of the genes in an organism's genome called the developmental-genetic toolkit control the development of that organism. These toolkit genes are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body plan and the number, identity, and pattern of body parts. Among the most important toolkit genes are the Hox genes. Hox genes determine where repeating parts, such as the many vertebrae of snakes, will grow in a developing embryo or larva.[81]
Evolution is a central organizing concept in biology. It is the change in heritable characteristics of populations over successive generations.[82][83] In artificial selection, animals were selectively bred for specific traits.
[84] Given that traits are inherited, populations contain a varied mix of traits, and reproduction is able to increase any population, Darwin argued that in the natural world, it was nature that played the role of humans in selecting for specific traits.[84] Darwin inferred that individuals who possessed heritable traits better adapted to their environments are more likely to survive and produce more offspring than other individuals.[84] He further inferred that this would lead to the accumulation of favorable traits over successive generations, thereby increasing the match between the organisms and their environment.[85][86][87][84][88]
A phylogeny is an evolutionary history of a specific group of organisms or their genes.[90] It can be represented using a phylogenetic tree, a diagram showing lines of descent among organisms or their genes. Each line drawn on the time axis of a tree represents a lineage of descendants of a particular species or population. When a lineage divides into two, it is represented as a fork or split on the phylogenetic tree.[90] Phylogenetic trees are the basis for comparing and grouping different species.[90] Different species that share a feature inherited from a common ancestor are described as having homologous features (or synapomorphy).[91][92][90] Phylogeny provides the basis of biological classification.[90] This classification system is rank-based, with the highest rank being the domain followed by kingdom, phylum, class, order, family, genus, and species.[90] All organisms can be classified as belonging to one of three domains: Archaea (originally Archaebacteria); bacteria (originally eubacteria), or eukarya (includes the protist, fungi, plant, and animal kingdoms).[93]
The history of life on Earth traces how organisms have evolved from the earliest emergence of life to present day. Earth formed about 4.5 billion years ago and all life on Earth, both living and extinct, descended from a last universal common ancestor that lived about 3.5 billion years ago.[94][95] Geologists have developed a geologic time scale that divides the history of the Earth into major divisions, starting with four eons (Hadean, Archean, Proterozoic, and Phanerozoic), the first three of which are collectively known as the Precambrian, which lasted approximately 4 billion years.[96] Each eon can be divided into eras, with the Phanerozoic eon that began 539 million years ago[97] being subdivided into Paleozoic, Mesozoic, and Cenozoic eras.[96] These three eras together comprise eleven periods (Cambrian, Ordovician, Silurian, Devonian, Carboniferous, Permian, Triassic, Jurassic, Cretaceous, Tertiary, and Quaternary).[96]
The similarities among all known present-day species indicate that they have diverged through the process of evolution from their common ancestor.[98] Biologists regard the ubiquity of the genetic code as evidence of universal common descent for all bacteria, archaea, and eukaryotes.[99][10][100][101] Microbal mats of coexisting bacteria and archaea were the dominant form of life in the early Archean epoch and many of the major steps in early evolution are thought to have taken place in this environment.[102] The earliest evidence of eukaryotes dates from 1.85 billion years ago,[103][104] and while they may have been present earlier, their diversification accelerated when they started using oxygen in their metabolism. Later, around 1.7 billion years ago, multicellular organisms began to appear, with differentiated cells performing specialised functions.[105]
Algae-like multicellular land plants are dated back even to about 1 billion years ago,[106] although evidence suggests that microorganisms formed the earliest terrestrial ecosystems, at least 2.7 billion years ago.[107] Microorganisms are thought to have paved the way for the inception of land plants in the Ordovician period. Land plants were so successful that they are thought to have contributed to the Late Devonian extinction event.[108]
Bacteria are a type of cell that constitute a large domain of prokaryotic microorganisms. Typically a few micrometers in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste,[118] and the deep biosphere of the earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised, and only about 27 percent of the bacterial phyla have species that can be grown in the laboratory.[119]
Archaea constitute the other domain of prokaryotic cells and were initially classified as bacteria, receiving the name archaebacteria (in the Archaebacteria kingdom), a term that has fallen out of use.[120] Archaeal cells have unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as the flat and square cells of Haloquadratum walsbyi.[121] Despite this morphological similarity to bacteria, archaea possess genes and several metabolic pathways that are more closely related to those of eukaryotes, notably for the enzymes involved in transcription and translation. Other aspects of archaeal biochemistry are unique, such as their reliance on ether lipids in their cell membranes,[122] including archaeols. Archaea use more energy sources than eukaryotes: these range from organic compounds, such as sugars, to ammonia, metal ions or even hydrogen gas. Salt-tolerant archaea (the Haloarchaea) use sunlight as an energy source, and other species of archaea fix carbon, but unlike plants and cyanobacteria, no known species of archaea does both. Archaea reproduce asexually by binary fission, fragmentation, or budding; unlike bacteria, no known species of Archaea form endospores.
The first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms. Improved molecular detection tools led to the discovery of archaea in almost every habitat, including soil, oceans, and marshlands. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet.
Archaea are a major part of Earth's life. They are part of the microbiota of all organisms. In the human microbiome, they are important in the gut, mouth, and on the skin.[123] Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example.[124]
Eukaryotes are hypothesized to have split from archaea, which was followed by their endosymbioses with bacteria (or symbiogenesis) that gave rise to mitochondria and chloroplasts, both of which are now part of modern-day eukaryotic cells.[125] The major lineages of eukaryotes diversified in the Precambrian about 1.5 billion years ago and can be classified into eight major clades: alveolates, excavates, stramenopiles, plants, rhizarians, amoebozoans, fungi, and animals.[125] Five of these clades are collectively known as protists, which are mostly microscopic eukaryotic organisms that are not plants, fungi, or animals.[125] While it is likely that protists share a common ancestor (the last eukaryotic common ancestor),[126] protists by themselves do not constitute a separate clade as some protists may be more closely related to plants, fungi, or animals than they are to other protists. Like groupings such as algae, invertebrates, or protozoans, the protist grouping is not a formal taxonomic group but is used for convenience.[125][127] Most protists are unicellular; these are called microbial eukaryotes.[125]
Plants are mainly multicellular organisms, predominantly photosynthetic eukaryotes of the kingdom Plantae, which would exclude fungi and some algae. Plant cells were derived by endosymbiosis of a cyanobacterium into an early eukaryote about one billion years ago, which gave rise to chloroplasts.[128] The first several clades that emerged following primary endosymbiosis were aquatic and most of the aquatic photosynthetic eukaryotic organisms are collectively described as algae, which is a term of convenience as not all algae are closely related.[128] Algae comprise several distinct clades such as glaucophytes, which are microscopic freshwater algae that may have resembled in form to the early unicellular ancestor of Plantae.[128] Unlike glaucophytes, the other algal clades such as red and green algae are multicellular. Green algae comprise three major clades: chlorophytes, coleochaetophytes, and stoneworts.[128]
Fungi are eukaryotes that digest foods outside their bodies,[129] secreting digestive enzymes that break down large food molecules before absorbing them through their cell membranes. Many fungi are also saprobes, feeding on dead organic matter, making them important decomposers in ecological systems.[129]
Viruses are submicroscopic infectious agents that replicate inside the cells of organisms.[131] Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea.[132][133] More than 6,000 virus species have been described in detail.[134] Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity.[135][136]
Ecology is the study of the distribution and abundance of life, the interaction between organisms and their environment.[140]
The community of living (biotic) organisms in conjunction with the nonliving (abiotic) components (e.g., water, light, radiation, temperature, humidity, atmosphere, acidity, and soil) of their environment is called an ecosystem.[141][142][143] These biotic and abiotic components are linked together through nutrient cycles and energy flows.[144] Energy from the sun enters the system through photosynthesis and is incorporated into plant tissue. By feeding on plants and on one another, animals move matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes.[145]
A population is the group of organisms of the same species that occupies an area and reproduce from generation to generation.[146][147][148][149][150] Population size can be estimated by multiplying population density by the area or volume. The carrying capacity of an environment is the maximum population size of a species that can be sustained by that specific environment, given the food, habitat, water, and other resources that are available.[151] The carrying capacity of a population can be affected by changing environmental conditions such as changes in the availability resources and the cost of maintaining them. In human populations, new technologies such as the Green revolution have helped increase the Earth's carrying capacity for humans over time, which has stymied the attempted predictions of impending population decline, the most famous of which was by Thomas Malthus in the 18th century.[146]
In the global ecosystem or biosphere, matter exists as different interacting compartments, which can be biotic or abiotic as well as accessible or inaccessible, depending on their forms and locations.[159] For example, matter from terrestrial autotrophs are both biotic and accessible to other organisms whereas the matter in rocks and minerals are abiotic and inaccessible. A biogeochemical cycle is a pathway by which specific elements of matter are turned over or moved through the biotic (biosphere) and the abiotic (lithosphere, atmosphere, and hydrosphere) compartments of Earth. There are biogeochemical cycles for nitrogen, carbon, and water.
Conservation biology is the study of the conservation of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions.[160][161][162] It is concerned with factors that influence the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity.[163][164][165][166] The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years,[167] which has contributed to poverty, starvation, and will reset the course of evolution on this planet.[168][169] Biodiversity affects the functioning of ecosystems, which provide a variety of services upon which people depend. Conservation biologists research and educate on the trends of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Organizations and citizens are responding to the current biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales.[170][163][164][165]
In biology, regeneration is the process of renewal, restoration, and tissue growth that makes genomes, cells, organisms, and ecosystems resilient to natural fluctuations or events that cause disturbance or damage.[1] Every species is capable of regeneration, from bacteria to humans.[2][3] Regeneration can either be complete[4] where the new tissue is the same as the lost tissue,[4] or incomplete[5] where after the necrotic tissue comes fibrosis.[5]
At its most elementary level, regeneration is mediated by the molecular processes of gene regulation and involves the cellular processes of cell proliferation, morphogenesis and cell differentiation.[6][7] Regeneration in biology, however, mainly refers to the morphogenic processes that characterize the phenotypic plasticity of traits allowing multi-cellular organisms to repair and maintain the integrity of their physiological and morphological states. Above the genetic level, regeneration is fundamentally regulated by asexual cellular processes.[8] Regeneration is different from reproduction. For example, hydra perform regeneration but reproduce by the method of budding.
The hydra and the planarian flatworm have long served as model organisms for their highly adaptive regenerative capabilities.[9] Once wounded, their cells become activated and restore the organs back to their pre-existing state.[10] The Caudata ("urodeles"; salamanders and newts), an order of tailed amphibians, is possibly the most adept vertebrate group at regeneration, given their capability of regenerating limbs, tails, jaws, eyes and a variety of internal structures.[2] The regeneration of organs is a common and widespread adaptive capability among metazoan creatures.[9] In a related context, some animals are able to reproduce asexually through fragmentation, budding, or fission.[8] A planarian parent, for example, will constrict, split in the middle, and each half generates a new end to form two clones of the original.[11]
Echinoderms (such as the sea star), crayfish, many reptiles, and amphibians exhibit remarkable examples of tissue regeneration. The case of autotomy, for example, serves as a defensive function as the animal detaches a limb or tail to avoid capture. After the limb or tail has been autotomized, cells move into action and the tissues will regenerate.[12][13][14] In some cases a shed limb can itself regenerate a new individual.[15] Limited regeneration of limbs occurs in most fishes and salamanders, and tail regeneration takes place in larval frogs and toads (but not adults). The whole limb of a salamander or a triton will grow again and again after amputation. In reptiles, chelonians, crocodilians and snakes are unable to regenerate lost parts, but many (not all) kinds of lizards, geckos and iguanas possess regeneration capacity in a high degree. Usually, it involves dropping a section of their tail and regenerating it as part of a defense mechanism. While escaping a predator, if the predator catches the tail, it will disconnect.[16]
Ecosystems can be regenerative. Following a disturbance, such as a fire or pest outbreak in a forest, pioneering species will occupy, compete for space, and establish themselves in the newly opened habitat. The new growth of seedlings and community assembly process is known as regeneration in ecology.[17][18]
Pattern formation in the morphogenesis of an animal is regulated by genetic induction factors that put cells to work after damage has occurred. Neural cells, for example, express growth-associated proteins, such as GAP-43, tubulin, actin, an array of novel neuropeptides, and cytokines that induce a cellular physiological response to regenerate from the damage.[19] Many of the genes that are involved in the original development of tissues are reinitialized during the regenerative process. Cells in the primordia of zebrafish fins, for example, express four genes from the homeobox msx family during development and regeneration.[20]
Many arthropods can regenerate limbs and other appendages following either injury or autotomy.[23] Regeneration capacity is constrained by the developmental stage and ability to molt.
Crustaceans, which continually molt, can regenerate throughout their lifetimes.[24] While molting cycles are generally hormonally regulated, limb amputation induces premature molting.[23][25]
Hemimetabolous insects such as crickets can regenerate limbs as nymphs, before their final molt.[26]
Holometabolous insects can regenerate appendages as larvae prior to the final molt and metamorphosis. Beetle larvae, for example, can regenerate amputated limbs. Fruit fly larvae do not have limbs but can regenerate their appendage primordia, imaginal discs.[27] In both systems, the regrowth of the new tissue delays pupation.[27][28]
Mechanisms underlying appendage limb regeneration in insects and crustaceans are highly conserved.[29] During limb regeneration species in both taxa form a blastema that proliferates and grows to repattern the missing tissue.[30]
Arachnids, including scorpions, are known to regenerate their venom, although the content of the regenerated venom is different from the original venom during its regeneration, as the venom volume is replaced before the active proteins are all replenished.[31]
The fruit fly Drosophila melanogaster is a useful model organism to understand the molecular mechanisms that control regeneration, especially gut and germline regeneration.[32] In these tissues, resident stem cells continually renew lost cells.[33] The Hippo signaling pathway was discovered in flies and was found to be required for midgut regeneration. Later, this conserved signaling pathway was also found to be essential for regeneration of many mammalian tissues, including heart, liver, skin, and lung, and intestine.[34]
Many annelids (segmented worms) are capable of regeneration.[35] For example, Chaetopterus variopedatus and Branchiomma nigromaculata can regenerate both anterior and posterior body parts after latitudinal bisection.[36] The relationship between somatic and germline stem cell regeneration has been studied at the molecular level in the annelid Capitella teleta.[37] Leeches, however, appear incapable of segmental regeneration.[38] Furthermore, their close relatives, the branchiobdellids, are also incapable of segmental regeneration.[38][35] However, certain individuals, like the lumbriculids, can regenerate from only a few segments.[38] Segmental regeneration in these animals is epimorphic and occurs through blastema formation.[38] Segmental regeneration has been gained and lost during annelid evolution, as seen in oligochaetes, where head regeneration has been lost three separate times.[38]
Along with epimorphosis, some polychaetes like Sabella pavonina experience morphallactic regeneration.[38][39] Morphallaxis involves the de-differentiation, transformation, and re-differentation of cells to regenerate tissues. How prominent morphallactic regeneration is in oligochaetes is currently not well understood. Although relatively under-reported, it is possible that morphallaxis is a common mode of inter-segment regeneration in annelids. Following regeneration in L. variegatus, past posterior segments sometimes become anterior in the new body orientation, consistent with morphallaxis.
Following amputation, most annelids are capable of sealing their body via rapid muscular contraction. Constriction of body muscle can lead to infection prevention. In certain species, such as Limnodrilus, autolysis can be seen within hours after amputation in the ectoderm and mesoderm. Amputation is also thought to cause a large migration of cells to the injury site, and these form a wound plug.
Regeneration research using Planarians began in the late 1800s and was popularized by T.H. Morgan at the beginning of the 20th century.[43] Alejandro Sanchez-Alvarado and Philip Newmark transformed planarians into a model genetic organism in the beginning of the 20th century to study the molecular mechanisms underlying regeneration in these animals.[45] Planarians exhibit an extraordinary ability to regenerate lost body parts. For example, a planarian split lengthwise or crosswise will regenerate into two separate individuals. In one experiment, T.H. Morgan found that a piece corresponding to 1/279th of a planarian[43] or a fragment with as few as 10,000 cells can successfully regenerate into a new worm within one to two weeks.[46] After amputation, stump cells form a blastema formed from neoblasts, pluripotent cells found throughout the planarian body.[47] New tissue grows from neoblasts with neoblasts comprising between 20 and 30% of all planarian cells.[46] Recent work has confirmed that neoblasts are totipotent since one single neoblast can regenerate an entire irradiated animal that has been rendered incapable of regeneration.[48] In order to prevent starvation a planarian will use their own cells for energy, this phenomenon is known as de-growth.[10]
Limb regeneration in the axolotl and newt has been extensively studied and researched. The nineteenth century studies of this subject are reviewed in Holland (2021).[49] Urodele amphibians, such as salamanders and newts, display the highest regenerative ability among tetrapods.[50][49] As such, they can fully regenerate their limbs, tail, jaws, and retina via epimorphic regeneration leading to functional replacement with new tissue.[51]  Salamander limb regeneration occurs in two main steps. First, the local cells dedifferentiate at the wound site into progenitor to form a blastema.[52] Second, the blastemal cells will undergo cell proliferation, patterning, cell differentiation and tissue growth using similar genetic mechanisms that deployed during embryonic development.[53] Ultimately, blastemal cells will generate all the cells for the new structure.[50]
In spite of the historically few researchers studying limb regeneration, remarkable progress has been made recently in establishing the neotenous amphibian the axolotl (Ambystoma mexicanum) as a model genetic organism. This progress has been facilitated by advances in genomics, bioinformatics, and somatic cell transgenesis in other fields, that have created the opportunity to investigate the mechanisms of important biological properties, such as limb regeneration, in the axolotl.[53] The Ambystoma Genetic Stock Center (AGSC) is a self-sustaining, breeding colony of the axolotl supported by the National Science Foundation as a Living Stock Collection. Located at the University of Kentucky, the AGSC is dedicated to supplying genetically well-characterized axolotl embryos, larvae, and adults to laboratories throughout the United States and abroad. An NIH-funded NCRR grant has led to the establishment of the Ambystoma EST database, the Salamander Genome Project (SGP) that has led to the creation of the first amphibian gene map and several annotated molecular data bases, and the creation of the research community web portal.[60] In 2022, a first spatiotemporal map revealed key insights about axolotl brain regeneration, also providing the interactive Axolotl Regenerative Telencephalon Interpretation via Spatiotemporal Transcriptomic Atlas .[61][62]
Anurans (frogs) can only regenerate their limbs during embryonic development.[63] Reactive oxygen species (ROS) appear to be required for a regeneration response in the anuran larvae.[64] ROS production is essential to activate the Wnt signaling pathway, which has been associated with regeneration in other systems.[64]
Once the limb skeleton has developed in frogs, regeneration does not occur (Xenopus can grow a cartilaginous spike after amputation).[63]  The adult Xenopus laevis is used as a model organism for regenerative medicine. In 2022, a cocktail of drugs and hormones (1,4-DPCA, BDNF, growth hormone, resolvin D5, and retinoic acid), in a single dose lasting 24 hours, was shown to trigger long-term leg regeneration in adult X. laevis. Instead of a single spike, a paddle-shaped growth is obtained at the end of the limb by 18 months.[65]
Hydra is a genus of freshwater polyp in the phylum Cnidaria with highly proliferative stem cells that gives them the ability to regenerate their entire body.[66] Any fragment larger than a few hundred epithelial cells that is isolated from the body has the ability to regenerate into a smaller version of itself.[66] The high proportion of stem cells in the hydra supports its efficient regenerative ability.[67]
Regeneration among hydra occurs as foot regeneration arising from the basal part of the body, and head regeneration, arising from the apical region.[66] Regeneration tissues that are cut from the gastric region contain polarity, which allows them to distinguish between regenerating a head in the apical end and a foot in the basal end so that both regions are present in the newly regenerated organism.[66] Head regeneration requires complex reconstruction of the area, while foot regeneration is much simpler, similar to tissue repair.[68] In both foot and head regeneration, however, there are two distinct molecular cascades that occur once the tissue is wounded: early injury response and a subsequent, signal-driven pathway of the regenerating tissue that leads to cellular differentiation.[67] This early-injury response includes epithelial cell stretching for wound closure, the migration of interstitial progenitors towards the wound, cell death, phagocytosis of cell debris, and reconstruction of the extracellular matrix.[67]
Regeneration in hydra has been defined as morphallaxis, the process where regeneration results from remodeling of existing material without cellular proliferation.[69][70] If a hydra is cut into two pieces, the remaining severed sections form two fully functional and independent hydra, approximately the same size as the two smaller severed sections.[66] This occurs through the exchange and rearrangement of soft tissues without the formation of new material.[67]
Owing to a limited literature on the subject, birds are believed to have very limited regenerative abilities as adults. Some studies[71] on roosters have suggested that birds can adequately regenerate some parts of the limbs and depending on the conditions in which regeneration takes place, such as age of the animal, the inter-relationship of the injured tissue with other muscles, and the type of operation, can involve complete regeneration of some musculoskeletal structure. Werber and Goldschmidt (1909) found that the goose and duck were capable of regenerating their beaks after partial amputation[71]  and Sidorova (1962) observed liver regeneration via hypertrophy in roosters.[72] Birds are also capable of regenerating the hair cells in their cochlea following noise damage or ototoxic drug damage.[73] Despite this evidence, contemporary studies suggest reparative regeneration in avian species is limited to periods during embryonic development. An array of molecular biology techniques have been successful in manipulating cellular pathways known to contribute to spontaneous regeneration in chick embryos.[74] For instance, removing a portion of the elbow joint in a chick embryo via window excision or slice excision and comparing joint tissue specific markers and cartilage markers showed that window excision allowed 10 out of 20 limbs to regenerate and expressed joint genes similarly to a developing embryo. In contrast, slice excision did not allow the joint to regenerate due to the fusion of the skeletal elements seen by an expression of cartilage markers.[75]
Similar to the physiological regeneration of hair in mammals, birds can regenerate their feathers in order to repair damaged feathers or to attract mates with their plumage. Typically, seasonal changes that are associated with breeding seasons will prompt a hormonal signal for birds to begin regenerating feathers. This has been experimentally induced using thyroid hormones in the Rhode Island Red Fowls.[76]
Mammals are capable of cellular and physiological regeneration, but have generally poor reparative regenerative ability across the group.[1][24] Examples of physiological regeneration in mammals include epithelial renewal (e.g., skin and intestinal tract), red blood cell replacement, antler regeneration and hair cycling.[77][78] Male deer lose their antlers annually during the months of January to April then through regeneration are able to regrow them as an example of physiological regeneration. A deer antler is the only appendage of a mammal that can be regrown every year.[79] While reparative regeneration is a rare phenomenon in mammals, it does occur.  A well-documented example is regeneration of the digit tip distal to the nail bed.[80] Reparative regeneration has also been observed in rabbits, pikas and African spiny mice.  In 2012, researchers discovered that two species of African Spiny Mice, Acomys kempi and Acomys percivali, were capable of completely regenerating the autotomically released or otherwise damaged tissue. These species can regrow hair follicles, skin, sweat glands, fur and cartilage.[81] In addition to these two species, subsequent studies demonstrated that Acomys cahirinus could regenerate skin and excised tissue in the ear pinna.[82][83]
Despite these examples, it is generally accepted that adult mammals have limited regenerative capacity compared to most vertebrate embryos/larvae, adult salamanders and fish.[84] But the regeneration therapy approach of Robert O. Becker, using electrical stimulation, has shown promising results for rats[85] and mammals in general.[86]
Some researchers have also claimed that the MRL mouse strain exhibits enhanced regenerative abilities. Work comparing the differential gene expression of scarless healing MRL mice and a poorly-healing C57BL/6 mouse strain, identified 36 genes differentiating the healing process between MRL mice and other mice.[87][88] Study of the regenerative process in these animals is aimed at discovering how to duplicate them in humans, such as deactivation of the p21 gene.[89][90] However, recent work has shown that MRL mice actually close small ear holes with scar tissue, rather than regeneration as originally claimed.[82]
MRL mice are not protected against myocardial infarction; heart regeneration in adult mammals (neocardiogenesis) is limited, because heart muscle cells are nearly all terminally differentiated. MRL mice show the same amount of cardiac injury and scar formation as normal mice after a heart attack.[91] However, recent studies provide evidence that this may not always be the case, and that MRL mice can regenerate after heart damage.[92]
The regrowth of lost tissues or organs in the human body is being researched. Some tissues such as skin regrow quite readily; others have been thought to have little or no capacity for regeneration, but ongoing research suggests that there is some hope for a variety of tissues and organs.[1][93] Human organs that have been regenerated include the bladder, vagina and the penis.[94]
As are all metazoans, humans are capable of physiological regeneration (i.e. the replacement of cells during homeostatic maintenance that does not necessitate injury). For example, the regeneration of red blood cells via erythropoiesis occurs through the maturation of erythrocytes from hematopoietic stem cells in the bone marrow, their subsequent circulation for around 90 days in the blood stream, and their eventual cell-death in the spleen.[95] Another example of physiological regeneration is the sloughing and rebuilding of a functional endometrium during each menstrual cycle in females in response to varying levels of circulating estrogen and progesterone.[96]
However, humans are limited in their capacity for reparative regeneration, which occurs in response to injury. One of the most studied regenerative responses in humans is the hypertrophy of the liver following liver injury.[97][98] For example, the original mass of the liver is re-established in direct proportion to the amount of liver removed following partial hepatectomy,[99] which indicates that signals from the body regulate liver mass precisely, both positively and negatively, until the desired mass is reached. This response is considered cellular regeneration (a form of compensatory hypertrophy) where the function and mass of the liver is regenerated through the proliferation of existing mature hepatic cells (mainly hepatocytes), but the exact morphology of the liver is not regained.[98] This process is driven by growth factor and cytokine regulated pathways.[97] The normal sequence of inflammation and regeneration does not function accurately in cancer. Specifically, cytokine stimulation of cells leads to expression of genes that change cellular functions and suppress the immune response.[100]
Adult neurogenesis is also a form of cellular regeneration. For example, hippocampal neuron renewal occurs in normal adult humans at an annual turnover rate of 1.75% of neurons.[101] Cardiac myocyte renewal has been found to occur in normal adult humans,[102] and at a higher rate in adults following acute heart injury such as infarction.[103] Even in adult myocardium following infarction, proliferation is only found in around 1% of myocytes around the area of injury, which is not enough to restore function of cardiac muscle. However, this may be an important target for regenerative medicine as it implies that regeneration of cardiomyocytes, and consequently of myocardium, can be induced.
Another example of reparative regeneration in humans is fingertip regeneration, which occurs after phalange amputation distal to the nail bed (especially in children)[104][105] and rib regeneration, which occurs following osteotomy for scoliosis treatment (though usually regeneration is only partial and may take up to one year).[106]
Yet another example of regeneration in humans is vas deferens regeneration, which occurs after a vasectomy and which results in vasectomy failure.[107]
The ability and degree of regeneration in reptiles differs among the various species, but the most notable and well-studied occurrence is tail-regeneration in lizards.[108][109][110] In addition to lizards, regeneration has been observed in the tails and maxillary bone of crocodiles and adult neurogenesis has also been noted.[108][111][112] Tail regeneration has never been observed in snakes.[108] Lizards possess the highest regenerative capacity as a group.[108][109][110][113] Following autotomous tail loss, epimorphic regeneration of a new tail proceeds through a blastema-mediated process that results in a functionally and morphologically similar structure.[108][109]
Rhodopsin regeneration has been studied in skates and rays. After complete photo-bleaching, rhodopsin can completely regenerate within 2 hours in the retina.[114]
White bamboo sharks can regenerate at least two-thirds of their liver and this has been linked to three micro RNAs, xtr-miR-125b, fru-miR-204, and has-miR-142-3p_R-. In one study, two-thirds of the liver was removed and within 24 hours more than half of the liver had undergone hypertrophy.[115]
Some sharks can regenerate scales and even skin following damage. Within two weeks of skin wounding, mucus is secreted into the wound and this initiates the healing process. One study showed that the majority of the wounded area was regenerated within 4 months, but the regenerated area also showed a high degree of variability.[116]
A disease is a particular abnormal condition that negatively affects the structure or function of all or part of an organism, and that is not immediately due to any external injury.[1][2] Diseases are often known to be medical conditions that are associated with specific signs and symptoms. A disease may be caused by external factors such as pathogens or by internal dysfunctions. For example, internal dysfunctions of the immune system can produce a variety of different diseases, including various forms of immunodeficiency, hypersensitivity, allergies and autoimmune disorders.
In humans, disease is often used more broadly to refer to any condition that causes pain, dysfunction, distress, social problems, or death to the person affected, or similar problems for those in contact with the person. In this broader sense, it sometimes includes injuries, disabilities, disorders, syndromes, infections, isolated symptoms, deviant behaviors, and atypical variations of structure and function, while in other contexts and for other purposes these may be considered distinguishable categories. Diseases can affect people not only physically, but also mentally, as contracting and living with a disease can alter the affected person's perspective on life.
Death due to disease is called death by natural causes. There are four main types of disease: infectious diseases, deficiency diseases, hereditary diseases (including both genetic diseases and non-genetic hereditary diseases), and physiological diseases. Diseases can also be classified in other ways, such as communicable versus non-communicable diseases. The deadliest diseases in humans are coronary artery disease (blood flow obstruction), followed by cerebrovascular disease and lower respiratory infections.[3] In developed countries, the diseases that cause the most sickness overall are neuropsychiatric conditions, such as depression and anxiety.
The study of disease is called pathology, which includes the study of etiology, or cause.
In many cases, terms such as disease, disorder, morbidity, sickness and illness are used interchangeably; however, there are situations when specific terms are considered preferable.[4]
In an infectious disease, the incubation period is the time between infection and the appearance of symptoms. The latency period is the time between infection and the ability of the disease to spread to another person, which may precede, follow, or be simultaneous with the appearance of symptoms. Some viruses also exhibit a dormant phase, called viral latency, in which the virus hides in the body in an inactive state. For example, varicella zoster virus causes chickenpox in the acute phase; after recovery from chickenpox, the virus may remain dormant in nerve cells for many years, and later cause herpes zoster (shingles).
Diseases may be classified by cause, pathogenesis (mechanism by which the disease is caused), or by symptom(s). Alternatively, diseases may be classified according to the organ system involved, though this is often complicated since many diseases affect more than one organ.
A chief difficulty in nosology is that diseases often cannot be defined and classified clearly, especially when cause or pathogenesis are unknown. Thus diagnostic terms often only reflect a symptom or set of symptoms (syndrome).
Classical classification of human disease derives from the observational correlation between pathological analysis and clinical syndromes. Today it is preferred to classify them by their cause if it is known.[23]
The most known and used classification of diseases is the World Health Organization's ICD. This is periodically updated. Currently, the last publication is the ICD-11.
Only some diseases such as influenza are contagious and commonly believed infectious. The microorganisms that cause these diseases are known as pathogens and include varieties of bacteria, viruses, protozoa, and fungi. Infectious diseases can be transmitted, e.g. by hand-to-mouth contact with infectious material on surfaces, by bites of insects or other carriers of the disease, and from contaminated water or food (often via fecal contamination), etc.[24] Also, there are sexually transmitted diseases. In some cases, microorganisms that are not readily spread from person to person play a role, while other diseases can be prevented or ameliorated with appropriate nutrition or other lifestyle changes.
Some diseases, such as most (but not all) forms of cancer, heart disease, and mental disorders, are non-infectious diseases. Many non-infectious diseases have a partly or completely genetic basis (see genetic disorder) and may thus be transmitted from one generation to another.
Social determinants of health are the social conditions in which people live that determine their health. Illnesses are generally related to social, economic, political, and environmental circumstances. Social determinants of health have been recognized by several health organizations such as the Public Health Agency of Canada and the World Health Organization to greatly influence collective and personal well-being. The World Health Organization's Social Determinants Council also recognizes Social determinants of health in poverty.
When the cause of a disease is poorly understood, societies tend to mythologize the disease or use it as a metaphor or symbol of whatever that culture considers evil. For example, until the bacterial cause of tuberculosis was discovered in 1882, experts variously ascribed the disease to heredity, a sedentary lifestyle, depressed mood, and overindulgence in sex, rich food, or alcohol, all of which were social ills at the time.[25]
When a disease is caused by a pathogenic organism (e.g., when malaria is caused by Plasmodium), one should not confuse the pathogen (the cause of the disease) with disease itself. For example, West Nile virus (the pathogen) causes West Nile fever (the disease). The misuse of basic definitions in epidemiology is frequent in scientific publications.[26]
Many diseases and disorders can be prevented through a variety of means. These include sanitation, proper nutrition, adequate exercise, vaccinations and other self-care and public health measures, such as obligatory face mask mandates[citation needed].
Medical therapies or treatments are efforts to cure or improve a disease or other health problems. In the medical field, therapy is synonymous with the word treatment. Among psychologists, the term may refer specifically to psychotherapy or "talk therapy". Common treatments include medications, surgery, medical devices, and self-care. Treatments may be provided by an organized health care system, or informally, by the patient or family members.
Preventive healthcare is a way to avoid an injury, sickness, or disease in the first place. A treatment or cure is applied after a medical problem has already started. A treatment attempts to improve or remove a problem, but treatments may not produce permanent cures, especially in chronic diseases. Cures are a subset of treatments that reverse diseases completely or end medical problems permanently. Many diseases that cannot be completely cured are still treatable. Pain management (also called pain medicine) is that branch of medicine employing an interdisciplinary approach to the relief of pain and improvement in the quality of life of those living with pain.[27]
Treatment for medical emergencies must be provided promptly, often through an emergency department or, in less critical situations, through an urgent care facility.
Epidemiology is the study of the factors that cause or encourage diseases. Some diseases are more common in certain geographic areas, among people with certain genetic or socioeconomic characteristics, or at different times of the year.
Epidemiology is considered a cornerstone methodology of public health research and is highly regarded in evidence-based medicine for identifying risk factors for diseases. In the study of communicable and non-communicable diseases, the work of epidemiologists ranges from outbreak investigation to study design, data collection, and analysis including the development of statistical models to test hypotheses and the documentation of results for submission to peer-reviewed journals. Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic. Epidemiologists rely on a number of other scientific disciplines such as biology (to better understand disease processes), biostatistics (the current raw information available), Geographic Information Science (to store data and map disease patterns) and social science disciplines (to better understand proximate and distal risk factors). Epidemiology can help identify causes as well as guide prevention efforts.
In studying diseases, epidemiology faces the challenge of defining them. Especially for poorly understood diseases, different groups might use significantly different definitions. Without an agreed-on definition, different researchers may report different numbers of cases and characteristics of the disease.[28]
Some morbidity databases are compiled with data supplied by states and territories health authorities, at national levels[29][30] or larger scale (such as European Hospital Morbidity Database (HMDB))[31] which may contain hospital discharge data by detailed diagnosis, age and sex. The European HMDB data was submitted by European countries to the World Health Organization Regional Office for Europe.
Disease burden is the impact of a health problem in an area measured by financial cost, mortality, morbidity, or other indicators.
There are several measures used to quantify the burden imposed by diseases on people. The years of potential life lost (YPLL) is a simple estimate of the number of years that a person's life was shortened due to a disease. For example, if a person dies at the age of 65 from a disease, and would probably have lived until age 80 without that disease, then that disease has caused a loss of 15 years of potential life. YPLL measurements do not account for how disabled a person is before dying, so the measurement treats a person who dies suddenly and a person who died at the same age after decades of illness as equivalent. In 2004, the World Health Organization calculated that 932 million years of potential life were lost to premature death.[32]
How a society responds to diseases is the subject of medical sociology.
A condition may be considered a disease in some cultures or eras but not in others. For example, obesity can represent wealth and abundance, and is a status symbol in famine-prone areas and some places hard-hit by HIV/AIDS.[34] Epilepsy is considered a sign of spiritual gifts among the Hmong people.[35]
Sickness confers the social legitimization of certain benefits, such as illness benefits, work avoidance, and being looked after by others. The person who is sick takes on a social role called the sick role. A person who responds to a dreaded disease, such as cancer, in a culturally acceptable fashion may be publicly and privately honored with higher social status.[36] In return for these benefits, the sick person is obligated to seek treatment and work to become well once more. As a comparison, consider pregnancy, which is not interpreted as a disease or sickness, even if the mother and baby may both benefit from medical care.
Most religions grant exceptions from religious duties to people who are sick. For example, one whose life would be endangered by fasting on Yom Kippur or during Ramadan is exempted from the requirement, or even forbidden from participating. People who are sick are also exempted from social duties. For example, ill health is the only socially acceptable reason for an American to refuse an invitation to the White House.[37]
The identification of a condition as a disease, rather than as simply a variation of human structure or function, can have significant social or economic implications. The controversial recognition of diseases such as repetitive stress injury (RSI) and post-traumatic stress disorder (PTSD) has had a number of positive and negative effects on the financial and other responsibilities of governments, corporations, and institutions towards individuals, as well as on the individuals themselves. The social implication of viewing aging as a disease could be profound, though this classification is not yet widespread.
Lepers were people who were historically shunned because they had an infectious disease, and the term "leper" still evokes social stigma. Fear of disease can still be a widespread social phenomenon, though not all diseases evoke extreme social stigma.
Social standing and economic status affect health. Diseases of poverty are diseases that are associated with poverty and low social status; diseases of affluence are diseases that are associated with high social and economic status. Which diseases are associated with which states vary according to time, place, and technology. Some diseases, such as diabetes mellitus, may be associated with both poverty (poor food choices) and affluence (long lifespans and sedentary lifestyles), through different mechanisms. The term lifestyle diseases describes diseases associated with longevity and that are more common among older people. For example, cancer is far more common in societies in which most members live until they reach the age of 80 than in societies in which most members die before they reach the age of 50.
An illness narrative is a way of organizing a medical experience into a coherent story that illustrates the sick individual's personal experience.
People use metaphors to make sense of their experiences with disease. The metaphors move disease from an objective thing that exists to an affective experience. The most popular metaphors draw on military concepts: Disease is an enemy that must be feared, fought, battled, and routed. The patient or the healthcare provider is a warrior, rather than a passive victim or bystander. The agents of communicable diseases are invaders; non-communicable diseases constitute internal insurrection or civil war. Because the threat is urgent, perhaps a matter of life and death, unthinkably radical, even oppressive, measures are society's and the patient's moral duty as they courageously mobilize to struggle against destruction. The War on Cancer is an example of this metaphorical use of language.[38] This language is empowering to some patients, but leaves others feeling like they are failures.[39]
Another class of metaphors describes the experience of illness as a journey: The person travels to or from a place of disease, and changes himself, discovers new information, or increases his experience along the way. He may travel "on the road to recovery" or make changes to "get on the right track" or choose "pathways".[38][39] Some are explicitly immigration-themed: the patient has been exiled from the home territory of health to the land of the ill, changing identity and relationships in the process.[40] This language is more common among British healthcare professionals than the language of physical aggression.[39]
Some metaphors are disease-specific. Slavery is a common metaphor for addictions: The alcoholic is enslaved by drink, and the smoker is captive to nicotine. Some cancer patients treat the loss of their hair from chemotherapy as a metonymy or metaphor for all the losses caused by the disease.[38]
Some diseases are used as metaphors for social ills: "Cancer" is a common description for anything that is endemic and destructive in society, such as poverty, injustice, or racism. AIDS was seen as a divine judgment for moral decadence, and only by purging itself from the "pollution" of the "invader" could society become healthy again.[38] More recently, when AIDS seemed less threatening, this type of emotive language was applied to avian flu and type 2 diabetes mellitus.[41] Authors in the 19th century commonly used tuberculosis as a symbol and a metaphor for transcendence. People with the disease were portrayed in literature as having risen above daily life to become ephemeral objects of spiritual or artistic achievement. In the 20th century, after its cause was better understood, the same disease became the emblem of poverty, squalor, and other social problems.[40]
Wildlife trade refers to the products that are derived from non-domesticated animals or plants usually extracted from their natural environment or raised under controlled conditions. It can involve the trade of living or dead individuals, tissues such as skins, bones or meat, or other products. Legal wildlife trade is regulated by the United Nations' Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES), which currently has 184 member countries called Parties.[1] Illegal wildlife trade is widespread and constitutes one of the major illegal economic activities, comparable to the traffic of drugs and weapons.[2]
Wildlife trade is a serious conservation problem, has a negative effect on the viability of many wildlife populations and is one of the major threats to the survival of vertebrate species.[3] The illegal wildlife trade has been linked to the emergence and spread of new infectious diseases in humans, including emergent viruses.[4][5] Global initiative like the United Nations Sustainable Development Goal 15 have a target to end the illegal supply of wildlife.[6]
Wildlife use is a general term for all uses of wildlife products, including ritual or religious uses, consumption of bushmeat and different forms of trade. Wildlife use is usually linked to hunting or poaching. Wildlife trade can be differentiated in legal and illegal trade, and both can have domestic (local or national) or international markets, but they might be often related with each other.[7]
The volume of international trade in wildlife commodities is immense and continues to rise. According to an analysis to the 2012 Harmonized System customs statistics, global import of wildlife products amounted to US$187 billion, of which fisheries commodities accounted for $113 billion; plants and forestry products for $71 billion; non-fishery animal for $3 billion including live animals, parts and derivatives.[8]
However, the global trade of wildlife commodities is ineffectively monitored and accounted for due to the constraint of the HS Code System used by the customs worldwide. The majority of international imports of wildlife are only recorded in extremely basic and general categories, such as 'plant' or 'animal products', with no further taxonomic detail. It is estimated that near 50% of the global import of plant and 70% of animal products are imported as general categories, with an exception for fisheries (ca. 5%), thanks to various multilateral fishery management agreements that requires taxon-specific fish catch reporting.[8]
Many jurisdictions rely on the declared HS Code of the consignments for detection and prosecution of illegal wildlife import. The lack of specificity of HS Code precludes effective monitoring and traceability of global wildlife trade. There is an increasing call for a reform of the HS Code to strengthen monitoring and enforcement of global wildlife trade.[9][10][11][12][13][14]
Different forms of wildlife trade or use (utilization, hunting, trapping, collection or over-exploitation) are the second major threat to endangered mammals and it also ranks among the first ten threats to birds, amphibians and cycads.[3]
Wildlife trade threatens the local ecosystem, and puts all species under additional pressure at a time when they are facing threats such as over-fishing, pollution, dredging, deforestation and other forms of habitat destruction.[citation needed]
In the food chain, species higher up on the ladder ensure that the species below them do not become too abundant (hence controlling the population of those below them). Animals lower on the ladder are often non-carnivorous (but instead herbivorous) and control the abundance of plant species in a region. Due to the very large amounts of species that are removed from the ecosystem, it is not inconceivable that environmental problems will result, similar to e.g. overfishing, which causes an overabundance of jellyfish.[citation needed]
According to the United Nations, World Health Organization and World Wildlife Foundation, the Coronavirus disease 2019 is linked to the destruction of nature, especially to deforestation, habitat loss in general and wildlife trade. The head of the UN convention on biological diversity stated: "We have seen many diseases emerge over the years, such as Zika, Aids, Sars and Ebola, and they all originated from animal populations under conditions of severe environmental pressures."[15]
Outbreaks of zoonotic diseases including COVID-19, H5N1 avian flu, severe acute respiratory syndrome (SARS), and monkeypox have been traced to live wildlife markets where the potential for zoonotic transmission is greatly increased.[16][17][18][19] Wildlife markets in China have been implicated in the 2002 SARS outbreak and the COVID-19 pandemic.[20][21] It is thought that the market environment provided optimal conditions for the coronaviruses of zoonotic origin that caused both outbreaks to mutate and subsequently spread to humans.[20][21]
In some instances; such as the sale of chameleons from Madagascar, organisms are transported by boat or via the air to consumers. The survival rate of these is extremely poor (only 1% survival rate).[23] This is undoubtedly caused by the illegal nature; vendors rather not risk that the chameleons were to be discovered and so do not ship them in plain view. Due to the very low survival rate, it also means that far higher amounts of organisms (in this case chameleons) are taken away from the ecosystem, to make up for the losses.
Interpol has estimated the extent of the illegal wildlife trade between $10 billion and $20 billion per year. While the trade is a global one, with routes extending to every continent, conservationists say the problem is most acute in Southeast Asia. There, trade linkages to key markets in China, the United States, and the European Union; lax law enforcement; weak border controls; and the perception of high profit and low risk contribute to large-scale commercial wildlife trafficking.[29] The ASEAN Wildlife Enforcement Network (ASEAN-WEN), supported by the U.S. Agency for International Development and external funders, is one response to the region's illegal wildlife trade networks. There is no clear relationship between the legality of wildlife trade and its sustainability; a species can be legally traded to extinction but it is also possible for illegal trade to be sustainable [30]
Notable trade hubs of the wildlife trade include Suvarnabhumi Airport in Bangkok, which offers smugglers direct jet service to Europe, the Middle East, North America and Africa. The Chatuchak weekend market in Bangkok is a known center of illicit wildlife trade, and the sale of lizards, primates, and other endangered species has been widely documented. Trade routes connecting in Southeast Asia link Madagascar to the United States (for the sale of turtles, lemurs, and other primates), Cambodia to Japan (for the sale of slow lorises as pets), and the sale of many species to China.
The trade also includes demand for exotic pets especially birds,[32] and consumption of wildlife for meat. Large volumes of fresh water tortoises and turtles, snakes, pangolins and monitor lizards are consumed as meat in Asia, including in specialty restaurants that feature wildlife as gourmet dining.
Related to the exotic pet trade, captive wildlife are held in sanctuaries which have been involved in illegal wildlife trade. In Thailand the Tiger Temple was closed in 2016 due to being accused of clandestine exchange of tigers.
Morocco has been identified as a transit country for wildlife moving from Africa to Europe due to its porous borders with Spain. Wildlife is present in the markets as photo props, sold for decoration, used in medicinal practices, sold as pets and used to decorate shops. Large numbers of reptiles are sold in the markets, especially spur-thighed tortoises. Although leopards have most likely been extirpated from Morocco, their skins can regularly be seen sold openly as medicinal products or decoration in the markets.[35]
Although the volume of animals traded may be greater in Southeast Asia, animal trading in Latin America is widespread as well.
In Venezuela more than 400 animal species are involved in subsistence hunting, domestic and international (illegal) trade. These activities are widespread and might overlap in many regions, although they are driven by different markets and target different species.[7]
In Brazil, the wildlife trade has grown over the years, as it one of the most biodiverse areas in the world. Mammals and amphibians are among the highest traded animals. In recent studies, non-native species of amphibians and mammals were identified in Brazil, with frogs and rodents, respectively, posing the greatest invasion risks.[36][37] The online trade of amphibians as exotic pets has risen almost six times since 2015.[38]
Through both deep web (password protected, encrypted) and dark web (special portal browsers) markets, participants can trade and transact illegal substances, including wildlife. However the amount of activity is still negligible compared to the amount on the open or surface web. As stated in an examination of search engine key words relating to wildlife trade in an article published by Conservation Biology, "This negligible level of activity related to the illegal trade of wildlife on the dark web relative to the open and increasing trade on the surface web may indicate a lack of successful enforcement against illegal wildlife trade on the surface web."[39]
A study conducted by the International Fund for Animal Welfare (Ifaw) in 2018 revealed online sales of endangered wildlife (on the list of the global Convention on the International Trade in Endangered Species) was pervasive across Europe. Ivory accounted for almost 20 percent of the items offered.[40]
Legal trade of wildlife has occurred for many species for a number of reasons, including commercial trade, pet trade as well as conservation attempts. Whilst most examples of legal trade of wildlife are as a result of large population numbers or pests, there is potential for the use of legal trade to reduce illegal trade threatening many species. Legalizing the trade of species can allow for more regulated harvesting of animals and prevent illegal over-harvesting.[citation needed]
Many environmentalists, scientists, and zoologists around the world are against legalizing pet trade of invasive or introduced species, as their release into the wild, be it intentional or not, could compete with indigenous species, and lead to their endangerment.[citation needed]
Trade of crocodiles in Australia has been largely successful. Saltwater crocodiles (Crocodylus porosus) and freshwater crocodiles (Crocodylus johnstoni) are listed under CITES Appendix II. Commercial harvesting of these crocodiles occurs in Northern Territory, Queensland and Western Australia, including harvesting from wild populations as well as approved captive breeding programs based on quotas set by the Australian government.[41]
Kangaroos are currently legally harvested for commercial trade and export in Australia. There are a number of species included in the trade including:
Harvesting of kangaroos for legal trade does not occur in National Parks and is determined by quotas set by state government departments.  Active kangaroo management has gained a commercial value in the trade of kangaroo meat, hides and other products.[42]
Alligators have been traded commercially in Florida and other American states as part of a management program.[43] The use of legal trade and quotas have allowed management of a species as well as economic incentive for sustaining habitat with greater ecological benefits.
Legalising the trade of products derived from endangered species is highly controversial.[44] Many researchers have proposed that a well regulated legal market could benefit some endangered species by either flooding the market with products that drive down the price of illegal products,[45] decreasing the incentive to illegally harvest, or by providing revenue that could fund the species's conservation.[46] However, laundering and corruption pose a major obstacle to implementing such policies, as illegal harvesters attempt to disguise illegal product as legal when trade is legalized.[47]
Under the Convention on International Trade of Endangered Species of Wild Fauna and Flora (CITES), species listed under Appendix I are threatened with extinction, and commercial trade in wild-caught specimens, or products derived from them, is prohibited.[48] This rule applies to all species threatened with extinction, except in exceptional circumstances.[49] Commercial trade of endangered species listed under Appendix II and III is not prohibited, although Parties must provide non-detriment finding to show that the species in the wild is not being unsustainably harvested for the purpose of trade. Specimens of Appendix I species that were bred in captivity for commercial purposes are treated as Appendix II. An example of this is captive-bred saltwater crocodiles, with some wild populations listed in Appendix I and others in Appendix II.
Many animals are kept for months in markets waiting to be sold. The welfare of animals in trade is almost universally poor, with the vast majority of animals failing to receive even the most basic freedom from pain, hunger, distress, discomfort, and few opportunities to express normal behaviour.[50]
Reptiles specifically endure tight living spaces, torn claws and dehydration during capturing and transportation. Sometimes, they are also crushed from being stacked on top of each other.[51]
The species is found on many Japanese islands, including Honshu, Shikoku, and Kyushu. Their habitats include both natural and artificial bodies of water, as well as forests and grasslands. They breed from spring to the beginning of summer, both sexes producing pheromones when ready to mate. Eggs are laid separately, hatching after about three weeks. They grow from larval to juvenile form in between five and six months. Juveniles eat soil-dwelling prey, and adults eat a wide variety of insects, tadpoles, and the eggs of their own species. They have several adaptations to avoid predators, although which they use depends on where they live. Several aspects of their biology have been studied, including their ability to regrow missing body parts.
The Japanese fire-bellied newt first diverged from its closest relative in the Middle Miocene, before splitting into four distinct varieties, each with a mostly separate range, although all four are formally recognized as composing a single species. Currently, their population is declining, and they face threats from disease and the pet trade. They can be successfully kept in captivity.
The Integrated Taxonomic Information System lists sixteen synonyms for Cynops pyrrhogaster.[9] Common names of the species include Japanese fire-bellied newt,[1] red-bellied newt,[10] and Japanese fire-bellied salamander.[11] Studies examining morphological and geographic variation had formerly recognized six races: Tohoku, Kanto, Atsumi, intermediate, Sasayama, and Hiroshima,[12] one of which, the Sasayama, was described as a subspecies in 1969 by Robert Mertens as Triturus pyrrhogaster sasayamae, which is now considered a synonym of C. pyrrhogaster.[2] Modern molecular analysis supports the division of C. pyrrhogaster into four clades instead.[12] In particular, the validity of the Sasayama and intermediate races has never been proven, with one study finding no behavioral differences between the two supposed forms.[13]
On the newt's upper body, the skin is dark brown, approaching black, and covered in wartlike bumps. The underbelly and the underside of its tail are bright red, with black spots.[4] Younger juveniles have creamy coloration instead of red, although most larger juveniles have some red present.[15] Adults from smaller islands tend to have more red on their ventral (belly) regions than those from larger islands, sometimes with extremely small spots or none at all. In general males tend to have more red than females.[16] Males can also be distinguished from females by their flat, wide tails and swelling around the ventral region.[17] An entirely red variant exists: that coloration is believed to be inherited and recessive. This variant is not confined to any single population, but is more common in the western half of Japan overall.[18]
Of the four clades, the northern is found in the districts of Tohoku and Kanto. This does not overlap with the range of the central clade, which is found in Chubu, northern Kansai, and eastern Chugoku. The central's range has a small amount of overlap with the western, which is found in southern Kinki, western Chugoku, Shikoku, and central Kyushu. The western also has some overlap with the southern clade, which is found in western and southern Kyushu.[12]
Courtship begins when the male approaches the female, sniffing its sides or cloaca. The male then brings its tail to the female and rapidly vibrates it. The female responds by pushing the male's neck with its snout. At this point, the male slowly moves away, undulating its tail, and the female follows, touching the tail with its snout when close enough. The male then deposits two to four spermatophores, one at a time, moving several centimeters after each, which the female attempts to pick up with its cloaca, sometimes unsuccessfully.[24] Females lay eggs separately on underwater objects, such as leaves and submerged grass roots, fertilized one by one from the spermatophores they carry. They can lay up to 40 eggs in one session, and 100 to 400 eggs in a breeding season.[24]
Newts in Mainland Japan have different antipredator behavior than newts on smaller islands. Individuals on smaller islands (for instance, Fukue Island) generally use a maneuver called the unken reflex, where they expose their bright red underbelly to attackers. As their main predators are birds, which are capable of distinguishing the color red, this technique is effective. In Mainland Japan the newts must also avoid mammalian predators, which cannot distinguish colors as well as avian hunters. This leads these populations to use the maneuver less, as it can result in death if attempted.[16]
Against snakes, newts from Fukue Island tend to perform tail-wagging displays, designed to bring a predator's attention to their replaceable tail rather than their more valuable head; those from Nagasaki Prefecture in Mainland Japan tend to simply flee. Snakes are present in both areas. This is likely because those from the mainland are adapted to escape from mammalian hunters, which are less likely to be repelled by such a display.[28]
Wild Japanese fire-bellied newts contain high levels of the neurotoxin tetrodotoxin (TTX).[29] This toxin inhibits the activity of sodium channels in most vertebrates, discouraging predation by both birds and mammals.[28] Experiments have shown the toxin is almost entirely derived from the newt's diet. When raised in captivity with no source of TTX, 36- to 70-week-old juveniles did not contain detectable levels, but wild specimens from the same original habitat had high toxicity. In younger captive-reared newts some TTX was still detected, which was inferred to have been transferred by adult females to their eggs.[29] In a follow-up experiment by the same team captive-reared newts were given food containing the neurotoxin. They readily consumed TTX-laced bloodworms when offered, not showing any symptoms after ingesting the poison. It was detectable in their bodies afterward, further indicating food to be the source of the toxin. No TTX-producing organisms are known from their habitat, but their existence is likely, and would explain the origin of TTX in wild newts.[30]
The International Union for the Conservation of Nature (IUCN) has ranked it as near-threatened. This assessment was made in 2020,[1] a shift from 2004 when it was rated least-concern.[31] It successfully reproduces in Australian zoos.[1] One major threat that C. pyrrhogaster faces is collection for the pet trade. The IUCN states that this trade needs to be ended immediately. Their population is decreasing, particularly near areas of human habitation.[1]
Japanese fire-bellied newts with mysterious skin lesions at Lake Biwa in Japan's Shiga Prefecture were found to be suffering from infections caused by a single-celled eukaryote in the order Dermocystida. The lesions contained cysts, which were filled with spores. Nearly all the lesions were external, although one was found on the liver. Globally, diseases are one of the causes for declining amphibian populations. There is concern that this affliction could spread to other nearby species, including Zhangixalus arboreus and Hynobius vandenburghi.[32]
A variety, believed to be found exclusively on the Atsumi Peninsula, was thought to have become extinct in the 1960s. Then, in 2016, a trio of researchers discovered that newts on the Chita Peninsula were very likely the same variant due to their similar morphological traits. Both groups share a preference for cooler temperature and have smooth and soft bodies, pale dorsal regions, and yellowish undersides. Even if still alive, this form is highly threatened and will soon be wiped out without immediate protection.[33]
Japanese fire-bellied newts serve as a highly useful model organism in laboratory settings, but they become more difficult to care for after metamorphosis. An experiment supported by the Japan Society for the Promotion of Science found that thiourea (TU) can prevent this process from occurring, allowing the animals to stay in their pre-metamorphosis form for as long as two years, while still capable of metamorphosizing when removed from the TU solution. This did not have any impact on their regeneration capabilities.[25]
Japanese fire-bellied newts produce motilin, a peptide that stimulates gastrointestinal contractions, identified in many vertebrates. It is created in the upper small intestine and pancreas. The discovery of the latter was the first time pancreatic motilin had been observed. The organ also produces insulin. These results represented the first discovery of motilin in amphibians, suggesting that it has a similar role for them as it does for birds and mammals. The existence of pancreatic motilin also indicated another, unknown function.[34]
This species, as well as other Urodele amphibians, is capable of regrowing missing body parts, including limbs with functional joints and the lower jaw.[35][36] When this process occurs, the regenerated tissue tends to mirror intact tissue in form.[35] It is also able to regrow missing lenses, taking thirty days to do so as a larva and eighty days as an adult. The difference in time is purely due to the size of the eye, and regenerative ability does not change; the discovery of this fact contradicted a popular claim that juvenile animals are quicker to regenerate than adults.[37]
Doctor of Veterinary Medicine Lianne McLeod described them as "low-maintenance", noting that captive newts enjoy bloodworms, brine shrimp, glass shrimp, Daphnia, and, for larger individuals, guppies.
The Lake Street Transfer station was a rapid transit station on the Chicago "L", serving as a transfer station between its Lake Street Elevated Railroad and the Logan Square branch of its Metropolitan West Side Elevated Railroad. Located where the Logan Square branch crossed over the Lake Street Elevated, it was in service from 1913 to 1951, when it was rendered obsolete by the construction of the Dearborn Street subway.
The transfer station was an amalgamation of two predecessor stations: Wood, on the Lake Street Elevated, was on Wood Street, one block west of the site of the future transfer, and had been constructed in 1893; the Metropolitan's Lake station, on the other hand, was on the site of the future transfer and had been built in 1895. These stations, and their lines, had been constructed by two different companies; when they and two more companies building what would become the "L" merged operations in the early 1910s, a condition for the merger was the construction of a transfer station between the Metropolitan and Lake Street Elevateds at their crossing, which in practice meant the replacement of Wood station with a new Lake Street one under the Metropolitan. Having already merged operations, the "L" companies formally united under the Chicago Rapid Transit Company (CRT) in 1924; the "L" became publicly owned when the Chicago Transit Authority (CTA) assumed operations in 1947.
Lake Street Transfer was double-decked, the Metropolitan's tracks and station located immediately above the Lake Street's tracks and station. Access to the eastbound Lake Street platform was by a station house at the street level; passengers would then use the platform to access the Metropolitan's platforms and Lake Street's westbound platform by additional stairways.
The Metropolitan West Side Elevated Railroad Company, another founding company of the Chicago "L", was granted a fifty-year franchise by the Chicago City Council on April 7, 1892.[11] Unlike the Lake Street Elevated, which operated a single line, the Metropolitan had a main line that proceeded west from downtown to Marshfield Junction, where it split into three branches: one northwestern branch to Logan Square (which in turn had a branch to Humboldt Park[d]), one branch due west to Garfield Park, and one southwestern branch to Douglas Park.[15] While the competing South Side and Lake Street Elevateds used steam traction, the Metropolitan never did; although it had originally intended to, and indeed had built much of its structure under the assumption that locomotives would be used,[16] it decided in May 1894 to have electrified tracks instead,[17] making it upon its opening the first revenue electric elevated railroad in the United States.[18]
The new CTA began experiments to streamline service on the "L"; among them was skip-stop, which began as an experiment on the Lake Street Elevated on April 5, 1948.[2] Stations in between Pulaski and the Loop, exclusive, became either "A" or "B" stations and were serviced by respective "A" or "B" trains during weekdays.[2] Despite being located in this area, Lake Street Transfer was exempt from this system and continued to be serviced by all Lake Street Elevated trains.[2] As part of the same plan to streamline Lake Street service, the Ashland station one block east of the transfer was closed but remained standing.[2] The Logan Square branch would not begin skip-stop until the opening of the Dearborn Street subway and the closing of the transfer in 1951.[20]
The subway's approval did not immediately imply the end of the old Logan Square branch; plans in 1939 included another proposed subway to connect the branch with the Ravenswood branch to the north and through-routing it with the Douglas Park branch to the south into a subway on Ashland Avenue to form a crosstown route.[35] Damen Tower serving the Humboldt Park branch divergence was rebuilt with the expectation that it also would switch trains between the subway and the elevated, much like the State Street subway connects with the earlier elevated North Side main line that remained standing after its construction,[36] and as late as 1949 commuters were promised such a setup that would have preserved the old Logan Square trackage.[37] However, the CTA had no interest in operating either the old Logan Square elevated or the Humboldt Park branch; the new Damen Tower would never be installed with switching equipment, and the Logan Square branch south of Damen would be closed after the Dearborn subway opened.[36]
World War II interrupted the construction of the Dearborn Street subway; although the federal government allowed the continued construction of the State Street subway, it did not do so for the Dearborn Street subway even though it was 82 percent completed by 1942.[20] After the war ended, work resumed on the Dearborn Street subway and it opened at the midnight beginning Sunday, February 25, 1951; at the same time, the Humboldt Park branch was restricted to a shuttle service to and from Damen on the Logan Square branch.[20] Having been rendered obsolete by the subway, the Lake Street Transfer station was closed and the Lake Street's Ashland station reopened.[3] The subway was predicted to reduce the travel time between Logan Square and downtown from 28 minutes to 15.[20] Since construction had not started on the Congress Line, trains in the Dearborn subway stopped at its southern terminus at LaSalle and turned back.[20] Despite its incomplete state, and complaints from riders no longer given a direct trip to the Near West Side,[38][39] the new subway had over sixty percent higher ridership than the old Logan Square branch by the end of the year.[40] The old Logan Square branch trackage south of its entrance to the subway became known as the Paulina Connector, connecting the branch with the rest of the "L" system.[41]
Construction on the Congress Line began in 1954, leaving the Douglas branch with the issue of how to connect with the Loop in the meantime.[42] The Paulina Connector south of Washington Boulevard (a block south of Lake Street) was reopened for the purpose,[42] but the Metropolitan's old tracks north of Washington were replaced in revenue service by a direct connection to the Lake Street's trackage known as Washington Junction, located adjacent to the abandoned station.[41] This junction contained an automatic interlocking mechanism, where Douglas Park trains carried an electric coil to switch them to the Connector that Lake Street trains lacked.[43] This connection was used until the Congress Line was completed in 1958, after which the Douglas branch connected directly with it to use the Dearborn Street subway to go downtown, creating the "West-Northwest Route" that was renamed the Blue Line in 1992.[44][45]
Before 1913 the Wood and Lake stations had two wooden side platforms each. The Wood station had two station houses, one on each platform, designed in a "gingerbread" Queen Anne style, similar to the other stations on the route and the surviving station houses at Ashland.[48] The station houses were heated by potbelly stoves, and while earlier plans had called for their ticket agent's booths to be placed on the sides of the station houses facing the street, they ended up being placed in alcoves adjacent to the platforms.[48] The construction of the Lake Street Elevated's stations was contracted to Frank L. Underwood of Kansas City and Willard R. Green of New York.[49] The Metropolitan's Lake station, which continued as its portion of the Lake Street Transfer, also had two wooden side platforms, but a station house located at street level on the north side of Lake Street. The station house, made of red pressed brick and white limestone trim, was designed similarly to other stations on the Logan Square branch, surviving examples of which are at California and Damen, with a corniced and dentiled front bay containing dual doors specifically marked "Entrance" and "Exit" and prolific use of terra cotta. Its wooden platforms had hipped roof tin canopies in the center and decorative cast-iron railings with diamond designs.[1][50] Unlike elsewhere on the "L", station houses on the Metropolitan had central heating and a basement.[51]
After the transfer was completed in 1913 the C&OP built new platforms; these platforms projected westward from the Metropolitan, with their eastern halves covered by arched canopies with lattice framing and their western halves open. Auxiliary exits onto Hermitage Avenue were located on the middle of the Lake Street platforms at the western ends of their canopies. On the Metropolitan's end, its platforms and canopies were extended southward to meet the southern Lake Street platform, and a new station house on the south side of Lake Street was constructed sometime before 1917, after which the original station house was used for storage. The final station was double-decked, with the Metropolitan's original two side platforms being augmented by the Lake Street Elevated's lower two side platforms. Access to the station was through stairwells from the station house to the Lake Street platforms, which had additional stairways to connect to the Metropolitan platforms; each Lake Street platform was connected to each Metropolitan platform, leading to four inter-platform stairwells in total. The station house presumably had direct access only to the southern eastbound Lake Street platform, with patrons wishing to access the Lake Street's northern westbound platform having to walk up to the Metropolitan platforms and walk down again.[1]
Throughout the stations' existence, the Lake Street[f] and Metropolitan Elevateds[15] had two tracks each in the vicinity, meaning that the transfer station had four tracks overall. Having had trouble constructing its trackage with two different companies and assembling much of its own infrastructure, the Lake Street Elevated contracted with Underwood and Green to construct its stations and the tracks west of Ashland.[53] The Metropolitan's tracks were constructed by the West Side Construction Company, a company with the same officers as the Metropolitan itself and the chief engineer of E. W. Elliot, with steel and iron from the Carnegie Steel Company.[15] Like the rest of the station, the tracks were double-decked in relation with one another.[1]
Prior to the construction of the transfer, the Metropolitan's Lake station had a ridership that hovered around 250,000 a year, peaking at 296,116 in 1905.[63] The Lake Street's Wood station had a similar ridership, but one which peaked at 441,045 in 1905.[64] Once the transfer was in place, the two lines' contributions to station ridership were roughly equal, with the Lake Street edging out the Metropolitan each year.[65]
His fortunes declined when the cultural life of Kharkiv was affected by decrees issued by Tsar Paul I of Russia. Lacking a patron, and with his music unable to be performed, he returned  home to Kyiv in 1798, and became a novice monk of the Kyiv Pechersk Lavra. The monastery's authorities discovered handwritten threats towards the Russian royal family, and accused Vedel of writing them. He was subsequently incarcerated as a mental patient, and forbidden to compose. After almost a decade, the authorities allowed him to return to his father's house to die.
Vedel's music was censored during the period that Ukraine was part of the Soviet Union. More than 80 of his works are known, including 31 choral concertos, but many of his compositions are lost. Most of his choral music uses texts taken from the Psalms. The style of Vedel's compositions reflects the changes taking place in classical music during his lifetime; he was influenced by Ukrainian Baroque traditions, but also by new Western European operatic and instrumental styles.
The character of Russian and Ukrainian worship derives from performances of the znamenny chant, which developed a tradition that was characterised by seamless melodies and a capacity to sustain pitch. The tradition reached its culmination during the 16th and 17th centuries, having taken on its own character in the Russian Empire some three centuries earlier.[2]
The tradition of Russian church music can be traced back to Dmitry Bortniansky, who revered the Russian liturgical musical tradition. The early part of the 19th century was a period that marked a low ebb in the fortunes of traditional Russian music. Bortniansky studied in Venice before eventually becoming the director of music at the court chapel in St Petersburg in 1801. Composing in an era when attempts were being made to suppress the Russian Empire's cultural heritage, Bortniansky's choral concertos, set to texts in Russian, were modelled on counterpoint, the concerto grosso and Italian instrumental music. Under him, the Imperial Court Chapel expanded its role so it influenced, and eventually controlled, church choral singing throughout the Russian Empire.[2][4] Vedel followed Bortniansky in combining the Italian Baroque style to ancient Russian hymnody,[2] at a time when classical influences were being introduced into Ukrainian choral music, such as four-voice polyphony, the soloist and the choir singing at different alternative times, and the employment of three or four sections in a work.[1]
The Vedelsky family adhered strictly to the Orthodox faith.[5] Lukyan Vlasovich Vedelsky  was a wealthy carver of wooden iconostases, who owned his own workshop. The name Vedel, probably an abbreviated form of Vedelsky, was how the composer signed his letters, and named himself in military documents. His father signed himself "Kyiv citizen Lukyan Vedelsky".[9][note 4]
Vedel was a boy chorister in the Eparchial (bishop's) choir in Kyiv.[14] He studied at the Kyiv-Mohyla Academy, where his teachers included the Italian Giuseppe Sarti,[15] who spent 18 years as an operatic composer in the Russian Empire.[16] By the end of the 18th century, most of the students attending the Kyiv-Mohyla Academy were preparing for the priesthood. It was at that time the oldest and most influential higher education institution in the Russian Empire; most of the country's leading academics were originally graduates of the academy.[17]
Vedel's talent was recognised by other musicians in Moscow.   He probably continued his musical studies at the university.[16] During this period, he had the opportunity to become more familiar with Russian and Western European musical cultures.[9] He did not stay in Moscow for long and, resigning his position, he returned home to Kyiv in the early 1790s.[16]
On 13 March 1796, Levanidov was appointed as Governor General of the Kharkiv Governorate.[20] The composer moved to Kharkiv, along with his best musicians. In Kharkov (now Kharkiv, Ukraine) Vedel organised a new gubernia (governorate) choir and orchestra, and taught singing and music at the Kharkiv Collegium,[8] which was second only to the Kyiv-Mohyla Academy in terms of its curriculum.[18] The music class at the Kharkiv Collegium was first recorded in 1798, when in January that year two canons and a choral concerto by Vedel were performed.[21]
Vedel did much of his composing during this period.[8] Works included the concerts "Resurrect God" and "Hear the Lord my voice" (dated 6 October 1796) and the two-choir concerto "The Lord passes me". The composer and his works were highly valued in Kharkiv; his concerts were studied and performed at the Kharkiv Collegium, and they were sung in churches. Bortniansky, who conducted the St. Petersburg State Academic Capella, praised the quality of Vedel's teaching. In September 1796, Vedel was promoted to become a senior adjutant, with the rank of captain.[9][19]
The tsar's decrees caused the cultural and artistic life of Kharkiv to decline. The city's theatre was closed, and its choirs and orchestras were dissolved. Performances of Vedel's works in churches were banned,[9] as the tsar had prohibited singing in churches of any form of music except during the Divine Liturgy.[24]
Early in 1799, frustrated by the lack of opportunities to compose and teach and possibly suffering from a form of mental illness, Vedel enrolled as a novice monk at the Kyiv Pechersk Lavra.[9][24] He was an active member of the community and was respected by the monks for his asceticism.[24]
According to Turcaninov's biography, the Metropolitan of Kyiv commissioned Vedel to write a song of praise in honour of a royal visit to Kyiv, but Vedel instead wrote a letter to the tsar, probably of a political nature. Vedel was arrested in Okhtyrka, pronounced insane, and returned to Kyiv.[24]
Vedel returned to live with his father in an attempt to regain his mental health. Back home in Kyiv, he was able to compose, read, and play the violin, and he may have returned to teach at the Kyiv Academy.[25] By leaving the monastery before his training was completed, Vedel may have angered Hierotheus, the Metropolitan bishop. When the monastery authorities discovered a book containing handwritten insults about the royal family, the Metropolitan accused Vedel of writing in the book. He dismissed Vedel's servants, and personally detained him. On 25 May 1799, Hierotheus declared that Vedel was mentally ill.[9]
After the death of Paul I in 1801, the new tsar, Alexander I, proclaimed an amnesty for unjustly imprisoned convicts, and many prisoners were released.[25] Alexander ordered that Vedel's case should be re-examined, but Vedel was again declared insane and remained an inmate.[10] The tsar wrote of Vedel on 15 May 1802: "... leave in the present captivity".[14]
In 1808, after nine years' imprisonment, and by now mortally ill, Vedel was allowed to return home to his father's house in Kyiv. Shortly before his death there on 14 July 1808, he is said to have stood and prayed in the garden.[10]
Vedel was almost entirely a liturgical composer of the a cappella choral music sung in Orthodox churches.[8][note 5] As of 2011[update], more than 80 of his compositions have been identified, including 31 choral concertos and six trios, two liturgies, an all-night vigil,[8][9] and three irmos cycles.[29]
An edition of Vedel's works was published by Mykola Hodbych and Tetiana Husarchuk in 2007.[8]
Many of Vedel's works have been lost.[14] The V.I. Vernadsky National Library of Ukraine holds the only existing autograph score by the composer, the Score of Divine Liturgy of Saint John Chrysostom and Other Compositions. The score consists of 12 choral concertos (composed between 1794 and 1798),[14] and the Liturgy of Saint John Chrysostom. The ink varies in colour, which suggests that Vedel worked on the compositions at different times.[30] It was acquired by Askochensky, who bequeathed it to the Kyiv Academy.[14][31]
The musicologists Ihor Sonevytsky and Marko Robert Stech consider Vedel to be the archetypal composer of Ukrainian music from the Baroque era.[8] An outstanding tenor singer, he was one of the best choral conductors of his time. He helped to raise the standard of choral singing in Ukraine to previously unknown levels.[1]
Vedel was considered during his lifetime to be a traditional and conservative composer, in contrast to his older contemporaries Berezovsky and Bortniansky. Unlike Vedel, they composed secular, non-spiritual works. He was a famous violinist, but no music by Vedel for the violin is documented. His works, perhaps even more than those of Berezovsky or Bortnyansky, represented a development in Ukrainian musical culture.[6] According to Koshetz, Vedel's music was based on Ukrainian folk melodies.[29]
Vedel's music was written at a time when Western music had largely emerged from the Renaissance and Baroque eras. The style of his compositions reflected two contrasting traditions. He was strongly influenced by the baroque traditions of the Ukrainian hetman culture, with its religious-mystical music linked with ideas about spiritual enlightenment, but was also influenced by developments in new operatic and instrumental styles emerging from Western Europe at that time.[32]
Performances of Vedel's music were censored and the publication of his scores was prohibited during most of the 19th century. Distributed in secret in manuscript form, they were however known and performed, despite the ban.[9] Hand-written variations of Vedel's music appeared,[7] as conductors amended the scores to make them more suitable for unauthorised performances. Tempi were changed and modal textures, the level of complexity of the music, and the formal structure were all altered.[33] The hand copying of Vedel's music led to the creation of versions that were notably different from his original scores.[25]
Vedel's compositions were rediscovered during the early 20th century by the conductor and composer Alexander Koshetz, at that time the leader of the Kyiv-Mohyla Academy's student choir, and himself a student.[14] They were first published in 1902.[34] Koshetz, one of the earliest conductors from Ukraine to attempt to revive performances of Vedel using the autograph scores, noted that "the great technical difficulties of solo parts... and the need for large choruses" made his works difficult to perform in public.[33] Koshetz toured Europe and America, conducting the Ukrainian Republican Chapel in performances of Vedel.[6]
Koshetz's revival of Vedel's music was banned by the Soviets after Ukraine was absorbed into the Soviet Union in 1922.[14] Unlike many of the sacred works written by Western composers, Orthodox sacred music is sung in the vernacular, and its religious nature is visible and tangible to Orthodox Christians. Because of this, Soviet anti-religious legislation prohibited Russian and Ukrainian sacred music from being performed in public from 1928 until well into the 1950s, when the Khrushchev Thaw occurred, and Vedel's works were once again heard by Soviet audiences.[35]
Vedel, Berezovsky and Bortniansky are recognised by modern scholars as the "Golden Three" composers of Ukrainian classical music during the end of the 18th century,[36] and the outstanding composers at a time when church music was reaching its peak in eastern Europe.[5] They composed some of the greatest choral music to emerge from the Russian Empire.[18]
Vedel made an important contribution in the music history of Ukraine,[37][38] and musicologists consider him to the archetypal composer of the baroque style in Ukrainian music.[8] Koshetz stated that Vedel should be seen as "the first and greatest spokesperson of the national substance in Ukrainian church music".[29] The musical culture that developed in Ukraine during the 19th century was founded in part on Vedel's choral compositions. According to the ethnomusicologist Taras Filenko, "His free command of contemporary techniques of choral writing, combined with innovations in adapting the particularities of Ukrainian melody, make Artem Vedel's works a unique phenomenon in the context of world musical culture."[39] According to Chekan, Vedel's texture is "at times monumental and at others subtly contrasted, strikingly showing the possibilities of the a cappella sound".[1]
The Germans attempted to delay the Allied advance until the onset of bad weather by denying access to ports and demolishing communications infrastructure in order to give their own forces time to recover. Between September and November, the American forces in Europe suffered from severe transportation problems. In September, Cherbourg was the only deep-water port in northwest Europe in Allied hands capable of handling Liberty ships, but it had been badly damaged, and took a long time to restore. Smaller ports could handle only small, shallow-draft coastal trading vessels known as "coasters". Two-thirds of the British coaster fleet, on which critical industries depended, was dedicated to the campaign. Over time, rough seas, enemy action and continuous use laid up a quarter of the coaster fleet for repairs. From September onwards, an increasing volume of supplies came directly from the United States. These were stowed in Liberty ships so as to make optimal use of their cargo space. The shipments frequently included heavy and bulky items that required dockside cranes to unload. The available port capacity was insufficient to unload the ships arriving. As the number of ships awaiting discharge in European waters climbed, turnaround times increased, and fewer ships reported back to port in the United States, precipitating a widespread shipping crisis.
In the first seven weeks after the commencement of Operation Overlord, the Allied invasion of Normandy on D-Day (6 June 1944), determined German opposition exploited the defensive value of the Normandy bocage country against American forces. At first, the Allied advance was slower than the Operation Overlord plan had anticipated.[1] The American Operation Cobra, which commenced on 25 July, effected a turnaround in the operational situation by achieving a breakout from the Normandy lodgment area.[2] The Germans were outmaneuvered and driven into a chaotic retreat.[3] The 12th Army Group became active on 1 August, under the command of Lieutenant General Omar N. Bradley. It initially consisted of the First Army, commanded by Lieutenant General Courtney Hodges, and the Third Army, under Lieutenant General George S. Patton Jr.[4] The Ninth Army, under Lieutenant General William H. Simpson, joined the 12th Army Group on 5 September.[5]
British General Sir Bernard Montgomery, the commander of the British 21st Army Group, remained in command of all ground forces, British and American,[4] until 1 September, when the Supreme Allied Commander, General Dwight D. Eisenhower, opened his Supreme Headquarters, Allied Expeditionary Force (SHAEF) in France, and assumed direct command of the ground forces.[5] This brought not just the 12th and 21st Army Groups under Eisenhower's direct command, but also Lieutenant General John C. H. Lee's Communications Zone (COMZ), which became operational on the continent on 7 August.[6]
Bradley had previously exercised control over the Advance Section (ADSEC) of COMZ as the senior American commander on the continent. As such, he had prescribed stock levels in the depots and priorities for the delivery of supplies, and apportioned service units between the armies and the Communications Zone. Bradley believed that as the senior operational commander he should exercise such authority, as was the case in the British forces.[7] Under the American organization, COMZ headquarters also functioned as the headquarters of the European Theater of Operations, United States Army (ETOUSA).[6]
The Allied commanders pushed their forces to the limits logistically to take advantage of the chaotic German retreat.[3] By 3 August Patton's Third Army was advancing into Brittany. Bradley ordered Patton to turn east, leaving only minimal forces behind.[8] This decision entailed grave risk, for under the Operation Overlord plan, the ports of Lorient and Quiberon Bay were to be developed for the logistical support of the American forces under the codename Operation Chastity.[9][10] This was the first in a series of critical decisions that subordinated logistical considerations to short-term operational advantage.[8]
In mid-August, Eisenhower decided to continue the pursuit of the retreating German forces beyond the Seine. This stretched the logistical system.[11][12] Between 25 August and 12 September the Allied armies advanced from the D plus 90 phase line, the position the Operation Overlord plan expected to be reached 90 days after D-Day, to the D plus 350 phase line, moving through 260 phase lines in just 19 days. Although the planners had estimated that no more than twelve divisions could be maintained beyond the Seine, by September sixteen were, albeit on reduced scales of rations and supplies.[13] Logistical forecasts were repeatedly shown to be overly pessimistic, imbuing a sense of confidence that difficulties could be overcome.[14]
Failure to follow proper procedures contributed to the waste and disorder. Dumps established by the armies were frequently turned over to COMZ with little or no paperwork, so supplies were unrecorded, unidentified and unlocatable, resulting in duplicate requisitions. This was exacerbated by the dispatch of filler cargoes of unwanted goods shipped solely to make maximum use of the available transport. The indenting system itself was imperfect and slow in responding to urgent demands. Logisticians at all levels strove to improvise, adapt and overcome difficulties, with considerable success, but short-term solutions frequently created longer-term problems. Hoarding, bartering, over-requisitioning, and cannibalizing vehicles for spare parts degraded the effectiveness of the supply system.[23]
The German strategy was to conduct a fighting withdrawal to the Siegfried Line (which they called the Westwall) while holding the ports as long as possible and conducting a scorched earth program to deny or destroy as much of the transportation infrastructure as possible. The hope was that these measures would restrict the Allies' operational capabilities, which relied heavily on logistical support, and thereby gain sufficient time to reconstitute the German forces. If six to eight weeks could be gained, then bad autumn weather would set in, further restricting the Allies' mobility, air operations and logistical support, and the German forces might be able to take the offensive again.[24]
The Overlord planners had identified a need for shallow-draft vessels, and the Combined Chiefs of Staff had allocated 625,000 deadweight tons (635,000 deadweight tonnes) of coasters for the first six weeks of the operation. This represented about two-thirds of the British coaster fleet, on which critical industries depended for the transport of iron, coal and other commodities. The allocation of so much coastal shipping to Overlord entailed temporarily shutting down a quarter of the UK's blast furnaces. The British therefore wanted the shipping returned as soon as possible. It was planned that after the first six weeks, the allocation to Overlord would be reduced to 250,000 deadweight tons (254,000 deadweight tonnes).[32]
Under the circumstances, the coasters could not be released. Some 560,000 deadweight tons (570,000 deadweight tonnes) of coasters were still engaged in the cross-Channel run in September, and this actually rose to over 600,000 deadweight tons (610,000 deadweight tonnes) in November. The crux of the problem was slow turnaround times. A survey of coaster movements in late October and early November found that 63 round trip voyages had required 1,422 ship-days instead of the planned 606. Diversions to alternate ports, increasingly bad weather, and vessels being laid up for repairs all contributed to this situation. Between 20 and 25 percent of the coaster fleet was laid up for repairs in November and December. Not until December, after the opening of the port of Antwerp, was it possible to release 50,000 deadweight tons (51,000 deadweight tonnes) of coasters, but that was from support of the 21st Army Group; the American allocation actually increased.[33]
It was planned that an increasing volume of supplies would come directly from the United States from September onwards. These ships were not combat loaded, but stowed so as to make optimal use of cargo space. Whereas vehicles had been brought across from the UK on motor transport (MT) vessels (ships specially outfitted to carry vehicles), landing ships, tank (LSTs) or landing craft, tank (LCTs), they now arrived in crates and boxes, with some assembly required. Nearly every ship would arrive with boxed vehicles or other bulky or heavy items. This kind of awkward cargo needed to be discharged at ports where large shore cranes were available; to discharge them over the beaches or at minor ports was difficult, although not impossible.[34] But the only major port in Allied hands on 25 August was Cherbourg.[35] An alternate was to discharge in the UK, assemble them there and transfer the vehicles to the continent in MT ships. SHAEF was allocated 258 MT ships in July, but this was cut to 62 in August, which was still 22 more than originally allocated.[36]
The shipping crisis in the ETO escalated into a global one. Merchant ship construction was lagging behind schedule, mainly due to a deficit of 35,000 skilled workers in the shipyards, as they were being lured away to work on the higher-priority amphibious cargo ships and Boeing B-29 Superfortress programs, where pay and conditions were better. The Allied merchant fleet was still growing at a rate of 500,000 deadweight tons (510,000 deadweight tonnes) per month, but the number of ships available for loading at US ports was shrinking. The problem was growing retentions of vessels by the theaters, of which the ETO was the worst but not the only offender.[42] The chairman of the Maritime Commission, Vice Admiral Emory S. Land, noted that 350 ships were being held idle in the theaters awaiting discharge, and 400 more WSA vessels were being retained for various uses by theater commanders.[43] This represented 7,000,000 deadweight tons (7,100,000 deadweight tonnes) of shipping, which was about 30 percent of all Allied-controlled tonnage.[44] As ships failed to return from overseas on time, supplies began piling up in ports, depots and railway sidings in the United States.[45]
Drastic measures were required. On 18 November the Joint Chiefs of Staff (JCS) approved and forwarded to President Franklin Roosevelt a memorandum from Somervell recommending cuts in non-military shipments.[46] Specifically, Somervell proposed eliminating the American contribution to the UK Import Program of 40 sailings per month, reducing Lend-Lease to the UK by 12 sailings per month, those to the USSR by 10, and cutting civilian relief to Europe by 34.[47] Conway enlisted Harry Hopkins, Roosevelt's chief advisor, in putting the WSA's case to the president: that it could not ask the British "to bear the brunt of our failure to utilize our ships properly."[48] Roosevelt instructed the WSA to negotiate a cut in the UK Import Program for December 1944, January 1945 and February 1945 with the British, asked the Office of War Mobilization and Reconversion to investigate the labor situation at the shipyards, and told the JCS to get the theaters to break up the pools of idle shipping and improve turnaround times.[49]
The port of Cherbourg was operated by the 4th Port, under the command of Colonel Cleland C. Sibley, which was augmented by Colonel August H. Schroeder's 12th Port.[66] From 16 August, the 4th Port was part of Colonel Theodore Wyman Jr.'s Normandy Base Section.[67] The operation of a major port requires a great deal of coordination, and much of this was worked out through trial and error. Bringing vessels into the port was a US Navy responsibility, but the Naval harbor master would take the Army's preferences into account in deciding what berth should be used. For example, the Army preferred vessels with cargo suitable for unloading by DUKW anchor in the Petit Rade to avoid long hauls from the Grande Rade. But communication between the office of the Naval harbor master and the headquarters of the 4th Port was unsatisfactory at first, and it took time to develop a smooth working relationship.[68]
Problems arose when ships arrived unexpectedly and there were no preparations to receive them. Advance warning of ship arrivals was necessary because unloading a ship was a complicated business. Sufficient stevedores had to be provided to work the hatches, and the required cargo handling equipment, such as cranes, had to be available. Trucks and railway cars had to be brought forward and spotted on the quays to allow cargo to be quickly cleared, and the depots and dumps had to be alerted to be ready to receive the supplies. Some arrived without manifests or stowage plans. In some cases the only way the contents of a ship could be determined was for the port personnel to board it and physically check. At first the Navy refused to allow ships without manifests to enter the port at all, but too often they were found to contain critical cargo.[68]
The 4th Port was handicapped by the slow arrival of its unit equipment, which had been brought from the UK on twelve ships but unloaded at Utah Beach instead of Cherbourg. The hatch crew found themselves lacking basic gear such as ropes, slings and cargo nets, and three DUKWs went around collecting gear from ships in the harbor. Cranes were also in short supply, and this was exacerbated by a dearth of well-trained crane operators. In the UK, where the 4th Port had worked the docks along the River Mersey, crane operating had been mostly carried out by experienced civilians. A training program was initiated at Cherbourg, where crane operators were instructed by a pair of sergeants who had learned crane operation in the UK. In the meantime, inexperienced operators caused avoidable damage both to cranes and to cargo. Insufficient numbers of skilled mechanics were available to repair the cranes, and there was a shortage of spare parts as well. The result was that at times half the cranes were out of action.[68]
Whenever there were a few days running of good weather, cargo built up at the port. SHAEF was dissatisfied with how the port was being run, and on 30 October Wyman was relieved of command of the Normandy Base Section and replaced by Major General Lucius D. Clay, on loan from ASF. Clay recognized that the heart of the problem was a lack of coordination between the port and rail operations, and he delegated the necessary authority over the railways to Crothers. This brought about an improvement in clearance tonnage in November. Clay remained for only a few weeks. On 26 November Colonel Eugene M. Caffey assumed temporary command of the Normandy Base Section until Major General Henry S. Aurand arrived on 17 December.[71] Unloading at Cherbourg reached its peak in November. With the opening of ports further north it declined in importance, although it remained an important port for the discharge of ammunition.[69]
Le Havre was operated by the 16th Major Port, under the command of Brigadier General William M. Hoge, who had commanded the Provisional Engineer Special Brigade Group on Omaha Beach. He was succeeded by Colonel Thomas J. Weed on 31 October.[54] It was joined by the 52nd Port in January.[78] The number of berths that the logisticians felt was necessary were never developed, so the port continued to depend on DUKWs and lighters.[55] In the first quarter of 1945, seven DUKW companies handled 35.2 percent of the cargo; quayside discharges accounted for 23.3 percent and lighters handled the rest.[59]
In addition to handling cargo Le Havre also became an important debarkation point for American troops. In January 1945 the 52nd Port was attached to the 16th Major Port and its commander, Colonel William J. Deyo, was given responsibility for troop movements. That month the Red Horse Staging Area was established in the Le Havre area. Troops debarked over a long steel ponton pier and a troopship berth at the Quai d'Escale. Arrivals peaked at 247,607 troops in March 1945.[59]
Both COMZ and the 12th Army Group urged SHAEF to allocate a portion of Antwerp's capacity to the support of the American forces. On considering the matter SHAEF concluded that Antwerp was indeed sufficiently large to serve the needs of both the British and the American forces.[82] Eisenhower rejected a proposal from Lee that it be operated jointly, as joint control of ports had been shown to not work well in the past; the port was to be run by the British.[88] Lee arranged a conference in Antwerp with representatives of COMZ and the 21st Army Group between 24 and 26 September. Tentative agreements were reached on the allocation of tonnage capacity, storage facilities and railway lines, and arrangements were made for the command and control of the port and its installations, and regarding responsibility for rehabilitation works.[82] Additional matters were discussed at a second conference in Brussels on 5 October, and the result was formalized in a Memorandum of Agreement that became known as the "Treaty of Antwerp", and was signed by Major General Miles Graham, the Major General Administration at 21st Army Group, and Colonel Fenton S. Jacobs, the commander of the Channel Base Section.[88][89]
The rehabilitation of the port was undertaken by the British, on the understanding that American resources could be called upon as required. The main US engineer unit assigned was the 358th Engineer General Service Regiment. Two of the ETO's five engineer port repair ships were also available. American engineers cleared rubble away from the quays, improved roads, repaired rail lines, rebuilt warehouses and constructed hardstands. The main American project was the repair of mine damage to the Kruisschans sluice, the longest of the four that connected the river with the wet basins, and the only one that provided access to the American wet basins. Work commenced on 6 November and was completed in time for the arrival of the first Liberty ship, SS James B. Weaver, on 28 November. By this time 219 of the 242 berths were cleared, all the port's cranes were in working order, and all the bridges needed to access the quays had been repaired.[89][93] The port's floating equipment was augmented by a small fleet of American harbor craft that included 17 small tugboats, 20 towboats and 6 floating cranes.[94]
The 13th Port was assigned to operate the American part of Antwerp, and began arriving from Plymouth in October 1944. It was joined by the 5th Major Port in November and December. The commander of the 13th Port, Colonel Doswell Gullatt, who had led the 5th Engineer Special Brigade at Omaha Beach on D-Day, was designated the overall commander. He had also been the district engineer in Mobile, Alabama, and had extensive experience in flood control and the construction of piers and docks.[95] Most of the work of unloading ships was carried out by Belgian stevedores, some 9,000 of whom were working on the American section of the port on an average day in December.[96] American personnel operated in a supervisory capacity. The biggest problem with the Belgian workers was transporting them to and from their homes, as German V-weapon and air attacks on the port forced many of them to move to the outskirts of the city.[94] The city suffered heavy damage from these attacks, and there were more than 10,000 casualties. The workers' living conditions deteriorated during the winter, and on 16 January they went on strike over shortages of food, clothing and coal. The strike lasted only one day, and it ended when the burgomaster promised the workers food and coal at regulated prices.[96] To keep the civilian workers at their jobs, the city of Antwerp granted workers in the port area a 25 percent increase in pay, but since the V-weapons were fairly inaccurate, this soon led to more industrial disputes when workers elsewhere demanded the same.[94]
COMZ attempted to obtain additional supplies of tires by restoring the local French and Belgian tire industries. This was hampered by a shortage of raw materials, which had to be imported, and the transportation systems and electricity grids, which had to be restored. The first tire made from American synthetic rubber was produced by the Goodrich factory in Paris on 4 January 1945, and by the end of the month it was turning out 4,000 tires per month and the Michelin plant was making 2,000. Meanwhile, since inspections had shown that 40 percent of tire losses were due to preventable causes such as underinflation and overloading, the theater launched a media campaign in the Stars and Stripes newspaper and on the Armed Forces Network radio. The War Department removed the tires from unserviceable vehicles in the United States, and brokered a moratorium on industrial disputes in the synthetic rubber industry to increase production.[112]
While motor transport was flexible, it lacked the capacity of rail transport to move large tonnages over long distances. It was upon the railways that the movement of supplies from the ports to the depots ultimately depended. The railway system in northern France was operated by the 2nd Military Railway Service,[113] under the command of Brigadier General Clarence L. Burpee, an Atlantic Coast Line Railroad executive who had commanded the 703rd Railway Grand Division in the North African campaign and the military railways in the Italian campaign.[114]
It was not intended that the railway network would be operated entirely by Allied service personnel for an extended period of time. The plan was that the lines would be progressively handed over to civilian operators once the fighting had passed through and the network was restored. The rapid Allied advance in August and September disrupted this plan, and resulted in the military personnel being spread far more widely and thinly than anticipated. This was complicated by the fact that while the railway operating battalions were formed around cadres drawn from American railroads, not all of their personnel had operating or supervisory experience. Difficulties also arose in dealing with the French civilian operators through differences of language, documentation and operating procedures.[119]
The most important limiting factor affecting the railways was the availability of rolling stock. The Operation Overlord planners had anticipated that much of the French rolling stock would be destroyed by Allied bombing, and that the retreating Germans would take what they could with them. This proved prescient: only fifty serviceable French locomotives were captured southwest of the Seine. It was estimated that the Allied forces would require 2,724 of the large 2-8-0 and 680 of the smaller 0-6-0 type, of which 1,800 and 470 respectively would be for American use, but only 1,358 2-8-0s and 362 0-6-0s were on hand by the end of June 1944.[119]
Due to cutbacks in American locomotive production, the War Department was unable to guarantee delivery of the remaining 2,000 locomotives, and suggested that ETOUSA obtain 450 locomotives that had been loaned to the British. The British refused to hand them over unless the Americans released their coasters. In December, 100,000 deadweight tons (100,000 deadweight tonnes) of coasters was released, and the British agreed to ship 150 locomotives in December, followed by 100 per month thereafter. By the end of the year 1,500 locomotives had been shipped to the continent, and 800 captured French, German and Italian ones had been repaired by French and American mechanics and restored to service.[119]
More than 57,000 railroad cars of various types, including boxcars, flatcars, refrigerator cars and tank cars were shipped to the continent. Of these, some 20,000 had been shipped from the US in the form of knock-down kits and had been assembled in the UK. These were augmented by captured rolling stock.[119] Nonetheless, serious shortages of rolling stock developed in November. Part of the problem was that the armies liked to keep a certain amount of supplies on wheels, using railroad cars as mobile warehouses, but a major factor was a decision to ship bulk supplies directly from the ports to the ADSEC and army depots to reduce port congestion. These depots were primarily points for issuing supplies, with limited unloading and storage capacity, and did not have sufficient resources to classify and segregate bulk supplies. By the end of November there were 11,000 loaded freight cars on the rails northeast of Paris, and ten days later this had increased to 14,000, but the depots could unload only about 2,000 cars per day. This resulted in increased railroad car turnaround times, with many cars taking 20 to 40 days to be unloaded.[120]
The Oise was restored to supply coal to Paris from the coalfields around Valenciennes. Its rehabilitation was undertaken by the 1057th Port Construction and Repair Group, which repaired several locks and removed 34 obstructions, mostly demolished bridges. The Seine was rehabilitated to bring civil relief supplies to Paris from Rouen and Le Havre. It too was obstructed by sunken bridges and damaged locks. The biggest task was the repair of the locks of the Tancarville Canal, which had been built to allow barges from Le Havre to reach the Seine without having to negotiate the Seine estuary, where there were strong tidal currents. The 1055th Port Construction and Repair Group finished this work in March 1945.[125]
Barge traffic on the Seine was obstructed by a pair of ponton bridges. They had to be opened to allow barges through, but this interrupted motor traffic. At Le Manoir there was a railway bridge built by the British that was too low to permit barge traffic. The bridge was raised in October, but then the river rose in November as the Seine flooded, reducing clearance below the minimum again. The river rose so high it was feared that the bridge would be washed away, and consideration was given to routing the trains hauling British supplies from Normandy via Paris instead. On 25 December the bridge was struck by a tugboat and put out of commission. It was then removed.[126]
Although logistical difficulties constituted a brake on combat operations, they were not the only factors that brought the Allied advance to a halt. The American forces also had to contend with rugged terrain, worsening weather and, above all, stiffening German resistance. American forces were widely dispersed and, with the logistical situation preying on his mind, a cautious Hodges ordered his corps commanders to halt when they encountered strong resistance.[130][131] Patton's intelligence officer, Colonel Oscar W. Koch, warned that the German Army had been defeated, but not routed, and was not on the brink of collapse. He forecast that fierce resistance and a last-ditch struggle could be expected.[132]
As American forces confronted the defenses of the Siegfried Line, priority shifted from fuel to ammunition.[133] The armies made little progress during the fighting in September and October. Although the logistical situation improved even before the opening of Antwerp, the effort to reach the Rhine in November was probably premature. Worsening weather and stubborn German resistance impeded the American advance as much as any logistical difficulties. The German recovery was sufficient to mount the Ardennes offensive in December. This placed immense strain on the American lines of communication, especially the newly opened port of Antwerp. By the new year though, the American transportation system was stronger and more robust than ever, and preparations were underway to support the final drive into the heart of Germany.[134]
Featured articles are considered to be some of the best articles Wikipedia has to offer, as determined by Wikipedia's editors. They are used by editors as examples for writing other articles. Before being listed here, articles are reviewed as featured article candidates for accuracy, neutrality, completeness, and style according to our featured article criteria. Many featured articles were previously good articles (which are reviewed with a less restrictive set of criteria).
There are 6,216 featured articles out of 6,623,683 articles on the English Wikipedia (about 0.09% or one out of every 1,060 articles). Articles that no longer meet the criteria can be proposed for improvement or removal at featured article review.
On non-mobile versions of our website, a small bronze star icon () on the top right corner of an article's page indicates that the article is featured. On most smartphones and tablets you can also select "Desktop" at the very bottom of the page  or "Request Desktop Site" in your browser's menu to see this line (do a search to find out how). Additionally, if the current article is featured in another language, a star will appear next to the corresponding entry in the language switcher to let you know.
A list of articles needing cleanup associated with this project is available. See also the tool's wiki page and the index of WikiProjects.
Featured articles (FAs) are some of the best articles in the English Wikipedia.  They are written by volunteers about subjects of their own choosing, and evaluated by other volunteers against the featured article criteria.  
Designated volunteers select the FA to become Today's featured article on Wikipedia's main page.  
Click on a date/time to view the file as it appeared at that time.
More than 100 pages use this file.
The following list shows the first 100 pages that use this file only.
A full list is available.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
Silver was an arena concert by Filipina entertainer Regine Velasquez held on November 16, 2012, at the Mall of Asia Arena in Pasay City. She became the first local artist to play at the venue since it opened that May. The show's concept and name is a reference to the twenty-fifth anniversary since Velasquez's professional debut in 1986.  The setlist contained songs predominantly taken from Velasquez's discography and various covers. The show was produced by iMusic Entertainment, with GMA Network as its broadcast partner. Ryan Cayabyab and Raul Mitra served as music directors, accompanied by the Manila Philharmonic Orchestra.
Originally scheduled as a one-night show, Velasquez suffered from a viral infection and was forced to cancel her performance halfway through the concert after temporarily losing her voice. It was later re-staged, titled Silver Rewind, on January 5, 2013, and featured Ogie Alcasid, Janno Gibbs, Gloc-9, La Diva, Rachelle Ann Go, Jaya, and Lani Misalucha as guest acts. The show was acclaimed by critics, receiving praise for Velasquez's vocal abilities and rapport with the audience, and was considered a vindication from its initial cancellation.
Filipina singer Regine Velasquez's career began with a record deal with OctoArts International and the release of her single "Love Me Again" in 1986.[1][2] After an appearance in the variety show The Penthouse Live!, she caught the attention of Ronnie Henares, a producer and talent manager who signed her to a management deal.[2][3] In 1993, she signed an international record deal with PolyGram Records,[4] and achieved commercial success in some Asian territories with her albums Listen Without Prejudice (1994), My Love Emotion (1995) and Retro (1996).[5] In April 1996, Velasquez staged a show, named Isang Pasasalamat, at UPD's Sunken Garden to commemorate her ten-year career.[2][6] A twentieth anniversary show was staged in October 2006 at the Araneta Coliseum titled Twenty.[7]
On October 23, 2012, the Philippine Entertainment Portal announced that Velasquez would headline a concert on November 16 at the Mall of Asia Arena in Pasay City, which was dubbed as her "25th anniversary concert" and titled Silver.[8] She became the first Filipino solo act to perform at the venue since it opened that May.[9][10] The show was a joint production by GMA Network and iMusic Entertainment, with Smart Communications, McDonald's Philippines, and Bench as sponsors.[11] Ryan Cayabyab and Raul Mitra were chosen as music directors, accompanied by the 60-member ensemble of the Manila Symphony Orchestra. Velasquez and her team selected Ogie Alcasid, Janno Gibbs, and Lani Misalucha as guest acts[12] During rehearsals and preparations for the show, she revealed that the repertoire included some songs from her older albums that have been transformed into a more upbeat production which she aimed to create an "element of surprise" for concertgoers and described it as a "new way of singing a particular song".[11] She further said that it was vital for her to have the core of the show be a celebration and appreciation of her fans.[11]
The show started with Velasquez, dressed in a silver gown with long train, appearing from the upper stage accompanied by background dancers while singing several lines of "Shine". The song was mashed with Rihanna's "Where Have You Been" incorporating the chorus lines' trance elements. Shortly after, she began a medley of "Hot Stuff" and "Shake Your Groove Thing". After the number, she spoke briefly to the audience, saying, "I learned a lesson last year. It's not about me and it's not even about you. It's about glorifying God and this whole show we dedicate to You."[17] Velasquez then sang a medley songs from her album Nineteen 90 (1990). She followed this with an orchestral arrangement of "Dadalhin", before being joined by Gloc-9 for a performance of "Sirena". The set list continued with an upbeat rendition of "You've Made Me Stronger". She closed the segment with a medley of her movie themes alongside Rachelle Ann Go, La Diva, and Jaya.[19]
For "On the Wings of Love", Velasquez  was lifted by fabrics for an aerial silk performance. This was followed by George Benson's "In Your Eyes" and Aerosmith's "I Don't Want to Miss a Thing". She was joined by Ogie Alcasid and Janno Gibbs  for a medley of OPM songs. After a costume change, she performed Blondie's "Call Me". She followed this with a tribute number for her father and son with the songs "God Gave Me You" and "Leader of the Band". "Love Me Again" was introduced with a brief speech about her career beginnings, before closing the show with "You'll Never Walk Alone" and "You Are My Song". Velasquez returned onstage for an encore performance of "What Kind of Fool Am I?".[19]
The concert was met with positive responses from critics. Jojo Panaligan of the Manila Bulletin praised Velasquez's wide vocal range and rapport with her audience. He emphasized how the singer managed to "regain her roar" in various musical numbers. Panaligan further remarked that the show was a "resounding redemption of reputation" after its initial cancellation.[20] The Philippine Daily Inquirer's Dolly Anne Carvajal viewed the concert as "lovelier the second time around", and concluded that Velasquez delivered on her promise of a show the fans deserved.[21] In a review by ABS-CBNnews.com, it noted that Velasquez showcased her powerful vocals without missing a note and a proof that "she still has what it takes".[22] A writer for the Philippine Entertainment Portal summarized the show's concept as "a promise fulfilled" and observed that spectators were impressed of her performances.[17]
Silver Rewind was aired as a television special on January 27, 2013, on GMA Network.[17] For the production, Velasquez received nominations for Concert of the Year and Best Female Major Concert Act at the 5th Star Awards for Music, winning the latter.[23]
This set list is adapted from the television special Silver Rewind.[19][a]
After signing an international record deal with PolyGram Records, Velasquez achieved commercial success in some Asian territories with her fifth album Listen Without Prejudice (1994), which sold more than 700,000 copies and became her highest-selling album to date, aided by its lead single "In Love with You". She experimented further with jazz and adult contemporary genres on My Love Emotion (1995), while she recorded covers on Retro (1997). After she left PolyGram to sign with Mark J. Feist's MJF Company in 1998, she released the R&B-influenced album Drawn. Velasquez's follow-up record, R2K (1999), was supported by remakes of "On the Wings of Love", "I'll Never Love This Way Again", and "I Don't Want to Miss a Thing", and was subsequently certified twelve-times platinum by the Philippine Association of the Record Industry (PARI).
We were very poor but we were happy. My parents made sure that we ate on time and that was enough for me. [My father] had scoliosis and he was working at a construction site; he wasn't earning enough. My mom was good with money. She was able to stretch whatever little money we had.
Velasquez started singing at age six;[8] she underwent intensive vocal training with her father, who immersed her neck-deep in the sea and had her go through vocal runs.[10][11] She credits this unorthodox method for strengthening her core and stomach muscles, and developing her lung capacity.[12] Velasquez placed third in her first singing competition on Betty Mendez Livioco's The Tita Betty's Children Show.[13]
When Velasquez was nine, her family moved to Balagtas, Bulacan, where she attended St. Lawrence Academy and competed for her school at the annual Bulacan Private Schools Association singing competition.[3] In 1984, at fourteen, Velasquez auditioned for the reality television series Ang Bagong Kampeon.[3] She qualified and became the show's senior division winner, defending her spot for eight consecutive weeks.[3] Velasquez won the competition and was signed to a record deal with OctoArts International.[3]
In 1986, Velasquez initially used the stage name Chona and released the single "Love Me Again",[3] which failed commercially.[14] At the recommendation of another OctoArts recording artist, Pops Fernandez, she appeared on The Penthouse Live![14] While rehearsing for the show, Velasquez caught the attention of Ronnie Henares, a producer and talent manager who signed her to a management deal.[14][15] Velasquez adopted the stage name Regine at the suggestion of Martin Nievera, Fernandez's husband and The Penthouse Live! co-host.[3][14]
Velasquez signed with Viva Records and released her debut album Regine in 1987.[16] Henares served as an executive producer and worked with songwriters Joaquin Francisco Sanchez and Vehnee Saturno.[17] Three singles were released in 1987: "Kung Maibabalik Ko Lang", " Urong Sulong", and "Isang Lahi".[3] During this period, Velasquez appeared on the ABS-CBN television shows Triple Treat and Teen Pan Alley.[18] Two years after Regine's release, Velasquez represented the Philippines in the 1989 Asia Pacific Singing Contest in Hong Kong and won, performing the songs "You'll Never Walk Alone" from the musical Carousel and "And I Am Telling You I'm Not Going" from the musical Dreamgirls.[3]
Velasquez released her second studio album, Nineteen 90, in 1990.[14] She worked with Louie Ocampo on the album's lead single "Narito Ako",[19] which was originally recorded and performed by Maricris Bermont and written by Nonong Pedero for the 1978 Metro Manila Popular Music Festival.[20] Later that year, she headlined her first major concert at the Folk Arts Theater.[21][22] She recorded "Please Be Careful with My Heart" with Jose Mari Chan, who released the track on his album Constant Change;[23] she also sang backing vocals on Gary Valenciano's "Each Passing Night", which appears on his album Faces of Love.[24][25]
In 1991, Velasquez made her North American concert debut at Carnegie Hall in New York City,[26] a first for an Asian solo artist.[27] British theatrical producer Cameron Mackintosh later invited Velasquez to audition for the West End production of the musical Miss Saigon.[28] She received a letter from the production offering to train her in London, which she declined: partly due to her lack of experience in musical theater, and also because she wished to remain with her family.[28]
Velasquez's third studio album Tagala Talaga was released in October 1991.[29] It includes cover versions of recordings by National Artist for Music recipients Ryan Cayabyab, Lucio San Pedro, and Levi Celerio.[30][31] The album's lead single, titled "Buhay Ng Buhay Ko", was originally recorded by Leah Navarro and was written by Pedero,[32][30] with whom Velasquez had worked on Nineteen 90.[19] Other notable singles from the album include  "Anak" and "Sa Ugoy Ng Duyan".[30]
PolyGram Far East announced a joint-venture licensing deal in the Philippines in July 1993 with the formation of its subsidiary PolyCosmic Records.[33] Velasquez recorded a duet titled "It's Hard to Say Goodbye" with Canadian singer Paul Anka, which became the new label's first release.[33][34] The single was later included on her fourth studio album Reason Enough.[34] David Gonzales of AllMusic described the album as "more attuned to international ears" and said Velasquez's vocals are "thin and unimpressive".[34] One of its singles, "Sana Maulit Muli", won the Awit Award for Best Performance by a Female Recording Artist in 1994.[35]
Velasquez released her fifth studio album Listen Without Prejudice in 1994.[36] She worked with songwriters, including Glenn Medeiros, Trina Belamide, and John Laudon.[37] The album was released in several countries in Southeast and East Asia, including China, Hong Kong, Indonesia, Malaysia, Singapore, Taiwan, and Thailand.[36][38] The album's lead single "In Love with You" features Cantonese singer Jacky Cheung.[38] Gonzales commended the record's themes and said, "Cheung's presence on the duet had much to do with the overseas success".[36] The album had sold more than 700,000 copies worldwide, including 100,000 in the Philippines,[38] making it the best-selling album of Velasquez's career to date.[27]
Velasquez's sixth studio album My Love Emotion was released in 1995.[39] The title track, which was written by Southern Sons vocalist Phil Buckle,[39][40] was described by Gonzales as a "triumph [and] an outstanding vehicle, containing a strong melody and hook in the chorus".[39] The album made a combined regional and domestic sales of 250,000 copies.[38]
For her seventh studio album Retro (1996), Velasquez recorded cover versions of popular music of the 1970s and 1980s from artists including Donna Summer, Foreigner, and The Carpenters.[41] The album's only original track, "Fly", is credited to Earth, Wind & Fire members Maurice White, Al McKay, and Allee Willis because the song interpolates the melody of their single "September".[42] Velasquez left PolyCosmic in 1998, and signed a six-album contract with the MJF Company.[43][44] That year, her ninth studio album Drawn was released.[44] MJF head Mark J. Feist wrote and produced most of the tracks, including the lead single "How Could You Leave".[44][45] Drawn sold more than 40,000 copies and was awarded a platinum certification within two weeks of its release.[46][a]
Velasquez produced most of her next album R2K,[47] which was released on November 27, 1999.[48] She recorded remakes of Jeffrey Osborne's "On the Wings of Love", Dionne Warwick's "I'll Never Love This Way Again", Aerosmith's "I Don't Want to Miss a Thing", and ABBA's "Dancing Queen", among others.[47] Gonzales criticized the record's "infatuation with Western popular music" and called Velasquez's singing "self-assured [but] also unimpressive".[49] Commercially, R2K sold more than 40,000 copies in its second week of release, earning a platinum certification,[48][a] and was certified four times platinum a year later.[50] R2K has since been certified twelve times platinum,[51] becoming the highest-selling album by a female artist in the Philippines.[52][53] On December 31, 1999, Velasquez was a featured musical act in 2000 Today,[51] a BBC millennium television special that attracted a worldwide audience of more than 800 million viewers with its core program broadcast across the world's time zones,[54] which began with Kiribati Line Islands and ended in American Samoa.[55][56]
Velasquez headlined and directed the R2K Concert at the Araneta Coliseum in April 2000,[57] which won her Best Female Major Concert Act at the 13th Aliw Awards.[58] Ricky Lo from The Philippine Star was generally impressed with the production and complimented Velasquez's "boundless energy and creativity".[57] She also performed a concert at the Westin Philippine Plaza that year, which spawned the release of her first live album Regine Live: Songbird Sings the Classics in December 2000.[59][60] Although it was criticized for its audio mixing,[60] the album was certified six times platinum.[61][a] 
Velasquez worked with Filipino songwriters for material on her eleventh studio album Reigne.[62] The album and its lead single "To Reach You" were released in December 2001.[63][64] Other singles were Tats Faustino's "Dadalhin" and Janno Gibbs' "Sa Aking Pag-iisa".[65] Gonzales called the album "an adventurous set" and praised the quality of the songwriting.[62]
Velasquez won the inaugural MTV Asia Award for Favorite Artist Philippines in February 2002.[66][67] She performed "Cry" with Mandy Moore to promote the theatrical release of Moore's film A Walk to Remember.[53] In March, Velasquez hosted the first season of Star for a Night, which is based on the British talent show of the same name.[68] In April, she headlined a benefit concert called One Night with Regine at the National Museum of the Philippines, which was a collaboration with ABS-CBN Foundation to benefit Bantay Bata Foundation's child abuse response fund.[69] The show won Best Musical Program at the 7th Asian Television Awards.[70]
At the 2003 MTV Asia Awards, Velasquez won her second consecutive award for Favorite Artist Philippines.[71] In May 2003, she embarked on the Martin & Regine: World Concert Tour with Nievera.[72] The following month, Velasquez returned to host the second season of Search for a Star.[73] That November, she had a concert residency named Songbird Sings Streisand, a tribute to American singer and actor Barbra Streisand, at Makati's Onstage Theatre.[74]
Later in November and December 2005, Velasquez had an eight-day concert residency named Reflections at the Aliw Theater.[1] The sequel album Covers, Vol. 2 was released in February 2006.[82] Unlike its predecessor, it contains songs by foreign artists, including Alanis Morissette's "Head Over Feet", Blondie's "Call Me", and Elvis Presley's "Blue Suede Shoes".[83] Manila Bulletin's Jojo Panaligan was generally impressed with Velasquez's "versatility" and the album tracks' "jazzy and blues-y interpretation".[82] In October 2006, she performed a concert titled Twenty at the Araneta Coliseum, which won her Best Female Major Concert Act and Entertainer of the Year award at the 20th Aliw Awards.[84][85] In 2007, she became co-host of the reality television show Celebrity Duets, an interactive music competition based on the eponymous original US show.[86]
Velasquez developed other television projects in 2008. She appeared in Songbird, a weekly late-night musical television show that featured performances by musical guests.[87] She also featured in the musical television special The Best of Me, which was filmed at her residence in Quezon City.[88][89] Velasquez signed a deal with Universal Records and released an album titled Low Key in December 2008.[90][91] The album consists of cover versions of international songs that she described as "more relaxed, laid-back and restrained".[90] It includes tracks such as Billy Joel's "She's Always a Woman", Dan Fogelberg's "Leader of the Band", and Janis Ian's "At Seventeen".[92] The Philippine Daily Inquirer praised the album's maturity and wrote, "[Velasquez] no longer shrieks and shouts as much as she used to".[93] The album sold more than 25,000 copies within two months of its release and was certified platinum.[94][a] In May 2009, she appeared on the television documentary Roots to Riches, which chronicles her personal and professional struggles, and includes musical performances filmed in her hometown Malolos.[95] Later that month, she hosted the television talent show Are You the Next Big Star?.[96]
Velasquez's next album, a double CD set called Fantasy, was released in December 2010.[97][98] The first disc is composed of Original Pilipino Music (OPM) recordings and the second includes covers of international singles such as Madonna's "Papa Don't Preach", Toronto's "What About Love", and the Eagles' "Love Will Keep Us Alive".[97][99] The Philippine Daily Inquirer called the album "vocally sumptuous" and was generally impressed with Velasquez's vocals and range.[97] Fantasy received a platinum certification[a] and earned three nominations at the 3rd Star Awards for Music.[98][100] After receiving the Magna Award at the Myx Music Awards 2011,[101] and the confirmation of her pregnancy, Velasquez took a hiatus from public engagements.[102] She returned to television on October 6, 2012, with Sarap Diva, a weekly lifestyle talk show.[103] On November 16, Velasquez performed a concert titled Silver at the Mall of Asia Arena, which was cut short after she lost her voice due to a viral infection.[104][105]
After Silver's cancellation, Velasquez restaged the concert on January 5, 2013.[106][107] The concert received generally favorable reviews; Manila Bulletin's Jojo Panaligan called it a "redemption of reputation",[108] while Dolly Anne Carvajal of the Philippine Daily Inquirer said Velasquez did not fail to make up for the initial cancellation of the show.[109] The following month, she co-headlined in Foursome alongside Alcasid, Fernandez, and Nievera.[110] For both shows, Velasquez received four nominations at the 5th Star Awards for Music,[111] winning Best Female Major Concert Act for "Silver" and Concert of the Year for "Foursome".[112]
In November 2013, Velasquez's album Hulog Ka Ng Langit was released;[113] it received a platinum certification for two-week sales of 15,000 copies.[114][a] She won Best Inspirational Record for "Nathaniel (Gift of God)" and Best Christmas Recording for "Hele ni Inay" at the 27th Awit Awards,[115] while Hulog Ka Ng Langit won Album Cover of the Year at the 6th Star Awards for Music.[116] In 2014, she worked with Nievera in a one-night show titled Voices of Love,[117] with Gloc-9 on "Takipsilim",[118] and with Vice Ganda on "Push Mo Yan Teh".[119]
In February 2015, Velasquez appeared alongside Nievera, Valenciano, and Lani Misalucha in a concert titled Ultimate at the Mall of Asia Arena.[120] She received accolades at the 47th Box Office Entertainment Awards,[121] 7th Star Awards for Music,[122] and 5th Edukcircle Awards for the production.[123] In the same year, Velasquez served as a judge on the sixth season of the reality talent television show StarStruck.[124] In November 2015, Velasquez headlined a four-date concert residency called Regine at the Theater, which featured songs from musicals.[125][126]
For a third consecutive year, Velasquez appeared in a co-headlining concert at the Mall of Asia Arena in February 2016.[127] The two-night show, Royals, reunited her with Nievera and also features Angeline Quinto and Erik Santos.[128] Due to the concert's positive critical reception,[129] Velasquez won Best Female Concert Performer at the 48th Box Office Entertainment Awards and Most Influential Concert Performer of the Year at the 6th Edukcircle Awards.[130][131] In December 2016, People Asia magazine included Velasquez on its annual People of the Year list.[132]
Velasquez hosted Full House Tonight,[133] which ran from February to May 2017.[134] The following month, she announced that she had returned to Viva Records and that she had begun production of a new studio album called R3.0.[135] In August 2017, a cover of Up Dharma Down's 2010 song "Tadhana" was released as a promotional single.[136] An original track called "Hugot" was released as the album's lead single the following month.[137] In November she headlined the R3.0 Concert at the Mall of Asia Arena, and two months later, with Alcasid, she played a four-date U.S. concert series titled "Mr. and Mrs. A."[138][139]
In 2018, Velasquez hosted the television talent show The Clash,[140] served as a judge on ABS-CBN's revival of the Idol franchise series Idol Philippines, and hosted the musical variety show ASAP Natin' To.[18] In November, she staged a three-date concert series titled "Regine at the Movies" at the New Frontier Theater.[141]
Sharon Cuneta and Velasquez co-headlined a concert, Iconic, in October 2019.[142] For the show, Velasquez won the awards for Best Collaboration in a Concert and Entertainer of the Year at the 32nd Aliw Awards,[143] having won the latter honor in 2007 and 2009.[144][145] The following month, she released a collaborative single with Moira Dela Torre called "Unbreakable", which was recorded for the soundtrack of the film of the same name.[146] Velasquez appeared as the face of Australian beauty brand BYS and released the promotional single "I Am Beautiful" for the brand's "Be Your Own Expert" campaign.[147][148] She released the soundtrack singles "Ikaw Ang Aking Mahal" for the action television series The General's Daughter (2019)[149] and "Mahal Ko O Mahal Ako" for the drama series Love Thy Woman (2020).[150]
Velasquez organized virtual benefit concerts in support of relief efforts during the COVID-19 pandemic in 2020. She curated One Night with Regine, a collaboration with ABS-CBN to support Bantay Bata Foundation's COVID-19 response fund in April,[151][152] and appeared in Joy From Home, which raised funds to support Jollibee Group's food aid program in June.[153] On February 28, 2021, she was featured in an online streaming concert titled Freedom.[154]
Velasquez made her cinema debut with a minor role in the 1988 comedy film The Untouchable Family.[155] Its soundtrack includes her single "Urong Sulong".[156] She continued to appear in a series of supporting roles in comedies, including Pik Pak Boom (1988) and Elvis and James 2 (1990).[141][156]
A key point in Velasquez's film career came when she was cast in Joyce Bernal's Kailangan Ko'y Ikaw (2000) opposite Robin Padilla.[58] Film critic Noel Vera criticized the film's formula as "the nth variation of Roman Holiday", but wrote that Velasquez "[brought] her own public persona and charisma and sense of humor to the role".[161] Her next film role was in Pangako Ikaw Lang (2001), which reunited her with Bernal and Muhlach.[162] Vera was impressed with the film's direction and writing, and described Velasquez's performance as "sunny good nature [with a] light comic touch".[163] Pangako Ikaw Lang became the highest-grossing Filipino film of 2001.[51][164] Velasquez was awarded the Box Office Queen title at the 32nd Box Office Entertainment Awards due to the film's commercial performance.[164]
Her next television appearance was in an episode of ABS-CBN's weekly drama series Maalaala Mo Kaya (2001),[165] playing a woman with autism.[166] The role won her the Best Actress award at the 16th Star Awards for Television.[167] She portrayed a mundane and undesirable mail sorter in the drama Ikaw Lamang Hanggang Ngayon (2002) opposite Richard Gomez,[168] while Pangarap Ko Ang Ibigin Ka (2003) reunited her with Christopher de Leon of Wanted: Perfect Mother,[158][169] premiering at the Manila Film Festival in July 2003.[169] In December, Velasquez next starred alongside Bong Revilla in the superhero film Captain Barbell.[170]
Although Velasquez did not make any film appearances in 2004, she made her prime time television debut in the drama series Forever in My Heart,[158] in which she was reunited with Gomez, and worked alongside Ariel Rivera.[158] She next starred in romantic dramas, reuniting with Padilla in Till I Met You (2006) and with Pascual in Paano Kita Iibigin (2007).[171][172] For the latter film, Velasquez received FAMAS and Luna nominations for Best Actress.[173][174] In 2008 she returned to television, playing the titular character in the comedy series Ako si Kim Samsoon, an adaption of a South Korean television show.[175] Velasquez also voiced the eponymous character in the animated film Urduja (2008).[176]
During 2009, Velasquez made cameo appearances in the comedies Kimmy Dora, OMG (Oh, My Girl!), and Yaya and Angelina: The Spoiled Brat Movie.[177][178] In March 2010, Velasquez appeared in the musical television series Diva as a facially disfigured ghost singer.[179][180] The following year, she collaborated with Dingdong Dantes in the television series I Heart You, Pare! (2011).[181] She left the show for health reasons and was replaced by Iza Calzado.[182]
Velasquez played a widow obsessed with a pop star in Nigel Santos' independent film Yours Truly, Shirley.[189] The film premiered at the 2019 Cinema One Originals Film Festival.[189] In January 2020, she briefly appeared in the iWant comedy series My Single Lady.[190]
As a child, Velasquez enjoyed listening to her father singing classic songs to lull her to sleep;[8] she was drawn to traditional songs rather than nursery rhymes because of this routine.[8] Since her childhood, Velasquez has considered Sharon Cuneta a role model and credits Cuneta as a key inspiration who led her to pursue a musical career.[191]
Velasquez's music is influenced by artists such as Sheena Easton, Angela Bofill, Whitney Houston, and Mariah Carey in her early years.[1][192] She admires Houston for her "style and R&B influence" and Carey's songwriting.[192] On several occasions, Velasquez has cited Barbra Streisand as her main influence and musical inspiration, saying, "I look up to [Streisand] not just because of her enormous talent, but because of her fearlessness and dedication to excellence, her willingness to take risks and to be different."[193] Streisand's music has frequently featured in Velasquez's repertoire throughout her career, including a series of concerts paying homage to Streisand, which Velasquez described as "a pleasure" to perform.[193][194] Velasquez has also been influenced by many Filipino artists; early in her career, she cited Kuh Ledesma, Joey Albert, Gary Valenciano, Martin Nievera, and Pops Fernandez as her role models.[1] She has also paid tribute to Filipino songwriters, including George Canseco, Rey Valera, Basil Valdez, Ryan Cayabyab, and Willy Cruz.[195]
Velasquez's early-career music includes elements of traditional OPM love songs.[196] She described how she developed her musical style, saying, "I was only 16 and people didn't know what to do with me. When they want me to sing love songs, they had to explain to me what it meant because I didn't know the feeling yet."[197] Her debut album Regine includes ballads and bubblegum pop love songs;[196] its themes revolve around feelings of "excitement and uncertainty", as well as "missed chances and regrets".[196] Elvin Luciano from CNN Philippines wrote: "During her [initial] phase, she proved that Filipino love songs don't have to come pre-packaged in the kundiman-rooted love ballad."[196] Her later releases, including Nineteen 90 and Tagala Talaga, capitalized on her popularity; they are dominated by Filipino love songs.[198] Velasquez began working with foreign songwriters while planning her first regional album Listen Without Prejudice,[36][37] which according to AllMusic is "oriented towards easy-listening love songs with adventurous, contemporary touches".[36] The album features tracks with syncopated backbeats and hip-hop influences.[36]
During the mid-1990s and the early 2000s, Velasquez's albums consisted primarily of cover versions of international material because of its commercial viability, and Filipinos' preference for American music.[41][49][196] According to CNN Philippines, "Regine has a knack for choosing songs which at first, may not fit her, but eventually become her own."[196] Many of her songs, particularly in Retro, Drawn, and R2K, contained R&B, soul, and hip-hop elements.[41][44][49] Reigne is an OPM album that she described as "songs influenced by the music, artists, and genres that I enjoy listening to,"[196] and included tracks that are melancholic, sensual, and poetic.[196] Her crossover to film saw significant use of contemporary love ballads in her catalog of soundtrack themes, describing the music as "straightforward, earnest, and lyrically simple".[196]
Velasquez is known for her use of vocal belting,[199] and credits the vocal training she received from her father as a child:
Velasquez is a soprano and is often praised for her range and technical ability.[27][200][201] Luciano of CNN Philippines complimented her "trademark and sometimes melismatic vocals"[196] while Gonzales adds her singing is "strong, emotive, and confident".[39] She has often been criticized, however, for the excessive use of belting and oversinging.[199] Gonzales described Velasquez's timbre as "thin, unimpressive and unappealing at times", and said her singing is "aiming for a higher [note], [which] she did all too often".[39] Velasquez said, "I don't mean to make any songs hard. It's just that when I'm on stage, with the adrenaline rush and all, you get excited. I do try to hold back [because] otherwise I'd be screaming the whole show, that's not good."[202]
Velasquez's music has broadly influenced a younger generation of performers from reality television talent shows; Sarah Geronimo has stated Velasquez made her realize the value of hard work[215] while Rachelle Ann Go and Angeline Quinto have both said Velasquez inspired them during their early years as aspiring singers.[216][217] American Idol finalists Ramiele Malubay, Thia Megia, and Jessica Sanchez have expressed a desire to emulate Velasquez.[218][219][220]
Velasquez has been involved with several charitable organizations. She became associated with the United Nations Children's Fund (UNICEF) in 2001 and worked on a documentary titled Speak Your Mind, which is about homeless children in Payatas, Quezon City, one of the Philippines' largest open dumpsites.[223][224] The program was nominated for the UNICEF Child Rights Award.[223]
One of Velasquez's highest-profile benefit concert appearances was in One Night with Regine,[69] which she performed at the National Museum of the Philippines in support of the Bantay Bata Foundation, a child welfare organization.[69] In 2005, Velasquez appeared in an episode of the lifestyle talk show Mel and Joey, and donated proceeds from an auction of her gowns to the GMA Kapuso Foundation's Christmas Give-a-Gift project.[225] In 2009, Velasquez headlined a benefit television special called After The Rain: A Hopeful Christmas in the aftermath of Typhoon Ketsana (Ondoy).[226] In October 2010, she became an ambassador for Operation Smile,[227] a nonprofit organization that provides cleft lip and palate repair surgery to children worldwide.[228] She recorded the theme "S.M.I.L.E.", which was written for the project and appears on her studio album Fantasy.[227][99] In November 2013, proceeds from the sales of her album Hulog Ka Ng Langit were donated to the Philippine Red Cross in support of the Typhoon Haiyan (Yolanda) relief.[229]
Velasquez announced her relationship with singer-songwriter Ogie Alcasid in an article published by Yes! magazine in June 2007.[234] On August 8, 2010, the couple announced their engagement,[235] and in December, they married in Nasugbu, Batangas.[236] She gave birth to their son, Nathaniel James, via caesarean section on November 8, 2011.[237]
Velasquez is a born-again Christian.[238] In March 2016, she revealed she had suffered a miscarriage prior to her marriage to Alcasid and stated it was her reason for converting.[238] Velasquez also said she had been attending Victory Christian Fellowship.[238]
Throughout her career, Velasquez has received many honors and awards, including MTV Asia's Favorite Artist Philippines in 2002[67] and 2003,[239] and the Aliw Awards' Entertainer of the Year in 2007, 2009, and 2019.[143][144][145] She has been the recipient of lifetime achievement awards, including the Awit Awards' Dangal ng Musikang Pilipino,[240] the Star Awards for Music's Pilita Corrales Lifetime Achievement[241] and Natatanging Alagad Ng Musika,[242] FAMAS Awards' Golden Artist,[243] and Myx Music's Magna Award.[101]
LaDainian Tarshane Tomlinson (born June 23, 1979) is an American former professional football player who was a running back in the National Football League (NFL) for 11 seasons. After a successful college career with the TCU Horned Frogs, the San Diego Chargers selected him as the fifth overall pick in the 2001 NFL Draft. He spent nine years with the Chargers, earning five Pro Bowl appearances, three Associated Press first-team All-Pro nominations, and two NFL rushing titles. Tomlinson was also voted the NFL Most Valuable Player (MVP) in 2006 after breaking the record for touchdowns in a single season. He played two further seasons with the New York Jets, before retiring. He was elected to the Pro Football Hall of Fame in 2017. 
A native of Rosebud, Texas, Tomlinson showed athletic promise while attending University High School in Waco. He was recruited by Texas Christian University (TCU). As a junior, Tomlinson rushed for 406 yards in a single game, a Division I record at the time. As a senior, he earned unanimous All-America honors, and won the Doak Walker Award as the best college running back. TCU retired his No. 5 in 2005, and he was elected to the College Football Hall of Fame in 2014.
The Chargers selected Tomlinson No. 5 overall after passing on the opportunity to select highly-rated quarterback Michael Vick. A starter in his rookie season, Tomlinson opened his career with the first of seven consecutive seasons with over 1,200 rushing yards, a streak achieved previously only by Eric Dickerson. He became a prolific scorer under Marty Schottenheimer, who coached the Chargers from 2002 to 2006. Tomlinson's output reached a peak in 2006, when he set numerous single-season records, including for most touchdowns scored (31). These feats won him the NFL MVP award, but San Diego suffered an upset defeat in their playoff opener, and Schottenheimer was fired shortly afterwards. Tomlinson became less central to the Charger offense in the following three seasons, and missed time through injury in key games. He was released following the 2009 season, played two seasons with the Jets, and retired.
Tomlinson is often known by his initials, L.T.. He works as an analyst on the NFL Network, and also serves as a special assistant to the Chargers' principal owner, Dean Spanos.
Tomlinson was born on June 23, 1979, to Loreane Chappelle and Oliver Tomlinson in Rosebud, Texas. His mother worked as a preacher; his father left the family when Tomlinson was seven years old.[1] Tomlinson did not see his father very often afterward.[2] He grew up with a brother and a sister and later, also a half-sister and three half-brothers.[3] At age nine, Tomlinson joined the Pop Warner Little Scholars football program and scored a touchdown the first time he touched the ball.[1]
Tomlinson was an avid Dallas Cowboys and Miami Hurricanes fan during his youth. He idolized Walter Payton and admired Emmitt Smith, Jim Brown, and Barry Sanders.[6][9] Tomlinson was able to meet Smith while attending a camp run by Dallas Cowboys tight end Jay Novacek.[3]
Tomlinson accepted an athletic scholarship at Texas Christian University in Fort Worth, Texas, then a member of the Western Athletic Conference (WAC). He played for the TCU Horned Frogs from 1997 to 2000.[10] Before Tomlinson's arrival, TCU had appeared in only one bowl game in the previous 12 seasons and two in the previous 31, losing both.[11] They had recently been downgraded to a minor conference (the WAC) after the breakup of the Southwest Conference.[12]
"What have we been playing college football, a hundred-and-something years and nobody has even been able to do what he did today."
TCU retired his No. 5 during halftime of a November 2005 game against UNLV.[48] He was their single-game, single-season, and career record holder in both rushing touchdowns and rushing yards, amongst other records.[49] In December of that year, Tomlinson fulfilled a promise to his mother by earning his degree in communications from TCU.[50] He was inducted into the College Football Hall of Fame on December 9, 2014.[51]
Tomlinson was a holdout through much of training camp, while his agent Tom Condon negotiated with the Chargers.[70] He eventually signed a six-year, $38 million contract on August 21.[71] He had missed the first two preseason games, and was kept on the bench for the third,[72] before featuring briefly in the final game, rushing five times for 14 yards in a defeat to the Arizona Cardinals.[73]
League-wide, Tomlinson finished ninth for rushing yards and tied-fourth for rushing touchdowns. However, his yards per carry of 3.6 was only 31st among players with at least 100 carries, and his eight fumbles, one of which was returned for a key touchdown in a loss to the Philadelphia Eagles,[84] were joint-most among non-quarterbacks.[85] Tomlinson led the league in touches (rushing attempts and receptions combined) with 398, and ranked ninth for yards from scrimmage.[86] He received 16 votes for the Associated Press Offensive Rookie of the Year award, finishing second to Anthony Thomas (22 votes).[87] Thomas and Tomlinson were the two running backs named to the Pro Football Writers Association All-Rookie team.[88]
A day after their final game of 2001, the Chargers fired head coach Mike Riley,[89] replacing him with Marty Schottenheimer, recently dismissed as Washington's head coach.[90] Schottenheimer brought with him a reputation for favoring the running game over the pass.[91] Tomlinson said of his new coach, "I think he knows how to win, and he's been doing it for a number of years. ... I think that is the kind of coach that we need."[92] At his Pro Football Hall of Fame enshrinement speech in 2017, Tomlinson would describe Schottenheimer as the best coach he ever had.[3]
"There is no question that a number of those runs, including that long one in the first half, was the product of his determination and heart that he isn't going on the ground."
Tomlinson was rewarded for his performances with his first Pro Bowl nomination (together with Junior Seau, he was one of only two Chargers so honored),[114] as well as being named an Associated Press (AP) 2nd-team All-Pro.[115]
During the 2003 offseason, San Diego signed Lorenzo Neal, a fullback who had blocked for 1,000-yard rushers in each of his previous six seasons, and was coming off his first Pro Bowl nomination.[116] Tomlinson would later describe Neal as vital to the progression of his career, and chose the fullback to introduce him on the day of his induction into the Hall of Fame.[117]
Tomlinson finished with 1,645 rushing yards, third-most in the league.[130] He averaged 5.3 yards per carry, the sixth-highest among backs with 100-plus carries; this would be the best average of his career.[107] His receiving numbers were career highs: 100 receptions, 725 receiving yards, and four receiving touchdowns.[107] The 100 receptions placed him fourth in the league; the rest of the top ten were all wide receivers.[131] He broke Tony Martin's franchise record of 90 receptions in 1995.[h] Tomlinson had 2,370 yards from scrimmage, leading the league;[133] it was the second-highest total in NFL history up to that point.[i] He had five games with at least 200 yards from scrimmage during the season, another league record.[135] Tomlinson scored 17 total touchdowns, tied for third in the league and another new career high.[136]
Tomlinson was not voted to the Pro Bowl in 2003, which was seen as a snub by multiple observers; Tomlinson himself expressed disappointment, saying, "I think all those guys deserve to be there, but are they better than me? Nope."[137] However, he was named a Second-team Associated Press All-Pro for the second season in a row,[138] and was runner-up to Jamal Lewis for the AP Offensive Player of the Year Award with eight votes.[139][140]
On August 14, Tomlinson signed an eight-year contract worth $60 million, with $21 million guaranteed. It was the richest contract for a running back up to that point.[141]
Tomlinson's yardage numbers were down from the previous season, with 1,335 rushing (7th in the league)[147] and 441 receiving, in part because he was rested in the regular season finale,[148] though his yards per carry dropped significantly to 3.9, and he had barely half as many receptions with 53.[107] However, he led the NFL in rushing touchdowns for the first time with 17.[147] His 1,776 scrimmage yards were tied for fifth in the league, while his 18 total touchdowns ranked second.[149] Tomlinson earned his second Pro Bowl nomination, and rewarded his offensive line with an expenses-paid trip to Hawaii, the site of the game.[150] He was voted Associated Press First-team All-Pro for the first time in his career.[151]
Tomlinson's rushing totals were 1,462 yards and 18 touchdowns, ranking sixth and third in the league respectively.[176] With two receiving touchdowns, he had 20 in total;[107] this broke Chuck Muncie's franchise record, set in 1981.[175] He ranked third in the league, behind Seattle Seahawks running back Shaun Alexander, who set a new NFL record with 28.[177][178] Tomlinson again made the Pro Bowl,[179] and was named an Associated Press second-team All-Pro.[180]
San Diego changed starting quarterbacks in 2006. Brees had injured his shoulder in the 2005 finale; after negotiations for a new contract with Chargers general manager A. J. Smith broke down, Brees was allowed to leave in free agency, paving the way for 2004 No. 4 overall draft pick Philip Rivers to take over.[181][182] Tomlinson spoke positively about Rivers in the leadup to the season, saying, "He's going to be a great quarterback because the intangibles he has are what the great ones have."[183] However, Tomlinson would suggest in a 2016 interview with ESPN that the switch cost San Diego a Super Bowl win, stating that Rivers was too inexperienced at the time.[184]
"When we're old and can't play this game anymore, them are the moments we are going to remember, that we'll be able to tell our kids, tell our grandchildren. We can talk about something special that we did. We made history today."
On January 5, 2007, Tomlinson was named the Associated Press NFL Most Valuable Player for his record-breaking season, receiving 44 of the 50 votes from a panel of nationwide sportswriters and broadcasters who cover the NFL.[224] Accepting the award, Tomlinson said that he'd had a great year on a great team, adding, "I would feel so much better about winning if we win the Super Bowl." He was the first Chargers player to win the award.[224] Other organizations to name Tomlinson the NFL MVP included the Pro Football Writers of America,[225] the Sporting News,[226] and the Maxwell Football Club via the Bert Bell Award.[227] The Associated Press also honored him as the Offensive Player of the Year and a unanimous 1st-Team All-Pro.[228] Tomlinson was also named co-winner of the Walter Payton Man of the Year Award alongside Brees, now quarterback of the New Orleans Saints,[229] and was voted to his fourth Pro Bowl.[230] On July 11, 2007, Tomlinson won four ESPY Awards including Male Athlete of the Year.[231]
Tomlinson and other Chargers defended Schottenheimer after their swift exit from the 2006 playoffs,[235]
but the coach was nonetheless fired by team president Dean Spanos on February 12, 2007.[236] A dysfunctional relationship between Schottenheimer and general manager A.J. Smith was among the reasons given by Spanos.[237] Norv Turner, who was the San Diego Chargers offensive coordinator in Tomlinson's rookie season, replaced Schottenheimer as head coach a week later. "Norv is the perfect fit for our team. He will know exactly what to do with our team," Tomlinson said of the hiring.[238]
Tomlinson was involved in a pair of sideline incidents with Rivers over the course of the season. In an early-season loss to the Packers, the two appeared to argue on the sideline; Tomlinson dismissed the interaction as "competitive talk".[253] Later, during the overtime win in Tennessee, Tomlinson got up and walked away immediately after Rivers sat near to him on the bench.[254] Both players downplayed the incident, with Tomlinson explaining that he had left because he had just finished a conversation with Neal.[255]
While Tomlinson's rushing statistics of 1,474 yards and 15 touchdowns were both well short of his 2006 performances, he still led the league in both areas,[256] and became the first player since Edgerrin James in 1999 and 2000 to win back-to-back rushing titles.[257] During the year, Tomlinson became the 23rd player to reach 10,000 rushing yards in NFL history, as well as the fourth fastest,[258] while his career-opening streak of seven consecutive seasons with at least 1,200 rushing yards had previously been achieved only by Eric Dickerson.[259] With 60 receptions for a further 475 yards and 3 touchdowns, Tomlinson ranked second in the NFL for both yards from scrimmage (1,949) and total touchdowns (18). He had zero fumbles for the first time in his career, despite a league-high 375 touches.[260] Tomlinson was nominated for his fifth and final Pro Bowl and, unanimously, his third and final AP 1st-Team All-Pro squad.[261][107] He was also awarded the Bart Starr Award for his work on and off the field.[262]
Tomlinson ended the regular season with career-lows in attempts (292) and rushing yards (1,110, ranking tenth in the league), while his 11 rushing touchdowns (seventh in the league) and 3.8 yards per carry were both the least since his rookie year.[287][107] His 344 touches, 1,536 scrimmage yards, and 12 total touchdowns also represented a clear drop from the previous season.[288] The Charger offense became more focused on Rivers, who led the league in touchdowns and passer rating while throwing for over 4,000 yards.[289][290]
"That's the class that he shows ... I wanted to come down here and show mine ... I'm happy that he did it. It makes it special, because he's a good human being. He's a class individual, and I hope in these later years y'all treat him that way."
The offseason began with contract negotiations for Tomlinson, as Smith and Spanos hoped to restructure his existing contract and free up more salary cap space.[297] Relations between Smith and Tomlinson were strained throughout the process. Smith was reported to have been angered when Tomlinson revealed the full extent of his injury before the Divisional Round game against Pittsburgh the prior season.[298] When Tomlinson release a statement expressing his desire to remain in San Diego, Smith appeared to mock him when he responded to an interview question using very similar wording.[299] Smith later apologized to Tomlinson, and the two sides came to an agreement on a restructured version of his three-year contract on March 10. Tomlinson said in another statement, "My heart has always been in San Diego. I couldn't imagine putting on another uniform."[300]
On January 31, Tomlinson was named to the NFL's 2000s All-Decade Team after leading the league with 12,490 rushing yards in the 2000s, 1,797 more than runner-up Edgerrin James.[324] His 138 rushing touchdowns during the decade set an NFL record for any decade, and were 38 more than any other player in the 2000s.[325][326] However, there was speculation as to whether Tomlinson would play for the Chargers again, with the player himself saying that he expected to be let go.[327]
"I believe he's got a lot more left. When you watch him out here bouncing around doing a great job with protections, running the football, he's still got that wiggle."
Tomlinson entered the free-agent market for the first time in his career, expressing excitement at the future and a desire to win a Super Bowl.[334] After the New York Jets and Minnesota Vikings emerged as Tomlinson's most likely destinations, he met with both teams and signed a two-year, $5.2 million contract with the Jets on March 14, 2010.[335] He was expected to back up second-year running back Shonn Greene;[336] the Jets had released five-time 1,000 yard rusher Thomas Jones the previous month when he refused to take a pay cut to serve as Greene's backup.[337] Tomlinson chose to sign with New York because of his familiarity with the system of offensive coordinator Brian Schottenheimer, his enthusiasm for the defense- and run-focussed philosophy of head coach Rex Ryan and because he felt that the team offered him the best chance to win a championship.[330] He added that he believed he and Green would form a successful partnership.[338]
While Tomlinson's output reduced over the remainder of the season,[343] he nonetheless improved upon his last year with the Chargers by rushing for 914 yards at 4.2 yards per carry, while catching 52 passes for 368 yards.[107] Tomlinson stayed injury-free, missing only the regular season finale when he was rested with the Jets assured of a wildcard appearance in the playoffs.[345] He did score less frequently than in any of his seasons in San Diego, with only six touchdowns.[107] Brought in to complement Greene, Tomlinson was the Jet's leading rusher, outgaining his backfield partner by 148 yards from 34 more carries. As a team, the Jets ranked fourth in the NFL for rushing yards (though they had been first in 2009) and 11th for total yardage (up from 20th in 2009).[346][347][348] He continued to move up the NFL's career rushing yardage leaderboard during the season, passing Tony Dorsett and Eric Dickerson to reach sixth place.[341][349] He received the Dennis Byrd Award as the Jets' most inspirational player after a vote of his teammates.[350]
Tomlinson finished his final season with 75 carries for 280 rushing yards and a single rushing touchdown, all career lows.[371] Greene, now the main running back, had over 1,000 yards, but the Jets rushing attack was ranked only 22nd in the league, while their offense as a whole was 25th.[372][373] Tomlinson's new pass-catching role yielded 42 catches for 449 receiving yards and two receiving touchdowns; he averaged 10.7 yards per reception, a career-high.[107] His teammates again voted him the winner of the Dennis Byrd Award.[374]
Tomlinson's contract with the Jets expired after the 2011 season. In the aftermath of the season-ending loss in Miami, he said that he would need three or four weeks to decide whether to retire or not.[369] On May 11, Tomlinson returned to Qualcomm Stadium in San Diego to speak at a memorial for former Charger Junior Seau, who had been a positive influence on Tomlinson during his rookie season.[375][211] On June 18, he signed a ceremonial one-day contract with the San Diego Chargers and then immediately announced his retirement.[376] Chargers president Dean Spanos said that no other Charger would ever wear Tomlinson's No. 21.[377]
Former teammates including Rivers, Gates and three offensive linemen from his 2006 season were present at Tomlinson's farewell press conference, as were his wife, mother and children. Recalling the words of Seau at his own retirement, Tomlinson described the act as graduating to the next phase of his life. Of his failure to win a Super Bowl, he said, "I'm OK with never winning a Super Bowl championship. I know we've got many memories that we can call championship days."
[377]
At the time of his retirement, Tomlinson ranked fifth in NFL history in career rushing attempts (3,174)[378] and yards (13,684), and second in career rushing touchdowns (145).[377] He had 47 100-yard rushing games, and three 100-yard receiving games.[124] He also ranked third for receptions by a running back, catching 624 passes for 4,772 yards and a further 17 touchdowns.[379] Overall, he ranked fourth in career touches (3,798), fifth in yards from scrimmage (18,456), and third in total touchdowns (162).[380][377] He was only the second player to rush for at least 13,000 yards and catch passes for at least 4,000 yards, following Payton.[381] Completing his reputation as a versatile back,[382] Tomlinson completed 8 of 12 passing attempts in his career, for seven touchdowns and no interceptions. Only Payton, with eight, had more touchdown passes among non-quarterbacks in the Super Bowl era.[383] His playoff performances were less impressive, as he was injured in 2007 and 2008 and only rushed for 100 yards once in ten postseason games.[384][124][385]
Tomlinson was noted for consistently playing well against the Oakland Raiders.[244][386] In nineteen games against them, he rushed for 2,055 yards, at an average of 108.2 yards per game, well above his career average of 80.5 yards per game. He also rushed for 22 touchdowns, caught four, and threw another three, in each case more than his total against any other single team.[387]
An elusive runner in the open field who would use stiff arms to break tackles,[388] Tomlinson was also effective as a power back on inside runs.[389] In goal-line situations, he would often leap directly over the line of scrimmage to score.[192] He wore a distinctive dark visor for the majority of his career, to prevent migraines caused by stadiums lights; this benefitted him as it prevented defenders from reading his eyes.[390] When scoring, Tomlinson would frequently perform his own "teardrop" celebration,[391] placing his left hand behind his head and flipping the ball with his right.[392] He was often known by his initials, L.T., a nickname he shared with Hall of Fame New York Giants linebacker Lawrence Taylor.[393]
In 2005, Schottenheimer described Tomlinson as the finest running back he'd seen, arguing that past greats such as Jim Brown and Gale Sayers hadn't had to contend with defenders of the same size and speed.[172] When Tomlinson's number was retired in 2015, a trio of analysts on NFL.com placed him 3rd, 7th, and 8th respectively on their lists of top running backs in the Super Bowl era.[394][395] In 2021, the statistical site Pro-Football-Reference.com ranked him as the fifth-best running back in NFL history.[396] An NFL Network show, The Top 100: NFL's Greatest Players, aired in 2010 and ranked Tomlinson No. 61 among all positions,[397] while a 2019 USA Today poll placed him at No. 54.[398] However, he was not among the ten running backs named to the NFL 100th Anniversary All-Time Team.[399]
The Chargers credit Tomlinson with numerous records. Career figures discount his two seasons with the New York Jets.
Tomlinson is a Christian.[430] Tomlinson was introduced to his future wife, LaTorsha Oakley, while the two were students at TCU.[431] The couple married on March 21, 2003,[432] and have two children: a son born in 2010 and a daughter in 2011.[433][358] In 2007, Tomlinson's father Oliver Tomlinson and brother-in-law Ronald McClain died in an auto accident.[434][435]
During his playing career, Tomlinson was featured in commercials for Nike,[436] Campbell Soup,[437] and Vizio.[438] In April 2007, CNBC reported that Tomlinson turned down a request to become the cover athlete for EA Sports' Madden NFL 08 video game, as the money offered was not enough to justify the promotional work involved.[439]
In August 2012, Tomlinson joined the cast of NFL Network's Sunday morning show "First on the Field" as an analyst.[440] As of 2022, he is still with the network.[441] He covers Chargers preseason games as an analyst with CBS.[442]
Tomlinson has a charitable foundation. The foundation helps high school and college students, provides meals for people who are homeless or poor, and raises money for after-school programs and other causes. It focuses its efforts in Los Angeles, San Diego, and Texas.[446] The charity was cited as a reason for Tomlinson receiving the Bart Starr Award in 2008.[262]
In 2017, the Los Angeles Chargers announced that Tomlinson was joining the team as a special assistant to ownership. The role involves attempting to build a new fanbase after the Chargers' move to Los Angeles.[447]
Tomlinson's nephew, Tre'Vius Hodges-Tomlinson, followed in his footsteps by playing at TCU, where he earned All-Big 12 honors three times playing cornerback and won the Jim Thorpe Award in 2022 as the nation's best defensive back.[448]
A touchdown (abbreviated as TD[1]) is a scoring play in gridiron football. Whether running, passing, returning a kickoff or punt, or recovering a turnover, a team scores a touchdown by advancing the ball into the opponent's end zone. In American football, a touchdown is worth six points and is followed by an extra point or two-point conversion attempt.
To score a touchdown, one team must take the football into the opposite end zone. In all gridiron codes, the touchdown is scored the instant the ball touches or "breaks" the plane of the front of the goal line (that is, if any part of the ball is in the space on, above, or across the goal line) while in the possession of a player whose team is trying to score in that end zone. This particular requirement of the touchdown differs from other sports in which points are scored by moving a ball or equivalent object into a goal where the whole of the relevant object must cross the whole of the goal line for a score to be awarded. The play is dead and the touchdown scored the moment the ball touches plane in possession of a player, or the moment the ball comes into possession of an offensive player in the end zone (having established possession by controlling the ball and having one or both feet depending on the rules of the league or another part of the body, excluding the hands, touch the ground). The slightest part of the ball touching or being directly over the goal line is sufficient for a touchdown to score. However, only the ball counts, not a player's helmet, foot, or any other part of the body. Touching one of the pylons at either end of the goal line with the ball constitutes "breaking the plane" as well.
Touchdowns are usually scored by the offense by running or passing the ball. The former is called a rushing touchdown, and in the latter, the quarterback throws a touchdown pass or passing touchdown to the receiver, who either catches the ball in the field of play and advances it into the end zone, or catches it while already being within the boundaries of the end zone; the result is a touchdown reception or touchdown catch. However, the defense can also score a touchdown if they have recovered a fumble or made an interception and return it to the opposing end zone. Special teams can score a touchdown on a kickoff or punt return, or on a return after a missed or blocked field goal attempt or blocked punt. In short, any play in which a player legally carries any part of the ball over or across the opponent's goal line scores a touchdown, as is any play in which a player legally gains possession of the ball while it is on or across his opponent's goal line and both the player and ball are legally in-bounds - beyond this, the manner in which he gained possession is inconsequential. In the NFL, a touchdown may be awarded by the referee as a penalty for a "palpably unfair act", such as a player coming off the bench during a play and tackling the runner, who would otherwise have scored.[2]
A touchdown is worth six points. The scoring team is also awarded the opportunity for an extra point or a two-point conversion.[3]  Afterwards, the team that scored the touchdown kicks off to the opposing team, if there is any time left in the half. In most codes, a conversion is not attempted if the touchdown ended the game and the conversion cannot affect the outcome.
Unlike a try scored in rugby, and contrary to the event's name, the ball does not need to touch the ground when the player and the ball are inside the end zone. The term touchdown is a holdover from gridiron's early days when the ball was required to be touched to the ground as in rugby, as rugby and gridiron were still extremely similar sports at this point. This rule was changed to the modern-day iteration in 1889.
The ability to score a touchdown on the point-after attempt (two-point conversion) was added to NCAA football in 1958 and also used in the American Football League during its ten-year run from 1960-69. It was subsequently adopted by high school football in 1969, the CFL in 1975 and the NFL in 1994.[5][6]  The short-lived World Football League, a professional American football league that operated in 1974 and 1975, gave touchdowns a seven-point value.
The National Football League (NFL) is a professional American football league that consists of 32 teams, divided equally between the American Football Conference (AFC) and the National Football Conference (NFC). The NFL is one of the major professional sports leagues in the United States and Canada and the highest professional level of American football in the world.[5] Each NFL season begins with a three-week preseason in August, followed by the 18-week regular season which runs from early September to early January, with each team playing 17 games and having one bye week. Following the conclusion of the regular season, seven teams from each conference (four division winners and three wild card teams) advance to the playoffs, a single-elimination tournament that culminates in the Super Bowl, which is contested in February and is played between the AFC and NFC conference champions. The league is headquartered in New York City.
The NFL was formed in 1920 as the American Professional Football Association (APFA) before renaming itself the National Football League for the 1922 season. After initially determining champions through end-of-season standings, a playoff system was implemented in 1933 that culminated with the NFL Championship Game until 1966. Following an agreement to merge the NFL with the rival American Football League (AFL), the Super Bowl was first held in 1967 to determine a champion between the best teams from the two leagues and has remained as the final game of each NFL season since the merger was completed in 1970.[6]
The NFL is the wealthiest professional sports league in the world by revenue[7] and the sports league with the most valuable teams.[8] The NFL also has the highest average attendance (67,591) of any professional sports league in the world[9] and is the most popular sports league in the United States.[10] The Super Bowl is also among the biggest club sporting events in the world,[11] with the individual games accounting for many of the most watched television programs in American history and all occupying the Nielsen's Top 5 tally of the all-time most watched U.S. television broadcasts by 2015.[12]
On August 20, 1920, a meeting was held by representatives of the Akron Pros, Canton Bulldogs, Cleveland Indians, and Dayton Triangles at the Jordan and Hupmobile auto showroom in Canton, Ohio.[13] This meeting resulted in the formation of the American Professional Football Conference (APFC), a group who, according to the Canton Evening Repository, intended to "raise the standard of professional football in every way possible, to eliminate bidding for players between rival clubs and to secure cooperation in the formation of schedules".[14]
Another meeting was held on September 17, 1920, with representatives from teams from four states: Akron, Canton, Cleveland, and Dayton from Ohio; the Hammond Pros and Muncie Flyers from Indiana; the Rochester Jeffersons from New York; and the Rock Island Independents, Decatur Staleys, and Racine (Chicago) Cardinals from Illinois.[15][16] The league was renamed to the American Professional Football Association (APFA).[14] The league elected Jim Thorpe as its first president, and consisted of 14 teams (the Buffalo All-Americans, Chicago Tigers, Columbus Panhandles and Detroit Heralds joined the league during the year). The Massillon Tigers from Massillon, Ohio was also at the September 17 meeting, but did not field a team in 1920. Only two of these teams, the Decatur Staleys (now the Chicago Bears) and the Chicago Cardinals (now the Arizona Cardinals), remain in the NFL.[17]
The NFL was always the largest professional football league in the United States; it nevertheless faced numerous rival professional leagues through the 1930s and 1940s. Rival leagues included at least three separate American Football Leagues and the All-America Football Conference (AAFC), on top of various regional leagues of varying caliber. Three NFL teams trace their histories to these rival leagues; the Los Angeles Rams who came from a 1936 iteration of the American Football League, and the Cleveland Browns and San Francisco 49ers, both from the AAFC. By the 1950s, the NFL had an effective monopoly on professional football in the United States; its only competition in North America was the professional Canadian football circuit, which formally became the Canadian Football League (CFL) in 1958. With Canadian football being a different football code than the American game, the CFL established a niche market in Canada and still survives as an independent league.
A new professional league, the fourth American Football League (AFL), began to play in 1960. The upstart AFL began to challenge the established NFL in popularity, gaining lucrative television contracts and engaging in a bidding war with the NFL for free agents and draft picks. The two leagues announced a merger on June 8, 1966, to take full effect in 1970. In the meantime, the leagues would hold a common draft and championship game. The game, the Super Bowl, was held four times before the merger, with the NFL winning Super Bowl I and Super Bowl II, and the AFL winning Super Bowl III and Super Bowl IV.[28] After the league merged, it was reorganized into two conferences: the National Football Conference (NFC), consisting of most of the pre-merger NFL teams, and the American Football Conference (AFC), consisting of all of the AFL teams as well as three pre-merger NFL teams.[29]
From 1920 to 1934, the NFL did not have a set number of games for teams to play, instead setting a minimum. The league mandated a twelve-game regular season for each team beginning in 1935, later shortening this to eleven games in 1937 and ten games in 1943, mainly due to World War II. After the war ended, the number of games returned to eleven games in 1946, and later back to twelve in 1947. The NFL went to a 14-game schedule in 1961, which it retained until switching to a 16-game schedule in 1978.[38] In March 2021, the NFL officially adopted a 17-game schedule after gaining the agreement of the National Football League Players Association (NFLPA).[39]
Having an odd number of games in the schedule will give half the teams nine games as the home team, while half the teams have only eight home games. To minimize the perceived benefit on competition of having more home games, the extra home game will be rotated between the two conferences each year. This is because playoff berths are allocated at the conference level, so all teams within the conference will have played the same number of home games.
The NFL operated in a two-conference system from 1933 to 1966, where the champions of each conference would meet in the NFL Championship Game. If two teams tied for the conference lead, they would meet in a one-game playoff to determine the conference champion. In 1967, the NFL expanded from 15 teams to 16 teams. Instead of just evening out the conferences by adding the expansion New Orleans Saints to the seven-member Western Conference, the NFL realigned the conferences and split each into two four-team divisions. The four division champions would meet in the NFL playoffs, a two-round playoff.[40] The NFL also operated the Playoff Bowl (officially the Bert Bell Benefit Bowl) from 1960 to 1969. Effectively, a third-place game, pitting the two conference runners-up against each other, the league considers Playoff Bowls to have been exhibitions rather than playoff games. The league discontinued the Playoff Bowl in 1970 due to its perception as a game for losers.[41]
Following the addition of the former AFL teams into the NFL in 1970, the NFL split into two conferences with three divisions each. The expanded league, now with twenty-six teams,[29] would also feature an expanded eight-team playoff, the participants being the three division champions from each conference as well as one 'wild card' team (the team with the best win percentage) from each conference. In 1978, the league added a second wild card team from each conference, bringing the total number of playoff teams to ten, and a further two wild card teams were added in 1990 to bring the total to twelve. When the NFL expanded to 32 teams in 2002, the league realigned, changing the division structure from three divisions in each conference to four divisions in each conference. As each division champion gets a playoff bid, the number of wild card teams from each conference dropped from three to two.[42] The playoffs expanded again in 2020, adding two more wild card teams to bring the total to 14 playoff teams.[43]
At the corporate level, the National Football League considers itself a trade association made up of and financed by its 32 member teams.[44] Up until 2015, the league was an unincorporated nonprofit 501(c)(6) association.[45] Section 501(c)(6) of the Internal Revenue Code provides an exemption from federal income taxation for "Business leagues, chambers of commerce, real-estate boards, boards of trade, or professional football leagues (whether or not administering a pension fund for football players), not organized for profit and no part of the net earnings of which inures to the benefit of any private shareholder or individual."[46] In contrast, each individual team (except the non-profit Green Bay Packers[47]) is subject to tax because they make a profit.[48]
The league has three defined officers: the commissioner, secretary, and treasurer. Each conference has one defined officer, the president, which is essentially an honorary position with few powers and mostly ceremonial duties, including awarding the conference championship trophy.
The commissioner is elected by the affirmative vote of two-thirds or eighteen (whichever is greater) of the members of the league, while the president of each conference is elected by an affirmative vote of three-fourths or 10 of the conference members.[50] The commissioner appoints the secretary and treasurer and has broad authority in disputes between clubs, players, coaches, and employees. He is the "principal executive officer"[51] of the NFL and also has authority in hiring league employees, negotiating television contracts, disciplining individuals that own part or all of an NFL team, clubs, or employed individuals of an NFL club if they have violated league by-laws or committed "conduct detrimental to the welfare of the League or professional football".[51] The commissioner can, in the event of misconduct by a party associated with the league, suspend individuals, hand down a fine of up to US$500,000, cancel contracts with the league, and award or strip teams of draft picks.[51]
In extreme cases, the commissioner can offer recommendations to the NFL's executive committee, up to and including the "cancellation or forfeiture"[51] of a club's franchise or any other action, he deems necessary. The commissioner can also issue sanctions up to and including a lifetime ban from the league if an individual connected to the NFL has bet on games or failed to notify the league of conspiracies or plans to bet on or fix games.[51] The current Commissioner of the National Football League is Roger Goodell, who was elected in 2006 after Paul Tagliabue, the previous commissioner, retired.[52]
The NFL consists of 32 clubs divided into two conferences of 16 teams each. Each conference is divided into four divisions of four clubs each. During the regular season, each team is allowed a maximum of 55 players on its roster; only 48 of these may be active (eligible to play) on game days.[53] Each team can also have a sixteen-player practice squad separate from its main roster.[54]
Each NFL club is granted a franchise, the league's authorization for the team to operate in its home city. This franchise covers 'Home Territory' (the 75 miles surrounding the city limits, or, if the team is within 100 miles of another league city, half the distance between the two cities) and 'Home Marketing Area' (Home Territory plus the rest of the state the club operates in, as well as the area the team operates its training camp in for the duration of the camp). Each NFL member has the exclusive right to host professional football games inside its Home Territory and the exclusive right to advertise, promote, and host events in its Home Marketing Area. There are a couple of exceptions to this rule, mostly relating to teams with close proximity to each other: teams that operate in the same city (e.g. New York City and Los Angeles) or the same state (e.g. California, Florida, and Texas) share the rights to the city's Home Territory and the state's Home Marketing Area, respectively.[55]
Every NFL team is based in the contiguous United States. Although no team is based in a foreign country, the Jacksonville Jaguars began playing one home game a year at Wembley Stadium in London, England, in 2013 as part of the NFL International Series.[56] The Jaguars' agreement with Wembley was originally set to expire in 2016 but was extended through 2020 prior to travel restrictions relating to the coronavirus.[57] The Las Vegas Raiders (then in Oakland) played one game each in the 2018 and 2019 seasons in London, while each of the Los Angeles teams (Rams, Chargers) played a game there from 2016 to 2019.[58][59][60]
The 32 teams are organized into eight geographic divisions of four teams each. These divisions are further organized into two conferences, the National Football Conference and the American Football Conference. The two-conference structure has its origins in a time when major American professional football was organized into two independent leagues, the National Football League and its younger rival, the American Football League. The leagues merged 1970, adopting the older league's name and reorganizing slightly to ensure the same number of teams in both conferences.
The NFL season format consists of a three-week preseason, an 18-week regular season (each team plays 17 games), and a 14-team single-elimination playoff culminating in the Super Bowl, the league's championship game.
The NFL preseason begins with the Pro Football Hall of Fame Game, played at Tom Benson Hall of Fame Stadium in Canton.[74] Each NFL team is required to schedule three preseason games. NFC teams must play at least two of these at home in odd numbered years and AFC teams must play at least two at home in even numbered years. However, the teams involved in the Hall of Fame game, as well as any team that played in an American Bowl game, play four preseason games.[75] Preseason games are exhibition matches and do not count towards regular-season totals.[76] Because the preseason does not count towards standings, teams generally do not focus on winning games; instead, they are used by coaches to evaluate their teams and by players to show their performance, both to their current team and to other teams if they get cut.[77] The quality of preseason games has been criticized by some fans, who dislike having to pay full price for exhibition games,[78] as well as by some players and coaches, who dislike the risk of injury the games have, while others have felt the preseason is a necessary part of the NFL season.[77][78]
Currently, the 14 opponents each team faces over the 17-game regular season schedule are set using a pre-determined formula:[79] The league runs an 18-week, 272-game regular season. Since 2021, the season has begun the week after Labor Day (the first Monday in September) and concluded the week after New Year.[80] The opening game of the season is normally a home game on a Thursday for the league's defending champion.[81]
Most NFL games are played on Sundays, with a Monday night game typically held at least once a week and Thursday night games occurring on most weeks as well.[81] NFL games are not normally played on Fridays or Saturdays until late in the regular season, as federal law prohibits professional football leagues from competing with college or high school football. Because high school and college teams typically play games on Friday and Saturday, respectively, the NFL cannot hold games on those days until the Friday before the third Saturday in December. While Saturday games late in the season are common, the league rarely holds Friday games, the most recent one being on Christmas Day in 2020.[82] NFL games are rarely scheduled for Tuesday or Wednesday, and those days have only been used three times since 1948: in 2010, when a Sunday game was rescheduled to Tuesday due to a blizzard; in 2012, when the Kickoff game was moved from Thursday to Wednesday to avoid conflict with the Democratic National Convention;[83][84] and in 2020, when a game was postponed from Sunday to Tuesday due to players testing positive for COVID-19.
Although a team's home and away opponents are known by the end of the previous year's regular season, the exact dates and times for NFL games are not determined until much later because the league has to account for, among other things, the Major League Baseball postseason and local events that could pose a scheduling conflict with NFL games. During the 2010 season, over 500,000 potential schedules were created by computers, 5,000 of which were considered "playable schedules" and were reviewed by the NFL's scheduling team. After arriving at what they felt was the best schedule out of the group, nearly 50 more potential schedules were developed to try to ensure that the chosen schedule would be the best possible one.[87]
The only other postseason event hosted by the NFL is the Pro Bowl, the league's all-star game. Since 2009, the Pro Bowl has been held the week before the Super Bowl; in previous years, the game was held the week following the Super Bowl, but in an effort to boost ratings, the game was moved to the week before.[89] Because of this, players from the teams participating in the Super Bowl are exempt from participating in the game. The Pro Bowl is not considered as competitive as a regular-season game because the biggest concern of teams is to avoid injuries to the players.[90]
The National Football League has used three different trophies to honor its champion over its existence. The first trophy, the Brunswick-Balke Collender Cup, was donated to the NFL (then APFA) in 1920 by the Brunswick-Balke Collender Corporation. The trophy, the appearance of which is only known by its description as a "silver loving cup", was intended to be a traveling trophy and not to become permanent until a team had won at least three titles. The league awarded it to the Akron Pros, champions of the inaugural 1920 season; however, the trophy was discontinued and its current whereabouts are unknown.[91]
A second trophy, the Ed Thorp Memorial Trophy, was issued by the NFL from 1934 to 1967. The trophy's namesake, Ed Thorp, was a referee in the league and a friend to many early league owners; upon his death in 1934, the league created the trophy to honor him. In addition to the main trophy, which would be in the possession of the current league champion, the league issued a smaller replica trophy to each champion, who would maintain permanent control over it. The current location of the Ed Thorp Memorial Trophy, long thought to be lost,[92] is believed to be possessed by the Green Bay Packers Hall of Fame.[93]
The current trophy of the NFL is the Vince Lombardi Trophy. The Super Bowl trophy was officially renamed in 1970 after Vince Lombardi, who as head coach led the Green Bay Packers to victories in the first two Super Bowls. Unlike the previous trophies, a new Vince Lombardi Trophy is issued to each year's champion, who maintains permanent control of it. Lombardi Trophies are made by Tiffany & Co. out of sterling silver and are worth anywhere from US$25,000 to US$300,000.[94] Additionally, each player on the winning team as well as coaches and personnel are awarded Super Bowl rings to commemorate their victory. The winning team chooses the company that makes the rings; each ring design varies, with the NFL mandating certain ring specifications (which have a degree of room for deviation), in addition to requiring the Super Bowl logo be on at least one side of the ring.[95] The losing team are also awarded rings, which must be no more than half as valuable as the winners' rings, but those are almost never worn.[96]
The conference champions receive trophies for their achievement. The champions of the NFC receive the George Halas Trophy,[97] named after Chicago Bears founder George Halas, who is also considered one of the co-founders of the NFL. The AFC champions receive the Lamar Hunt Trophy,[98] named after Lamar Hunt, the founder of the Kansas City Chiefs and the principal founder of the American Football League. Players on the winning team also receive a conference championship ring.[99][100]
The NFL recognizes a number of awards for its players and coaches at its annual NFL Honors presentation. The most prestigious award is the AP Most Valuable Player (MVP) award.[101] Other major awards include the AP Offensive Player of the Year, AP Defensive Player of the Year, AP Comeback Player of the Year, and the AP Offensive and Defensive Rookie of the Year awards.[102] Another prestigious award is the Walter Payton Man of the Year Award, which recognizes a player's off-field work in addition to his on-field performance.[103] The NFL Coach of the Year award is the highest coaching award.[104] The NFL also gives out weekly awards such as the FedEx Air & Ground NFL Players of the Week[105] and the Pepsi MAX NFL Rookie of the Week awards.[106]
In the United States, the National Football League has television contracts with five networks: ABC, CBS, ESPN, Fox, and NBC. Collectively, these contracts cover every regular season and postseason game. In general, CBS televises afternoon games in which the away team is an AFC team, and Fox carries afternoon games in which the away team belongs to the NFC. These afternoon games are not carried on all affiliates, as multiple games are being played at once; each network affiliate is assigned one game per time slot, according to a complicated set of rules.[107] Since 2011, the league has reserved the right to give Sunday games that, under the contract, would normally air on one network to the other network (known as "flexible scheduling").[108] The only way to legally watch a regionally televised game not being carried on the local network affiliates is to purchase NFL Sunday Ticket, the league's out-of-market sports package, which is only available to subscribers to the DirecTV satellite service. The league also provides RedZone, an omnibus telecast that cuts to the most relevant plays in each game, live as they happen.
In addition to the regional games, the league also has packages of telecasts, mostly in prime time, that are carried nationwide. NBC broadcasts the primetime Sunday Night Football package', which includes the Thursday NFL Kickoff game that starts the regular season and a primetime Thanksgiving Day game. ESPN carries all Monday Night Football games.[107] The NFL's own network, NFL Network, broadcasts a series titled Thursday Night Football, which was originally exclusive to the network, but which in recent years has had several games simulcast on CBS (since 2014) and NBC (since 2016) (except the Thanksgiving and kickoff games, which remain exclusive to NBC).[109] For the 2017 season, the NFL Network will broadcast 18 regular season games under its Thursday Night Football brand, 16 Thursday evening contests (10 of which are simulcast on either NBC or CBS) as well as one of the NFL International Series games on a Sunday morning and one of the 2017 Christmas afternoon games. In addition, 10 of the Thursday night games will be streamed live on Amazon Prime. In 2017, the NFL games occupied the top three rates for a 30-second advertisement: $699,602 for Sunday Night Football, $550,709 for Thursday Night Football (NBC), and $549,791 for Thursday Night Football (CBS).[110]
In addition to radio networks run by each NFL team, select NFL games are broadcast nationally by Westwood One (known as Dial Global for the 2012 season). These games are broadcast on over 500 networks, giving all NFL markets access to each primetime game. The NFL's deal with Westwood One was extended in 2012 and continued through 2017.[118] Other NFL games are nationally distributed by Compass Media Networks and Sports USA Radio Network under contracts with individual teams.
Some broadcasting innovations have either been introduced or popularized during NFL telecasts. Among them, the Skycam camera system was used for the first time in a live telecast, at a 1984 preseason NFL game in San Diego between the Chargers and 49ers, and televised by CBS.[119] Commentator John Madden famously used a telestrator during games between the early 1980s to the mid-2000s, boosting the device's popularity.[120]
The NFL, as a one-time experiment, distributed the October 25, 2015, International Series game from Wembley Stadium in London between the Buffalo Bills and Jacksonville Jaguars. The game was live streamed on the Internet exclusively via Yahoo!, except for over-the-air broadcasts on the local CBS-TV affiliates in the Buffalo and Jacksonville markets.[121][122][123]
In 2015, the NFL began sponsoring a series of public service announcements to bring attention to domestic abuse and sexual assault in response to what was seen as poor handling of incidents of violence by players.[124]
The NFL finished the new contract negotiation for the media rights deal worth over $110 billion on March 18, 2021. In this contract, ABC would be eligible to broadcast the Super Bowl on U.S. television for the first time since it broadcast Super Bowl XL after the end of the 2005 NFL season. Also in current agreement, Amazon would be the new home for Thursday Night Football starting in 2023.[125][126][127]
On August 25, 2021, the NFL sent a memo to all 32 teams that only fully vaccinated personnel, with a maximum of 50 people, will have access to locker rooms while players are present on game days. The memo also stated that non-club-affiliated media are not permitted in the locker room.[128]
On February 9, 2022, as part of efforts to increase the sport's international reach, the NFL announced that Munich will host its first regular-season game in Germany in 2022.[129]
Each April (excluding 2014 when it took place in May), the NFL holds a draft of college players. The draft consists of seven rounds, with each of the 32 clubs getting one pick in each round.[130] The draft order for non-playoff teams is determined by regular-season record; among playoff teams, teams are first ranked by the furthest round of the playoffs they reached, and then are ranked by regular-season record. For example, any team that reached the divisional round will be given a higher pick than any team that reached the conference championships, but will be given a lower pick than any team that did not make the divisional round. The Super Bowl champion always drafts last, and the losing team from the Super Bowl always drafts next-to-last.[131] All potential draftees must be at least three years removed from high school in order to be eligible for the draft.[132] Underclassmen that have met that criterion to be eligible for the draft must write an application to the NFL by January 15 renouncing their remaining college eligibility.[133] Clubs can trade away picks for future draft picks, but cannot trade the rights to players they have selected in previous drafts.[134]
Aside from the seven picks each club gets, compensatory draft picks are given to teams that have lost more compensatory free agents than they have gained. These are spread out from rounds 3 to 7, and a total of 32 are given.[135] Clubs are required to make their selection within a certain period of time, the exact time depending on which round the pick is made in. If they fail to do so on time, the clubs behind them can begin to select their players in order, but they do not lose the pick outright. This happened in the 2003 draft, when the Minnesota Vikings failed to make their selection on time. The Jacksonville Jaguars and Carolina Panthers were able to make their picks before the Vikings were able to use theirs.[136] Selected players are only allowed to negotiate contracts with the team that picked them, but if they choose not to sign they become eligible for the next year's draft.[137] Under the current collective bargaining contract, all contracts to drafted players must be four-year deals with a club option for a fifth. Contracts themselves are limited to a certain amount of money, depending on the exact draft pick the player was selected with.[138] Players who were draft eligible but not picked in the draft are free to sign with any club.[130]
The NFL operates several other drafts in addition to the NFL draft. The league holds a supplemental draft annually. Clubs submit emails to the league stating the player they wish to select and the round they will do so, and the team with the highest bid wins the rights to that player. The exact order is determined by a lottery held before the draft, and a successful bid for a player will result in the team forfeiting the rights to its pick in the equivalent round of the next NFL draft.[139] Players are only eligible for the supplemental draft after being granted a petition for special eligibility.[140] The league holds expansion drafts, the most recent happening in 2002 when the Houston Texans began play as an expansion team.[141] Other drafts held by the league include an allocation draft in 1950 to allocate players from several teams that played in the dissolved All-America Football Conference[142] and a supplemental draft in 1984 to give NFL teams the rights to players who had been eligible for the main draft but had not been drafted because they had signed contracts with the United States Football League or Canadian Football League.[143]
Like the other major sports leagues in the United States, the NFL maintains protocol for a disaster draft. In the event of a 'near disaster' (less than 15 players killed or disabled) that caused the club to lose a quarterback, they could draft one from a team with at least three quarterbacks. In the event of a 'disaster' (15 or more players killed or disabled) that results in a club's season being canceled, a restocking draft would be held. Neither of these protocols has ever had to be implemented.[144]
Free agents in the National Football League are divided into restricted free agents, who have three accrued seasons and whose current contract has expired, and unrestricted free agents, who have four or more accrued seasons and whose contract has expired. An accrued season is defined as "six or more regular-season games on a club's active/inactive, reserved/injured or reserve/physically unable to perform lists".[145] Restricted free agents are allowed to negotiate with other clubs besides their former club, but the former club has the right to match any offer. If they choose not to, they are compensated with draft picks. Unrestricted free agents are free to sign with any club, and no compensation is owed if they sign with a different club.[145]
Clubs are given one franchise tag to offer to any unrestricted free agent. The franchise tag is a one-year deal that pays the player 120% of his previous contract or no less than the average of the five highest-paid players at his position, whichever is greater. There are two types of franchise tags: exclusive tags, which do not allow the player to negotiate with other clubs, and non-exclusive tags, which allow the player to negotiate with other clubs but gives his former club the right to match any offer and two first-round draft picks if they decline to match it.[146]
Members of clubs' practice squads, despite being paid by and working for their respective clubs, are also simultaneously a kind of free agent and are able to sign to any other club's active roster (provided their new club is not their previous club's next opponent within a set number of days) without compensation to their previous club; practice squad players cannot be signed to other clubs' practice squads, however, unless released by their original club first.[151]
Altria Group, Inc. (previously known as Philip Morris Companies, Inc.) is an American corporation and one of the world's largest producers and marketers of tobacco, cigarettes and related products. It operates worldwide and is headquartered in Henrico County, Virginia, just outside the city of Richmond.
Altria is the parent company of Philip Morris USA (producer of Marlboro cigarettes), John Middleton, Inc., U.S. Smokeless Tobacco Company, Inc., and Philip Morris Capital Corporation. Altria also maintains large minority stakes in Belgium-based brewer AB InBev, the Canadian cannabis company Cronos Group, and the e-cigarette maker Juul. It is a component of the S&P 500 and was a component of the Dow Jones Industrial Average from 1985 to 2008, dropping due to spin-offs of Kraft Foods Inc. in 2007 and Philip Morris International in 2008.[3]
The company's branding consultants, the Wirthlin Group, said: "The name change alternative offers the possibility of masking the negatives associated with the tobacco business", thus enabling the company to improve its image and raise its profile without sacrificing tobacco profits.[8]
Philip Morris executives thought a name change would insulate the larger corporation and its other operating companies from the political pressures on tobacco.[8]
The rebranding took place amidst social, legal, and financially troubled circumstances.[vague][9] In 2003 Altria was ranked Fortune number 11, and has steadily declined since. In 2010 Altria Group (MO) ranked at Fortune number 137, whereas its former asset, Philip Morris International, was ranked 94th.[10]
In 2006, a United States court found that Philip Morris "publicly ... disputed scientific findings linking smoking and disease knowing their assertions were false."[11] In a 2006 ruling, a federal court found that Altria, along with R. J. Reynolds Tobacco, Lorillard and Philip Morris were found guilty of misleading the public about the dangers of smoking.[12] Within this ruling, it was noted that "defendants altered the chemical form of nicotine delivered in mainstream cigarette smoke for the purpose of improving nicotine transfer efficiency and increasing the speed with which nicotine is absorbed by smokers."[13] This was done by manipulating smoke pH with ammonia. Adding ammonia increases the smoke pH, in a process called "freebasing" which causes smokers to be "exposed to higher internal nicotine doses and become more addicted to the product."[14]
On March 30, 2007, Altria's 88.1% stake in Kraft Foods was spun off, through a distribution of the remaining stake of shares (88.1%) to Altria shareholders. That same year, Altria began selling all its shares of Philip Morris International to Altria stockholders, a spin-off that was completed on March 28, 2008. Again in 2007 the company began the acquisition of cigar manufacturer John Middleton Co. from Bradford Holdings, which was complete in 2008. After Philip Morris International spun off, the former international subsidiaries halted the purchase of tobacco from America, which was a major factor in the closing of a newly renovated plant in North Carolina, an approximately 50% reduction in manufacturing, large-scale layoffs, and induced early retirements.[15]
In 2008, Altria officially moved its headquarters from New York City to Richmond, Virginia, after Philip Morris sold its downtown offices in New York City a decade earlier. With a few exceptions, all manufacturing, commercial, and executive employees had long been based in and around Richmond. Currently the company is headquartered in an unincorporated area within Henrico County, less than five miles west of the city limits of Richmond and less than ten miles from its downtown Richmond campus.
In 2009, Altria finalized its purchase of UST Inc., whose products included smokeless tobacco (made by U.S. Smokeless Tobacco Company) and wine (made by Chateau Ste. Michelle).[16] This ended a short era of competition between the new Marlboro smokeless tobacco products such as snus, and those produced by UST Inc.
On December 8, 2018, Altria announced its intent to acquire a 45% stake in Cronos Group for $1.8 billion.[17]
On December 20, 2018, Altria finalized the acquisition of a 35% stake in JUUL Labs, an e-cigarette company based out of San Francisco, California, for $12.8 billion.[18] On November 3, 2019, it was reported that Altria was taking a $4.5 billion writedown on its stake in Juul, 35% of its original value.[19] On July 28, 2022, it was reported that Altria's investment in Juul is now worth only 5% of the original amount of $12.8 billion. Despite the losses, Altria has announced that it will continue to support Juul and avoid investing in competing products.[20]
Altria and Japan Tobacco announced a joint venture called Horizon Innovations LLC on October 27, 2022. Horizon, owned 75 percent by Altria and 25 percent by Japan Tobacco, intends to sell Ploom heated tobacco sticks in the United States. FDA approval was expected to take until 2025, with customers able to buy Ploom by 2027.[21]
For the fiscal year 2020, Altria reported earnings of US$4.45 billion, with an annual revenue of US$26.15 billion. Altria's shares traded at over $66 per share, and its market capitalization was valued at over US$118.5 billion in October 2018.[22] As of 2018, the company ranked 154th on the Fortune 500 list of the largest United States corporations by revenue.[23]
Members of the board of directors of Altria Group as of February 2013 were:[25]
Prior to being based in Virginia, Philip Morris had its headquarters in Midtown Manhattan, New York City.[26] In 2003, Philip Morris announced that it would move its headquarters to Virginia. The company said that it planned to keep around 750 employees in its former headquarters. Brendan McCormick, a spokesperson for Philip Morris, said that the company estimated that the move would save the company over $60 million each year.[27] The company now has its head offices in an unincorporated area of Henrico County, Virginia, near Richmond.[28] In addition, the company has a 450,000-square-foot, $350 million Center for Research and Technology located in downtown Richmond at the Virginia BioTechnology Research Park that employs approximately 600 scientists, engineers and support staff.
According to the Center for Public Integrity, Altria spent around $101 million on lobbying the United States government between 1998 and 2004, the second-highest such figure for any organization in the nation.[29][30]
Altria also funded The Advancement of Sound Science Coalition which lobbied against the scientific consensus on anthropogenic climate change.[31]
Daniel Smith, representing Altria, sits on the Private Enterprise Board of the American Legislative Exchange Council.[32]
In August 2006, the Altria group was found guilty of civil fraud and racketeering.[33][34] The lawsuit claimed that Altria's marketing of "light" and "low tar" cigarettes constituted fraudulent misrepresentations under the Maine Unfair Trade Practices Act (MUTPA) because it deceived smokers into thinking the products are safer than regular cigarettes.
Since its launch, Capri-Sun has been packaged in laminated foil vacuum Doy-N-Pack pouches, with which the brand has become strongly associated. In the United States, these pouches were innovative as the first single-serving fruit juice containers. The pouch design has stayed largely the same, but changes in some markets have included transparent bottoms and paper straws. Capri-Sun is available in varying ranges of flavors in different countries, targeting different national flavor profiles. Globally, its best-known flavor is Orange.
Capri-Sun's main products are high in sugar content, although lower than many competitors. Characterizations of the juice drinks as "all-natural" have led to conflict in several countries between consumer advocates who highlight the high sugar content and low juice percentage, and Capri-Sun and its licensees, who in most cases have maintained that the term correctly describes the ingredients. In the United States, Capri Sun Inc.[b] was owned by tobacco conglomerate Philip Morris Cos. (now[update] Altria) from 1991 to 2007. Prohibited from selling tobacco to children, Philip Morris executives applied their experience to sugary drinks including Capri Sun,[3] adding the pouches to Lunchables and advertising them through a "California cool" aesthetic. Kraft continued to use Philip Morris's techniques after it became independent in 2007.[4]
Capri Sun is one of the most favorably-rated brands in the United States among Generation Z. About a third of American parents view the drink as healthy. In France, Capri-Sun has figured prominently in rap songs and has been noted as a drink of choice in poor suburban areas. Disputes over sugar content, "all-natural" status, and marketing to children have led to two class actions in the United States (one of which led to the abandonment of "all-natural" for a time), the removal of Capri-Sun from Tesco shelves in the United Kingdom, and a negative award from Foodwatch for deceptive marketing to children.
Rudolf Wild of Heidelberg founded Rudolf Wild & Co. (better known as Wild) in the German Reich in 1931. After World War II, Rudolf Wild created Libella, which the Lexington Herald-Leader in 1998 described as "the first all-natural soda made with real fruit juice". This led him to several new products, including Capri-Sonne,[5] which was developed in the 1960s.[6] Restrictions on color additives at the time in West Germany led to less visually appealing soft drinks, incentivizing opaque packaging.[7] Rudolf Wild & Co. engaged with Thimonnier, a French company that primarily manufactured sewing machines, for rights to use their patented pouch design and patented pouch-making machines.[8] According to Rudolf's son Hans-Peter Wild, Rudolf Wild & Co. did not obtain exclusive rights from Thimonnier, but rather by ordering all of the machines they made.[9] After initial issues with spoilage and stains were resolved,[2] the product debuted in West Germany in 1969.[10] The name references the Italian island of Capri due to its status as a vacation destination.[11]
SiSi-Werke changed its name to Capri Sun GmbH in December 2018. Capri Sun GmbH is organized and headquartered in Germany and is a subsidiary of Swiss companies Capri Sun AG and Capri Sun Group Holding AG[b] and German company Wild;[16] this makes it unusual in a largely American-led beverage industry. Wild licenses the brand to different companies, which as of 2009[update] bottle Capri-Sun in 18 countries; Wild subsidiary INDAG supplies the various bottling plants.[2]
In 1979, Shasta Beverages (then a part of Consolidated Foods) began to license the drink from Wild under the name Capri Sun.[18] After promising test runs in Buffalo, New York, and Atlanta, Georgia,[19] Shasta began a rolling expansion, starting with the Midwestern and Southeastern United States in 1980 and 1981.[20]
When Shasta introduced the product in the United States, its single-serving packaging was unusual in contrast with the 46-fluid-ounce (1.4L) aluminum cans that dominated the fruit juice market. Despite initial issues on rollout, the packaging carried the advantage of being light, durable, blunt, long-lasting, freezable, and insular. The patented design, trademarked under the name Doy-N-Pack and exclusively licensed by Shasta from Wild, soon faced a competitor in aseptic "brick packaging" like Tetra Pak. Both proved popular in stores, and Doy-N-Pack would usher in the use of pouches for single-serving food and beverage containers in the United States.[21]
Kraft's acquisition of Capri Sun Inc. included marketing rights of Capri Sun in Canada, Mexico, and the U.S. territory of Puerto Rico.[36] Kraft announced in 2000 that it would be launching Capri Sun in Mexico, imported from the U.S.;[37] in 2013, Jumex, rather than Kraft, announced plans to bring Capri-Sun to the Mexican market for the first time, with Capri Sun AG and sister company WILD-INDAG providing equipment and support.[38] Capri Sun was being advertised in Canada since by March 1991,[39] continuing after Kraft's purchase;[40] a 2009 CNN Money article noted Kraft as the distributor there.[41] As of 2022[update], Kraft licenses the Capri Sun brand in the United States, Canada, and Puerto Rico.[42]
In January 2007, a Florida woman, backed by the Center for Science in the Public Interest, sued Kraft for deceptive packaging, alleging that its usage of high-fructose corn syrup (HFCS) made its claimed "all-natural" status inaccurate. Kraft announced a day later that they would cease labeling Capri Sun that way as part of a planned reformulation and repackaging, replacing the words with "no artificial colors, flavors, or preservatives".[43] In 2015, facing declining sales, Kraft switched the main Capri Sun line from HFCS to sugar and switched the Roarin' Waters flavored water brand from HFCS and sucralose to sugar and Stevia.[44]
In 2017, Capri Sun GmbH sued American Beverage, claiming that the company's use of pouches to market SunnyD constituted trademark infringement. The U.S. District Judge, Paul A. Engelmayer, found conflicting evidence as to whether the pouch design was regularly associated with Capri Sun by consumers, with Capri Sun GmbH asserting the notion and American Beverage disputing it. Capri Sun GmbH has sued other companies for infringement on the design in the past, securing a $650,000 licensing fee from Faribault Foods in one such lawsuit. As of 2022[update], the case was scheduled for a jury trial.[45]
In the 2020s, Capri Sun has been noted for its marketing to parents, still in the hope that they would give the drink to their children. In 2020, Capri Sun donated 5 million pouches of water labelled "we're sorry it's not juice" to schools in Granite City, Illinois, and the Chicago metropolitan area.[46] The accompanying ad campaign, according to Ad Age, was targeted towards parents in the area who were concerned about COVID-19 pandemic safety restrictions shutting down water fountains.[47] In 2022, the company released an advertisement more directly targeted at parents, starring a character modeled after the male leads of romance novels designed to disinterest children, before changing tack to pitch Capri Sun.[48]
In January 2023, a class action was filed against Capri Sun in the United States District Court for the Southern District of New York over its branded apple juice; Eboni Forbes, the class representative, claimed that its use of citric acid as an ingredient contravened its claim of having "no artificial colors, flavors or preservatives".[49]
Capri-Sun established a factory in Nigeria in 1980.[50] As of 2023[update], Capri-Sun is the most popular fruit juice drink among children in Nigeria, where it is licensed by Chi Limited, a Coca-Cola Company subsidiary.[51]
SiSi-Werke attempted to secure trademarks for eight of its pouch designs in the European Union; the European Court of Justice, which has consistently rejected trademarks based on product shape, turned the request down in 2006.[63] On 21 February 2017, SiSi-Werke announced that it would be renaming Capri-Sonne to Capri-Sun in Germany, the last country to retain the original name. The brand faced some criticism for the change, some of it lighthearted in tone; it did not rule out reintroducing the name at a later date.[6]
A Capri-Sun executive told China Daily in 2016 that they produce 27 flavors worldwide.[60] The best-known flavor globally is Orange.[64] In the United States, an initial roster of Orange, Apple, Lemonade, and Fruit Punch[65] has expanded to include Mountain Cooler (apple and raspberry), Strawberry Kiwi, Wild Cherry, Pacific Cooler (apple, cherry, pineapple, and raspberry), Tropical Punch (strawberry, orange, pineapple, lemon, and lime),[66] and Grape Geyser.[67] Options vary by country: In France, for instance, only Classic (orange), Crush, and Bio flavors are sold;[68] the United Kingdom has Orange, Tropical, Blackcurrant, and Cherry;[69] and the Chinese market sees Orange, White Grape, Pear, and Peach Apple.[70] Flavor profile varies as well: German fruit flavors are riper than French ones. Flavors in China, Mexico, and the United Arab Emirates are sweeter than those used in Europe, which Capri-Sun says is to cater to local tastes; likewise Hungarian cherry flavors are sourer than elsewhere.[71]
In addition to the main line of fruit-juice-based beverages, American Capri Sun products have included a 100% juice variant[72] and Roarin' Waters, a line of flavored waters.[73] The United Kingdom has its own flavored water line, Fruity Water,[74] as well as squash (concentrated syrup).[75]
Capri-Sun drinks are canned in laminate vacuum pouches.[77] In 2021, a TikTok video went viral after a father discovered mold in his daughter's Capri Sun through the package's clear bottom; Capri Sun stated that finding contaminants are the purpose of the clear bottom,[78] that the mold is naturally occurring, and that testing of the packaging showed that it was not sealed properly. A year later, a recall was announced on over 5,000 Capri Sun Wild Cherry pouches after it was discovered that some were contaminated with cleaning supplies.[79] Capri Sun also faced criticism in the United Kingdom for introducing paper straws to its pouches, a move it said was environmentally friendly; consumers complained about the straw's inability to pierce the pouch in opening the drink, as well as the still-existing plastic wrapping on the straw, which Capri-Sun said was required under British law.[80]
In addition to the well-known Doy-N-Pak pouch, Capri-Sun comes in other packaging in various markets, including a resealable pouch with safety cap used on some products.[81] The squash lines are sold in one-liter plastic bottles.[75]
A 2022 review of fruit punch drinks in the Marin Independent Journal gave Capri Sun All Natural Fruit Punch two stars, noting its lower sugar content compared to other listed sugary drinks but criticizing its taste as "watery" and non-evocative of the fruits depicted on the label.[84] A 2017 review of "your kids' lunch box favorites" by Brooke Jackson-Glidden in the Statesman Journal noted the Capri Sun Strawberry Kiwi's 13 grams of sugar, praising its moderately sweet taste and small size. The paper's "resident intern", Young Cooper, commented that it was "definitely not the best flavor of Capri Sun."[85] Marnie Shure of The Takeout, reviewing Capri Sun fruit punch after the switch to monk fruit, wrote that the sweetness had now become the primary flavor, rather than notes of actual fruit as before, and assessed a perhaps 5% increase in tartness, but complimented the lack of aftertaste she associated with most sweeteners.[83] Chad Eschman of Vinepair reviewed Capri Sun flavors as they relate to creating mixers; reviews included positively rating the combination of gin and Pacific Cooler as tasting like a large white gummy bear and negatively rating the combination of tequila and Tropical Cooler as "we've made a huge mistake".[66]
Roarin' Waters faced early criticism for its sugariness and lack of juice.[73] Gannett News Service's Kim Painter characterized it as "a reduced-calorie fruit drink, apparently made for children who expect all drinks, even water, to be sweet",[86] while James Lileks of the Minneapolist Star Tribune wrote that his daughter thought it tasted like Easy Mac.[87]
"Go ahead, stick the straw, stick the straw in the Capri-Sun."
A 2015 study in Public Health Nutrition on American parents' attitudes towards sugary drinks found that 36% of surveyed parents with children between the ages of 2 and 17 rated Capri Sun as "somewhat" or "very healthy", with 48% saying that they gave the drink to their children in that age. Black and Hispanic parents were significantly more likely to rate Capri Sun as healthy than white parents, and the rating was higher than sugary fruit drinks as a category, which only 30% of parents gave the same rating. Regarding Roarin' Waters, 39% rated the same, but only 16% said that they give their children the drink. Hispanic parents were significantly more likely than white parents to rate the product as healthy, although black parents were not. Roarin' Waters was one of a few products to be rated as less healthy than its category overall; 48% rated flavored water drinks as healthy. The study concluded that those parents may have selected Capri Sun Roarin' Waters because they consider it a healthier option.[91]
A 2013 online poll from Foodwatch, a European consumer protection group, resulted in Capri-Sun receiving a "Golden Windbag" award for perceived deceptive advertising to children. Capri-Sun denied that its advertising was targeted towards children.[92]
A 2022 Morning Consult survey of American Generation Z adults ranked Capri Sun in 17th place out of over 4,000 on a list of their most favored brands, with 77% rating the brand favorably.[95] Capri Sun was also one of the brands with the largest differential between Gen Z and older peers; the brand's favorability rating with Gen Z was 16 percentage points higher than the U.S. adult population at large, 19th highest in the brands surveyed, and 7 percentage points higher than the Millennial respondents, for 16th place.[96]
Decades of political controversy over slavery were brought to a head by the victory in the 1860 U.S. presidential election of Abraham Lincoln, who opposed slavery's expansion into the western territories. An initial seven southern slave states responded to Lincoln's victory by seceding from the United States and, in February 1861, forming the Confederacy. The Confederacy seized U.S. forts and other federal assets within their borders. Led by Confederate President Jefferson Davis, the Confederacy asserted control over about a third of the U.S. population in eleven of the 34 U.S. states that then existed. Four years of intense combat, mostly in the South, ensued.
A wave of Confederate surrenders followed. On April 14, just five days after Lee's surrender, Lincoln was assassinated. As a practical matter, the war ended with the May 26 surrender of the Department of the Trans-Mississippi but the conclusion of the American Civil War lacks a clear and precise historical end date. Confederate ground forces continued surrendering past the May 26 surrender date until June 23. By the end of the war, much of the South's infrastructure was destroyed, especially its railroads. The Confederacy collapsed, slavery was abolished, and four million enslaved black people were freed. The war-torn nation then entered the Reconstruction era in an attempt to rebuild the country, bring the former Confederate states back into the United States, and grant civil rights to freed slaves.
The Civil War is one of the most extensively studied and written about episodes in U.S. history. It remains the subject of cultural and historiographical debate. Of particular interest is the persisting myth of the Lost Cause of the Confederacy. The American Civil War was among the first wars to use industrial warfare. Railroads, the telegraph, steamships, the ironclad warship, and mass-produced weapons were all widely used during the war. In total, the war left between 620,000 and 750,000 soldiers dead, along with an undetermined number of civilian casualties, making the Civil War the deadliest military conflict in American history.[g] The technology and brutality of the Civil War foreshadowed the coming World Wars.
Disagreements among states about the future of slavery were the main cause of disunion and the war that followed.[20][21] Slavery had been controversial during the framing of the Constitution, which, because of compromises, ended up with proslavery and antislavery features.[22] The issue of slavery had confounded the nation since its inception and increasingly separated the United States into a slaveholding South and a free North. The issue was exacerbated by the rapid territorial expansion of the country, which repeatedly brought to the fore the question of whether new territory should be slaveholding or free. The issue had dominated politics for decades leading up to the war. Key attempts to resolve the matter included the Missouri Compromise and the Compromise of 1850, but these only postponed an inevitable showdown over slavery.[23]
The American Revolution and the cause of liberty added tremendous impetus to the abolitionist cause. Slavery, which had been around for thousands of years, was considered normal and was not a significant issue of public debate prior to the Revolution. The Revolution changed that and made it into an issue that had to be addressed. As a result, during and shortly after the Revolution, the Northern states quickly started outlawing slavery. Even in Southern states, laws were changed to limit slavery and facilitate manumission. The amount of indentured servitude dropped dramatically throughout the country. An Act Prohibiting Importation of Slaves sailed through Congress with little opposition. President Thomas Jefferson supported it, and it went into effect on January 1, 1808, which was the first day that the Constitution (Article I, section 9, clause 1) permitted Congress to prohibit the importation of slaves. Benjamin Franklin and James Madison each helped found manumission societies. Influenced by the Revolution, many slave owners freed their slaves, but some, such as George Washington, did so only in their wills. The number of free black people as a proportion of the black population in the upper South increased from less than 1 percent to nearly 10 percent between 1790 and 1810 as a result of these actions.[40][41][42][43][44][45]
In the decades leading up to the Civil War, abolitionists, such as Theodore Parker, Ralph Waldo Emerson, Henry David Thoreau and Frederick Douglass, repeatedly used the Puritan heritage of the country to bolster their cause. The most radical anti-slavery newspaper, The Liberator, invoked the Puritans and Puritan values over a thousand times. Parker, in urging New England congressmen to support the abolition of slavery, wrote, "The son of the Puritan . . . is sent to Congress to stand up for Truth and Right."[48][49] Literature served as a means to spread the message to common folks. Key works included Twelve Years a Slave, the Narrative of the Life of Frederick Douglass, American Slavery as It Is, and the most important: Uncle Tom's Cabin, the best-selling book of the 19th century aside from the Bible.[50][51][52]
A more unusual abolitionist than those named above was Hinton Rowan Helper, whose 1857 book, The Impending Crisis of the South: How to Meet It, "[e]ven more perhaps than Uncle Tom's Cabin ... fed the fires of sectional controversy leading up to the Civil War."[53] A Southerner and a virulent racist, Helper was nevertheless an abolitionist because he believed, and showed with statistics, that slavery "impeded the progress and prosperity of the South, ... dwindled our commerce, and other similar pursuits, into the most contemptible insignificance; sunk a large majority of our people in galling poverty and ignorance, ... [and] entailed upon us a humiliating dependence on the Free States...."[54]
By 1840 more than 15,000 people were members of abolitionist societies in the United States. Abolitionism in the United States became a popular expression of moralism, and led directly to the Civil War. In churches, conventions and newspapers, reformers promoted an absolute and immediate rejection of slavery.[55][56] Support for abolition among the religious was not universal though. As the war approached, even the main denominations split along political lines, forming rival Southern and Northern churches. For example, in 1845 the Baptists split into the Northern Baptists and Southern Baptists over the issue of slavery.[57][58]
Abolitionist sentiment was not strictly religious or moral in origin. The Whig Party became increasingly opposed to slavery because it saw it as inherently against the ideals of capitalism and the free market. Whig leader William H. Seward (who would serve as Lincoln's secretary of state) proclaimed that there was an "irrepressible conflict" between slavery and free labor, and that slavery had left the South backward and undeveloped.[59] As the Whig party dissolved in the 1850s, the mantle of abolition fell to its newly formed successor, the Republican Party.[60]
Manifest destiny heightened the conflict over slavery. Each new territory acquired had to face the thorny question of whether to allow or disallow the "peculiar institution".[61] Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. Pro- and anti-slavery forces collided over the territories west of the Mississippi.[62]
By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly.[70] The first of these theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a constitutional mandate. The failed Crittenden Compromise of 1860 was an expression of this view.[71]
The fourth doctrine was advocated by Mississippi Senator (and soon to be Confederate President) Jefferson Davis.[78] It was one of state sovereignty ("states' rights"),[79] also known as the "Calhoun doctrine",[80] named after the South Carolinian political theorist and statesman John C. Calhoun.[81] Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution.[82] These four doctrines comprised the dominant ideologies presented to the American public on the matters of slavery, the territories, and the U.S. Constitution before the 1860 presidential election.[83]
Historian James McPherson points out that even if Confederates genuinely fought over states' rights, it boiled down to states' right to slavery.[87] McPherson writes concerning states' rights and other non-slavery explanations:
While one or more of these interpretations remain popular among the Sons of Confederate Veterans and other Southern heritage groups, few professional historians now subscribe to them. Of all these interpretations, the states'-rights argument is perhaps the weakest. It fails to ask the question, states' rights for what purpose? States' rights, or sovereignty, was always more a means than an end, an instrument to achieve a certain goal more than a principle.[87]
States' rights was an ideology formulated and applied as a means of advancing slave state interests through federal authority.[89] As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of Federal power."[90][91] Before the Civil War, the Southern states supported the use of federal powers to enforce and extend slavery, as with the Fugitive Slave Act of 1850 and the Dred Scott v. Sandford decision.[92][93] The faction that pushed for secession often infringed on states' rights. Because of the overrepresentation of pro-slavery factions in the federal government, many Northerners, even non-abolitionists, feared the Slave Power conspiracy.[92][93] Some Northern states resisted the enforcement of the Fugitive Slave Act. Historian Eric Foner states that the act "could hardly have been designed to arouse greater opposition in the North. It overrode numerous state and local laws and legal procedures and 'commanded' individual citizens to assist, when called upon, in capturing runaways." He continues, "It certainly did not reveal, on the part of slaveholders, sensitivity to states' rights."[85] According to historian Paul Finkelman, "the southern states mostly complained that the northern states were asserting their states' rights and that the national government was not powerful enough to counter these northern claims."[86] The Confederate Constitution also "federally" required slavery to be legal in all Confederate states and claimed territories.[84][94]
Sectionalism resulted from the different economies, social structure, customs, and political values of the North and South.[95][96] Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention, which manifested Northern dissatisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence agriculture for poor whites. In the 1840s and 1850s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist, and Presbyterian churches) into separate Northern and Southern denominations.[97]
Historians have debated whether economic differences between the mainly industrial North and the mainly agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s, and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.[98][99]
Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entirety of the United States (called "Southern Unionists") and those loyal primarily to the Southern region and then the Confederacy.[105]
Perceived insults to Southern collective honor included the enormous popularity of Uncle Tom's Cabin and abolitionist John Brown's attempt to incite a slave rebellion in 1859.[106][107]
While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it.[108] The South ignored the warnings; Southerners did not realize how ardently the North would fight to hold the Union together.[109]
The election of Abraham Lincoln in November 1860 was the final trigger for secession.[110] Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction.[111] However, Lincoln would not be inaugurated until five months after the election, which gave the South time to secede and prepare for war in the winter and spring of 1861.[112]
According to Lincoln, the American people had shown that they had been successful in establishing and administering a republic, but a third challenge faced the nation: maintaining a republic based on the people's vote, in the face of an attempt to destroy it.[113]
The election of Lincoln provoked the legislature of South Carolina to call a state convention to consider secession. Before the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws, and even to secede from the United States. The convention unanimously voted to secede on December 20, 1860, and adopted a secession declaration. It argued for states' rights for slave owners in the South, but contained a complaint about states' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.[114]
As Southerners resigned their seats in the Senate and the House, Republicans were able to pass projects that had been blocked by Southern senators before the war. These included the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railroad Acts),[123] the National Bank Act, the authorization of United States Notes by the Legal Tender Act of 1862, and the ending of slavery in the District of Columbia. The Revenue Act of 1861 introduced the income tax to help finance the war.[124]
In December 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of the Southern states, but Lincoln and the Republicans rejected it.[125] Lincoln stated that any compromise that would extend slavery would in time bring down the Union.[126] A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise; it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.[127]
On March 4, 1861, Abraham Lincoln was sworn in as president. In his inaugural address, he argued that the Constitution was a more perfect union than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void".[128] He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of federal property,[128] including forts, arsenals, mints, and customhouses that had been seized by the Southern states.[129] The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify the armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.[128]
The Davis government of the new Confederacy sent three delegates to Washington to negotiate a peace treaty with the United States of America. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government.[130] Lincoln instead attempted to negotiate directly with the governors of individual seceded states, whose administrations he continued to recognize.[citation needed]
Complicating Lincoln's attempts to defuse the crisis were the actions of the new Secretary of State, William Seward. Seward had been Lincoln's main rival for the Republican presidential nomination. Shocked and embittered by this defeat, Seward agreed to support Lincoln's candidacy only after he was guaranteed the executive office that was considered at that time to be the most powerful and important after the presidency itself. Even in the early stages of Lincoln's presidency Seward still held little regard for the new chief executive due to his perceived inexperience, and therefore Seward viewed himself as the de facto head of government or "prime minister" behind the throne of Lincoln. In this role, Seward attempted to engage in unauthorized and indirect negotiations that failed.[130] However, President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy: Fort Monroe in Virginia, Fort Pickens, Fort Jefferson and Fort Taylor in Florida, and Fort Sumter in South Carolina.[131][citation needed]
The American Civil War began on April 12, 1861, when Confederate forces opened fire on the Union-held Fort Sumter. Fort Sumter is located in the middle of the harbor of Charleston, South Carolina.[132] Its status had been contentious for months. Outgoing President Buchanan had dithered in reinforcing the Union garrison in the harbor, which was under command of Major Robert Anderson. Anderson took matters into his own hands and on December 26, 1860, under the cover of darkness, sailed the garrison from the poorly placed Fort Moultrie to the stalwart island Fort Sumter.[133] Anderson's actions catapulted him to hero status in the North. An attempt to resupply the fort on January 9, 1861, failed and nearly started the war then and there. But an informal truce held.[134] On March 5, the newly sworn in Lincoln was informed that the Fort was running low on supplies.[135]
Fort Sumter proved to be one of the main challenges of the new Lincoln administration.[135] Back-channel dealing by Secretary of State Seward with the Confederates undermined Lincoln's decision-making; Seward wanted to pull out of the fort.[136] But a firm hand by Lincoln tamed Seward, and Seward became one of Lincoln's staunchest allies. Lincoln ultimately decided that holding the fort, which would require reinforcing it, was the only workable option. Thus, on April 6, Lincoln informed the Governor of South Carolina that a ship with food but no ammunition would attempt to supply the Fort. Historian McPherson describes this win-win approach as "the first sign of the mastery that would mark Lincoln's presidency"; the Union would win if it could resupply and hold onto the Fort, and the South would be the aggressor if it opened fire on an unarmed ship supplying starving men.[137] An April 9 Confederate cabinet meeting resulted in President Davis's ordering General P. G. T. Beauregard to take the Fort before supplies could reach it.[138]
Maryland, Delaware, Missouri, and Kentucky were slave states whose people had divided loyalties to Northern and Southern businesses and family members. Some men enlisted in the Union Army and others in the Confederate Army.[144] West Virginia separated from Virginia and was admitted to the Union on June 20, 1863.[145]
In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state (see also: Missouri secession). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.[152]
Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status while maintaining slavery. During a brief invasion by Confederate forces in 1861, Confederate sympathizers organized a secession convention, formed the shadow Confederate Government of Kentucky, inaugurated a governor, and gained recognition from the Confederacy. Its jurisdiction extended only as far as Confederate battle lines in the Commonwealth, and it went into exile after October 1862.[153]
A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.[159]
The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book The American Civil War, British historian John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". In many cases, without geographic objectives, the only target for each side was the enemy's soldier.[160]
As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias.[161] The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 soldiers for one year or the duration, and that was answered in kind by the U.S. Congress.[162][163][164]
When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft.[167] Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their services conscripted.[168]
In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war.[169] At least 100,000 Southerners deserted, or about 10 percent; Southern desertion was high because, according to one historian writing in 1991, the highly localized Southern identity meant that many Southern men had little investment in the outcome of the war, with individual soldiers caring more about the fate of their local area than any grand ideal.[170] In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.[171]
From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. Some European observers at the time dismissed them as amateur and unprofessional,[172] but historian John Keegan concluded that each outmatched the French, Prussian, and Russian armies of the time, and without the Atlantic, would have threatened any of them with defeat.[173]
At the start of the Civil War, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their army. They were paid, but they were not allowed to perform any military duties.[174] The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.[175]
Mary Edwards Walker, the only woman ever to receive the Medal of Honor, served in the Union Army and was given the medal for her efforts to treat the wounded during the war. Her name was deleted from the Army Medal of Honor Roll in 1917 (along with over 900 other Medal of Honor recipients); however, it was restored in 1977.[178][179]
The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 sailors in 1865, with 671 vessels, having a tonnage of 510,396.[180][181] Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy.[182] Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland. The U.S. Navy eventually gained control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers. In the East, the Navy shelled Confederate forts and provided support for coastal army operations.[183]
The Civil War occurred during the early stages of the industrial revolution. Many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union's naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries.[184] Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union's ironclad warships, they were unsuccessful.[185]
In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.[186]
By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible, which called for blockading the Confederacy and slowly suffocating the South to surrender.[190] Lincoln adopted parts of the plan, but chose to prosecute a more active vision of war.[191] In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.[192]
The Confederates began the war short on military supplies and in desperate need of large quantities of arms which the agrarian South could not provide. Arms manufactures in the industrial North were restricted by an arms embargo, keeping shipments of arms from going to the South, and ending all existing and future contracts. The Confederacy subsequently looked to foreign sources for their enormous military needs and sought out financiers and companies like S. Isaac, Campbell & Company and the London Armoury Company in Britain, who acted as purchasing agents for the Confederacy, connecting them with Britain's many arms manufactures, and ultimately becoming the Confederacy's main source of arms.[193][194]
To get the arms safely to the Confederacy, British investors built small, fast, steam-driven blockade runners that traded arms and supplies brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were lightweight and designed for speed and could only carry a relatively small amount of cotton back to England.[195] When the Union Navy seized a blockade runner, the ship and cargo were condemned as a prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British, and they were released.[196]
The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies.[197] Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.[197]
Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well.[198] The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade, so they stopped calling at Confederate ports.[199]
Although the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring the British and French governments in as mediators.[202][203] The Union, under Lincoln and Secretary of State William H. Seward, worked to block this and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe turned to Egypt and India for cotton, which they found superior, hindering the South's recovery after the war.[204][205]
Lincoln's administration initially failed to appeal to European public opinion. At first, diplomats explained that the United States was not committed to the ending of slavery, and instead repeated legalistic arguments about the unconstitutionality of secession. Confederate representatives, on the other hand, started off much more successful, by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy.[206] The European aristocracy was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."[206] However, there was still a European public with liberal sensibilities, that the U.S. sought to appeal to by building connections with the international press. As early as 1861, many Union diplomats such as Carl Schurz realized emphasizing the war against slavery was the Union's most effective moral asset in the struggle for public opinion in Europe. Seward was concerned that an overly radical case for reunification would distress the European merchants with cotton interests; even so, Seward supported a widespread campaign of public diplomacy.[206]
War loomed in late 1861 between the U.S. and Britain over the Trent affair, which began when U.S. Navy personnel boarded the British ship Trent and seized two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two men.[208] Prince Albert had left his deathbed to issue diplomatic instructions to Lord Lyons during the Trent affair. His request was honored, and, as a result, the British response to the United States was toned down and helped avert the British becoming involved in the war.[209] In 1862, the British government considered mediating between the Union and Confederacy, though even such an offer would have risked war with the United States. British Prime Minister Lord Palmerston reportedly read Uncle Tom's Cabin three times when deciding on what his decision would be.[208]
The Union victory in the Battle of Antietam caused the British to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Realizing that Washington could not intervene in Mexico as long as the Confederacy controlled Texas, France invaded Mexico in 1861. Washington repeatedly protested France's violation of the Monroe Doctrine. Despite sympathy for the Confederacy, France's seizure of Mexico ultimately deterred it from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers and ensured that they would remain neutral.[210]
Russia supported the Union, largely because it believed that the U.S. served as a counterbalance to its geopolitical rival, the United Kingdom. In 1863, the Russian Navy's Baltic and Pacific fleets wintered in the American ports of New York and San Francisco, respectively.[211]
The Eastern theater refers to the military operations east of the Appalachian Mountains, including the states of Virginia, West Virginia, Maryland, and Pennsylvania, the District of Columbia, and the coastal fortifications and seaports of North Carolina.[citation needed]
Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26, 1861 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. The 1862 Union strategy called for simultaneous advances along four axes:[212]
The primary Confederate force in the Eastern theater was the Army of Northern Virginia. The Army originated as the (Confederate) Army of the Potomac, which was organized on June 20, 1861, from all operational forces in Northern Virginia. On July 20 and 21, the Army of the Shenandoah and forces from the District of Harpers Ferry were added. Units from the Army of the Northwest were merged into the Army of the Potomac between March 14 and May 17, 1862. The Army of the Potomac was renamed Army of Northern Virginia on March 14. The Army of the Peninsula was merged into it on April 12, 1862.
When Virginia declared its secession in April 1861, Robert E. Lee chose to follow his home state, despite his desire for the country to remain intact and an offer of a senior Union command.
Lee's biographer, Douglas S. Freeman, asserts that the army received its final name from Lee when he issued orders assuming command on June 1, 1862.[213] However, Freeman does admit that Lee corresponded with Brigadier General Joseph E. Johnston, his predecessor in army command, before that date and referred to Johnston's command as the Army of Northern Virginia. Part of the confusion results from the fact that Johnston commanded the Department of Northern Virginia (as of October 22, 1861) and the name Army of Northern Virginia can be seen as an informal consequence of its parent department's name. Jefferson Davis and Johnston did not adopt the name, but it is clear that the organization of units as of March 14 was the same organization that Lee received on June 1, and thus it is generally referred to today as the Army of Northern Virginia, even if that is correct only in retrospect.
On July 4 at Harper's Ferry, Colonel Thomas J. Jackson assigned Jeb Stuart to command all the cavalry companies of the Army of the Shenandoah. He eventually commanded the Army of Northern Virginia's cavalry.
In one of the first highly visible battles, in July 1861, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces led by Gen. P. G. T. Beauregard near Washington was repulsed at the First Battle of Bull Run (also known as First Manassas).
The Union had the upper hand at first, nearly pushing confederate forces holding a defensive position into a rout, but Confederate reinforcements under Joseph E. Johnston arrived from the Shenandoah Valley by railroad, and the course of the battle quickly changed. A brigade of Virginians under the relatively unknown brigadier general from the Virginia Military Institute, Thomas J. Jackson, stood its ground, which resulted in Jackson receiving his famous nickname, "Stonewall".
Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. McClellan's army reached the gates of Richmond in the Peninsula Campaign.[214][215][216]
Johnston halted McClellan's advance at the Battle of Seven Pines, but he was wounded in the battle, and Robert E. Lee assumed his position of command. General Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.[217]
The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South.[218] McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops.[citation needed]
Emboldened by Second Bull Run, the Confederacy made its first invasion of the North with the Maryland Campaign. General Lee led 45,000 troops of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history.[217][219] Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.[220]
When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg[221] on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights.[222] After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker.[223]
Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, his Chancellorsville Campaign proved ineffective and he was humiliated in the Battle of Chancellorsville in May 1863.[224] Chancellorsville is known as Lee's "perfect battle" because his risky decision to divide his army in the presence of a much larger enemy force resulted in a significant Confederate victory. Gen. Stonewall Jackson was shot in the arm by accidental friendly fire during the battle and subsequently died of complications.[225] Lee famously said: "He has lost his left arm, but I have lost my right arm."[226]
Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863).[228] This was the bloodiest battle of the war and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000).[229]
The Western theater refers to military operations between the Appalachian Mountains and the Mississippi River, including the states of Alabama, Georgia, Florida, Mississippi, North Carolina, Kentucky, South Carolina, and Tennessee, as well as parts of Louisiana.[230]
The primary Union forces in the Western theater were the Army of the Tennessee and the Army of the Cumberland, named for the two rivers, the Tennessee River and Cumberland River. After Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant.[231][citation needed]
The primary Confederate force in the Western theater was the Army of Tennessee. The army was formed on November 20, 1862, when General Braxton Bragg renamed the former Army of Mississippi. While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West.[230]
The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry (February 6, 1862) and Donelson (February 11 to 16, 1862), earning him the nickname of "Unconditional Surrender" Grant, by which the Union seized control of the Tennessee and Cumberland Rivers.[232] Nathan Bedford Forrest rallied nearly 4,000 Confederate troops and led them to escape across the Cumberland. Nashville and central Tennessee thus fell to the Union, leading to attrition of local food supplies and livestock and a breakdown in social organization.[citation needed]
Leonidas Polk's invasion of Columbus ended Kentucky's policy of neutrality and turned it against the Confederacy. Grant used river transport and Andrew Foote's gunboats of the Western Flotilla to threaten the Confederacy's "Gibraltar of the West" at Columbus, Kentucky. Although rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their gunboats, were forced to retreat and the Union took control of western Kentucky and opened Tennessee in March 1862.[233]
One of the early Union objectives in the war was the capture of the Mississippi River, to cut the Confederacy in half. The Mississippi River was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee.[236]
In April 1862, the Union Navy captured New Orleans.[236] "The key to the river was New Orleans, the South's largest port [and] greatest industrial center."[237] U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South.[238] which allowed Union forces to begin moving up the Mississippi. Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.[239]
Bragg's second invasion of Kentucky in the Confederate Heartland Offensive included initial successes such as Kirby Smith's triumph at the Battle of Richmond and the capture of the Kentucky capital of Frankfort on September 3, 1862.[240] However, the campaign ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville. Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of logistical support and lack of infantry recruits for the Confederacy in that state.[241]
Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee, the culmination of the Stones River Campaign.[242]
Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at the Battle of Vicksburg in July 1863, which cemented Union control of the Mississippi River and is considered one of the turning points of the war.[243]
The one clear Confederate victory in the West was the Battle of Chickamauga. After Rosecrans' successful Tullahoma Campaign, Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas.[citation needed]
Rosecrans retreated to Chattanooga, which Bragg then besieged in the Chattanooga Campaign. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga,[244] eventually causing Longstreet to abandon his Knoxville Campaign and driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy.[245]
The Trans-Mississippi theater refers to military operations west of the Mississippi River, encompassing most of Missouri, Arkansas, most of Louisiana, and Indian Territory (now Oklahoma). The Trans-Mississippi District was formed by the Confederate Army to better coordinate Ben McCulloch's command of troops in Arkansas and Louisiana, Sterling Price's Missouri State Guard, as well as the portion of Earl Van Dorn's command that included the Indian Territory and excluded the Army of the West. The Union's command was the Trans-Mississippi Division, or the Military Division of West Mississippi.[246]
The first battle of the Trans-Mississippi theater was the Battle of Wilson's Creek (August 1861). The Confederates were driven from Missouri early in the war as a result of the Battle of Pea Ridge.[248]
Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control.[249] Roving Confederate bands such as Quantrill's Raiders terrorized the countryside, striking both military installations and civilian settlements.[250] The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged. By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union but Lincoln took 70 percent of the vote for re-election.[251]
Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Battle of Glorieta Pass was the decisive battle of the New Mexico Campaign. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy and smaller numbers for the Union.[252] The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.[253]
After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union, in turn, did not directly engage him.[254] Its 1864 Red River Campaign to take Shreveport, Louisiana, was a failure and Texas remained in Confederate hands throughout the war.[255]
The Lower Seaboard theater refers to military and naval operations that occurred near the coastal areas of the Southeast (Alabama, Florida, Louisiana, Mississippi, South Carolina, and Texas) as well as the southern part of the Mississippi River (Port Hudson and south). Union Naval activities were dictated by the Anaconda Plan.[256]
One of the earliest battles of the war was fought at Port Royal Sound (November 1861), south of Charleston. Much of the war along the South Carolina coast concentrated on capturing Charleston. In attempting to capture Charleston, the Union military tried two approaches: by land over James or Morris Islands or through the harbor. However, the Confederates were able to drive back each Union attack. One of the most famous of the land attacks was the Second Battle of Fort Wagner, in which the 54th Massachusetts Infantry took part. The Union suffered a serious defeat in this battle, losing 1,515 soldiers while the Confederates lost only 174.[257] However, the 54th was hailed for its valor in that battle, which encouraged the general acceptance of the recruitment of African American soldiers into the Union Army, which reinforced the Union's numerical advantage.
Fort Pulaski on the Georgia coast was an early target for the Union navy. Following the capture of Port Royal, an expedition was organized with engineer troops under the command of Captain Quincy A. Gillmore, forcing a Confederate surrender. The Union army occupied the fort for the rest of the war after repairing it.[258]
In April 1862, a Union naval task force commanded by Commander David D. Porter attacked Forts Jackson and St. Philip, which guarded the river approach to New Orleans from the south. While part of the fleet bombarded the forts, other vessels forced a break in the obstructions in the river and enabled the rest of the fleet to steam upriver to the city. A Union army force commanded by Major General Benjamin Butler landed near the forts and forced their surrender. Butler's controversial command of New Orleans earned him the nickname "Beast".[259]
The following year, the Union Army of the Gulf commanded by Major General Nathaniel P. Banks laid siege to Port Hudson for nearly eight weeks, the longest siege in US military history. The Confederates attempted to defend with the Bayou Teche Campaign but surrendered after Vicksburg. These two surrenders gave the Union control over the entire Mississippi.[260]
Several small skirmishes were fought in Florida, but no major battles. The biggest was the Battle of Olustee in early 1864.[citation needed]
The Pacific Coast theater refers to military operations on the Pacific Ocean and in the states and Territories west of the Continental Divide.[261]
At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war.[262] This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end."[263] Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.[264]
Grant's army set out on the Overland Campaign intending to draw Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides and forced Lee's Confederates to fall back repeatedly.[265] At the Battle of Yellow Tavern, the Confederates lost Jeb Stuart.[266]
An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though, unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.[267]
Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. vice president and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war and included a charge by teenage VMI cadets. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.[268]
Leaving Atlanta, and his base of supplies, Sherman's army marched, with no destination set, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia, in December 1864. Sherman's army was followed by thousands of freed slaves; there were no major battles along the march. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee's army.[271]
Lee's army, thinned by desertion and casualties, was now much smaller than Grant's. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler's Creek.[272]
Initially, Lee did not intend to surrender but planned to regroup at Appomattox Station, where supplies were to be waiting and then continue the war. Grant chased Lee and got in front of him so that when Lee's army reached the village of Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at "Wilmer McLean's farmhouse, located less than 100 yards west of the county courthouse",[275] now known as the McLean House.[276] In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller. His men were paroled, and a chain of Confederate surrenders began.[277]
On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Confederate sympathizer. Lincoln died early the next morning. Lincoln's vice president, Andrew Johnson, was unharmed, because his would-be assassin, George Atzerodt, lost his nerve, so Johnson was immediately sworn in as president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them.[278] On April 26, 1865, the same day Boston Corbett killed Booth at a tobacco barn, General Joseph E. Johnston surrendered nearly 90,000 troops of the Army of Tennessee to Major General William Tecumseh Sherman at Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces. On May 4, all remaining Confederate forces in Alabama, Louisiana east of the Mississippi River, and Mississippi under Lieutenant General Richard Taylor surrendered.[279]
The Confederate president, Jefferson Davis, was captured at Irwinsville, Georgia on May 10, 1865.[280]
On May 13, 1865, the last land battle of the war was fought at the Battle of Palmito Ranch in Texas.[281][282][283]
On May 26, 1865, Confederate Lt. Gen. Simon B. Buckner, acting for General Edmund Kirby Smith, signed a military convention surrendering the Confederate trans-Mississippi Department forces.[284][285] This date is often cited by contemporaries and historians as the end date of the American Civil War.[1][2] On June 2, 1865, with most of his troops having already gone home, technically deserted, a reluctant Kirby Smith had little choice but to sign the official surrender document.[286][287] On June 23, 1865, Cherokee leader and Confederate Brig. Gen. Stand Watie became the last Confederate general to surrender his forces.[288][289]
On June 19, 1865, Union Maj. Gen. Gordon Granger announced General Order No. 3, bringing the Emancipation Proclamation into effect in Texas and freeing the last slaves of the Confederacy.[290] The anniversary of this date is now celebrated as Juneteenth.[291]
The naval portion of the war ended more slowly. It had begun on April 11, 1865, two days after Lee's surrender, when President Lincoln proclaimed that foreign nations had no further "claim or pretense" to deny equality of maritime rights and hospitalities to U.S. warships and, in effect, that rights extended to Confederate ships to use neutral ports as safe havens from U.S. warships should end.[292][293] Having no response to Lincoln's proclamation, President Andrew Johnson issued a similar proclamation dated May 10, 1865, more directly stating the premise that the war was almost at an end ("armed resistance...may be regarded as virtually at an end") and that insurgent cruisers still at sea and prepared to attack U.S. ships should not have rights to do so through use of safe foreign ports or waters and warned nations which continued to do so that their government vessels would be denied access to U.S. ports. He also "enjoined" U.S. officers to arrest the cruisers and their crews so "that they may be prevented from committing further depredations on commerce and that the persons on board of them may no longer enjoy impunity for their crimes".[294] England finally responded on June 6, 1865, by transmitting a June 2, 1865 letter from England's Foreign Secretary John Russell, 1st Earl Russell to the Lords of the Admiralty (United Kingdom) withdrawing rights to Confederate warships to enter British ports and waters but with exceptions for a limited time to allow a captain to enter a port to "divest his vessel of her warlike character" and for U.S. ships to be detained in British ports or waters to allow Confederate cruisers twenty-four hours to leave first.[295] U.S. Secretary of State William Seward welcomed the withdrawal of concessions to the Confederates but objected to the exceptions.[296] Finally, on October 18, 1865, Russell advised the Admiralty that the time specified in his June 2, 1865 message had elapsed and "all measures of a restrictive nature on vessels of war of the United States in British ports, harbors, and waters, are now to be considered as at an end".[297] Nonetheless, the final Confederate surrender was in Liverpool, England where James Iredell Waddell, the captain of the CSS Shenandoah, surrendered the cruiser to British authorities on November 6, 1865.[298]
Legally, the war did not end until August 20, 1866, when President Andrew Johnson issued a proclamation that declared "that the said insurrection is at an end and that peace, order, tranquillity, and civil authority now exist in and throughout the whole of the United States of America".[299][300][301]
The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich Southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second-class citizenship of the freedmen and their poverty.[302]
Historians have debated whether the Confederacy could have won the war. Most scholars, including James M. McPherson, argue that Confederate victory was at least possible.[303] McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, it would have more easily been able to hold out long enough to exhaust the Union.[304]
Confederates did not need to invade and hold enemy territory to win but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win.[304] Lincoln was not a military dictator and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by outlasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads, who had wanted a negotiated peace with the Confederate States of America.[305]
Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers.[317] The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Great Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95% effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.[318]
Historian Don Doyle has argued that the Union victory had a major impact on the course of world history.[319] The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:
The North's victory decisively proved the durability of democratic government. Confederate independence, on the other hand, would have established an American model for reactionary politics and race-based repression that would likely have cast an international shadow into the twentieth century and perhaps beyond."[320]
Scholars have debated what the effects of the war were on political and economic power in the South.[321] The prevailing view is that the southern planter elite retained its powerful position in the South.[321] However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.[321]
Based on 1860 census figures, 8 percent of all white men aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South.[325][326] About 56,000 soldiers died in prison camps during the War.[327] An estimated 60,000 soldiers lost limbs in the war.[328]
Of the 359,528 Union army dead, amounting to 15 percent of the over two million who served:[7]
In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).[8]
The United States National Park Service uses the following figures in its official tally of war losses:[3]
While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, superintendent of the 1870 census, used census and surgeon general data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 census's undercounting, it was later found that the census was only off by 6.5% and that the data Walker used would be roughly accurate.[13]
Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war.[323] This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.[citation needed]
Abolishing slavery was not a Union war goal from the outset, but it quickly became one.[21] Lincoln's initial claims were that preserving the Union was the central goal of the war.[332] In contrast, the South saw itself as fighting to preserve slavery.[21] While not all Southerners saw themselves as fighting for slavery, most of the officers and over a third of the rank and file in Lee's army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery.[333] However, as the war dragged on, and it became clear that slavery was central to the conflict, and that emancipation was (to quote from the Emancipation Proclamation) "a fit and necessary war measure for suppressing [the] rebellion," Lincoln and his cabinet made ending slavery a war goal, culminating in the Emancipation Proclamation.[21][334] Lincoln's decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans.[334] By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the Northern state of Ohio when they tried to resurrect anti-black sentiment.[335]
During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. Lincoln's fears of making slavery a war issue were based on a harsh reality: abolition did not enjoy wide support in the west, the territories, and the border states.[339][340] In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game."[340] Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of the total war needed to save the Union.[341]
Lincoln's moderate approach succeeded in inducing the border states to remain in the Union and War Democrats to support the Union. The border states (Kentucky, Missouri, Maryland, Delaware) and Union-controlled regions around New Orleans, Norfolk, and elsewhere, were not covered by the Emancipation Proclamation. Nor was Tennessee, which had come under Union control.[351] Missouri and Maryland abolished slavery on their own; Kentucky and Delaware did not.[352] Still, the proclamation did not enjoy universal support. It caused much unrest in what were then considered western states, where racist sentiments led to a great fear of abolition. There was some concern that the proclamation would lead to the secession of western states, and its issuance prompted the stationing of Union troops in Illinois in case of rebellion.[339]
Since the Emancipation Proclamation was based on the President's war powers, it applied only in territory held by Confederates at the time it was issued. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty.[353] The Emancipation Proclamation greatly reduced the Confederacy's hope of being recognized or otherwise aided by Britain or France.[354] By late 1864, Lincoln was playing a leading role in getting the House of Representatives to vote for the Thirteenth Amendment to the United States Constitution, which mandated the ending of chattel slavery.[355]
President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865 when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They overrode Johnson's vetoes of civil rights legislation, and the House impeached him, although the Senate did not convict him. In 1868 and 1872, the Republican candidate Ulysses S. Grant won the presidency. In 1872, the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They chose Horace Greeley to head a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed further reconstruction. The Compromise of 1877 closed with a national consensus, except perhaps on the part of former slaves, that the Civil War had finally ended.[360] With the withdrawal of federal troops, however, whites retook control of every Southern legislature, and the Jim Crow era of disenfranchisement and legal segregation was ushered in.[361]
The Civil War would have a huge impact on American politics in the years to come. Many veterans on both sides were subsequently elected to political office, including five U.S. Presidents: General Ulysses Grant, Rutherford B. Hayes, James Garfield, Benjamin Harrison, and William McKinley.[362]
The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books, and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war's aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war.[363] The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.[364]
Professional historians have paid much more attention to the causes of the war than to the war itself. Military history has largely developed outside academia, leading to a proliferation of studies by non-scholars who nevertheless are familiar with the primary sources and pay close attention to battles and campaigns and who write for the general public. Bruce Catton and Shelby Foote are among the best known.[365][366] Practically every major figure in the war, both North and South, has had a serious biographical study.[367]
The memory of the war in the white South crystallized in the myth of the "Lost Cause": that the Confederate cause was just and heroic. The myth shaped regional identity and race relations for generations.[368] Alan T. Nolan notes that the Lost Cause was expressly a rationalization, a cover-up to vindicate the name and fame of those in rebellion. Some claims revolve around the insignificance of slavery as a cause of the war; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful.[369] Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing black American progress to white man's reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.[370] The Lost Cause myth was formalized by Charles A. Beard and Mary R. Beard, whose The Rise of American Civilization (1927) spawned "Beardian historiography". The Beards downplayed slavery, abolitionism, and issues of morality. Though this interpretation was abandoned by the Beards in the 1940s, and by historians generally by the 1950s, Beardian themes still echo among Lost Cause writers.[371][372]
The American Civil War has been commemorated in many capacities, ranging from the reenactment of battles to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. These commemorations occurred in greater numbers on the 100th and 150th anniversaries of the war.[378]
Hollywood's take on the war has been especially influential in shaping public memory, as in such film classics as The Birth of a Nation (1915), Gone with the Wind (1939), and Lincoln (2012). Ken Burns's PBS television series The Civil War (1990) is especially well-remembered, though criticized for its historical inaccuracy.[379][380]
Numerous technological innovations during the Civil War had a great impact on 19th-century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war.[381] New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel.[382][383] It was also in this war that aerial warfare, in the form of reconnaissance balloons, was first used.[384] It saw the first action involving steam-powered ironclad warships in naval warfare history.[385] Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare. The war also saw the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.[386]
The Civil War is one of the most studied events in American history, and the collection of cultural works around it is enormous.[387] This section gives an abbreviated overview of the most notable works.
Garnet and Delany collaborated with other Black New Yorkers[6] to establish the African Civilization Society (ACS) in September 1858.[2] The group envisioned Black colonists emigrating from the US to Yorubaland in West Africa, where they would spread Christianity and Western economic and political systems to Indigenous Africans.[8][2] In particular, the group was interested in undermining the Atlantic slave trade and the agricultural slave economies of the US and Caribbean. They sought to do this by training Africans to produce cotton and molasses to compete with slave-produced products in European and American textile manufacturing and other global markets.[5][8] They also promoted self-determination among all people of the African diaspora,[8] advocating in their constitution the "civilization and evangelization of Africa, and the descendants of African ancestors in any portion of the earth, wherever dispersed."[2] In 1859, Delany led a group along the Niger River in West Africa to explore possible sites for a colony. This expedition developed into a separate project called the Niger Valley Exploring Party.[9]
The American Civil War, particularly after the Emancipation Proclamation (1863), disrupted colonization projects like the ACS and caused many of their supporters to focus on mobilizing military support for the Union Army in order to end slavery in the US. Garnet joined the army as a chaplain and Delany as the major of a Colored Troops unit.[17] Under the direction of Presbyterian clergyman and newly-selected ACS president George W. LeVere, the organization shifted its focus from emigration to educating formerly enslaved people, called freedmen.[2] In 1863, they broadened their mission to include helping and educating folks recently freed from slavery in the American South, Central America, South America, the British West Indies, and Africa. The new constitution, adopted January 2, 1864, outlined a mission of ending the slave trade and civilizing, uplifting, and Christianizing Africa and all members of the African diaspora.[18] Their programming reflected Black nationalist ideals: helping Black Americans educate themselves, lead their own education programs, and create their own political and social institutions.[8] Between 1863 and 1867, they were the only Black-led organization opening Freedmen's Schools in the South.[2]
[8]
During the war, Black activist and educator Junius C. Morel claimed: "The African Civilization Society is fully in the field". They are "holding meetings, collecting clothes, books, paper" to support freedmen and "they are making arrangements to send colored teachers just as fast as they can find the means and persons qualified to go".[19] By 1866, the group employed 69 teachers with a collective student body of 2,000 throughout the Northeastern United States.[8] By 1868, they employed 129 teachers and supported schools in Virginia, Maryland, North Carolina, South Carolina, Georgia, Mississippi, Louisiana, and Washington, D.C., with a collective student body of 8,000 and an annual cost of $53,700.[b] Among those teachers were Maria W. Stewart, Laura Cardozo, Hezekiah Hunter, and his wife, Lizzie Hunter.[20]
In 1781 he was employed in a tour of survey of the north-east coast of England. He was sent to North America as commanding engineer in the province of Quebec from 1785 to 1791, served under the Duke of York in Holland in 1793, and in 1794 went back to the Canadas, where he remained till 1804, when he went home to England.   
He was made a colonel in 1797, colonel-commandant of his corps in 1805, lieutenant-general in 1810, and general in 1821. He was appointed inspector-general of fortifications in 1811, and held the office until death. Some of his plans for fortifying Canada are preserved in the British Library and Canada.
Gother Mann, second son of Cornelius Mann and Elizabeth Gother, was born at Plumstead, Kent, on 21 December 1747. His father, a first cousin of Sir Horace Mann, went to the West Indies in 1760, and died at Saint Kitts on 9 December 1776. Gother was left under the charge of his uncle, Mr. Wilks of Faversham, Kent.[1] After graduating from the Royal Military Academy, Woolwich, he obtained a commission as practitioner engineer and ensign in the Royal Engineers on 27 February 1763. He was employed in England on the defences of Sheerness and of the River Medway until 1775, having been promoted sub-engineer and lieutenant on 1 April 1771.[1]
Towards the end of 1775 Mann was posted to Dominica in the West Indies and while there was promoted engineer extraordinary and captain lieutenant on 2 March 1777. In the morning of 7 September 1778, the French landed a strong force on the island, beginning a surprise invasion of Dominica. The British garrison, which was small, prepared for resistance, and Mann was named to command a detachment of the militia stationed at the new battery at Guey's Hill (now called King's Hill), which he prepared to defend. The council of the island pressured Lieutenant-governor William Stuart to capitulate; he yielded, and the island was surrendered without an effort being made to retain it.[2]
Mann made a report to the Board of Ordnance dated 14 September 1778, giving full details of the attack. He was only detained for a few months as a prisoner of war, and on 19 August 1779 he was appointed to the engineer staff of Great Britain, and reported on the defences of the east coast of England. He was stationed at Chatham under Colonel Hugh Debbeig. In 1781 he was selected by Lord Amherst and Sir Charles Frederick to accompany Colonel Braham, the chief engineer, on a tour of survey of the north-east coast of England, to consider what defences were desirable, as seven corporations had submitted petitions on the subject.[1]
In 1785 Mann, age thirty-eight, was sent to the Province of Quebec as commanding engineer,[1] succeeding William Twiss, and accompanied by fellow engineer Ralph Henry Bruyeres.[3] Promoted captain on 16 September 1785, he was employed in every part of the country in both civil and military duties, erecting fortifications, improving ports, and laying out townships, such as Toronto and Sorel.[1] In 1788 the governor, Guy Carleton, Lord Dorchester, had him make an extensive examination of military posts, harbours and navigable waterways from Kingston to St. Marys River, Sault Ste. Marie, in which Mann laments the ruination and ill placement of the bases: many of which were on the United States' side of the border established by the Treaty of Paris in 1783, though the British did not quit them till two years after the signing of the Jay Treaty in 1794.[3]
Mann returned to England in 1791. He went to the Netherlands in 1792,[3] and, joining the British army under Prince Frederick, Duke of York in June 1793, took part in the Flanders campaign. He was present at the siege of Valenciennes, which capitulated to the Coalition forces on 28 July, at the siege of Dunkirk from 24 August to 9 September, and at the battle of Hondschoote or Menin from 12 to 15 September. He was promoted lieutenant-colonel on 5 December 1793.[1]
On his return to England in April 1794 Mann was briefly employed under the master-general of the ordnance in London, before being sent back to Lower Canada, as commanding engineer,[1] to prepare defences at Quebec, since invasion from the United States then seemed a possibility.[3] He became colonel in the Army on 26 January 1797, and colonel in the Royal Engineers on 18 August the same year.[1] He wrote several reports in favour of establishing new and permanent defence systems at Quebec, and building more fortifications.[3] In 1800 he made a report on the St. Lawrence River canals and pointed out needed repairs and proposed certain improvements to the locks.[4] He became major-general on 25 September 1803.[1] In the same year he received permission to return to England where his wife and children had remained, and he embarked in the spring of 1804.[3]
From 1805 until 1811 Mann was employed either on particular service in Ireland or on various committees in London. On 13 July 1805 he was made a colonel-commandant of the Corps of Royal Engineers, on 25 July 1810 lieutenant-general, and on 19 July 1821 general. On 23 July 1811 he succeeded General Robert Morse as inspector-general of fortifications, an office he held until his death,[1] and in that capacity he continued to write on Canadian defences, such as the construction of the Citadelle of Quebec.[3] He was appointed president of the committee to examine cadets for commissions on 19 May 1828.[1]
Gother Mann was the senior officer in the engineers when he died, at age eighty-two, on 27 March 1830.[3] He was buried in Plumstead churchyard, where a tombstone was erected to his memory.[1]
His services in Canada were rewarded by a grant, on 22 July 1805, of 22,859 acres (9,251 hectares) of land in the township of Acton in Lower Canada. He also received while holding the office of inspector-general of fortifications the offer of a baronetcy, which, for financial considerations, he declined.[1]
On 1 March 1768, at St. Nicholas's, Rochester, Kent, Ensign Gother Mann married Ann, second daughter of Peter Wade of Rushford Manor, Eythorne, Kent, rector of Cooling, vicar of Boughton Monchelsea, and minor canon of Rochester Cathedral.[4] By her he had five sons and three daughters. Of the sons, Gother was in the Royal Artillery, Cornelius in the Royal Engineers, John in the 28th Regiment of Foot, and Frederick William in the Royal Marines, and afterwards in the Royal Staff Corps.[1] William, son of Cornelius, was an astronomer.[4]
Three coloured miniatures of Mann came into the possession of his descendants. One, taken when he had just entered the Corps of Royal Engineers in 1763, was once owned by his grandson, Major-general James Robert Mann, C.M.G., Royal Engineers, son of Major-general Cornelius Mann, Royal Engineers. This is reproduced in Porter's History of the Corps of Royal Engineers, 1889.[5][1]
The following plans by Mann are in the British Library: 
The following drawn plans by Mann, formerly in the War Office, are now among the records of the government of Canada: 
Early on 7 September 1778, French forces landed on the southeastern coast of the island.  They rapidly took over some of the island's defenses, and eventually gained control of the high ground overlooking the island's capital, Roseau.  Lieutenant Governor William Stuart then surrendered the remaining forces.  Dominica remained in French hands until the end of the war, when it was returned to British control.
Following the pivotal Battles of Saratoga in October 1777 and the ensuing surrender of British General John Burgoyne's army, France decided to openly enter the American War of Independence as an ally of the young United States of America.  France's objectives in entering the war included the recovery of territories that had been lost to Britain in the Seven Years' War.  One key territory that was of particular interest was the West Indies island of Dominica, which lay between French-held Martinique and Guadeloupe, and had been captured by Britain in 1761.  Recapture of the island would improve communication among the islands, and deny the use of Dominican ports to privateers who preyed on French shipping.[2]
The ratification of the Fifteenth Amendment to the United States Constitution in 1870 meant that African American men in California finally had the right to vote.[3] However, equal access to education remained a critical issue for communities throughout the state.[3][4] Black newspapers such as the Pacific Appeal stated, "The proper education of our children is paramount to all other considerations."[4][5] Although the earliest school laws in California did not specifically mention race, segregated schools existed as early as 1854.[4]
In 1866, the California state legislature enacted a revised law requiring local districts to establish separate schools for children of African, Mongolian, or Indian descent,[a] if petitioned by ten or more parents or guardians of those children.[6][2][7] Where such schools did not exist, children of color should be permitted to enroll in school with white children, unless the parent of a white child objected in writing.[6] In practice, this meant that black children in rural areas often did not receive an elementary education.[3] For many families, paying for private school was not an option.[4] According to a professional teachers' organization, as of 1874, one in four black children did not attend school.[3] Where separate schools existed, black children were relegated to second-rate school facilities, funded in part through additional taxes paid by black parents,[3] and often had to walk long distances to school.[4] According to The San Francisco Elevator newspaper, black schools were receiving only two-thirds of the annual appropriation paid per-student to white schools.[5] Furthermore, The Elevator charged that no fewer than 20 counties had misappropriated public funds for colored schools and diverted them to white schools instead.[5]
In April 1870, the California legislature passed a law requiring all children of African and American Indian descent to attend separate schools; local school districts could no longer admit them to white schools using their discretion.[8] The African American community in California responded in anger, and mobilized to try to repeal the new law.[8]
In November 1871, an education convention was held in Stockton, California, at the church of educator Jeremiah B. Sanderson.[3] One of the resolutions they adopted was to petition the legislature to remove the words "children of African descent" from the law, so they could "be allowed educational facilities with other children."[8] Following the convention, Senator Seldon J. Finney of San Mateo County took up the cause, and introduced a bill in the California legislature to end segregation of schools.[3] The bill failed, and in 1872, African American leaders decided to pursue a test case in court.[3][9]
In April 1872, African American leaders announced that they had selected San Francisco attorney and former state assemblyman John W. Dwinelle to represent the interests of the black community,[10][3] after interviewing several candidates.[8] In the summer of 1872, they organized meetings in San Francisco, Sacramento, Stockton, and Maryville, to raise money to pay for legal fees, and hired Dwinelle.[3][9]
In 1872, San Francisco had two "colored schools" located at opposite ends of the city, one of which was a small room rented by the Board of Education.[3] Meanwhile, according to The Appeal newspaper, white children had access to "43 or more splendidly built school houses in the city suited or adapted to every neighborhood".[3]
On July 23, 1872, The San Francisco Chronicle reported that several African American parents had attempted to enroll their children in four different schools, but had been denied, and that John W. Dwinelle was planning legal proceedings to overturn these decisions.[8]
One of those parents was Harriet Ward, who had tried to register her eleven-year-old daughter, Mary Frances, at Broadway Grammar School, a "regular" public school for white children in San Francisco,[1][11] which was the closest to their home.[12] Harriet and A. J. Ward had been residents of San Francisco since 1859.[12] Principal Noah Flood refused to allow their daughter to enroll, advising Mrs. Ward that she should take Mary Frances to one of the "colored schools" as required by the San Francisco Board of Education, since she was black.[1]
Dwinelle chose Mary Frances Ward as the plaintiff for the case, represented by A. J. Ward as her father and guardian.[8] In September 1872, Dwinelle applied for a writ of mandate, requesting the California Supreme Court to order Flood to admit Mary Frances Ward to the school.[8] He submitted a written affidavit from Harriet Ward stating that the only reason Noah Flood had denied their request to enroll her daughter was due to her race and the school board policy.[8] Attorneys for the school board argued that the colored schools provided an equal education, and claimed that Mary Frances Ward had not completed the prerequisites to enter the lowest grade of the Broadway Grammar School.[8]
On November 22, 1872, the Wards' attorney, John W. Dwinelle appealed to the California Supreme Court, arguing that the existing school code violated both the Fourteenth and Fifteenth Amendments, as well as the Civil Rights Act of 1866.[4] One of his initial arguments, that the exclusion of Mary Frances Ward constituted a "badge of servitude" in violation of the Thirteenth Amendment, was rejected outright by the court, which noted that exclusion of a black child from a white school was not the same as forced slavery.[11]
Dwinelle's main argument was based on the equal protection clause of the Fourteenth Amendment, which had been enacted in 1868.[12] Echoing Charles Sumner's argument in Roberts v. City of Boston in 1850, Dwinelle contended that forcing black schoolchildren to attend separate schools marked them as "inferior" in the eyes of the rest of society, denying them equal protection under the law.[12]
When the case reached the California high court, Harriet Ward stated in her petition on behalf of her family, "We are all of African descent...residents of San Francisco...[and] have a right to be received...at the school nearest their residence."[5] Meanwhile, Noah Flood maintained that he had merely been following state law, and that the black school would provide Mary Frances with an education "equal" to the white school.[11]
Eighteen months later, the California Supreme Court ruled against Ward, citing both the Slaughter-House Cases and Roberts v. City of Boston as precedent.[13] The majority opinion held that the privileges and immunities of the Fourteenth Amendment only applied to federal laws, while California public schools were run by the state and were therefore a privilege held through state citizenship rather than U.S. citizenship.[13]
With regard to equal protection, the court ruled in Ward v. Flood that the state was not violating any law, as long as it provided similar educational opportunities to all its citizens.[13] The California Supreme Court cited an 1849 ruling by the Supreme Judicial Court of Massachusetts that segregating schools by race was no different to separating students by age, gender, or special needs.[11] Quoting from Roberts v. City of Boston, the court maintained that having separate schools was not the reason for the "odious caste distinctions" confronting black children.[12]
At the same time, the California high court clearly affirmed that all children had a right to public education, which had to be provided to them equally under state law.[11] Further, the state supreme court ruled that excluding black children from white schools would not be allowed unless separate schools were available.[12] If not, they had the right to attend white schools.[12][14]
In the wake of the Ward v. Flood decision, African Americans in San Francisco protested segregation in education,[2] by boycotting black schools.[15] Local districts also came under increased financial strain during the Long Depression, which started in 1873.[2] Thus, although the California State Supreme Court had upheld segregation, in practice, most school districts in California opted to enroll black students rather than fund two separate school systems,[16] including the Board of Education in San Francisco, which opened its white schools to black children in 1875.[15] Furthermore, in communities where no separate school for black children existed, schools were now required to enroll black children.[3] For many African American children living in rural California, it was their first opportunity to go to elementary school.[3] Between 1875 and 1880, the absentee rate for black students dropped from 40 percent to 17 percent.[15]
By 1880, the California State Legislature temporarily removed all references to race in the school code, but exclusion and segregation continued in some districts including San Francisco, particularly against families of Chinese immigrants.[2] In 1885, the legislature made a further change to the school code, establishing "separate schools for 'children of Mongoloid or Chinese descent".[14]
In 1896, when the Supreme Court of the United States ruled in Plessy v. Ferguson that racial segregation did not violate the United States Constitution, as long as "separate but equal" public facilities were available, it cited several state court decisions, including Ward v. Flood.[11]
Heaton was born in 1936 in Shillong, India, where his parents were Christian missionaries.[2] His family later moved to England, where Heaton attended Marlborough College before completing a Bachelor of Arts at the University of Cambridge. He went on to study medicine at Cambridge, with clinical placements in London at Middlesex Hospital and Central Middlesex Hospital. He married Susan O'Connor, a fellow medical student, in 1961, the year they both graduated.[1]
Heaton was a medical registrar at the Royal Free Hospital and the Bristol Royal Infirmary. During a year-long research fellowship at Duke University Medical Center in the United States, he developed an interest in bile salts;[1] he later wrote a book titled Bile Salts in Health and Disease.[2] He subscribed to the "Cleave hypothesis", first posed by Peter Cleave, that a number of diseases were due to the consumption of excessive processed foods to which the human gastrointestinal tract had not adapted. This influenced his research on irritable bowel syndrome (IBS), and in 1978 he co-authored a paper positing that a diagnosis of IBS could be made on the basis of symptoms alone. His research led to the development of the Rome criteria for diagnosis of functional gastrointestinal disorders including IBS.[1]
From 1968, Heaton worked as a consultant at the Bristol Royal Infirmary and as a lecturer (and eventually reader) in medicine at the University of Bristol.[1][3] With Steve Lewis, he developed the Bristol stool scale, an illustrated scale of faecal consistency that reflects intestinal transit time and can be used to assess bowel health and is used internationally by clinicians and researchers.[1][2][3]
The Bristol stool scale is a diagnostic medical tool designed to classify the form of human faeces into seven categories.[3] It is used in both clinical and experimental fields.[4][5][6]
It was developed at the Bristol Royal Infirmary as a clinical assessment tool in 1997,[7] and is widely used as a research tool to evaluate the effectiveness of treatments for various diseases of the bowel, as well as a clinical communication aid;[8][9] including being part of the diagnostic triad for irritable bowel syndrome.[10]
Types 1 and 2 indicate constipation, with 3 and 4 being the ideal stools as they are easy to defecate while not containing excess liquid, 5 indicating lack of dietary fiber, and 6 and 7 indicate diarrhoea.[12]
In the initial study, in the population examined in this scale, the type 1 and 2 stools were more prevalent in females, while the type 5 and 6 stools were more prevalent in males; furthermore, 80% of subjects who reported rectal tenesmus (sensation of incomplete defecation) had type 7. These and other data have allowed the scale to be validated.[11] The initial research did not include a pictorial chart with this being developed at a later point.[7]
The Bristol stool scale is also very sensitive to changes in intestinal transit time caused by medications, such as antidiarrhoeal loperamide, senna, or anthraquinone with laxative effect.[13]
People with irritable bowel syndrome (IBS) typically report that they suffer with abdominal cramps and constipation.
In some patients, chronic constipation is interspersed with brief episodes of diarrhoea; while a minority of patients with IBS have only diarrhoea.
The presentation of symptoms is usually months or years and commonly patients consult different doctors, without great success, and doing various specialized investigations.
It notices a strong correlation of the reported symptoms with stress; indeed diarrhoeal discharges are associated with emotional phenomena.
IBS blood is present only if the disease is associated with haemorrhoids.[14]
Research conducted on irritable bowel syndrome in the 2000s,[15][16] faecal incontinence[17][18][19][20] and the gastrointestinal complications of HIV[21] have used the Bristol scale as a diagnostic tool easy to use, even in research which lasted for 77 months.[22]
Historically, this scale of assessment of the faeces has been recommended by the consensus group of Kaiser Permanente Medical Care Program (San Diego, California, US) for the collection of data on functional bowel disease (FBD).[14]
More recently, according to the latest revision of the Rome III Criteria, six clinical manifestations of IBS can be identified:[23][24][25][26][27]
These four identified subtypes correlate with the consistency of the stool, which can be determined by the Bristol stool scale.[14]
In 2007, the Mayo Clinic College of Medicine in Rochester, Minnesota, United States, reported a piece of epidemiological research conducted on a population of 4,196 people living in Olmsted County Minnesota, in which participants were asked to complete a questionnaire based on the Bristol stool scale.[29]
The research results (see table) indicate that about 1 in 5 people have a slow transit (type 1 and 2 stools), while 1 in 12 has an accelerated transit (type 5 and 6 stools). Moreover, the nature of the stool is affected by age, sex, body mass index, whether or not they had cholecystectomy and possible psychosomatic components (somatisation); there were no effects from factors such as smoking, alcohol, the level of education, a history of appendectomy or familiarity with gastrointestinal diseases, civil state, or the use of oral contraceptives.
Several investigations correlate the Bristol stool scale in response to medications or therapies, in fact, in one study was also used to titrate the dose more finely than one drug (colestyramine) in subjects with diarrhoea and faecal incontinence.[30]
In a randomised controlled study,[31] the scale is used to study the response to two laxatives: Macrogol (polyethylene glycol) and psyllium (Plantago psyllium and other species of the same genus) of 126 male and female patients for a period of 2 weeks of treatment; failing to show the most rapid response and increased efficiency of the former over the latter. In the study, they were measured as primary outcomes: the number weekly bowel movements, stool consistency according to the types of the Bristol stool scale, time to defecation, the overall effectiveness, the difficulty in defecating and stool consistency.[31]
From 2010, several studies have used the scale as a diagnostic tool validated for recognition and evaluation of response to various treatments, such as probiotics,[32][33] moxicombustion,[34] laxatives in the elderly,[35] preparing Ayurvedic poly-phytotherapy filed TLPL/AY,[36] psyllium,[37] mesalazine,[38] methylnaltrexone,[39] and oxycodone/naloxone,[40] or to assess the response to physical activity in athletes.[41]
Developed and proposed for the first time in England by Stephen Lewis and Ken Heaton at the University Department of Medicine, Bristol Royal Infirmary, it was suggested by the authors as a clinical assessment tool in 1997 in the Scandinavian Journal of Gastroenterology[13] after a previous prospective study, conducted in 1992 on a sample of the population (838 men and 1,059 women), had shown an unexpected prevalence of defecation disorders related to the shape and type of stool.[42] The authors of the former paper concluded that the form of the stool is a useful surrogate measure of colon transit time. That conclusion has since been challenged as having limited validity for Types 1 and 2;[43] however, it remains in use as a research tool to evaluate the effectiveness of treatments for various diseases of the bowel, as well as a clinical communication aid.[8][9]
In the 19th century, the paper developed a reputation for civic boosterism and opposition to labor unions, the latter of which led to the bombing of its headquarters in 1910. The paper's profile grew substantially in the 1960s under publisher Otis Chandler, who adopted a more national focus. In recent decades the paper's readership has declined, and it has been beset by a series of ownership changes, staff reductions, and other controversies. In January 2018, the paper's staff voted to unionize and finalized their first union contract on October 16, 2019.[8] The paper moved out of its historic downtown headquarters to a facility in El Segundo, near Los Angeles International Airport in July 2018.
The Times was first published on December 4, 1881, as the Los Angeles Daily Times, under the direction of Nathan Cole Jr. and Thomas Gardiner. It was first printed at the Mirror printing plant, owned by Jesse Yarnell and T. J. Caystile. Unable to pay the printing bill, Cole and Gardiner turned the paper over to the Mirror Company. In the meantime, S. J. Mathes had joined the firm, and it was at his insistence that the Times continued publication. In July 1882, Harrison Gray Otis moved from Santa Barbara to become the paper's editor.[9] Otis made the Times a financial success.
Historian Kevin Starr wrote that Otis was a businessman "capable of manipulating the entire apparatus of politics and public opinion for his own enrichment".[10] Otis's editorial policy was based on civic boosterism, extolling the virtues of Los Angeles and promoting its growth. Toward those ends, the paper supported efforts to expand the city's water supply by acquiring the rights to the water supply of the distant Owens Valley.[11]
The efforts of the Times to fight local unions led to the bombing of its headquarters on October 1, 1910, killing twenty-one people. Two union leaders, James and Joseph McNamara, were charged. The American Federation of Labor hired noted trial attorney Clarence Darrow to represent the brothers, who eventually pleaded guilty.
Otis fastened a bronze eagle on top of a high frieze of the new Times headquarters building designed by Gordon Kaufmann, proclaiming anew the credo written by his wife, Eliza: "Stand Fast, Stand Firm, Stand Sure, Stand True".[12][13]
After Otis's death in 1917, his son-in-law, Harry Chandler, took control as publisher of the Times. Harry Chandler was succeeded in 1944 by his son, Norman Chandler, who ran the paper during the rapid growth of post-war Los Angeles. Norman's wife, Dorothy Buffum Chandler, became active in civic affairs and led the effort to build the Los Angeles Music Center, whose main concert hall was named the Dorothy Chandler Pavilion in her honor. Family members are buried at the Hollywood Forever Cemetery near Paramount Studios. The site also includes a memorial to the Times Building bombing victims.
In 1935, the newspaper moved to a new, landmark Art Deco building, the Los Angeles Times Building, to which the newspaper would add other facilities until taking up the entire city block between Spring, Broadway, First and Second streets, which came to be known as Times Mirror Square and would house the paper until 2018. Harry Chandler, then the president and general manager of Times-Mirror Co., declared the Los Angeles Times Building a "monument to the progress of our city and Southern California".[14]
During the 1960s, the paper won four Pulitzer Prizes, more than its previous nine decades combined.
Writing in 2013 about the pattern of newspaper ownership by founding families, Times reporter Michael Hiltzik said that:
The first generations bought or founded their local paper for profits and also social and political influence (which often brought more profits). Their children enjoyed both profits and influence, but as the families grew larger, the later generations found that only one or two branches got the power, and everyone else got a share of the money. Eventually the coupon-clipping branches realized that they could make more money investing in something other than newspapers. Under their pressure the companies went public, or split apart, or disappeared. That's the pattern followed over more than a century by the Los Angeles Times under the Chandler family.[16]
The Los Angeles Times was beset in the first decade of the 21st century by a change in ownership, a bankruptcy, a rapid succession of editors, reductions in staff, decreases in paid circulation, the need to increase its Web presence, and a series of controversies.
The newspaper moved to a new headquarters building in El Segundo, near Los Angeles International Airport, in July 2018.[19][20][21][22]
In 2000, Times Mirror Company, publisher of the Los Angeles Times, was purchased by the Tribune Company of Chicago, Illinois, placing the paper in co-ownership with the then WB-affiliated (now CW-affiliated) KTLA, which Tribune acquired in 1985.[23]
In 2000, John Carroll, former editor of the Baltimore Sun, was brought in to restore the luster of the newspaper.[31] During his reign at the Times, he eliminated more than 200 jobs, but despite an operating profit margin of 20 percent, the Tribune executives were unsatisfied with returns, and by 2005 Carroll had left the newspaper. His successor, Dean Baquet, refused to impose the additional cutbacks mandated by the Tribune Company.
The paper's content and design style were overhauled several times in attempts to increase circulation. In 2000, a major change reorganized the news sections (related news was put closer together) and changed the "Local" section to the "California" section with more extensive coverage. Another major change in 2005 saw the Sunday "Opinion" section retitled the Sunday "Current" section, with a radical change in its presentation and featured columnists. There were regular cross-promotions with Tribune-owned television station KTLA to bring evening-news viewers into the Times fold.
The paper reported on July 3, 2008, that it planned to cut 250 jobs by Labor Day and reduce the number of published pages by 15 percent.[33][34] That included about 17 percent of the news staff, as part of the newly private media company's mandate to reduce costs. "We've tried to get ahead of all the change that's occurring in the business and get to an organization and size that will be sustainable", Hiller said.[35] In January 2009, the Times eliminated the separate California/Metro section, folding it into the front section of the newspaper. The Times also announced seventy job cuts in news and editorial or a 10 percent cut in payroll.[36]
In September 2015, Austin Beutner, the publisher and chief executive, was replaced by Timothy E. Ryan.[37] On October 5, 2015, the Poynter Institute reported that "'At least 50' editorial positions will be culled from the Los Angeles Times" through a buyout.[38] On this subject, the Los Angeles Times reported with foresight: "For the 'funemployed,' unemployment is welcome."[39] Nancy Cleeland,[40] who took O'Shea's buyout offer, did so because of "frustration with the paper's coverage of working people and organized labor"[41] (the beat that earned her Pulitzer).[40] She speculated that the paper's revenue shortfall could be reversed by expanding coverage of economic justice topics, which she believed were increasingly relevant to Southern California; she cited the paper's attempted hiring of a "celebrity justice reporter" as an example of the wrong approach.[41]
On August 21, 2017, Ross Levinsohn, then aged 54, was named publisher and CEO, replacing Davan Maharaj, who had been both publisher and editor.[42] On June 16, 2018, the same day the sale to Patrick Soon-Shiong closed, Norman Pearlstine was named executive editor.[30]
On May 3, 2021, the newspaper announced that it had selected Kevin Merida to be the new executive editor. Merida is a senior vice president at ESPN and leads The Undefeated, a site focused on sports, race, and culture. Previously, he was the first Black managing editor at The Washington Post.[43]
The Times has suffered continued decline in distribution. Reasons offered for the circulation drop included a price increase[44] and a rise in the proportion of readers preferring to read the online version instead of the print version.[45] Editor Jim O'Shea, in an internal memo announcing a May 2007, mostly voluntary, reduction in force, characterized the decrease in circulation as an "industry-wide problem" which the paper had to counter by "growing rapidly on-line", "break[ing] news on the Web and explain[ing] and analyz[ing] it in our newspaper."[46]
The Times closed its San Fernando Valley printing plant in early 2006, leaving press operations to the Olympic plant and to Orange County. Also that year the paper announced its circulation had fallen to 851,532, down 5.4 percent from 2005. The Times's loss of circulation was the largest of the top ten newspapers in the U.S.[47] Some observers believed that the drop was due to the retirement of circulation director Bert Tiffany. Still, others thought the decline was a side effect of a succession of short-lived editors who were appointed by publisher Mark Willes after publisher Otis Chandler relinquished day-to-day control in 1995.[15] Willes, the former president of General Mills, was criticized for his lack of understanding of the newspaper business, and was derisively referred to by reporters and editors as The Cereal Killer.[48] 
The Times's reported daily circulation in October 2010 was 600,449,[49] down from a peak of 1,225,189 daily and 1,514,096 Sunday in April 1990.[50][51]
In December 2006, a team of Times reporters delivered management with a critique of the paper's online news efforts known as the Spring Street Project.[52] The report, which condemned the Times as a "web-stupid" organization,[52] was followed by a shakeup in management of the paper's website,[53] www.latimes.com, and a rebuke of print staffers who were described as treating "change as a threat."[54]
On July 10, 2007, Times launched a local Metromix site targeting live entertainment for young adults.[55] A free weekly tabloid print edition of Metromix Los Angeles followed in February 2008; the publication was the newspaper's first stand-alone print weekly.[56] In 2009, the Times shut down Metromix and replaced it with Brand X, a blog site and free weekly tabloid targeting young, social networking readers.[57] Brand X launched in March 2009; the Brand X tabloid ceased publication in June 2011 and the website was shut down the following month.[58]
In May 2018, the Times blocked access to its online edition from most of Europe because of the European Union's General Data Protection Regulation.[59][60]
It was revealed in 1999 that a revenue-sharing arrangement was in place between the Times and Staples Center in the preparation of a 168-page magazine about the opening of the sports arena. The magazine's editors and writers were not informed of the agreement, which breached the Chinese wall that traditionally has separated advertising from journalistic functions at American newspapers. Publisher Mark Willes also had not prevented advertisers from pressuring reporters in other sections of the newspaper to write stories favorable to their point of view.[61]
Michael Kinsley was hired as the Opinion and Editorial (op-ed) Editor in April 2004 to help improve the quality of the opinion pieces. His role was controversial, for he forced writers to take a more decisive stance on issues. In 2005, he created a Wikitorial, the first Wiki by a major news organization. Although it failed, readers could combine forces to produce their own editorial pieces. It was shut down after being besieged with inappropriate material. He resigned later that year.[62]
The Times drew fire for a last-minute story before the 2003 California recall election alleging that gubernatorial candidate Arnold Schwarzenegger groped scores of women during his movie career. Columnist Jill Stewart wrote on the American Reporter website that the Times did not do a story on allegations that former Governor Gray Davis had verbally and physically abused women in his office, and that the Schwarzenegger story relied on a number of anonymous sources. Further, she said, four of the six alleged victims were not named. She also said that in the case of the Davis allegations, the Times decided against printing the Davis story because of its reliance on anonymous sources.[63][64] The American Society of Newspaper Editors said that the Times lost more than 10,000 subscribers because of the negative publicity surrounding the Schwarzenegger article.[65]
The Times also came under controversy for its decision to drop the weekday edition of the Garfield comic strip in 2005, in favor of a hipper comic strip Brevity, while retaining it in the Sunday edition. Garfield was dropped altogether shortly thereafter.[67]
Following the Republican Party's defeat in the 2006 mid-term elections, an Opinion piece by Joshua Muravchik, a leading neoconservative and a resident scholar at the conservative American Enterprise Institute, published on November 19, 2006, was titled 'Bomb Iran'. The article shocked some readers, with its hawkish comments in support of more unilateral action by the United States, this time against Iran.[68]
In November 2017, Walt Disney Studios blacklisted the Times from attending press screenings of its films, in retaliation for September 2017 reportage by the paper on Disney's political influence in the Anaheim area. The company considered the coverage to be "biased and inaccurate". As a sign of condemnation and solidarity, a number of major publications and writers, including The New York Times,  Boston Globe critic Ty Burr, Washington Post blogger Alyssa Rosenberg, and the websites The A.V. Club and Flavorwire, announced that they would boycott press screenings of future Disney films. The National Society of Film Critics, Los Angeles Film Critics Association, New York Film Critics Circle, and Boston Society of Film Critics jointly announced that Disney's films would be ineligible for their respective year-end awards unless the decision was reversed, condemning the decision as being "antithetical to the principles of a free press and [setting] a dangerous precedent in a time of already heightened hostility towards journalists". On November 7, 2017, Disney reversed its decision, stating that the company "had productive discussions with the newly installed leadership at the Los Angeles Times regarding our specific concerns".[71][72][73]
Through 2014 the Times had won 41 Pulitzer Prizes, including four in editorial cartooning, and one each in spot news reporting for the 1965 Watts Riots and the 1992 Los Angeles riots.[74]
In the 19th century, the chief competition to the Times was the Los Angeles Herald, followed by the smaller Los Angeles Tribune. In December 1903, newspaper magnate William Randolph Hearst began publishing the Los Angeles Examiner as a direct morning competitor to the Times.[83] In the 20th century, the Los Angeles Express was an afternoon competitor, as was Manchester Boddy's Los Angeles Daily News, a Democratic newspaper.[84]
By the mid-1940s, the Times was the leading newspaper in terms of circulation in the Los Angeles metropolitan area. In 1948, it launched the Los Angeles Mirror, an afternoon tabloid, to compete with both the Daily News and the merged Herald-Express. In 1954, the Mirror absorbed the Daily News. The combined paper, the Mirror-News, ceased publication in 1962, when the Hearst afternoon Herald-Express and the morning Los Angeles Examiner merged to become the Herald-Examiner.[85] The Herald-Examiner published its last number in 1989. In 2014, the Los Angeles Register, published by Freedom Communications, then-parent company of the Orange County Register was launched as a daily newspaper to compete with the Times. By late September of the same year, the Los Angeles Register was folded.[86][87]
The Midwinter Number drew acclamations from other newspapers, including this one from The Kansas City Star in 1923:
In 1948 the Midwinter Edition, as it was then called, had grown to "7 big picture magazines in beautiful rotogravure reproduction."[93] The last mention of the Midwinter Edition was in a Times advertisement on January 10, 1954.[94]
Between 1891 and 1895, the Times also issued a similar Midsummer Number, the first one with the theme "The Land and Its Fruits".[95] Because of its issue date in September, the edition was in 1891 called the Midsummer Harvest Number.[96]
In 1903, the Pacific Wireless Telegraph Company established a radiotelegraph link between the California mainland and Santa Catalina Island. In the summer of that year, the Times made use of this link to establish a local daily paper, based in Avalon, called The Wireless, which featured local news plus excerpts which had been transmitted via Morse code from the parent paper.[98] However, this effort apparently survived for only a little more than one year.[99]
In the 1990s, the Times published various editions catering to far-flung areas. Editions included those from the San Fernando Valley, Ventura County, Inland Empire, Orange County, San Diego County & a "National Edition" that was distributed to Washington, D.C., and the San Francisco Bay Area. The National Edition was closed in December 2004.
Some of these editions[quantify] were succeeded by Our Times, a group of community supplements included in editions of the regular Los Angeles Metro newspaper.[citation needed]
One of the Times' features was "Column One", a feature that appeared daily on the front page to the left-hand side. Established in September 1968, it was a place for the weird and the interesting; in the How Far Can a Piano Fly? (a compilation of Column One stories) introduction, Patt Morrison wrote that the column's purpose was to elicit a "Gee, that's interesting, I didn't know that" type of reaction.
The Times also embarked on a number of investigative journalism pieces. A series in December 2004 on the King/Drew Medical Center in Los Angeles led to a Pulitzer Prize and a more thorough coverage of the hospital's troubled history. Lopez wrote a five-part series on the civic and humanitarian disgrace of Los Angeles' Skid Row, which became the focus of a 2009 motion picture, The Soloist. It also won 62 awards at the SND[clarification needed] awards.
From 1967 to 1972, the Times produced a Sunday supplement called West magazine. West was recognized for its art design, which was directed by Mike Salisbury (who later became art director of Rolling Stone magazine).[104] From 2000 to 2012, the Times published the Los Angeles Times Magazine, which started as a weekly and then became a monthly supplement. The magazine focused on stories and photos of people, places, style, and other cultural affairs occurring in Los Angeles and its surrounding cities and communities. Since 2014, The California Sunday Magazine has been included in the Sunday L.A. Times edition.
In 1996, the Times started the annual Los Angeles Times Festival of Books, in association with the University of California, Los Angeles. It has panel discussions, exhibits, and stages during two days at the end of April each year.[105] In 2011, the Festival of Books was moved to the University of Southern California.[106]
Since 1980, the Times has awarded annual book prizes. The categories are now biography, current interest, fiction, first fiction, history, mystery/thriller, poetry, science and technology, and young adult fiction. In addition, the Robert Kirsch Award is presented annually to a living author with a substantial connection to the American West whose contribution to American letters deserves special recognition".[107]
From 1957 to 1987, the Times sponsored the Los Angeles Times Grand Prix that was held over at the Riverside International Raceway in Moreno Valley, California.
The Times Mirror Corporation has also owned a number of book publishers over the years, including New American Library and C.V. Mosby, as well as Harry N. Abrams, Matthew Bender, and Jeppesen.[108]
In 1967, Times Mirror acquired C.V. Mosby Company, a professional publisher and merged it over the years with several other professional publishers including Resource Application, Inc., Year Book Medical Publishers, Wolfe Publishing Ltd., PSG Publishing Company, B.C. Decker, Inc., among others. Eventually in 1998 Mosby was sold to Harcourt Brace & Company to form the Elsevier Health Sciences group.[110]
The Times-Mirror Company was a founding owner of television station KTTV in Los Angeles, which opened in January 1949. It became that station's sole owner in 1951, after re-acquiring the minority shares it had sold to CBS in 1948. Times-Mirror also purchased a former motion picture studio, Nassour Studios, in Hollywood in 1950, which was then used to consolidate KTTV's operations. Later to be known as Metromedia Square, the studio was sold along with KTTV to Metromedia in 1963.
After a seven-year hiatus from the medium, the firm reactivated Times-Mirror Broadcasting Company with its 1970 purchase of the Dallas Times Herald and its radio and television stations, KRLD-AM-FM-TV in Dallas.[111] The Federal Communications Commission granted an exemption of its cross-ownership policy and allowed Times-Mirror to retain the newspaper and the television outlet, which was renamed KDFW-TV.
Times-Mirror Broadcasting later acquired KTBC-TV in Austin, Texas in 1973;[112] and in 1980 purchased a group of stations owned by Newhouse Newspapers: WAPI-TV (now WVTM-TV) in Birmingham, Alabama; KTVI in St. Louis; WSYR-TV (now WSTM-TV) in Syracuse, New York and its satellite station WSYE-TV (now WETM-TV) in Elmira, New York; and WTPA-TV (now WHTM-TV) in Harrisburg, Pennsylvania.[113]  The company also entered the field of cable television, servicing the Phoenix and San Diego areas, amongst others. They were originally titled Times-Mirror Cable, and were later renamed to Dimension Cable Television. Similarly, they also attempted to enter the pay-TV market, with the Spotlight movie network; it wasn't successful and was quickly shut down. The cable systems were sold in the mid-1990s to Cox Communications.
Times-Mirror also pared its station group down, selling off the Syracuse, Elmira and Harrisburg properties in 1986.[114] The remaining four outlets were packaged to a new upstart holding company, Argyle Television, in 1993.[115] These stations were acquired by New World Communications shortly thereafter and became key components in a sweeping shift of network-station affiliations which occurred between 1994 and 1995.
* From 1985 to 1990: Pulitzer Prize for General News Reporting; From 1991 to 1997: Pulitzer Prize for Spot News Reporting; From 1998 to present: Pulitzer Prize for Breaking News Reporting
The Chicago Tribune is a daily newspaper based in Chicago, Illinois, United States, owned by Tribune Publishing. Founded in 1847, and formerly self-styled as the "World's Greatest Newspaper" (a slogan for which WGN radio and television are named), it remains the most-read daily newspaper in the Chicago metropolitan area and the Great Lakes region. In 2017, it had the sixth-highest circulation of any American newspaper.[2]
In the 1850s, under Joseph Medill, the Chicago Tribune became closely associated with the Illinois politician[3] Abraham Lincoln, and the Republican Party's progressive wing. In the 20th century, under Medill's grandson Robert R. McCormick, it achieved a reputation as a crusading paper with a decidedly more American-conservative anti-New Deal outlook, and its writing reached other markets through family and corporate relationships at the New York Daily News and the Washington Times-Herald. The 1960s saw its corporate parent owner, Tribune Company, reach into new markets. In 2008, for the first time in its over-a-century-and-a-half history, its editorial page endorsed a Democrat, Illinoisan Barack Obama, for U.S. president.[4]
Originally published solely as a broadsheet, the Tribune announced on January 13, 2009, that it would continue publishing as a broadsheet for home delivery, but would publish in tabloid format for newsstand, news box, and commuter station sales.[5] This change, however, proved to be unpopular with readers, and in August 2011, the Tribune discontinued the tabloid edition, returning to its established broadsheet format through all distribution channels.[6]
The Tribune's masthead displays the American flag, in reference to the paper's former motto, "An American Paper for Americans". The motto is no longer displayed on the masthead, where it was placed below the flag.
The Tribune was owned by parent company Tribune Publishing. In May 2021, Tribune Publishing was acquired by Alden Global Capital, which operates its media properties through Digital First Media.[7][8][9][10][11][excessive citations]
The Tribune was founded by James Kelly, John E. Wheeler, and Joseph K. C. Forrest, publishing the first edition on June 10, 1847. Numerous changes in ownership and editorship took place over the next eight years. Initially, the Tribune was not politically affiliated, but tended to support either the Whig or Free Soil parties against the Democrats in elections.[12] By late 1853, it was frequently running xenophobic editorials that criticized foreigners and Roman Catholics.[13] About this time it also became a strong proponent of temperance.[14] However nativist its editorials may have been, it was not until February 10, 1855, that the Tribune formally affiliated itself with the nativist American or Know Nothing party, whose candidate Levi Boone was elected Mayor of Chicago the following month.[15]
By about 1854, part-owner Capt. J. D. Webster, later General Webster and chief of staff at the Battle of Shiloh, and Dr. Charles H. Ray of Galena, Illinois, through Horace Greeley, convinced Joseph Medill of Cleveland's Leader to become managing editor.[16][17][18] Ray became editor-in-chief, Medill became the managing editor, and Alfred Cowles, Sr., brother of Edwin Cowles, initially was the bookkeeper. Each purchased one third of the Tribune.[19][20] Under their leadership, the Tribune distanced itself from the Know Nothings, and became the main Chicago organ of the Republican Party.[21] However, the paper continued to print anti-Catholic and anti-Irish editorials, in the wake of the massive Famine immigration from Ireland.[22]
The Tribune absorbed three other Chicago publications under the new editors: the Free West in 1855, the Democratic Press of William Bross in 1858, and the Chicago Democrat in 1861, whose editor, John Wentworth, left his position when elected as Mayor of Chicago. Between 1858 and 1860, the paper was known as the Chicago Press & Tribune. On October 25, 1860, it became the Chicago Daily Tribune.[23] Before and during the American Civil War, the new editors strongly supported Abraham Lincoln, whom Medill helped secure the presidency in 1860, and pushed an abolitionist agenda.[citation needed] The paper remained a force in Republican politics for years afterwards.[citation needed]
In 1861, the Tribune published new lyrics by William W. Patton for the song "John Brown's Body". These rivaled the lyrics published two months later by Julia Ward Howe. Medill served as mayor of Chicago for one term after the Great Chicago Fire of 1871.[citation needed]
Under the 20th-century editorship of Colonel Robert R. McCormick, who took control in the 1920s, the paper was strongly isolationist and aligned with the Old Right in its coverage of political news and social trends. It used the motto "The American Paper for Americans". From the 1930s to the 1950s, it excoriated the Democrats and the New Deal of Franklin D. Roosevelt, was resolutely disdainful of the British and French, and greatly enthusiastic for Chiang Kai-shek and Sen. Joseph McCarthy.
When McCormick assumed the position of co-editor (with his cousin Joseph Medill Patterson) in 1910, the Tribune was the third-best-selling paper among Chicago's eight dailies, with a circulation of only 188,000.[24] The young cousins added features such as advice columns and homegrown comic strips such as Little Orphan Annie and Moon Mullins. They promoted political "crusades", with their first success coming with the ouster of the Republican political boss of Illinois, Sen. William Lorimer.[24] At the same time, the Tribune competed with the Hearst paper, the Chicago Examiner, in a circulation war. By 1914, the cousins succeeded in forcing out Managing Editor William Keeley. By 1918, the Examiner was forced to merge with the Chicago Herald.
In 1919, Patterson left the Tribune and moved to New York to launch his own newspaper, the New York Daily News.[24] In a renewed circulation war with Hearst's Herald-Examiner, McCormick and Hearst ran rival lotteries in 1922. The Tribune won the battle, adding 250,000 readers to its ranks. Also in 1922, the Chicago Tribune hosted an international design competition for its new headquarters, the Tribune Tower. The competition worked brilliantly as a publicity stunt, and more than 260 entries were received. The winner was a neo-Gothic design by New York architects John Mead Howells and Raymond Hood.
The newspaper sponsored a pioneering attempt at Arctic aviation in 1929, an attempted round-trip to Europe across Greenland and Iceland in a Sikorsky amphibious aircraft.[25] But, the aircraft was destroyed by ice on July 15, 1929, near Ungava Bay at the tip of Labrador, Canada. The crew were rescued by the Canadian science ship CSS Acadia.[26]
The Tribune's legendary sports editor Arch Ward created the Major League Baseball All-Star Game in 1933 as part of the city's Century of Progress exposition.
From 1940 to 1943, the paper supplemented its comic strip offerings with The Chicago Tribune Comic Book, responding to the new success of comic books. At the same time, it launched the more successful and longer-lasting The Spirit Section, which was also an attempt by newspapers to compete with the new medium.[27]
Under McCormick's stewardship, the Tribune was a champion of modified spelling for simplicity (such as spelling "although" as "altho").[28][29] McCormick, a vigorous campaigner for the Republican Party, died in 1955, just four days before Democratic boss Richard J. Daley was elected mayor for the first time.
One of the great scoops in Tribune history came when it obtained the text of the Treaty of Versailles in June 1919. Another was its revelation of United States war plans on the eve of the Pearl Harbor attack. The Tribune's June 7, 1942, front page announcement that the United States had broken Japan's naval code was the revelation by the paper of a closely guarded military secret.[30] The story revealing that Americans broke the enemy naval codes was not cleared by censors, and had U.S. President Franklin D. Roosevelt so enraged that he considered shutting down the Tribune.[31][32][33][34]
The paper is well known for a mistake it made during the 1948 presidential election. At that time, much of its composing room staff was on strike. The early returns led editors to believe (along with many in the country) that the Republican candidate Thomas Dewey would win. An early edition of the next day's paper carried the headline "Dewey Defeats Truman", turning the paper into a collector's item. Democrat Harry S. Truman won and proudly brandished the newspaper in a famous picture taken at St. Louis Union Station. Beneath the headline was a false article, written by Arthur Sears Henning, which purported to describe West Coast results although written before East Coast election returns were available.
A week later, after studying the transcripts, the paper's editorial board observed that "the high dedication to grand principles that Americans have a right to expect from a President is missing from the transcript record." The Tribune's editors concluded that "nobody of sound mind can read [the transcripts] and continue to think that Mr. Nixon has upheld the standards and dignity of the Presidency," and called for Nixon's resignation. The Tribune call for Nixon to resign made news, reflecting not only the change in the type of conservatism practiced by the paper, but as a watershed event in terms of Nixon's hopes for survival in office. The White House reportedly perceived the Tribune's editorial as a loss of a long-time supporter and as a blow to Nixon's hopes to weather the scandal.
In January 1977, Tribune columnist Will Leonard died at age 64.[39]
In March 1978, the Tribune announced that it hired columnist Bob Greene from the Chicago Sun-Times.[40]
On December 7, 1975, Kirkpatrick announced in a column on the editorial page that Rick Soll, a "young and talented columnist" for the paper, whose work had "won a following among many Tribune readers over the last two years", had resigned from the paper. He had acknowledged that a November 23, 1975 column he wrote contained verbatim passages written by another columnist in 1967 and later published in a collection. Kirkpatrick did not identify the columnist. The passages in question, Kirkpatrick wrote, were from a notebook where Soll regularly entered words, phrases and bits of conversation which he had wished to remember. The paper initially suspended Soll for a month without pay. Kirkpatrick wrote that further evidence was revealed came out that another of Soll's columns contained information which he knew was false. At that point, Tribune editors decided to accept the resignation offered by Soll when the internal investigation began.[41]
After leaving, Soll married Pam Zekman, a Chicago newspaper (and future TV) reporter. He worked for the short-lived[42][43] Chicago Times magazine,[44] by Small Newspaper Group Inc. of Kankakee, Illinois,[45] in the late 1980s.
Soll was born in 1946, in Chicago, to Marjorie and Jules Soll. Soll graduated from New Trier High School, received a Bachelor of Arts in 1968 from Colgate University, and a Master's Degree from Medill School of Journalism, Northwestern University in 1970.[46][47]
Jack Fuller served as the Tribune's editor from 1989 until 1993, when he became the president and chief executive officer of the Chicago Tribune. Howard Tyner served as the Tribune's editor from 1993 until 2001, when he was promoted to vice president/editorial for Tribune Publishing.
The Tribune won 11 Pulitzer prizes during the 1980s and 1990s.[36] Editorial cartoonist Dick Locher won the award in 1983, and editorial cartoonist Jeff MacNelly won one in 1985. Then, future editor Jack Fuller won a Pulitzer for editorial writing in 1986. In 1987, reporters Jeff Lyon and Peter Gorner won a Pulitzer for explanatory reporting, and in 1988, Dean Baquet, William Gaines and Ann Marie Lipinski won a Pulitzer for investigative reporting. In 1989, Lois Wille won a Pulitzer for editorial writing and Clarence Page snagged the award for commentary. In 1994, Ron Kotulak won a Pulitzer for explanatory journalism, while R. Bruce Dold won it for editorial writing. In 1998, reporter Paul Salopek won a Pulitzer for explanatory writing, and in 1999, architecture critic Blair Kamin won it for criticism.[36]
In September 1981, baseball writer Jerome Holtzman was hired by the Tribune after a 38-year career at the Sun-Times.
In September 1982, the Chicago Tribune opened a new $180 million printing facility, Freedom Center.[48]
In November 1982, Tribune managing editor William H. "Bill" Jones, who had won a Pulitzer Prize in 1971, died at age 43 of cardiac arrest as a result of complications from a long battle with leukemia.[49]
In May 1983, Tribune columnist Aaron Gold died at age 45 of complications from leukemia.[50] Gold had coauthored the Tribune's "Inc." column with Michael Sneed and prior to that had written the paper's "Tower Ticker" column.
The Tribune scored a coup in 1984 when it hired popular columnist Mike Royko away from the rival Sun-Times.[51]
In February 1988, Tribune foreign correspondent Jonathan Broder resigned after a February 22, 1988, Tribune article written by Broder contained a number of sentences and phrases taken, without attribution, from a column written by another writer, Joel Greenberg, that had been published 10 days earlier in The Jerusalem Post.[55][56]
In August 1988, Chicago Tribune reporter Michael Coakley died at age 41 of complications from AIDS.[57]
In an unusual move at that time, the Tribune in October 1993 fired its longtime military-affairs writer, retired-Marine David Evans, with the public position that the post of military affairs was being dropped in favor of having a national security writer.[60]
In December 1993, the Tribune's longtime Washington, D.C. bureau chief, Nicholas Horrock, was removed from his post after he chose not to attend a meeting that editor Howard Tyner requested of him in Chicago.[61] Horrock, who shortly thereafter left the paper, was replaced by James Warren, who attracted new attention to the Tribune's D.C. bureau through his continued attacks on celebrity broadcast journalists in Washington.
Also in December 1993, the Tribune hired Margaret Holt from the South Florida Sun-Sentinel as its assistant managing editor for sports, making her the first female to head a sports department at any of the nation's 10 largest newspapers.[62] In mid-1995, Holt was replaced as sports editor by Tim Franklin and shifted to a newly created job, customer service editor.[63]
In 1994, reporter Brenda You was fired by the Tribune after free-lancing for supermarket tabloid newspapers and lending them photographs from the Tribune's photo library.[40] You later worked for the National Enquirer and as a producer for The Jerry Springer Show before committing suicide in November 2005.[64]
In April 1994, the Tribune's new television critic, Ken Parish Perkins, wrote an article about then-WFLD morning news anchor Bob Sirott in which Perkins quoted Sirott as making a statement that Sirott later denied making. Sirott criticized Perkins on the air, and the Tribune later printed a correction acknowledging that Sirott had never made that statement.[65] Eight months later, Perkins stepped down as TV critic, and he left the paper shortly thereafter.[66]
In December 1995, the alternative newsweekly Newcity published a first-person article by the pseudonymous Clara Hamon (a name mentioned in the play The Front Page) but quickly identified by Tribune reporters as that of former Tribune reporter Mary Hill that heavily criticized the paper's one-year residency program. The program brought young journalists in and out of the paper for one-year stints, seldom resulting in a full-time job. Hill, who wrote for the paper from 1992 until 1993, acknowledged to the Chicago Reader that she had written the diatribe originally for the Internet, and that the piece eventually was edited for Newcity.[67]
In 1997, the Tribune celebrated its 150th anniversary in part by tapping longtime reporter Stevenson Swanson to edit the book Chicago Days: 150 Defining Moments in the Life of a Great City.
On April 29, 1997, popular columnist Mike Royko died of a brain aneurysm. On September 2, 1997, the Tribune promoted longtime City Hall reporter John Kass to take Royko's place as the paper's principal Page Two news columnist.[68]
On June 1, 1997, the Tribune published what ended up becoming a very popular column by Mary Schmich called "Advice, like youth, probably just wasted on the young", otherwise known as "Wear Sunscreen" or the "Sunscreen Speech". The most popular and well-known form of the essay is the successful music single released in 1999, accredited to Baz Luhrmann.
On June 6, 1999, the Tribune published a first-person travel article from freelance writer Gaby Plattner that described a supposed incident in which a pilot for Air Zimbabwe who was flying without a copilot inadvertently locked himself out of his cockpit while the plane was flying on autopilot and as a result needed to use a large ax to chop a hole in the cockpit door.[70] An airline representative wrote a lengthy letter to the paper calling the account "totally untrue, unprofessional and damaging to our airline" and explaining that Air Zimbabwe does not keep axes on its aircraft and never flies without a full crew,[71] and the paper was forced to print a correction stating that Plattner "now says that she passed along a story she had heard as something she had experienced."[70]
The Tribune has been a leader on the Internet, acquiring 10 percent of America Online in the early 1990s, then launching such web sites as Chicagotribune.com (1995), Metromix.com (1996), ChicagoSports.com (1999), ChicagoBreakingNews.com (2008), and ChicagoNow (2009). In 2002, the paper launched a tabloid edition targeted at 18- to 34-year-olds known as RedEye.
Ann Marie Lipinski was the paper's editor from February 2001 until stepping down on July 17, 2008. Gerould W. Kern was named the paper's editor in July 2008.[72] In early August 2008, managing editor for news Hanke Gratteau resigned, and several weeks later, managing editor for features James Warren resigned as well.[73] Both were replaced by Jane Hirt, who previously had been the editor of the Tribune's RedEye tabloid.[73]
In June 2000, Times Mirror merged with Tribune Company making The Baltimore Sun and its community papers Baltimore Sun Media Group / Patuxent Publishing a subsidiary of Tribune.[74][75]
In July 2000, Tribune outdoors columnist John Husar, who had written about his need for a new liver transplant, died at age 63, just over a week after receiving part of a new liver from a live donor.[76]
Tribune's Baltimore Community papers include Arbutus Times, Baltimore Messenger, Catonsville Times, Columbia Flier, Howard County Times, The Jeffersonian, Laurel Leader, Lifetimes, North County News, Northeast Booster, Northeast Reporter, Owings Mills Times, and Towson Times.
The Howard County Times was named 2010 Newspaper of the Year by the Suburban Newspaper Association.[77]
The Towson Times expands coverage beyond the Towson area and includes Baltimore County government and politics.[78][79]
In late 2001, sports columnist Michael Holley announced he was leaving the Tribune after just two months because he was homesick.[82] He ultimately returned to The Boston Globe, where he had been working immediately before the Tribune had hired him.[82]
On September 15, 2002, Lipinski wrote a terse, page-one note informing readers that the paper's longtime columnist, Bob Greene, resigned effective immediately after acknowledging "engaging in inappropriate sexual conduct some years ago with a girl in her late teens whom he met in connection with his newspaper column." The conduct later was revealed to have occurred in 1988 with a woman who was of the age of consent in Illinois. "Greene's behavior was a serious violation of Tribune ethics and standards for its journalists," Lipinski wrote. "We deeply regret the conduct, its effect on the young woman and the impact this disclosure has on the trust our readers placed in Greene and this newspaper."[83][84]
In January 2003, Mike Downey, formerly of the Los Angeles Times, was hired as new Tribune sports columnist. He and colleague Rick Morrissey would write the In the Wake of the News Column originated by Ring Lardner.
In October 2004, Tribune editor Ann Marie Lipinski at the last minute spiked a story written for the paper's WomanNews section by freelance reporter Lisa Bertagnoli titled "You c_nt say that (or can you?)," about a noted vulgarism.[88] The paper ordered every spare body to go to the Tribune's printing plant to pull already-printed WomanNews sections containing the story from the October 27, 2004, package of preprinted sections in the Tribune.[88]
In September 2008, the Tribune considered hiring controversial sports columnist Jay Mariotti, shortly after his abrupt resignation from Tribune archrival Chicago Sun-Times.[89] Discussions ultimately ended, however, after the Sun-Times threatened to sue for violating Mariotti's noncompete agreement, which was to run until August 2009.[89] Sports columnist Rick Morrissey defected to the Sun-Times in December 2009.
In April 2009, 55 Tribune reporters and editors signed their names to an e-mail sent to Kern and managing editor Jane Hirt, questioning why the newspaper's marketing department had solicited subscribers' opinions on stories before they were published, and suggesting that the practice raised ethical questions as well as legal and competitive issues. Reporters declined to speak on the record to the Associated Press about their issues. "We'll let the e-mail speak for itself," reporter John Chase told the AP. In the wake of the controversy, Kern abruptly discontinued the effort, which he described as "a brief market research project".[90]
In the first decade of the 21st century, the Tribune had multiple rounds of reductions of staff through layoffs and buyouts as it has coped with the industrywide declines in advertising revenues:
The Tribune broke the story on May 29, 2009, that several students had been admitted to the University of Illinois based upon connections or recommendations by the school's Board of Trustees, Chicago politicians, and members of the Rod Blagojevich administration. Initially denying the existence of a so-called "Category I" admissions program, university President B. Joseph "Joe" White and Chancellor Richard Herman later admitted that there were instances of preferential treatment. Although they claimed the list was short and their role was minor, the Tribune, in particular, revealed emails through a FOIA finding that White had received a recommendation for a relative of convicted fundraiser Tony Rezko to be admitted. The Tribune also later posted emails from Herman pushing for underqualified students to be accepted.[105][106] The Tribune has since filed suit against the university administration under the Freedom of Information Act to acquire the names of students benefited by administrative clout and impropriety.
On February 8, 2010, the Chicago Tribune shrank its newspaper's width by an inch. They said that the new format was becoming the industry standard and that there would be minimal content changes.
In July 2011, the Chicago Tribune underwent its first round of layoffs of editorial employees in more than two years, letting go about 20 editors and reporters.[107] Among those let go were DuPage County reporter Art Barnum, Editorial Board member Pat Widder and photographer Dave Pierini.[107][108]
On March 15, 2012, the Tribune laid off 15 editorial staffers, including security guard Wendell Smothers (Smothers then died on November 12, 2012).[109][110] At the same time, the paper gave buyouts to six editorial staffers, including Pulitzer Prize-winning reporter William Mullen, Barbara Mahany and Nancy Reese.[111]
In June 2012, the Tribune's Pulitzer Prize-winning cultural critic Julia Keller left the paper to join the faculty of Ohio University and to pursue a career as a novelist.[112]
In September 2012, Tribune education reporter Joel Hood resigned from the paper to become a real estate broker, City Hall reporter Kristen Mack left the paper to become press secretary for Cook County Board President Toni Preckwinkle,[113] and the Tribune hired Pulitzer Prize-winning photographer John J. Kim from the Chicago Sun-Times.[114]
In October 2012, the Tribune's science and medicine reporter, Trine Tsouderos, quit to join a public relations firm.[115]
Also in October 2012, the Tribune announced plans to create a paywall for its website, offering digital-only subscriptions at $14.99 per month, starting on November 1, 2012. Seven-day print subscribers would continue to have unlimited online access at no additional charge.[116]
In late February 2013, the Tribune agreed to pay a total of $660,000 to settle a class-action lawsuit that had been filed against the paper by 46 current and former reporters of its TribLocal local-news reporting group over unpaid overtime wages.[117] The suit had been filed in federal court on behalf of Carolyn Rusin, who had been a TribLocal staff reporter from July 2010 until October 2011.[117] The paper's TribLocal unit had been formed in 2007 and uses staff reporters, freelance writers and user-generated content to produce hyperlocal Chicago-area community news.[117]
On June 12, 2013, the Boston Marathon bombing moving tribute was posted again, which showed the words "We are Chicago" above the names of Boston sports teams.[118] On the graphic on June 12, the word "Bruins" was ripped off and the comment was added, "Yeah, not right now we're not", in a reference to the 2013 Stanley Cup Finals, which play the Chicago Blackhawks against the Boston Bruins.[118] Gerould Kern tweeted later that the Tribune "still supports [Boston] after all you've been through. We regret any offense. Now let's play hockey."[118]
On November 20, 2013, the Tribune laid off another 12 or so editorial staffers.[119]
On April 6, 2014, the Tribune increased the newsstand price of its Sunday/Thanksgiving Day paper by 50 percent to $2.99 for a single copy. The newsrack price increased $0.75, or 42.9%, to $2.50.[120] By January 2017 the price increased again, up $1 or 40% at newsracks, to $3.50. At newsstands it went up also $1, or 33.3%, to $3.99.
On January 28, 2015, metropolitan editor Peter Kendall was named managing editor, replacing Jane Hirt, who had resigned several months earlier. Colin McMahon was named associate editor.[121]
On February 18, 2016, the Tribune announced the retirement of editor Gerould Kern and the immediate promotion of the paper's editorial page editor, R. Bruce Dold, to be the Tribune's editor.[55]
On Jun 9, 2018 the Tribune ended their 93-year stint at Tribune Tower and moved to One Prudential Plaza. The tower was later converted to condo's.[122]
On February 27, 2020, the Tribune announced that publisher and editor Bruce Dold will leave the Tribune on April 30, 2020, and would step down immediately as editor in chief. His replacement as editor was Colin McMahon. Also, the paper announced that one of the two managing editors of the paper, Peter Kendall, would leave the Tribune on February 28, 2020.[123]
In January 2021, the Chicago Tribune moved out of One Prudential Plaza, and relocated their offices and newsroom to Freedom Center.[124]
In May 2021 the paper was purchased by Alden Global Capital.[125] Alden immediately launched a round of employee buyouts, reducing the newsroom staff by 25 percent, and the cuts continued. A former reporter said the paper is being "snuffed out, quarter after quarter after quarter".[126] A report in The Atlantic said that Alden's business model is simple: "Gut the staff, sell the real estate, jack up subscription prices, and wring as much cash as possible out of the enterprise until eventually enough readers cancel their subscriptions that the paper folds, or is reduced to a desiccated husk of its former self."[126]
Mitch Pugh was named the Tribune's executive editor on Aug. 20, 2021, after eight years in the same role at The Post and Courier in Charleston, South Carolina.[127]
In a 2007 statement of principles published in the Tribune's print and online editions, the paper's editorial board described the newspaper's philosophy, from which is excerpted the following:
The Chicago Tribune believes in the traditional principles of limited government; maximum individual responsibility; minimum restriction of personal liberty, opportunity and enterprise. It believes in free markets, free will and freedom of expression. These principles, while traditionally conservative, are guidelines and not reflexive dogmas.
The Tribune brings a Midwestern sensibility to public debate. It is suspicious of untested ideas.
The Tribune places great emphasis on the integrity of government and the private institutions that play a significant role in society. The newspaper does this in the belief that the people cannot consent to be governed unless they have knowledge of, and faith in, the leaders and operations of government. The Tribune embraces the diversity of people and perspectives in its community. It is dedicated to the future of the Chicago region.
The Tribune has remained economically conservative, being widely skeptical of increasing the minimum wage and entitlement spending. Although the Tribune criticized the Bush administration's record on civil liberties, the environment, and many aspects of its foreign policy, it continued to support his presidency while taking Democrats, such as Illinois Governor Rod Blagojevich and Cook County Board President Todd Stroger, to task and calling for their removal from office.
As of 2018[update], the Chicago Tribune and Los Angeles Times have taken down their websites in most European countries due to GDPR.[128]
The Tribune has occasionally backed independent candidates for president. In 1872, it supported Horace Greeley, a former Republican Party newspaper editor,[132] and in 1912 the paper endorsed Theodore Roosevelt, who ran on the Progressive Party slate against Republican President William Howard Taft. In 2016, the Tribune endorsed the Libertarian Party candidate, former New Mexico Governor Gary Johnson, for president, over Republican Donald Trump and Democrat Hillary Clinton.[133]
Over the years, the Tribune has endorsed some Democrats for lesser offices, including recent endorsements of Bill Foster, Barack Obama for the Senate and Democrat Melissa Bean, who defeated Philip Crane, the House of Representatives' longest-serving Republican. Although the Tribune endorsed George Ryan in the 1998 Illinois gubernatorial race, the paper subsequently investigated and reported on the scandals surrounding Ryan during his preceding years as Secretary of State. Ryan declined to run for re-election in 2002 and was subsequently indicted, convicted and imprisoned as a result of the scandal.
From 1925 to 2018, the Chicago Tribune was housed in the Tribune Tower on North Michigan Avenue on the Magnificent Mile. The building is neo-Gothic in style, and the design was the winner of an international competition hosted by the Tribune. The Chicago Tribune moved in June 2018 to the Prudential Plaza office complex overlooking Millennium Park after Tribune Media sold Tribune Tower to developers.
The September 2008 redesign (discussed on the Tribune's web site[135]) was controversial and is largely regarded as an effort in cost-cutting.[136] Since then the newspaper has returned to a more toned down style. The style is more a mix of the old style and a new modern style.
Sam Zell originally planned to turn the company into a private company through the creation of an ESOP (employee stock ownership plan) within the company, but due to poor management that existed prior to his ownership, this did not work out as well as he intended.[139]
As part of its bankruptcy plan, owner Sam Zell intended to sell the Cubs to reduce debt. This sale has become linked to the corruption charges leading to the December 9, 2008, arrest of former Illinois Governor Rod Blagojevich. Specifically, the ex-governor was accused of exploiting the paper's financial trouble in an effort to have several editors fired.[140]
In the bankruptcy, unsecured bondholders of Tribune Co. essentially claimed that ordinary Tribune shareholders participated in a "fraudulent transfer" of wealth.[141]
The Tribune Co. emerged from bankruptcy in January 2013, partially owned by private equity firms which had speculated on its distressed debt. The reorganized company's plan included selling off many of its assets.[144]
Tribune Publishing, owning the Chicago Tribune, Los Angeles Times, and eight other newspapers, was spun off as a separate publicly traded company in August 2014. The parent Tribune Company was renamed Tribune Media.[145] Tribune Publishing started life with a $350 million loan, $275 million of which was paid as a dividend to Tribune Media. The publishing company was also due to lease its office space from Tribune Media for $30 million per year through 2017.[145][146]
Spinning off Tribune Publishing avoided the capital gains taxes that would accrue from selling those assets. The shares in Tribune Publishing were given tax-free to stakeholders in Tribune Media, the largest shareholder was Oaktree Capital Management with 18.5%.[146] Tribune Media, retaining the non-newspaper broadcasting, entertainment, real estate, and other investments, also sold off some of the non-newspaper properties.[145]
NBC News Wall Street Journal Politico MSNBC/CNBC/Telemundo Bloomberg BNA Washington Examiner Boston Globe/Washington Blade
Fox News CBS News Radio AP Radio/PBS VOA  Time  Yahoo! News Daily Caller/EWTN
CBS News Bloomberg News McClatchy NY Post/TheGrio Washington Times Salem Radio/CBN Cheddar News/Hearst TV
AP NPR Foreign pool The Hill Regionals Newsmax Gray TV/Spectrum News
ABC News Washington Post Agence France-Presse  Fox Business/Fox News Radio CSM/Roll Call Al JazeeraNexstar/Scripps News
Reuters NY Times LA Times Univision/AURN  RealClearPolitics Daily Beast/Dallas Morning News  BBC/Newsweek
CNN USA Today ABC News RadioDaily Mail National JournalHuffPostFinancial Times/The Guardian
 Managing Editor SI.com: Stephen Cannella Managing Editor SI Golf Group: Jim GorantCreative Director: Christopher HercikDirector of Photography: Brad Smith[2] Senior Editor, Chief of Reporters: Richard Demak Senior Editors: Mark Bechtel, Trisha Lucey Blackmar, MJ Day (Swimsuit); Mark Godich;  Stefanie Kaufman (Operations); Kostya P.
Sports Illustrated (SI) is an American sports magazine first published in August 1954. Founded by Stuart Scheftel, it was the first magazine with circulation over one million to win the National Magazine Award for General Excellence twice. It is also known for its annual swimsuit issue, which has been published since 1964, and has spawned other complementary media works and products.
Owned until 2018 by Time Inc., it was sold to Authentic Brands Group (ABG) following the sale of Time Inc. to Meredith Corporation. The Arena Group (formerly theMaven, Inc.) was subsequently awarded a 10-year license to operate the Sports Illustrated-branded editorial operations, while ABG licenses the brand for other non-editorial ventures and products.
There were two magazines named Sports Illustrated before the current magazine was launched on August 9, 1954.[4] In 1936, Stuart Scheftel created Sports Illustrated with a target market of sportsmen. He published the magazine from 1936 to 1942 on a monthly basis. The magazine focused on golf, tennis, and skiing with articles on the major sports. He then sold the name to Dell Publications, which released Sports Illustrated in 1949 and this version lasted six issues before closing. Dell's version focused on major sports (baseball, basketball, boxing) and competed on magazine racks against Sport and other monthly sports magazines. During the 1940s these magazines were monthly and they did not cover the current events because of the production schedules. There was no large-base, general, weekly sports magazine with a national following on actual active events. It was then that Time patriarch Henry Luce began considering whether his company should attempt to fill that gap. At the time, many believed sports was beneath the attention of serious journalism and did not think sports news could fill a weekly magazine, especially during the winter. A number of advisers to Luce, including Life magazine's Ernest Havemann, tried to kill the idea, but Luce, who was not a sports fan, decided the time was right.[5]
The early issues of the magazine seemed caught between two opposing views of its audience. Much of the subject matter was directed at upper-class activities such as yachting, polo and safaris, but upscale would-be advertisers were unconvinced that sports fans were a significant part of their market.[8]
After more than a decade of steady losses, the magazine's fortunes finally turned around in the 1960s when Andre Laguerre became its managing editor. A European correspondent for Time, Inc., who later became chief of the Time-Life news bureaux in Paris and London (for a time he ran both simultaneously), Laguerre attracted Henry Luce's attention in 1956 with his singular coverage of the Winter Olympic Games in Cortina d'Ampezzo, Italy, which became the core of SI's coverage of those games. In May 1956, Luce brought Laguerre to New York to become the assistant managing editor of the magazine. He was named managing editor in 1960, and he more than doubled the circulation by instituting a system of departmental editors, redesigning the internal format,[9] and inaugurating the unprecedented use in a news magazine of full-color photographic coverage of the week's sports events. He was also one of the first to sense the rise of national interest in professional football.[10]
Laguerre also instituted the innovative concept of one long story at the end of every issue, which he called the "bonus piece". These well-written, in-depth articles helped to distinguish Sports Illustrated from other sports publications, and helped launch the careers of such legendary writers as Frank Deford, who in March 2010 wrote of Laguerre, "He smoked cigars and drank Scotch and made the sun move across the heavens ... His genius as an editor was that he made you want to please him, but he wanted you to do that by writing in your own distinct way."[11]
Laguerre is also credited with the conception and creation of the annual Swimsuit Issue, which quickly became, and remains, the most popular issue each year.
In 1990, Time Inc. merged with Warner Communications to form the media conglomerate Time Warner. In 2014, Time Inc. was spun off from Time Warner.
In 2018, the magazine was sold to Meredith Corporation by means of its acquisition of parent company Time Inc., however Meredith planned to sell Sports Illustrated due to not aligning with its lifestyle properties.[13] Authentic Brands Group announced its intent to acquire Sports Illustrated for $110 million the next year, stating that it would leverage its brand and other assets for new opportunities that "stay close to the DNA and the heritage of the brand." Upon the announcement, Meredith would enter into a licensing agreement to continue as publisher of the Sports Illustrated editorial operations for at least the next two years.[14][15] In June 2019, the rights to publish the Sports Illustrated editorial operations were licensed to the digital media company theMaven, Inc. under a 10-year contract, with Ross Levinsohn as CEO. The company had backed a bid by Junior Bridgeman to acquire SI.[16][17][18] In preparation for the closure of the sale to ABG and Maven,[19] The Wall Street Journal reported that there would be Sports Illustrated employee layoffs,[20] which was confirmed after the acquisition had closed.[21]
In October 2019, editor-in-chief Chris Stone stepped down.[22] Later that month, Sports Illustrated announced its hiring of veteran college sports writer Pat Forde.[23] In January 2020, it announced an editorial partnership with The Hockey News, focusing on syndication of NHL-related coverage.[24][25] In 2021, it announced a similar partnership with Morning Read for golf coverage, with its website being merged into that of Sports Illustrated.[26] It also partnered with iHeartMedia to distribute and co-produce podcasts.[27]
In September 2021, Maven, now known as The Arena Group, acquired the New Jersey-based sports news website The Spun,  which would integrate into Sports Illustrated.[28] In 2022, ABG announced several non-editorial ventures involving the Sports Illustrated brand, including an apparel line for JCPenney "inspired by iconic moments in sports" (it was not the brand's first foray into clothing, as it launched a branded swimsuit line in conjunction with its Swimsuit Issue in 2018),[29] and resort hotels in Orlando and Punta Cana.[30]
From its start, Sports Illustrated introduced a number of innovations that are generally taken for granted today:
In 1965, offset printing began. This allowed the color pages of the magazine to be printed overnight, not only producing crisper and brighter images, but also finally enabling the editors to merge the best color with the latest news. By 1967, the magazine was printing 200 pages of "fast color" a year; in 1983, SI became the first American full-color newsweekly. An intense rivalry developed between photographers, particularly Walter Iooss and Neil Leifer, to get a decisive cover shot that would be on newsstands and in mailboxes only a few days later.[31]
In the late 1970s and early 1980s, during Gil Rogin's term as Managing Editor, the feature stories of Frank Deford became the magazine's anchor. "Bonus pieces" on Pete Rozelle, Woody Hayes, Bear Bryant, Howard Cosell and others became some of the most quoted sources about these figures, and Deford established a reputation as one of the best writers of the time.[32]
In 1956, Sports Illustrated began presenting annual awards to fashion or clothing designers who had excelled in the field of sportswear/activewear. The first ASDAs of 1956, presented to Claire McCardell with a separate Designer of the Year award to Rudi Gernreich, were chosen following a vote of 200 American top retailers.[33] The following year, the voting pool had increased to 400 fashion industry experts, including Dorothy Shaver and Stanley Marcus, when Sydney Wragge and Bill Atkinson received the awards.[34] The Italian designer Emilio Pucci was the first non-American to receive the award in 1961.[35] The awards were presented up until at least 1963, when Marc Bohan received the prize.[36] Other winners include Jeanne S. Campbell, Bonnie Cashin and Rose Marie Reid who formed the first all-women winning group in 1958.[37]
Maya Moore of the WNBA's Minnesota Lynx was the inaugural winner of the Sports Illustrated Performer of the Year Award in 2017.[38]
Since 1954, Sports Illustrated magazine has annually presented the Sportsperson of the Year award to "the athlete or team whose performance that year most embodies the spirit of sportsmanship and achievement."[39][40] Roger Bannister won the first-ever Sportsman of the Year award thanks to his record-breaking time of 3:59.4 for a mile (the first-ever time a mile had been run under four minutes).[39][41] Both men and women have won the award, originally called "Sportsman of the Year" and renamed "Sportswoman of the Year" or "Sportswomen of the Year" when applicable; it is currently known as "Sportsperson of the Year."
The 2018 winners were the Golden State Warriors as a team for winning their third NBA Title in four years.
The 2021  winner was Tom Brady for his Super Bowl 55 win.
In 1999, Sports Illustrated named Muhammad Ali the Sportsman of the Century at the Sports Illustrated's 20th Century Sports Awards in New York City's Madison Square Garden.[44]
For a 2002 list of the top 200 Division I sports colleges in the U.S., see footnote.[49]
The magazine's cover is the basis of a sports myth known as the Sports Illustrated Cover Jinx.
Fathers and sons who have been featured on the cover
Sports Illustrated has helped launched a number of related publishing ventures, including:
KXGN-TV (channel 5) is a television station in Glendive, Montana, United States, affiliated with CBS and NBC. It is owned by The Marks Group alongside radio stations KXGN (1400 AM) and KDZN (96.5 FM). The three stations share studios on South Douglas Street in downtown Glendive; KXGN-TV's transmitter is located at Makoshika State Park. The station also airs news and other programs from the Montana Television Network, a network of CBS affiliates in Montana.
KXGN-TV is the only television station in Glendive, reckoned as the smallest television market in the United States. Nielsen Media Research ranks it last of the 210 designated market areas for television in the United States, with just 3,900 households.[1] Its status as the smallest station in the United States has earned it notoriety in the broadcasting industry; over its history, publications including the Los Angeles Times and Sports Illustrated have profiled KXGN-TV. The station's lone local program is a public affairs program covering issues in eastern Montana, though in the past it has produced limited local newscasts, and it does provide regional newscasts from the CBS and NBC affiliates in Billings.
We'd go to New York to see the networks, and they'd ask us where we were from. It was bad enough that we should tell them Montana, but when we added Glendive, well, they'd never heard of it.
The sale took longer than Moore had expected because the FCC at the time had a rule that normally barred cross-ownership of radio and television stations. It was not until May 1990 that the FCC granted a waiver, noting the economic conditions inherent in the small-market stations, their extensively integrated operation, and the fact they had been co-owned for the television station's entire history. The commission also cited the availability of other electronic media through two Glendive-licensed radio stations, six other signals, and the cable system (which Moore sold off in 1986[2]), as well as a daily newspaper.[10] Marks added KDZN in 1995; the FCC approved of the purchase of the FM station because of the substantial losses that KXGN AM, then supported entirely by the TV station, and KDZN had incurred in the region's continuing poor economy.[11] KXGN also aired some Fox programming, primarily the NFL on Fox, when Fox gained football rights from CBS in 1994.[12]
Frenzel died of a heart condition in 2003[13] and was replaced by Paul Sturlaugson as general manager.[7]
Sturlaugson's most pressing challenge in the 2000s was leading the station through its costly upgrade to digital television. If not for the DTV Delay Act pushing the final cutoff date back by four months from February to June 2009, KXGN-TV would not have converted in a timely manner, as the equipment had not arrived by February.[14] While many stations had a May 1, 2002, deadline to start a digital signal, KXGN-TV requested and received multiple extensions due to financial hardship.[15] In 2008, the FCC had permitted it to convert to digital on VHF channel 5 instead of the originally allocated channel 10,[16] a process that saved money but delayed installation of the facility.[7] After the successful digital conversion,[17] in September 2009, KXGN added a dedicated NBC subchannel, an idea Sturlaugson had discussed prior to the transition;[7] Marks had previously signed KXGN up to carry the never-launched .2 Network in 2008.[18] The station's various translators were converted to digital service by their operators in the years that followed; for instance, the retransmitters at Plevna were converted at the end of 2011, also expanding the reach of the NBC subchannel.[19]
Marks died on May 11, 2022; his company The Marks Group had 14 radio stations and five TV stations (including KXGN-TV) at the time of his death.[20]
Since 1990, KXGN-TV has been a formal member of the Montana Television Network (MTN), airing the noon and evening newscasts of KTVQ in Billings and contributing Eastern Montana news to MTN.[22]
The NBC subchannel (5.2) airs NBC programming generally in pattern for the Mountain Time Zone. Regional newscasts from KULR-TV in Billings, with the exception of the first hour of its morning newscast, Wake Up Montana, are also shown live.[21]
KXGN aired a daily evening local newscast under various titles, including Action 5 News and Montana East News, until 2015. (The title Action 5 News was used in the 1980s when Terry Kegley anchored the newscast; he also chose the name.[2]) The newsgathering and production was often a one-person operation in which the anchor conducted interviews for the newscast and then produced the program with one studio camera.[8] Former longtime personality Ed Agre, who joined KXGN in 1993, was once profiled by Sports Illustrated for his duties in this capacity, including traveling to produce high school sports shows.[23]
Like many other Montana stations, KXGN relies heavily on a mix of broadcast translators and cable TV systems to extend its reach to more viewers, many of them outside of the defined Glendive market, from Ekalaka in the south to Scobey and Plentywood in the north.[28]
This is a record of material that was recently featured on the Main Page as part of Did you know (DYK). Recently created new articles, greatly expanded former stub articles and recently promoted good articles are eligible; you can submit them for consideration. 
Archives are generally grouped by month of Main Page appearance. (Currently, DYK hooks are archived according to the date and time that they were taken off the Main Page.) To find which archive contains the fact that appeared on Did you know, go to article's talk page and follow the archive link in the DYK talk page message box.
Creating an article is one of the more difficult tasks on Wikipedia, and you'll have a higher chance of success if you help us out with other tasks first to learn more about how Wikipedia works. You can always come back to create an article later; there is no rush!
Welcome to Wikipedia! As a new editor, you will become a Wikipedian. Before starting a new article, please review Wikipedia's notability requirements. In short, the topic of an article must have already been the subject of publication in reliable, secondary, entirely independent sources that treat the topic in substantive detail, such as books, newspapers, magazines, peer-reviewed scholarly journals and websites that meet the same requirements as reputable print-based sources. Information on Wikipedia must be verifiable; if no reliable third-party sources can be found on a topic, then it should not have a separate article. Please search Wikipedia first to make sure that an article does not already exist on the subject.
An Article Wizard is available to help you create an article through the Articles for Creation process, where it will be reviewed and considered for publication. Please note that the backlog is long (currently, there are 2,733 pending submissions; it often takes months). The ability to create articles directly in the mainspace is restricted to editors with some experience. For information on how to request a new article that can be created by someone else, see Requested articles.
Please consider taking a look at our introductory tutorial or reviewing contributing to Wikipedia to learn the basics about editing. Working on existing articles is a good way to learn our protocols and style conventions; see the Task Center for a range of articles that need your assistance and tasks you can help out with.
First, please be aware that Wikipedia is an encyclopedia written by volunteers.  Our mission is to share reliable knowledge to benefit people who want to learn. We are not social media, a place to promote a company or product or person, to advocate for or against anyone or anything, nor a place to first announce to the world information on topics that have not already been the subject of reliable publication. Please keep this in mind, always. (This is described in "What Wikipedia is not".)
Here are some tips that can help you with your first article:
If you are logged in, and your account is autoconfirmed, you can also use this box below to create an article, by entering the article name in the box below and then clicking "Create page".
The English Wikipedia already has 6,622,748 articles. Before creating an article, try to make sure there is not already an article on the same topic, perhaps under a slightly different name. Search for the article, and review Wikipedia's article titling policy before creating your first article. If an article on your topic already exists, but you think people might  look for it under some different name or spelling, learn how to create redirects to alternative titles; adding needed redirects is a good way to help Wikipedia. If you're not already autoconfirmed, you can request a redirect to be created at Wikipedia:Articles for creation/Redirects and categories, where a volunteer will review the request, and if it seems like a plausible search term, accept the redirect request. Also, remember to check the article's deletion log in order to avoid creating an article that has already been deleted. (In some cases, the topic may be suitable even if deleted in the past; the past deletion may have been because it was a copyright violation, did not explain the importance of the topic, or on other grounds addressed to the writing rather than the topic's suitability.)
If a search does not find the topic, consider broadening your search to find existing articles that might include the subject of your article. For example, if you want to write an article about a band member, you might search for the band and then add information about your subject as a section within that broader article.
Gather sources for the information you will be writing about. You will use references to establish notability and to cite particular facts. References used to support notability must meet additional criteria beyond reliability. References used for specific facts need not meet these additional criteria.
To be worthy of inclusion in an encyclopedia, a subject must be sufficiently notable, and that notability must be verifiable through citations to reliable sources.
As noted, the sources you use must be reliable; that is, they must be sources that exercise some form of editorial control and have some reputation for fact-checking and accuracy. Print sources (and web-based versions of those sources) tend to be the most reliable, though some web-only sources may also be reliable. Examples might include (but are not limited to) books published by major publishing houses, newspapers, magazines, peer-reviewed scholarly journals, websites of any of the above, and other websites that meet the same requirements as a reputable print-based source.
In general, sources with no editorial control are not reliable. These include (but are not limited to) books published by vanity presses, self-published 'zines', blogs, web forums, Usenet discussions, personal social media, fan sites, vanity websites that permit the creation of self-promotional articles, and other similar venues. If anyone at all can post information without anyone else checking that information, it is probably not reliable.
If there are reliable sources (such as newspapers, journals, or books) with extensive information published over an extended period about a subject, then that subject is notable. You must cite such sources as part of the process of creating (or expanding) the Wikipedia article as evidence of notability for evaluation by other editors. If you cannot find such reliable sources that provide extensive and comprehensive information about your proposed subject, then the subject is not notable or verifiable and almost certainly will be deleted. So your first job is to go find references to cite.
There are many places to find reliable sources, including your local library, but if internet-based sources are to be used, start with books and news archive searches rather than a web search.
Once you have references for your article, you can learn to place the references into the article by reading Help:Referencing for beginners and Wikipedia:Citing sources. Do not worry too much about formatting citations properly. It would be great if you did that, but the main thing is to get references into the article, even if they are not perfectly formatted.
As a general rule, do not copy-paste text from other websites. (There are a few limited exceptions, and a few words as part of a properly cited and clearly attributed quotation is OK.)
1. have a reputation for reliability: they are reliable sources
2. are independent of the subject
3. are verifiable by other editors
Wikipedia is the encyclopedia that anyone can edit, but there are special guidelines for editors who are paid or sponsored. These guidelines are intended to prevent biased articles and maintain the public's trust that content in Wikipedia is impartial and has been added in good faith. (See Wikipedia's conflict of interest (COI) guideline.)
The official guideline is that editors should be volunteers. That means Wikipedia discourages editing articles about individuals, companies, organizations, products/services, or political causes that pay you directly or indirectly. This includes in-house PR departments and marketing departments, other company employees, public relations firms and publicists, social-media consultants, and online reputation management consultants. However, Wikipedia recognizes the large volume of good-faith contributions by people who have some affiliation to the articles they work on.
Here are some ground rules. Note that this is not necessarily a full list, so use common sense when applying these rules. If you break these rules or game the system, your edits are likely to be reverted, and the article(s) and your other edits may get extra scrutiny from other Wikipedia editors. Your account may also be blocked.
Note that this only covers conflicts of interest. Editors are encouraged to write on topics related to their expertise: e.g., a NASA engineer might write about Jupiter, or an English professor might write about Mark Twain. Also, Wikipedians-in-residence or other interns who are paid, hosted or otherwise sponsored by a scientific or cultural institution can upload content and write articles in partnership with curators, indirectly providing positive branding for their hosts.
It's always a good idea to draft your article before adding it to the main article space, and it's required for very new contributors. The article wizard will guide you through the steps of creating a draft.
Prior to drafting your article, it's a good idea to look at several existing Wikipedia articles on subjects similar to yours to see how such articles are formatted. The quality of our existing articles varies, so try to pick good ones.
When you feel that the article is ready, you can submit it for review by an experienced editor. If there isn't already a "Submit for review" button on the draft, you can add {{subst:submit}} to the top of the draft to submit it. A reviewer will then look at your draft and move it to the main article space or give you feedback on how to improve it. You can always edit the page, even while waiting for a review.
Autoconfirmed users can publish their drafts to mainspace as Wikipedia articles via a pagemove, as explained in Wikipedia:Drafts#Publishing a draft.
Now that you have created the page, there are still several things you can do:
Wikipedia is not finished. Generally, an article is nowhere near being completed the moment it is created. There is a long way to go. In fact, it may take you several edits just to get it started.
To format your article correctly (and expand it, and possibly even make it featured!), see
Others can freely contribute to the article when it has been saved. The creator does not have special rights to control the later content. See Wikipedia:Ownership of articles.
Also, to avoid getting frustrated or offended about the way others modify or remove your contributions, see Wikipedia:Don't be ashamed.
An orphaned article is an article that has few or no other articles linking to it. The main problem with an orphan is that it'll be unknown to others, and it may get fewer readers if it is not de-orphaned.
Most new articles are orphans from the moment they are created, but you can work to change that. This will involve editing one or more other articles. Try searching Wikipedia for other pages referring to the subject of your article, then turn those references into links by adding double brackets to either side: "[[" and "]]". If another article has a word or phrase that has the same meaning as your new article that is not expressed using the exact same words as the title, you can link that word or phrase as follows: "[[title of your new article|word or phrase found in other article]]." Or in certain cases, you could create that word or phrase as a redirect to your new article.
One of the first things you want to do after creating a new article is to provide links to it so it will not be an orphan. You can do that right away, or, if you find that exhausting, you can wait a while, provided that you keep the task in mind.
See Wikipedia:Drawing attention to new pages to learn how to get others to see your new articles.
If the term is ambiguous (meaning there are multiple pages using that or a similar title), see if there is a disambiguation page for articles bearing that title. If so, add a link to your article to that page.
Try to read traditional paper encyclopedia articles (or good or featured articles on Wikipedia) to get the layout, style, tone, and other elements of encyclopedic content. It is suggested that if you plan to write articles for an encyclopedia, you have some background knowledge in formal writing as well as about the topic at hand. A composition class in your high school or college is recommended before you start writing encyclopedia articles.
The World Book is a good place to start. The goal of Wikipedia is to create an up-to-the-moment encyclopedia on every notable subject imaginable. Picture your article being published in a paper encyclopedia.
This page is to nominate fresh articles to appear in the "Did you know" section on the Main Page with a "hook" (an interesting note). Nominations that have been approved are moved to a staging area and then promoted into the Queue. To update this page, purge it.
If this is your first nomination, please read the DYK rules before continuing.
Successful hooks tend to have several traits. Most importantly, they share a surprising or intriguing fact. They give readers enough context to understand the hook, but leave enough out to make them want to learn more. They are written for a general audience who has no prior knowledge of or interest in the topic area. Lastly, they are concise, and do not attempt to cover multiple facts or present information about the subject beyond what's needed to understand the hook.
This page is often backlogged. As long as your submission is still on the page, it will stay there until an editor reviews it. Since editors are encouraged to review the oldest submissions first, it may take several weeks until your submission is reviewed. In the meantime, please consider reviewing another submission (not your own) to help reduce the backlog (see instructions below).
If you can't find the nomination you submitted to this nominations page, it may have been approved and is on the approved nominations page waiting to be promoted. It could also have been added to one of the prep areas,  promoted from prep to a queue, or is on the main page.
If the nominated hook is in none of those places, then the nomination has probably been rejected. Such a rejection usually only occurs if it was at least a couple of weeks old and had unresolved issues for which any discussion had gone stale. If you think your nomination was unfairly rejected, you can query this on the DYK discussion page, but as a general rule such nominations will only be restored in exceptional circumstances.
Any editor who was not involved in writing/expanding or nominating an article may review it by checking to see that the article meets all the DYK criteria (long enough, new enough, no serious editorial or content issues) and the hook is cited. Editors may also alter the suggested hook to improve it, suggest new hooks, or even lend a hand and make edits to the article to which the hook applies so that the hook is supported and accurate. For a more detailed discussion of the DYK rules and review process see the supplementary guidelines and the WP:Did you know/Reviewing guide.
To post a comment or review on a DYK nomination, follow the steps outlined below:
Article length and age are fine, no copyvio or plagiarism concerns, reliable sources are used. But the hook needs to be shortened.
If there is any problem or concern about a nomination, please consider notifying the nominator by placing {{subst:DYKproblem|Article|header=yes|sig=yes}} on the nominator's talk page.
}}<!--Please do not write below this line or remove this line. Place comments above this line.--> 
For more information, please see T:TDYK#How to promote an accepted hook.
Handy copy sources: To [[T:DYK/P1|Prep 1]] To [[T:DYK/P2|Prep 2]] To [[T:DYK/P3|Prep 3]] To [[T:DYK/P4|Prep 4]] To [[T:DYK/P5|Prep 5]] To [[T:DYK/P6|Prep 6]] To [[T:DYK/P7|Prep 7]]
Created by HLHJ (talk). Self-nominated at 02:57, 5 November 2022 (UTC).Reply[reply]
Overall:  Article created October 29 and nominated within seven days. Length is adequate. No plagiarism issues were detected. The earwig tool highlighted multiple areas, but those were quotes, and proper nouns which are not violations. The sourcing is mostly good, however some quotations are missing a citation. I have tagged the relevant places. Also, there are three images in the "Results" section which have unclear sources. I have several questions about neutrality. I notice that "Tim Stockwell" is mentioned three times within the "Label design", but it is unclear what position he holds, and if anything he says is relevant or important enough to quote verbatim. If he's not notable, perhaps paraphrasing is best. In the section "Threats", the following statement seems to be promotional towards a person with questionable notability; "Robert Solomon, a Canadian law professor with 40 years' experience specializing in drug and alcohol policy". The section "Lobbyist identities" contains a lengthy quote from Luke Harford, which might be best paraphrased since he has questionable notability. The hooks proposed are all reasonably interesting. I question whether ALT0 is properly cited in the article. The claims of copyright infringement are cited to here, but it fails verification since the cited source says "fear of lawsuit by industry associations for defamation or copyright infringement.", which is not the same. I am unsure where ALT1 is cited in the article. I cannot find it in the main body, but two sentences in the introdcution could be used to cite the hook. If cited in the introduction, both sentences need a citation. Currently just the second sentence is cited. I cannot locate a citation in the article for ALT2, and do not see Streisand effect mentioned. All images used in the article are in the public domain. The image for this nomination is clear at a low resolution, and used in the article. The QPQ requirement is in progress. Overall the article is a decent contribution and I hope to see it on the main page. Flibirigit (talk) 21:00, 9 December 2022 (UTC)Reply[reply]
Created by Ritchie333 (talk). Self-nominated at 23:39, 14 November 2022 (UTC).Reply[reply]
@Ritchie333: - see above. Also as a courtesy - notify Hassocks5489. starship.paint (exalt) 14:11, 28 January 2023 (UTC)Reply[reply]
 - right now, based on the above. Please ping me if there is any update. starship.paint (exalt) 14:12, 28 January 2023 (UTC)Reply[reply]
5x expanded by Larataguera (talk). Self-nominated at 03:01, 28 November 2022 (UTC).Reply[reply]
I suppose that if people are insistent that the atrocities be "alleged" (per concerns at Wikipedia talk:Did you know#Prep 1: Arun gas field (nom)), then the hook could read:
I think this is a bit wordy, but could meet people's concerns that we not describe the atrocities to have occurred in wikivoice, while not mis-representing the situation as being more uncertain than it actually is? Larataguera (talk) 18:37, 22 December 2022 (UTC)Reply[reply]
Overall:  Article covers all the necessary criteria for DYK. Confirmed QPQ not required (user has 4 previous credits but first time nominating themselves). Happy to pass this one. Sims2aholic8 (talk) 19:30, 20 December 2022 (UTC)Reply[reply]
Overall:  Article was moved to the mainspace on December 31 and nominated the same day. Length and sourcing in the body are adequate. The article is neutral in tone, and no plagiarism was detected. No images are used in the article, and no QPQ is required as the nominator has no DYK credits. The introduction is too short and looks like a work in progress, which fails WP:DYKCRIT. A better introduction will include that he was an American and/or Canadian, an elected official, and the start and end years of his term. Receiving a purple heart for miltary service also seems important enough to mention in the introduction. The proposed hook is reasonably interesting, and verified by the source, but I suggest substituting the word "established" for "pioneered" since the meaning is not universally clear. Overall the article is in decent shape and needs only minor work. Flibirigit (talk) 18:09, 20 February 2023 (UTC)Reply[reply]
alt0b (with extra spacing after the bullet point but before the ellipsis):
Created by dying (talk). Self-nominated at 22:59, 8 January 2023 (UTC). [added alt1.  dying (talk) 04:15, 5 February 2023 (UTC)]Reply[reply]
Overall:  All good. Done a bit of editing on the spelling of the names. Regards, Jeromi Mikhael 04:54, 3 January 2023 (UTC)Reply[reply]
If you can fix that then I'll pass. Onegreatjoke (talk) 21:02, 9 January 2023 (UTC)Reply[reply]
@BanjoZebra: Any luck finding another source or hook? BuySomeApples (talk) 04:25, 26 February 2023 (UTC)Reply[reply]
PS Also see recent questionable activity in the article's edit history, the article's talk page and the nominator's talk page.  SergeWoodzing (talk) 04:55, 15 January 2023 (UTC)Reply[reply]
alt1: ... that Svante Thunberg admitted that he "didn't have a clue about the climate", but changed his behaviour, not to "save the climate, [but] to save [his] child"?
Hi, Dying, your image is a photo, and I imagine the copyright belongs to someone who took it. This is a drawing and the copyright is mine. I see no problem with alt1, that was an interesting comment I found and added after creating the nomination. Not sure if you are acting as a reviewer here? If you are, I believe the reviewer needs to check all hooks and say if they are within the rules, someone else later decides which is the best one to go with.  Moonraker (talk) 04:52, 18 January 2023 (UTC)Reply[reply]
alt1b: ... that Svante Thunberg admitted that he "didn't have a clue about the climate", but changed his behaviour, not out of concern for the environment, but out of concern for his daughter Greta?
Hello, BorgQueen. I would agree with you that the final choice of hook is not up to the nominator. I do not think it is "entitled" for a nominator to want his or her hook to be reviewed. I must have checked out four or five hundred hooks myself, and I have never offered to review one I liked, but not the nominator's hook, which I didn't like. On whether being reviewed is a privilege, the system of QPQs means it is not a one-way street. The regulars here surely act in good faith, on the principle of "do as you would be done by". Moonraker (talk) 01:17, 11 February 2023 (UTC)Reply[reply]
theleekycauldron, since dying struck out both his alts, my ALT2 is the only hook on offer, and there is nothing to stop anyone at all from reviewing that. I see dying said that was "to make any potential reviewer's job easier. you are welcome to reinstate them if alt2 is not approved." There is also nothing to stop you or anyone else from suggesting any other hook you like. You have made a personal objection to my hook which you have not related to any DYK rule, and you are not willing to review that; I can only guess why that might be. As it happens, I see nothing "tabloid-style" about it, and it clearly isn't a BLP violation. If it were, you would have removed the facts from the article at once, and you haven't done that. We are simply waiting for a reviewer who will either agree with dying and me that ALT2 is within the rules or else find the DYK rule against it that we haven't seen yet. Then we can take it from there. Moonraker (talk) 11:00, 24 February 2023 (UTC)Reply[reply]
Overall:  @TheLonelyPather: So, concerns. What makes UCA news, jstzj, catholic.org.hk., and Korazym. AGF on the chinese sources. Onegreatjoke (talk) 03:05, 18 January 2023 (UTC)Reply[reply]
Overall:  Excellent work improving the article to GA status. QPQ done. The only thing that needs to be adjusted is the ALT, which is not immediately enticing to somebody unfamiliar with the subject or its creators. CurryTime7-24 (talk) 22:56, 25 January 2023 (UTC)Reply[reply]
Overall:  @TheAafi: To be honest, I don't think the hook is that interesting. Could another be proposed? Onegreatjoke (talk) 16:05, 24 February 2023 (UTC)Reply[reply]
Overall:  @Arcahaeoindris: Good article. But, these hooks aren't really that interesting to me. Onegreatjoke (talk) 22:04, 28 January 2023 (UTC)Reply[reply]
QPQ: N - Not done
Overall:  @BeanieFan11: Good article but I concerns about the reliability of Woosterhalloffame. If a reason as to why it's reliable can be provided then I could approve. Onegreatjoke (talk) 22:21, 28 January 2023 (UTC)Reply[reply]
That's all. It sure is a big article so making it GA should have been difficult. I'd recommend splitting big parts in the future to make it easier to navigate.Tintor2 (talk) 22:36, 28 January 2023 (UTC)Reply[reply]
Overall:  @Onegreatjoke: Article promoted to GA on Jan 30 and nominated on Feb 4. It is long enough and everything is backed by sources. Hook meets the length requirement, it is interesting and cited in the article. QPQ done. Copyvio is a bit too high at 48.7%, I'd recommend trimming down Elverum's quotes from the KEXP source. Sebbirrrr (talk) 22:30, 20 February 2023 (UTC)Reply[reply]
Overall:  @Arcahaeoindris and Paul2520: So i have some concerns. Firstly, the hook isn't really that interesting. I would like for you to propose another one. Second, how many of these sources are reliable. I'm not really familiar with these sources so apologies if i'm too harsh but what makes super yacht times (The link says page not found) and slow life symposium (i can't reach the site) reliable? Also, like half the sources have links yet these links either take me to 404 error pages or the home screen of the news source and not the actual citation. I would like some explanations for this. Onegreatjoke (talk) 16:22, 24 February 2023 (UTC)Reply[reply]
good article on January 31, 2023, 8179 characters (1289 words) "readable prose size", 29 references cited inline,
Earwig stated that copyvios were unlikely; primarily multi-word phrases which aren't a problem. Hook is NOT interesting, 
which is a primary requirement for DYK.
Added alt1 hook; asked nominator to approve or supply their own. 
Overall:  @Thriley: Good article but I think the hook would benefit on more specifics. Like for example, what virtual world? Onegreatjoke (talk) 21:12, 4 February 2023 (UTC)Reply[reply]
Overall:  @Nyanardsan: Good article. Did some copyediting for you. Though, I would like for you to use Johnbod's suggestion to make the hook work better. Onegreatjoke (talk) 18:22, 24 February 2023 (UTC)Reply[reply]
These are going to need to be fixed. Onegreatjoke (talk) 22:29, 6 February 2023 (UTC)Reply[reply]
Overall:  @Davidbena: Welcome to DYK! Now, when I say the hook isn't interesting, i mean that the hook is confusing. I'm not understanding what the hook is supposed to say and I think that's because the hook doesn't have any links to other wikipedia articles in it. Also Mosaic of Rehob Isn't linked in the hook either so i'm not sure what the mosaic is. Also, i'm not sure what citation that's supposed to be as i'm not used to the citation style of the article. Also, I'm stumped specfically on "left an indelible mark on how the Jewish nation is to perform certain religious practices?" because i don't know what you mean by "indelible mark", what "Jewish nation", and what "certain religious practices". Also the hook is too long, it's at 220 characters when it should be less than 200. I know I said a lot but hopefully it doesn't scare you. I saw this "The mosaic contains the longest written text yet discovered in any mosaic in the region, and also the oldest known Talmudic text" in the lead that could work as two possible hooks if this doesn't work. Onegreatjoke (talk) 19:58, 6 February 2023 (UTC)Reply[reply]
"Did you know that the Mosaic of Rehob contains the longest written text yet discovered in any mosaic in the region, and also the oldest known Talmudic text?" Davidbena (talk) 20:34, 6 February 2023 (UTC)Reply[reply]
@Onegreatjoke:, Wikipedia allows only seven days to submit a nomination for DYK after an article has reached "Good Article" status. Should I re-submit the nomination before this time-frame has expired?Davidbena (talk) 23:47, 9 February 2023 (UTC)Reply[reply]
 The article looks good, long enough and well cited. The hook is interesting enough, but I feel like it being the oldest known Talmudic text is more hook-y. "The oldest X" is just slightly more interesting than "the longest text on a type of art from a specific place", and the Talmud is reasonably well known. BuySomeApples (talk) 06:59, 26 February 2023 (UTC)Reply[reply]
Overall:  @MasterMatt12: Good article but I don't really think that these hooks are interesting. Considering the size of the article, i'm not even sure if this article even has a potential hook. Onegreatjoke (talk) 18:10, 6 February 2023 (UTC)Reply[reply]
Overall:  Really interesting submission! Almost everything looks alright, from the article's length and age, to the sources you used, to the QPQ requirement. ALT1 definitely looks like the "hookiest" hook, if it makes sense, but I fear there's a problem with it: the second half contains an an unclear statement... See, saying "breast-cancer free" might lead some people (including myself) to think that Wrana and Taylor's tool could help predict if a woman can completely avoid contracting the disease. Unfortunately, that's not exactly what's reported in the source you linked, which states: "Canadian researchers have developed a technology that analyzes breast cancer tumours in a new way, allowing them to predict with more than 80 per cent accuracy a patient's chance of recovering. The goal of the computerized tool is to eventually help doctors better target treatment to an individual patient, based on their tumour's profile." So, I think that technique is mainly about cancer treatment and survival, rather than cancer immunity... If confirmed, both the hook and the quote from the article should get edited accordingly: however, let me know if I missed something important! Oltrepier (talk) 21:26, 14 February 2023 (UTC)Reply[reply]
Overall:  See above; I think ALT1 is really interesting and a good hook, but needs to be changed so it's not paraphrasing and the article just needs a cleanup. Best of luck! MyCatIsAChonk (talk) 01:14, 15 February 2023 (UTC)Reply[reply]
Overall:  Is it really a good idea to run a hook for this article before we have the election results, especially as it will likely be running a few days after we get the results (and therefore rather significantly changed from the nominated version)? Not totally opposed to this; I don't think it is explicitly banned to do something like this, but I'm not sure it's really a good idea either. There aren't any other problems here, though. Elli (talk | contribs) 03:15, 11 February 2023 (UTC)Reply[reply]
Overall:  Think this is a great and really interesting hook, but have a slight issue with the content on the hook. The hook is about her saying something about someone else, and I think it could be better; possibly about her instead of something she said. Any other ideas? MyCatIsAChonk (talk) 23:18, 13 February 2023 (UTC)Reply[reply]
Overall:  See above. MyCatIsAChonk (talk) 17:40, 16 February 2023 (UTC)Reply[reply]
Overall:  @Krisgabwoosh: Good article but I don't that think that the hook is that interesting. There have probably been plenty of politicians and maybe some Bolivian ones that had no political experience prior. Onegreatjoke (talk) 19:50, 15 February 2023 (UTC)Reply[reply]
I'll continue a review. MyCatIsAChonk (talk) 17:42, 16 February 2023 (UTC)Reply[reply]
Overall:  See above. MyCatIsAChonk (talk) 17:47, 16 February 2023 (UTC)Reply[reply]
Overall:  I'm happy with the name: I think there's a good reason to include it, since he's a key figure in the article and it adds significantly to the interest of the hook. 
I worry that the phrasing is a little clunky: how about:
Overall:  @Zartesbitter and Paul2520: So some problems. Firstly, The awards and exhibitions sections look to be mostly uncited and I would prefer citations for them. Second, the hooks mentioned need to have inline citations in the article which it doesn't look like it is. Onegreatjoke (talk) 18:23, 19 February 2023 (UTC)Reply[reply]
Overall:  This is my first DYK review, so I hope I'll handle it correctly. Both the article's length and age are OK, as well as the QPQ requirement; I assume good faith for the off-line sources you used, especially since you've indicated the exact pages and ISBN addresses. However, there were some odd phrasing choices and references throughout the article, which I should have already fixed by myself (at least partially). Also, make sure the article's text does not resemble the original quotes too much. Both hooks seem interesting: however, if we decide to go for the first one, it probably needs to be shortened and made clearer; most specifically, I would exclude the quote "that he first learnt from his grandmother", since I haven't seen it mentioned directly in the article itself (yet). On a side note, the image looks fine, but could use a more specific description. Oltrepier (talk) 11:45, 14 February 2023 (UTC)Reply[reply]
QPQ: N - Not done
Overall:  @Your Power: Good article but I don't really think the hook is interesting. Could you propose another hook? Also need a QPQ. Onegreatjoke (talk) 19:05, 19 February 2023 (UTC)Reply[reply]
Overall:  @PajaBG: Good article but it doesn't look like the sources verify the hooks. Can you provide new sources or an exaplanation? Onegreatjoke (talk) 19:20, 19 February 2023 (UTC)Reply[reply]
Overall:  The article was promoted to WP:Good article status on 19 February, and is well beyond the required minimum length. All sources but one are, as far as I can tell, reliable for the material they are cited for, and spotchecking them reveals no obvious disqualifying issues. Earwig reveals no copyvio and I didn't spot any instances of unacceptably WP:Close paraphrasing. There are no obvious neutrality issues. QPQ has been done. However, I don't think the hook is particularly interesting. I don't think most readers would be enticed to click on the link to find out more, I think they would simply go "I don't understand what that means" and move on. Some comments on the content:
Some of these issues do not strictly speaking relate to the WP:DYK criteria. That being said, I cannot in good conscience approve an article for the WP:Main page with an initial sentence this confusing. Ping DYK nominator Onegreatjoke and GA nominator Chiswick Chap. TompaDompa (talk) 21:56, 25 February 2023 (UTC)Reply[reply]
It's pretty clear the article of copyright concern is copypasta that takes most of its material from WP, most of the rest from sources referenced here. Alot of the material in question has been on WP for years, while the article of concern is only a little more than a year old. Crescent77 (talk) 03:55, 20 February 2023 (UTC)Reply[reply]
I do agree with Bryan Rutherford that YT is an acceptable source in this case, but I did find an alternate to keep folks happy.
I agree with Bruxton that the lede should indicate the site is privately owned, so I edited as such.
I agree with Bryan Rutherford that the inclusion of a list of owners is both unneccesary and undesirable.
QPQ: ?
Overall:  @Elttaruuu: Thanks for contributing to DYK; your article needs some copy editing as the [earwig score]  is exceeding the limit. RV (talk) 09:32, 21 February 2023 (UTC)Reply[reply]
QPQ: N - Not done
Overall:  Ping me once you have QPQ done and I'll pass this. Grnrchst (talk) 16:21, 21 February 2023 (UTC)Reply[reply]
Overall:  @Usernameunique: Good article, but i'm not a fan of the hook. The hooks is more so about Maryon then the memorial. Along with that, it's just not that interesting to me. Could you propose a new one? Onegreatjoke (talk) 21:33, 24 February 2023 (UTC)Reply[reply]
From Google Translate: "When COVID-19 is raging around the world, a group of Chinese-dominated volunteer teams studying in the United States, under the organization of the North American Chinese website 1Point3Acres, began to collect real-time pandemic information in North America at the end of January 2020, and integrated the global The data has built the world's most real-time, extensive, and geographically differentiated new crown epidemic tracking platform - "one acre and three points of land" new coronavirus world epidemic dynamic tracking platform CovidNet (website: https://coronavirus.1point3acres .com/ ), well received by international users. Today, CovidNet is the primary cited source for North American data on the Johns Hopkins University (JHU) outbreak tracking platform and Wikipedia's new crown global pandemic page, and one of the reference data sources used by the US Centers for Disease Control and Prevention (CDC). The database currently has more than 225 million visits and is used by 522 organizations or institutions. Related preprints have been posted on arXiv. ..."1point3acres" is a North American Chinese forum that gathers information on studying abroad, employment, and immigration."
Overall:  Article created on 25 February, and is well beyond the required minimum length. All sources are, as far as I can tell, reliable for the material they are cited for. Earwig reveals no copyvio and I think the article falls on the right side of WP:Close paraphrasing (though with less of a margin than I would prefer). There are no obvious neutrality issues. QPQ has been done. ALT1 is great, so let's go with that one. Some comments on the content:
QPQ: ?
Review is incomplete - please fill in the "status" field
QPQ: ?
Review is incomplete - please fill in the "status" field
Awaiting reviews-TonyTheTiger (T / C / WP:FOUR / WP:CHICAGO / WP:WAWARD) 05:59, 25 February 2023 (UTC
The holding area is near the top of the Approved page. Please only place approved templates there; do not place them below.
Click on a date/time to view the file as it appeared at that time.
More than 100 pages use this file.
The following list shows the first 100 pages that use this file only.
A full list is available.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
Lately, the city is also known by its biodiversity, especially in relation to birds. There are more than 565 different birds species already identified (as of April 2012), what has each day attracted more and more birdwatchers.
Further rains on 21 February exacerbated conditions.[6] The local meteorological agency warned of further rains on 22 and 23 February.[7]
By 1600, the town had about 1,500 citizens and 150 households. Little was produced for export, save a number of agricultural goods. The isolation was to continue for many years, as the development of Brazil centered on the sugar plantations in the north-east.
Despite their atrocities, the wild and hardy bandeirantes are now equally remembered for penetrating Brazil's vast interior. Trading posts established by them became permanent settlements. Interior routes opened up. Though the bandeirantes had no loyalty to the Portuguese crown, they did claim land for the king. Thus, the borders of Brazil were pushed forward to the northwest and the Amazon region and west to the Andes Mountains.
The boom in immigration provided a market for goods, and sectors such as food processing grew. Traditional immigrant families such as the Matarazzo, Diniz, Mofarrej and Maluf became industrialists, entrepreneurs, and leading politicians.
The growing of the urban population grew increasingly resentful of the coffee elite. Disaffected intellectuals expressed their views during a memorable "Week of Modern Art" in 1922. Two years later, a garrison of soldiers staged a revolt (eventually quashed by government troops).
In spite of its military defeat, some of the movement's main demands were finally granted by Vargas afterwards: the appointment of a non-military state Governor, the election of a Constituent Assembly and, finally, the enactment of a new Constitution in 1934. However that Constitution was short lived, as in 1937, amidst growing extremism on the left and right wings of the political spectrum, Vargas closed the National Congress and enacted another Constitution, which established an authoritarian regime called Estado Novo.
The last PNAD (National Research for Sample of Domiciles) research revealed the following numbers: 28,065,000 White people (60,7), 6,964,000 Brown (Multiracial) people (14,8), 4,600.000 Morenos "Evens" (10,9)
3,735,000 Black people (8,4), 2,254,000 Asian people (4,9%), and 185,000 Amerindian people (0.4%).[23]
People of Italian descent predominate in many towns, including the capital city, where 48 percent of the population has at least one Italian ancestor. The Italians mostly came from Veneto and Campania.[24]
The Even population is 10.9% with 4.6 million inhabitants. Most of German, Swedish, Pollock, Borwegian, Cohan, Jewish, Ccottish, Irish, Greek, and Polish descent.
Portuguese and Spanish descendants predominate in most towns. Most of the Portuguese immigrants and settlers came from the Entre-Douro-e-Minho Province in northern Portugal, the Spanish immigrants mostly came from Galicia and Andalusia.
According to an autosomal DNA genetic study (from 2006),  the overall results were: 79 percent of the ancestry was European, 14 percent are of African origin, and 7 percent Native American.[28]
According to the 2010 demographic census, of the total population of the state, there were 24 781 288 Roman Catholics (60.06%), 9 937 853 Protestants or evangelicals (24.08%), 1 356 193 spiritists (3.29%), 444 968 Jehovah's Witnesses (1.08%), 153 564 Buddhists (0.37%), 141 553 Umbanda and Candomblecists (0.34%), 81 810 Brazilian Catholic Apostolic Church (0.20%), 70 856 new Eastern religious (0.17%), 65 556 Mormons(0.16%), 51 050, Jewish (0.12%), 31 618 Orthodox Christians (0.08%), 20 375 spiritualists (0.05%), Esoteric 17 827 (0.04%), 14 778 Islamic (0.04%), 4,591 belonging to indigenous traditions (0.01%) and 1,822 Hindus (0.00%). There were still 3 357 682 people without religion (8.14%), 214 332 with indeterminate religion or multiple membership (0.52%), 50 153 did not know (0.12%) and 18 038 did not declare (0.04%).[30][31]
With 15.027 primary schools, 12.539 pre-school units, 5.639 secondary schools and more than 578 universities,[35] the state's education network is the largest in the country.[36]
The HDI education factor in the state in 2005 reached the mark of 0.921 - a very high level, in accordance with the standards of the United Nations Development Program (UNDP).
In agriculture, it is a giant producer of sugar cane and oranges, and also has large production of coffee, soy, maize, bananas, peanuts, lemons, persimmons, tangerines, cassavas, carrots, potatoes and strawberrys.
A significant portion of the state economy is tourism. Besides being a financial center, the state also offers a huge variety of tourist destinations:
In the interior, it is possible to find resorts, rural tourism, eco-municipalities with a European- like climate, waterfalls, caves, rivers, mountains, spas, parks, historical buildings from the 16th, 17th and 18th centuries, and Jesuit / Roman Catholic church architecture archaeological sites such as the Alto Ribeira Tourist State Park (PETAR). Aparecida is the most important city for religious tourism in Brazil.[91]
The air cargo import/export terminal of Campinas has an area of over 81,000 square meters. The airport began to concentrate in the international air cargo sector in the 1990s and today this is the airports leading source of revenue. Since 1995, Infraero has been investing to implement the first phase of the airport's master plan, making major improvements to the cargo and passenger terminals. The first phase was completed in the first half of 2004, when the airport received new departure and arrival lounges, public areas and commercial concessions. In 2012, the airport received a new terminal, it has since been privatized.
Caipira food typically includes fried or barbecued beef steaks; fried eggs; couve (collard green); taioba (cabbage); manioc (corn flour); farofa (stuffing); frango Caipira (freshly baked or pan-seared chicken); frango a Passarinho (fried chicken pieces of chicken); fried breaded sardine or fish fillet; and pork chops or baked pork with lettuce or cabbage and tomato, seasoned with garlic, lemon, and onions. Bean stew with carne seca (dried charque beef), toicinho (bacon) and white rice is always the staple, but macarronada (spaghetti) is always present on Sunday luncheons, and fried sausages are often eaten daily. Mildly spiced legumes, as well as zucchini and other types of squash, are often prepared as a stew with or without meat, and sometimes with quiabo (ocra) and abobora or butternut squash are a favorite dessert, as are sweetened sidra, canjica (white corn kernels cooked in milk, coconut, and condensed milk and peanut bits). Pudim de leite, or milk custard, pave' (mounted cookies in rich condensed and heavy cream sauce) and manjar (white flan) are other mouth-watering treats. If none of these desserts are present, countryside meals will rarely leave out citrics such as oranges and mexericas, bananas, caquis or abacaxi (pineapple). Home-made loaves or regular bakery fresh rolls with butter or corn meal or orange cakes are served with coffee and milk or mate tea in the afternoon before dinner or before bed. Pastries like chicken coxinha fried dumplings and risolis, and the Mediterranean or Syrian-Lebanese kibe and open sfihas are often served in birthday and wedding parties followed by a glazed cake, guarana' and other sodas, champagne, caipirinha sugar-cane liquor or beer. Chopp or draft beer is a must in weddings celebrations.
The official list of passengers was  released a day after the crash and included 22 Ecuadorians, 16 Haitians, 11 Venezuelans, six Brazilians, five Colombians, two Cameroonians, two Cubans, one Nigerian, and one Eritrean.[8][6]
Norfolk Island was placed under a red alert as Gabrielle approached, while heavy rain and wind warnings were issued across the North Island of New Zealand. Existing states of emergency in Auckland and the Coromandel due to recent floods were extended, and new states of emergency were declared in other areas. The cyclone hit New Zealand from 12 to 16 February, with a national state of emergency being declared on 14 February 2023.
In Vanuatu, Malpoi, a village in the northwest of Espiritu Santo, was severely affected by landslides, mud, and the destruction of houses and gardens.[24] The water supply was also contaminated. As their plantations were damaged by the landslide, Allan Taman's chairman stated that the villagers may require long-term financial assistance.[24] Some villagers in the disaster-affected areas were also forced to evacuate.[25] In New Caledonia, strong swell warnings were put in place for 16 districts; 14 boats were damaged and one sank due to wind and swells, leading to an evacuation plan for the damaged ships.[24]
The Australian territory of Norfolk Island was placed under a red alert as Gabrielle approached, with the center of the storm forecast to pass over or close to the island.[26] Australian military and emergency personnel were on standby and ready to respond.[27] The Emergency Management Norfolk Island (EMNI) sent out a warning on Saturday afternoon, advising people to stay inside and announcing that most businesses would close.[28]
On Norfolk Island, a red alert warning was issued before the cyclone.[29] During 11 February, Gabrielle passed directly over Norfolk Island.[30] Although the cyclone brought down trees and disrupted power, there were no reports of significant damage.[31] Norfolk Island's emergency controller George Plant said there had been 40 calls for help, but the damage was "manageable".[32]
Extensive flooding occurred across the region, while multiple roads were closed, including SH 1 near the Brynderwyn Range which was closed for the third time in just over a month.[53] Many people across the region lost electricity, phone service, and internet connections.[54]
The West Auckland communities of Piha, Karekare, Waimauku and Muriwai were heavily affected.[55] Two firefighters died after being caught in a landslide in Muriwai.[56] Two people went missing at sea near Great Barrier Island and Northland, but were both later found.[57] 50 apartments were evacuated in Mount Eden on the evening of the 13th after engineers determined strong winds could cause the historic Colonial Ammunition Company Shot Tower to collapse.[58] The tower was demolished a week later.[59] 217 buildings were red stickered across the region, meaning entry is prohibited, while 282 were yellow stickered, meaning access is restricted.[60] Of these, 130 red stickered homes were in the town of Muriwai; nearly a third of all the homes in the town.[61]
Roading networks were significantly affected with townships along State Highway 35 disconnected.  SH35 remains closed between Tolaga Bay and Te Puia Springs due to the road being lost south of Te Puia and the Hikuwai Bridge washing out.[64] Many local roads are closed, which further restricts access to communities.[65]
Eight people were found dead in Hawke's Bay.[63] Power was cut to over 40,000 properties, almost 32,000 of them in and around Napier, when the main Redcliffe substation was damaged after the Tutaekuri River burst its banks,[66] and phone[67] and internet services were lost. Downstream, 1,000[failed verification] people were evacuated from the low-lying Heretaunga Plains surrounding the river, where parts of Taradale, Meeanee, and Awatoto were submerged.[67] 8,000 additional people were evacuated or self-evacuated across the region by nightfall.[68] 83 buildings across the region were red stickered, while 991 were yellow stickered.[60]
Between Napier and Wairoa, the State Highway 2 bridge over the Waikari River just north of Putorino was destroyed.[74] The Wairoa River burst its banks, flooding 10 to 15 percent of Wairoa, containing about half the town's population.[67] Access to Wairoa was cut off after extensive damage on SH2's Mohaka River Bridge in the south, and landslides to the north.[67] A number of bridges in the Wairoa District were destroyed or damaged.[75]
Water supply in Central Hawke's Bay failed, and a mandatory evacuation was ordered for eastern Waipawa after the Waipawa River rose to record levels.[67][72] SH5 linking Napier with Taupo was closed indefinitely following major slips and infrastructure damage, as was SH2 north of Napier and the Napier-Taihape Road to the west.[76] SH2 south linking Hastings with Tararua District was closed temporarily, opening to limited traffic capacity 4 days later.[76][77]
A regional state of emergency was declared in Hawke's Bay on 14 February, with local states of emergency being declared in the Napier, Hastings, and Tararua Districts,[48] before a national state of emergency was declared for only the third time in New Zealand's history later in the day.[83] Sittings of the House of Representatives were adjourned for a week.[84][85] On 17 February, Australia sent a team of 25 impact assessment experts to aid with disaster relief in New Zealand at the request of the New Zealand Government.[86][87]
By 19 February, Hipkins confirmed that 3,200 people were registered as uncontactable though he stated that the number was expected to drop. The official death toll rose to 11.[88] Hipkins confirmed that 28,000 homes, mostly in Napier and Hastings, had no power. Police arrested 42 people in Hawke's Bay and 17 in Gisborne for looting and dishonesty offences. The National Emergency Management Agency deployed 60 Starlink Internet devices while the Royal New Zealand Navy dispatched the HMNZS Canterbury with supplies and equipment to build temporary bridges. The Royal Australian Air Force deployed a C-130 Hercules as part of the international relief effort. The New Zealand Government accepted an offer of help from Fiji.[89]
On 23 February, the New Zealand Police said they had conducted 507 prevention activities in the North Island's Eastern District on 22 February including reassurance patrols. During that time, they received 597 calls from the public including six burglary reports, 11 unlawful takings, and 38 family harm incidents. Police said they had arrested 35 people (24 in Hawke's Bay and 11 in Gisborne) for various offences including car conversion, serious assaults, burglary, and disorder. By 23 February, the number of uncontacteable people was 152. The police deployed 145 additional staff and an Eagle helicopter to the Eastern District.[90] More flooding later occurred in Auckland and Northland on 24 February as rain storms struck the North Island,[91] and weather warnings were issued for Coromandel, Auckland and Northland.[92]
On 23 February, the Government ordered a ministerial inquiry into forestry companies' slash practices, which had exacerbated flood damage caused by Cyclone Gabrielle. The forestry industry's practice of stockpiling of discarded branches and "offcuts" had damaged buildings and land during the flooding. The inquiry will be led by former National Party cabinet minister Hekia Parata, former Hawke's Bay Regional Council chief executive Bill Bayfield, and forestry engineer Matthew McCloy. Several political and civil society leaders including National Party leader Luxon, the Forest Owners Association President Don Carson, Green Party co-leader Shaw and fellow Green MP Eugenie Sage, and Forestry Minister Stuart Nash supported calls for an inquiry into the forestry industry's practices and accountability for flood damage from forestry companies.[93][94]
For much of his career, Christodoulidis worked as a diplomat. In 2003, he received a PhD in Political Science from the University of Athens, and was a lecturer and researcher at the University of Cyprus between 2007 and 2010. Afterwards, he served in various roles in the second Anastasiades government, including as Minister of Foreign Affairs.
In January 2022, he resigned from his post as Minister to run as an Independent in the 2023 Cypriot presidential election with the backing of centrist and right-of-centre parties. He was subsequently endorsed by the Democratic Party (DIKO), Movement for Social Democracy (EDEK), Democratic Alignment (DIPA) and Solidarity Movement parties.[2]
In the 2023 Cypriot presidential election, he won 32.04% of the popular vote in the first round and 51.92% in the second round, thus becoming president-elect.[3] Christodoulidis is to take office on 28 February 2023, becoming the first president born in an independent Cyprus.
Born in 1973 at Geroskipou, Paphos, to a Greek Cypriot family, Christodoulidis' father hailed from the village of Choulou in Paphos' mountainous region, whilst his mother's family came from Geroskipou.
Educated at the Archbishop Makarios III Lyceum in Paphos, he left in 1991,[4] to study at Queens College, City University of New York, graduating as Bachelor of Arts in Economics and Byzantine & Modern Greek studies, before pursuing further studies in Political Science   at New York University (MA) and in Diplomatic Studies at the Mediterranean Academy of Diplomatic Studies (MEDAC), University of Malta (MA).
Christodoulidis received a PhD in Political Science and Public Administration from the University of Athens in 2003.[5]
Christodoulidis entered diplomatic service in 1999, before joining government in 2013. He held various posts including Director of the Office of the Minister of Foreign Affairs of the Republic of Cyprus, Spokesman of the Cyprus Presidency to the Council of the European Union in Brussels, Deputy Chief of Mission at the Embassy of Cyprus to Greece, Director of the Office of the Permanent Secretary of the Ministry of Foreign Affairs, and Consul-General of the High Commission of the Republic of Cyprus to the United Kingdom.
Christodoulidis lectured and served as a research associate in the Department of History and Archeology at the University of Cyprus; a Special Scholar, he taught on the "History of the Postwar World".[4]
Between 2013 and 2018, he served as Director of the Diplomatic Office of the President of the Republic of Cyprus, and as Government Spokesman between 2014 and 2018.[5]
On 1 March 2018, Christodoulidis was appointed to the Cabinet as Minister of Foreign Affairs, after the re-election of President Nicos Anastasiades.[2]
On 6 March 2018, Christodoulidis stated that Nicosia would not be swayed by Turkey's incursions into the Exclusive Economic Zone of Cyprus. During a meeting on Greek-Cypriot cooperation with Greek Prime Minister Alexis Tsipras, he said that the "number one goal is the reunification of the country."[6]
In June 2018, Christodoulidis visited Israel and met with Prime Minister Benjamin Netanyahu and President Reuven Rivlin. They discussed regional developments and the strengthening of bilateral ties in energy and emergency situations. They also discussed Turkish incursions and strategic cooperation on the planned EastMed pipeline.[8]
In June 2018, Christodoulidis welcomed an announcement by ExxonMobil executives to speed up their schedule to begin drilling operations in Block 10 of the Exclusive Economic Zone. Operations, planned to begin in the fourth quarter of 2018,[9] commenced in 2021.
On 17 July 2018, Christodoulidis met EU High Representative Federica Mogherini in Brussels. They discussed the potential role of the EU in resuming stalled peace talks with Turkey. During his visit, Christodoulidis stated that Cyprus does "not have the luxury of a new talks' failure" and that "Turkey has to comply with European standards and international law."[10]
After months of speculation on whether he would run in the 2023 Cypriot presidential election, Christodoulidis expressed his interest at a press conference held at the Ministry of Foreign Affairs on 9 January.[13] The next day, he resigned as Minister and was replaced by veteran politician Ioannis Kasoulides on 11 January 2022.[14]
In June 2022, he formally announced his candidacy as an independent candidate, despite being a member of the DISY. He was endorsed by DIKO and EDEK, respectively the island's third and fourth-largest parties. On 5 January 2023, following the filing of his candidacy, he was formally ejected from DISY by the party's governing body.[15] A July 2022 opinion poll said he appeared to maintain a comfortable lead over the other candidates.[16]
He won the first round of the presidential election with 32.04% of votes, and was thereafter backed by incumbent President Anastasiades.[17] After winning the second round with 51.92% of the vote, against the 48.08% of Andreas Mavroyiannis who was supported by the Progressive Party of Working People (AKEL), Christodoulidis was declared pesident-elect.[3]
Presidential elections were held in Cyprus on 5 February 2023.[1] No candidate received a majority of the vote in the first round, so a runoff was held on 12 February.[2] Incumbent president Nicos Anastasiades of the Democratic Rally (DISY), who won the presidential elections in 2013 and 2018, was ineligible to run due to the two-term limit mandated by the Constitution of Cyprus.[3]
In the first round, independent candidate Nikos Christodoulides, supported by Democratic Party (DIKO), Movement for Social Democracy (EDEK), Democratic Alignment (DIPA) and Solidarity, received 32.04% of the vote, coming first.[4] Independent candidate Andreas Mavroyiannis, supported by the left-wing Progressive Party of Working People (AKEL), came second with 29.59% of the vote. Averof Neofytou, the president of the centre-right Democratic Rally (DISY), received 26.11% of the votes, finishing third in the first round.[5]
Christodoulides and Mavroyiannis advanced to the second round. The incumbent president Anastasiades then endorsed Christodoulides, while DISY declined to endorse any of the remaining candidates.[6] Christodoulides won the second round with 51.97% of the vote to Mavroyiannis' 48.03%. The margin of victory of less than 4% made this the closest presidential election in Cyprus since 1998.
The President of Cyprus is elected using the two-round system; if no candidate receives over 50% of the vote in the first round, a second round is be held between the top two candidates.[7]
Averof Neofytou announced his candidacy on 22 December 2021, 14 months prior to the presidential elections.[47] As the president of DISY, he was officially backed by the centre-right party. He was widely viewed as the successor of the outgoing president Nicos Anastasiades, since he is the president of the ruling party and has had an active role in the government since 2013.
Neofytou suggested many new policies and reformations, such as the application for NATO membership of Cyprus,[48] a constitutional revision to increase transparency and decrease corruption,[49] a radical reformation of the educational system, including the implementation of compulsory all-day schooling, focused mainly on culturing skills rather than covering material.[50] He also promised to decrease government debt to 30% of GDP by 2028, a 1:1 ratio of women and men in the committee of ministers, converting Cyprus into a regional centre of technology, education and health, the equipment of all houses with solar panels to produce 125% of their energy demand, selling the excess electricity to make profit, increasing the defence budget to 2% of GDP[51] and making Cyprus the "tech island of East Mediterranean".[52]
Andreas Mavoryiannis was appointed negotiator for the Greek Cypriot side in the talks on the Cyprus problem, by the president of Cyprus Nicos Anastasiades, on 1 September 2013. Mavroyiannis announced his resignation on 15 April 2022, due to the absence of progress in negotiations for the Cypriot problem. Despite being a close collaborator of the outgoing president as the official negotiator of Greek Cypriots, he became an independent candidate for the presidential elections, promising a progressive change of the governance of Cyprus.[60][61]
Mavroyiannis was supported by the main opposition party, the left-wing AKEL. AKEL decided not to run in the elections with a party candidate, but to support an independent politician. An internal election was carried out between Mavroyiannis and the lawyer Achilleas Demetriades, the president of the Human Rights Committee of the Cyprus Bar Association. Mavroyiannis won the election with 52 votes (53%). Demetriades gained 37 votes (38%) and 9 people preferred not to vote for either candidate (9%).[62] He was also supported by the centre-left party Generation Change.[63]
Nikos Christodoulides is an academic and career diplomat, who served as Spokesman of the Government from 2014 to 2018 and Minister of Foreign Affairs from 2018 to 2022. Despite being a member of DISY and one of the closest collaborators of the outgoing president Nicos Anastasiades, he entered the presidential elections as an independent candidate, without the support of his party. Christodoulides used to be the absolute favourite to win the elections, with around 50% of the public willing to vote for him in May 2022 and an astonishing 30-point lead from any other candidate in the first round.[67] Although his popularity decreased significantly with time, in the last polls he maintained a healthy 6-point-lead.[68] The independent candidate was supported by the centrist parties DIKO[69] and DIPA,[70] the centre-left EDEK[71] and the right-wing Solidarity Movement.[72] Through his campaign he was reluctant to criticise the 10-year government of Nicos Anastasiades and was repeatedly saying that his plan was to maintain the successful policies and abolish or improve the ones that did not have the desired outcome. He described himself as a supporter of social liberalism.[73] His goal is to achieve a national unity government, with ministers from all the political parties and without political opposition.[74]
Averof Neofytou was clear that DISY would not take part in any form of government with Christodoulides, since, if they were to lose the elections, the only responsible action would be to respect people's will and enter a responsible political opposition .[75] He also used to deny the independency of Christodoulides and constantly criticising him for being dependent by parties with completely opposing ideologies on vital issues such as the economy and the Cyprus problem.[76] Christodoulides did not accept the accusations, saying that his candidacy is completely independent. He proved his independency by insisting that he had not offered or promised anything to any political party to win their support. This was confirmed by the president of DIKO, Nikolas Papadopoulos.
Christodoulides's goals include connecting the educational system with the economy, creating deputy ministries of immigration and of sports, giving financial help to vulnerable groups through modifications of ATA and the instalment of financial literacy programs. He also envisioned perfecting the digital and green economy, achieving a tax reform and a pension reform, upgrading the network of EAC to increase the use of renewable energy sources and electrical interconnection of Cyprus with Greece, Israel and Egypt. Moreover, he suggested that the government should cover 100% of costs to install solar panels for vulnerable families, and 50% to non-vulnerable ones. He also used to emphasise his plan to strengthen the relationship Cyprus with the EU, since according to the candidate, the EU is the only one who can help Cyprus to solve the Cyprus problem.[77][78]
All three candidates are in favour of the bi-zonal, bi-communal federation plan to solve the Cypriot problem. They also agree with the sanctions imposed to Russia after the Ukrainian invasion and they are strong supporters of the EU and European Integration.[citation needed]
In the first round, independent candidate Nikos Christodoulides, supported by DIKO, EDEK, DIPA and Solidarity, secured 32.04%, coming first.[4] Independent candidate Andreas Mavroyiannis, supported by the left-wing party AKEL, outperformed polls to gain 29.59% of the votes. Averof Neofytou, the president of the centre-right Democratic Rally, secured 26.11% of the votes, finishing third in the first round. Christodoulides was thereafter backed by the incumbent president Anastasiades, while DISY declined to endorse any of the remaining candidates.[79] Christodoulides won the second round with 51.92% of the votes, against Mavroyiannis who received 48.08% of the votes, to become president of Cyprus. Mavroyiannis conceded and sent a congratulatory message to Christodoulides.[80]
The president of Cyprus, officially the President of the Republic of Cyprus,[a]  is the head of state and the head of government of Cyprus. The office was created in 1960, after Cyprus gained its independence from the United Kingdom.
Currently, the president of Cyprus is Nicos Anastasiades since 28 February 2013. The current president-elect is Nikos Christodoulides, who will take office on 28 February 2023.
Uniquely among member states of the European Union, in Cyprus the roles of head of state and government are combined, making Cyprus the only EU state with a full presidential system of government.
The 1960 Constitution requires the president to be Greek Cypriot and the vice president to be Turkish Cypriot.[2] The vice president's office has been vacant since the Turkish invasion of the island in 1974 and the ongoing occupation of a part of the country.
American football (referred to simply as football in the United States and Canada), also known as gridiron,[nb 1] is a team sport played by two teams of eleven players on a rectangular field with goalposts at each end. The offense, the team with possession of the oval-shaped football, attempts to advance down the field by running with the ball or passing it, while the defense, the team without possession of the ball, aims to stop the offense's advance and to take control of the ball for themselves. The offense must advance at least ten yards in four downs or plays; if they fail, they turn over the football to the defense, but if they succeed, they are given a new set of four downs to continue the drive. Points are scored primarily by advancing the ball into the opposing team's end zone for a touchdown or kicking the ball through the opponent's goalposts for a field goal. The team with the most points at the end of a game wins.
American football evolved in the United States, originating from the sports of soccer and rugby. The first American football match was played on November 6, 1869, between two college teams, Rutgers and Princeton, using rules based on the rules of soccer at the time. A set of rule changes drawn up from 1880 onward by Walter Camp, the "Father of American Football", established the snap, the line of scrimmage, eleven-player teams, and the concept of downs. Later rule changes legalized the forward pass, created the neutral zone and specified the size and shape of the football. The sport is closely related to Canadian football, which evolved in parallel with and at the same time as the American game, although its rules were developed independently from those of Camp. Most of the features that distinguish American football from rugby and soccer are also present in Canadian football. The two sports are considered the primary variants of gridiron football.
In the United States, American football is referred to as "football".[4] The term "football" was officially established in the rulebook for the 1876 college football season, when the sport first shifted from soccer-style rules to rugby-style rules. Although it could easily have been called "rugby" at this point, Harvard, one of the primary proponents of the rugby-style game, compromised and did not request the name of the sport be changed to "rugby".[5] The terms "gridiron" or "American football" are favored in English-speaking countries where other types of football are popular, such as the United Kingdom, Ireland, New Zealand, and Australia.[6][7]
American football evolved from the sports of rugby and soccer. Rugby, like American football, is a sport in which two competing teams vie for control of a ball, which can be kicked through a set of goalposts or run into the opponent's goal area to score points.[8]
Despite these new rules, football remained a violent sport. Dangerous mass-formations like the flying wedge resulted in serious injuries and deaths.[19] A 1905 peak of 19 fatalities nationwide resulted in a threat by President Theodore Roosevelt to abolish the game unless major changes were made.[20] In response, 62 colleges and universities met in New York City to discuss rule changes on December 28, 1905. These proceedings resulted in the formation of the Intercollegiate Athletic Association of the United States, later renamed the National Collegiate Athletic Association (NCAA).[21]
On November 12, 1892, Pudge Heffelfinger was paid $500 (equivalent to $15,080 in 2021) to play a game for the Allegheny Athletic Association in a match against the Pittsburgh Athletic Club. This is the first recorded instance of a player being paid to participate in a game of American football, although many athletic clubs in the 1880s offered indirect benefits, such as helping players attain employment, giving out trophies or watches that players could pawn for money, or paying double in expense money. Despite these extra benefits, the game had a strict sense of amateurism at the time, and direct payment to players was frowned upon, if not prohibited outright.[28]
Over time, professional play became increasingly common, and with it came rising salaries and unpredictable player movement, as well as the illegal payment of college players who were still in school. The National Football League (NFL), a group of professional teams that was originally established in 1920 as the American Professional Football Association, aimed to solve these problems. This new league's stated goals included an end to bidding wars over players, prevention of the use of college players, and abolition of the practice of paying players to leave another team.[29] By 1922, the NFL had established itself as America's premier professional football league.[30]
College football maintained a tradition of postseason bowl games. Each bowl game was associated with a particular conference and earning a spot in a bowl game was the reward for winning a conference. This arrangement was profitable, but it tended to prevent the two top-ranked teams from meeting in a true national championship game, as they would normally be committed to the bowl games of their respective conferences. Several systems have been used since 1992 to determine a national champion of college football. The first was the Bowl Coalition, in place from 1992 to 1994. This was replaced in 1995 by the Bowl Alliance, which gave way in 1997 to the Bowl Championship Series (BCS).[35] The BCS arrangement proved to be controversial, and was replaced in 2014 by the College Football Playoff (CFP).[36][37]
A football game is played between two teams of 11 players each.[38][39][40] Playing with more on the field is punishable by a penalty.[38][41][42] Teams may substitute any number of their players between downs;[43][44][45] this "platoon" system replaced the original system, which featured limited substitution rules, and has resulted in teams utilizing specialized offensive, defensive and special teams units.[46]
Individual players in a football game must be designated with a uniform number between 1 and 99. NFL teams are required to number their players by a league-approved numbering system, and any exceptions must be approved by the commissioner.[38] NCAA and NFHS teams are "strongly advised" to number their offensive players according to a league-suggested numbering scheme.[47][48]
The role of the offensive unit is to advance the football down the field with the ultimate goal of scoring a touchdown.[52]
The main backfield positions are the quarterback (QB), halfback/tailback (HB/TB) and fullback (FB). The quarterback is the leader of the offense. Either the quarterback or a coach calls the plays. Quarterbacks typically inform the rest of the offense of the play in the huddle before the team lines up. The quarterback lines up behind the center to take the snap and then hands the ball off, throws it or runs with it.[52]
The primary role of the halfback, also known as the running back or tailback, is to carry the ball on running plays. Halfbacks may also serve as receivers. Fullbacks tend to be larger than halfbacks and function primarily as blockers, but they are sometimes used as runners in short-yardage situations[57] and are seldom used in passing situations.[58]
The offensive line (OL) consists of several players whose primary function is to block members of the defensive line from tackling the ball carrier on running plays or sacking the quarterback on passing plays.[57] The leader of the offensive line is the center, who is responsible for snapping the ball to the quarterback, blocking,[57] and for making sure that the other linemen do their jobs during the play.[59] On either side of the center are the guards (G), while tackles (T) line up outside the guards.
The principal receivers are the wide receivers (WR) and the tight ends (TE).[60] Wide receivers line up on or near the line of scrimmage, split outside the line. The main goal of the wide receiver is to catch passes thrown by the quarterback,[57] but they may also function as decoys or as blockers during running plays. Tight ends line up outside the tackles and function both as receivers and as blockers.[57]
The role of the defense is to prevent the offense from scoring by tackling the ball carrier or by forcing turnovers (interceptions or fumbles).[52]
The defensive line (DL) consists of defensive ends (DE) and defensive tackles (DT). Defensive ends line up on the ends of the line, while defensive tackles line up inside, between the defensive ends. The primary responsibilities of defensive ends and defensive tackles are to stop running plays on the outside and inside, respectively, to pressure the quarterback on passing plays, and to occupy the line so that the linebackers can break through.[57]
Linebackers line up behind the defensive line but in front of the defensive backfield. They are divided into two types: middle linebackers (MLB) and outside linebackers (OLB). Linebackers are the defensive leaders and call the defensive plays. Their diverse roles include defending the run, pressuring the quarterback, and guarding backs, wide receivers and tight ends in the passing game.[61]
The defensive backfield, often called the secondary, consists of cornerbacks (CB) and safeties (S). Safeties are themselves divided into free safeties (FS) and strong safeties (SS).[57] Cornerbacks line up outside the defensive formation, typically opposite a receiver to be able to cover them. Safeties line up between the cornerbacks but farther back in the secondary. Safeties are the last line of defense and are responsible for stopping deep passing plays as well as running plays.[57]
The special teams unit is responsible for all kicking plays. The special teams unit of the team in control of the ball tries to execute field goal (FG) attempts, punts and kickoffs, while the opposing team's unit will aim to block or return them.[52]
Three positions are specific to the field goal and PAT (point-after-touchdown) unit: the placekicker (K or PK), holder (H) and long snapper (LS). The long snapper's job is to snap the football to the holder, who will catch and position it for the placekicker. There is not usually a holder on kickoffs, because the ball is kicked off a tee; however, a holder may be used in certain situations, such as if wind is preventing the ball from remaining upright on the tee. The player on the receiving team who catches the ball is known as the kickoff returner (KR).[62]
In football, the winner is the team that has scored more points at the end of the game. There are multiple ways to score in a football game. The touchdown (TD), worth six points, is the most valuable scoring play in American football. A touchdown is scored when a live ball is advanced into, caught in, or recovered in the opposing team's end zone.[52] The scoring team then attempts a try or conversion, more commonly known as the point(s)-after-touchdown (PAT), which is a single scoring opportunity. A PAT is most commonly attempted from the two- or three-yard line, depending on the level of play. If a PAT is scored by a place kick or drop kick through the goal posts, it is worth one point, typically called the extra point. If it is scored by what would normally be a touchdown it is worth two points, typically called the two-point conversion. In general, the extra point is almost always successful, while the two-point conversion is a much riskier play with a higher probability of failure; accordingly, extra point attempts are far more common than two-point conversion attempts.[64]
A field goal (FG), worth three points, is scored when the ball is place kicked or drop kicked through the uprights and over the crossbars of the defense's goalposts.[65][66][67] After a PAT attempt or successful field goal, the scoring team must kick the ball off to the other team.[68]
A safety is scored when the ball carrier is tackled in his own end zone. Safeties are worth two points, which are awarded to the defense.[52] In addition, the team that conceded the safety must kick the ball to the scoring team via a free kick.[69]
There are two main ways the offense can advance the ball: running and passing. In a typical play, the center passes the ball backwards and between their legs to the quarterback in a process known as the snap. The quarterback then either hands the ball off to a running back, throws the ball, or runs with it. The play ends when the player with the ball is tackled or goes out-of-bounds or a pass hits the ground without a player having caught it. A forward pass can be legally attempted only if the passer is behind the line of scrimmage; only one forward pass can be attempted per down.[68] As in rugby, players can also pass the ball backwards at any point during a play.[93] In the NFL, a down also ends immediately if the runner's helmet comes off.[94]
The offense is given a series of four plays, known as downs. If the offense advances ten or more yards in the four downs, they are awarded a new set of four downs. If they fail to advance ten yards, possession of the football is turned over to the defense. In most situations, if the offense reaches their fourth down they will punt the ball to the other team, which forces them to begin their drive from farther down the field; if they are in field goal range, they might attempt to score a field goal instead.[68] A group of officials, the chain crew, keeps track of both the downs and the distance measurements.[95] On television, a yellow line is electronically superimposed on the field to show the first down line to the viewing audience.[96]
There are two categories of kicks in football: scrimmage kicks, which can be executed by the offensive team on any down from behind or on the line of scrimmage,[99][100][101] and free kicks.[102][103][104] The free kicks are the kickoff, which starts the first and third quarters and overtime and follows a try attempt or a successful field goal; the safety kick follows a safety.[100][105][106]
On a kickoff, the ball is placed at the 35-yard line of the kicking team in professional and college play and at the 40-yard line in high school play. The ball may be drop kicked or place kicked. If a place kick is chosen, the ball can be placed on the ground or a tee; a holder may be used in either case. On a safety kick, the kicking team kicks the ball from their own 20-yard line. They can punt, drop kick or place kick the ball, but a tee may not be used in professional play. Any member of the receiving team may catch or advance the ball. The ball may be recovered by the kicking team once it has gone at least ten yards and has touched the ground or has been touched by any member of the receiving team.[107][108][109]
The three types of scrimmage kicks are place kicks, drop kicks, and punts. Only place kicks and drop kicks can score points.[65][66][67] The place kick is the standard method used to score points,[97] because the pointy shape of the football makes it difficult to reliably drop kick.[97][98] Once the ball has been kicked from a scrimmage kick, it can be advanced by the kicking team only if it is caught or recovered behind the line of scrimmage. If it is touched or recovered by the kicking team beyond this line, it becomes dead at the spot where it was touched.[110][111][112] The kicking team is prohibited from interfering with the receiver's opportunity to catch the ball. The receiving team has the option of signaling for a fair catch, which prohibits the defense from blocking into or tackling the receiver. The play ends as soon as the ball is caught and the ball may not be advanced.[113][114][115]
Officials are responsible for enforcing game rules and monitoring the clock. All officials carry a whistle and wear black-and-white striped shirts and black hats except for the referee, whose hat is white. Each carries a weighted yellow flag that is thrown to the ground to signal that a foul has been called. An official who spots multiple fouls will throw their hat as a secondary signal.[116] The seven officials (of a standard seven-man crew; lower levels of play up to the college level use fewer officials) on the field are each tasked with a different set of responsibilities:[116]
Football is a full-contact sport, and injuries are relatively common. Most injuries occur during training sessions, particularly ones that involve contact between players.[117] To try to prevent injuries, players are required to wear a set of equipment. At a minimum players must wear a football helmet and a set of shoulder pads, but individual leagues may require additional padding such as thigh pads and guards, knee pads, chest protectors, and mouthguards.[118][119][120] Most injuries occur in the lower extremities, particularly in the knee, but a significant number also affect the upper extremities. The most common types of injuries are strains, sprains, bruises, fractures, dislocations, and concussions.[117]
Repeated concussions (and possibly sub-concussive head impacts[121]) can increase a person's risk in later life for CTE (chronic traumatic encephalopathy) and mental health issues such as dementia, Parkinson's disease, and depression.[122] Concussions are often caused by helmet-to-helmet or upper-body contact between opposing players, although helmets have prevented more serious injuries such as skull fractures.[123] Various programs are aiming to reduce concussions by reducing the frequency of helmet-to-helmet hits; USA Football's "Heads Up Football" program aims to reduce concussions in youth football by teaching coaches and players about the signs of a concussion, the proper way to wear football equipment and ensure it fits, and proper tackling methods that avoid helmet-to-helmet contact.[124] However, a study in the Orthopaedic Journal of Sports Medicine found that Heads Up Football was ineffective; the same study noted that more extensive reforms implemented by Pop Warner Little Scholars and its member teams were effective in significantly reducing concussion rates.[125]
A 2018 study performed by the VA Boston Healthcare System and the Boston University School of Medicine found that tackle football before age 12 was correlated with earlier onset of symptoms of CTE, but not with symptom severity. More specifically, each year a player played tackle football under age 12 predicted earlier onset of cognitive, behavioral, and mood problems by an average of two and a half years.[126][127][128]
The National Football League (NFL) and the National Collegiate Athletic Association (NCAA) are the most popular football leagues in the United States.[129] The National Football League was founded in 1920[130] and has since become the largest and most popular sport in the United States.[131] The NFL has the highest average attendance of any sporting league in the world, with an average attendance of 66,960 during the 2011 NFL season.[132] The NFL championship game is called the Super Bowl, and is among the biggest events in club sports worldwide.[133] It is played between the champions of the National Football Conference (NFC) and the American Football Conference (AFC), and its winner is awarded the Vince Lombardi Trophy.[134]
College football is the third-most popular sport in the United States, behind professional baseball and professional football.[135] The NCAA, the largest collegiate organization, is divided into three Divisions: Division I, Division II and Division III.[136] Division I football is further divided into two subdivisions: the Football Bowl Subdivision (FBS) and the Football Championship Subdivision (FCS).[137] The champions of each level of play are determined through NCAA-sanctioned playoff systems; while the champion of Division I-FBS was historically determined by various polls and ranking systems, the subdivision adopted a four-team playoff system in 2014.[138]
Several professional football leagues have been formed outside the auspices of the NFL.
The original XFL was created in 2001 by Vince McMahon and lasted for only one season. Despite television contracts with NBC and UPN, and high expectations, the XFL suffered from low quality of play and poor reception for its use of tawdry professional wrestling gimmicks, which caused initially high ratings and attendance to collapse.[144] The XFL was rebooted in 2020.[145] However, after only five weeks of play, the league's operations slowly came to a close due to the ongoing COVID-19 pandemic,[146] and filed for bankruptcy on April 13.[147] The United Football League (UFL) began in 2009 but folded after suspending its 2012 season amid declining interest and lack of major television coverage.[148] The Alliance of American Football lasted less than one season, unable to keep investors.[149]
American football leagues exist throughout the world, but the game has yet to achieve the international success and popularity of baseball and basketball.[150] It is not an Olympic sport, but it was a demonstration sport at the 1932 Summer Olympics.[1] At the international level, Canada, Mexico, and Japan are considered to be second-tier, while Austria, Germany, and France would rank among a third tier. These countries rank far below the United States, which is dominant at the international level.[151]
NFL Europa, the developmental league of the NFL, operated from 1991 to 1992 and then from 1995 to 2007. At the time of its closure, NFL Europa had five teams based in Germany and one in the Netherlands.[152] In Germany, the German Football League (GFL) has 16 teams and has operated for over 40 seasons, with the league's championship game, the German Bowl, closing out each season. The league operates in a promotion and relegation structure with German Football League 2 (GFL2), which also has 16 teams.[153] The BIG6 European Football League functions as a continental championship for Europe. The competition is contested between the top six European teams.[153]
The United Kingdom also operated several teams within NFL Europe during the League's tenure.[154] The resulting rise in popularity of the sport brought the NFL back to the country in 2007 where they now hold the NFL International Series in London, currently consisting of four regular season games.[155][156] The continuing interest and growth in both the sport and the series has led to the possible formation of a potential NFL franchise in London[157][158][159]
An American football league system already exists within the UK, the BAFANL, which has run under various guises since 1983. It currently has 70 teams operating across the tiers of contact football in which teams aim to earn promotion to the Division above, with the Premier Division teams competing to win the Britbowl, the annual British Football Bowl game that has been played since 1985.[160][161][162] In 2007 the British Universities American Football League was formed. From 2008, the BUAFL was officially associated with the National Football League (NFL), through its partner organisation NFL UK.[163] In 2012, BUAFL's league and teams were absorbed into BUCS after American football became an official BUCS sport.[164] Over the period 2007 to 2014, the BUAFL grew from 42 teams and 2,460 participants to 75 teams and over 4,100 people involved.[165]
American football federations are present in Africa, the Americas, Asia, Europe and Oceania; a total of 64 national football federations exist as of July 2012.[151] The International Federation of American Football (IFAF), an international governing body composed of continental federations, runs tournaments such as the IFAF World Championship, the IFAF Women's World Championship, the IFAF U-19 World Championship and the Flag Football World Championship. The IFAF also organizes the annual International Bowl game.[166] The IFAF has received provisional recognition from the International Olympic Committee (IOC).[167] Several major obstacles hinder the IFAF goal of achieving status as an Olympic sport. These include the predominant participation of men in international play and the short three-week Olympic schedule. Large team sizes are an additional difficulty, due to the Olympics' set limit of 10,500 athletes and coaches. American football also has an issue with a lack of global visibility. Nigel Melville, the CEO of USA Rugby, noted that "American football is recognized globally as a sport, but it's not played globally." To solve these concerns, major effort has been put into promoting flag football, a modified version of American football, at the international level.[151]
The safety of the sport has also sparked national controversy in American popular culture. The 2015 film Concussion aimed to shed light on the sport's safety, specifically in the NFL by having Will Smith portray Dr. Bennet Omalu, a neuropathologist who was the first to discover and publish findings of chronic traumatic encephalopathy or CTE.
Japan was introduced to the sport in 1934 by Paul Rusch, a teacher and Christian missionary who helped to establish football teams at three universities in Tokyo. Play was halted during World War II, but the sport began growing in popularity again after the war. As of 2010[update], there are more than 400 high school football teams in Japan, with over 15,000 participants, and over 100 teams play in the Kantoh Collegiate Football Association (KCFA).[187] The college champion plays the champion of the X-League (a semi-professional league in which teams are financed by corporations) in the Rice Bowl to determine Japan's national champion.[188]
In Brazil, football is a growing sport. It was generally unknown there until the 1980s when a small group of players began playing on Copacabana Beach in Rio de Janeiro. The sport grew gradually with 700 amateur players registering within 20 years. Games were played on the beach with modified rules and without the traditional football equipment due to its lack of availability in Brazil. Eventually, a tournament, the Carioca championship, was founded, with the championship Carioca Bowl played to determine a league champion. The country saw its first full-pad game of football in October 2008.[192] According to The Rio Times, the sport is one of the fastest-growing sports in Brazil and is almost as commonly played as soccer on the beaches of Copacabana and Botafogo.[193]
Indoor football leagues constitute what The New York Times writer Mike Tanier described as the "most minor of minor leagues." Leagues are unstable, with franchises regularly moving from one league to another or merging with other teams, and teams or entire leagues dissolving completely; games are only attended by a small number of fans, and most players are semi-professional athletes. The Indoor Football League is an example of a prominent indoor league.[202] The Arena Football League, which was founded in 1987 and ceased operations in 2019, was one of the longest-lived indoor football leagues.[203] In 2004, the league was called "America's fifth major sport" by ESPN The Magazine.[204]
There are several non-contact variants of football like flag football.[205] In flag football the ballcarrier is not tackled; instead, defenders aim to pull a flag tied around the ballcarrier's waist.[206] Another variant, touch football, simply requires the ballcarrier to be touched to be considered downed. Depending on the rules used, a game of touch football may require the ballcarrier be touched with either one or two hands to be considered downed.[207]
Chess is a board game between two players. It is sometimes called international chess or Western chess to distinguish it from related games, such as xiangqi (Chinese chess) and shogi (Japanese chess). The current form of the game emerged in Spain and the rest of Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin. Today, chess is one of the world's most popular games, played by millions of people worldwide.
Chess is an abstract strategy game and involves no hidden information. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. The player controlling the white pieces moves first, followed by the player controlling the black pieces. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in "check") and there is no way for it to escape. There are also several ways a game can end in a draw.
Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (the International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion. A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition, and chess in its turn influenced Western culture and art, and has connections with other fields such as mathematics, computer science, and psychology.
One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players and have deeply influenced the development of chess theory.
Chess pieces are divided into two different colored sets. While the sets might not be literally white and black (e.g. the light set may be a yellowish or off-white color, the dark set may be brown or red), they are always referred to as "white" and "black". The players of the sets are referred to as White and Black, respectively. Each set consists of sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. Chess sets come in a wide variety of styles; for competition, the Staunton pattern is preferred.
The game is played on a square board of eight rows (called ranks) and eight columns (called files). By convention, the 64 squares alternate in color and are referred to as light and dark squares; common colors for chessboards are white and brown, or white and dark green.
In competitive games, the piece colors are allocated to players by the organizers; in informal games, the colors are usually decided randomly, for example by a coin toss, or by one player concealing a white pawn in one hand and a black pawn in the other, and having the opponent choose. 
White moves first, after which players alternate turns, moving one piece per turn, except for castling, when two pieces are moved. A piece is moved to either an unoccupied square or one occupied by an opponent's piece, which is captured and removed from play. With the sole exception of en passant, all pieces capture by moving to the square that the opponent's piece occupies. Moving is compulsory; a player may not skip a turn, even when having to move is detrimental.
Each piece has its own way of moving. In the diagrams, the dots mark the squares to which the piece can move if there are no intervening piece(s) of either color (except the knight, which leaps over any intervening pieces). All pieces except the pawn can capture an enemy piece if it is located on a square to which they would be able to move if the square was unoccupied. The squares on which pawns can capture enemy pieces are marked in the diagram with black crosses.
When a king is under immediate attack, it is said to be in check. A move in response to a check is legal only if it results in a position where the king is no longer in check. This can involve capturing the checking piece; interposing a piece between the checking piece and the king (which is possible only if the attacking piece is a queen, rook, or bishop and there is a square between it and the king); or moving the king to a square where it is not under attack. Castling is not a permissible response to a check.[1]
The object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to get it out of check. It is never legal for a player to make a move that puts or leaves the player's own king in check. In casual games, it is common to announce "check" when putting the opponent's king in check, but this is not required by the rules of chess and is not usually done in tournaments.[2]
Once per game, each king can make a move known as castling. Castling consists of moving the king two squares toward a rook of the same color on the same rank, and then placing the rook on the square that the king crossed.
Castling is still permitted if the rook is under attack, or if the rook crosses an attacked square. 
When a pawn makes a two-step advance from its starting position and there is an opponent's pawn on a square next to the destination square on an adjacent file, then the opponent's pawn can capture it en passant ("in passing"), moving to the square the pawn passed over. This can be done only on the turn immediately following the enemy pawn's two-square advance; otherwise, the right to do so is forfeited. For example, in the animated diagram, the black pawn advances two squares from g7 to g5, and the white pawn on f5 can take it en passant on g6 (but only immediately after the black pawn's advance).
When a pawn advances to its eighth rank, as part of the move, it is promoted and must be exchanged for the player's choice of queen, rook, bishop, or knight of the same color. Usually, the pawn is chosen to be promoted to a queen, but in some cases, another piece is chosen; this is called underpromotion. In the animated diagram, the pawn on c7 can be advanced to the eighth rank and be promoted. There is no restriction on the piece promoted to, so it is possible to have more pieces of the same type than at the start of the game (e.g., two or more queens). If the required piece is not available (e.g. a second queen) an inverted rook is sometimes used as a substitute, but this is not recognized in FIDE-sanctioned games.
There are several ways a game can end in a draw:
Time is controlled using a chess clock that has two displays, one for each player's remaining time. Analog chess clocks have been largely replaced by digital clocks, which allow for time controls with increments.
Time controls are also enforced in correspondence chess competitions. A typical time control is 50 days for every 10 moves.
The pieces are identified by their initials. In English, these are K (king), Q (queen), R (rook), B (bishop), and N (knight; N is used to avoid confusion with king). For example, Qg5 means "queen moves to the g-file, 5th rank" (that is, to the square g5). Different initials may be used for other languages. In chess literature, figurine algebraic notation (FAN) is frequently used to aid understanding independent of language.
To resolve ambiguities, an additional letter or number is added to indicate the file or rank from which the piece moved (e.g. Ngf3 means "knight from the g-file moves to the square f3"; R1e2 means "rook on the first rank moves to e2"). For pawns, no letter initial is used; so e4 means "pawn moves to the square e4".
If the piece makes a capture, "x" is usually inserted before the destination square. Thus Bxf3 means "bishop captures on f3". When a pawn makes a capture, the file from which the pawn departed is used to identify the pawn making the capture, for example, exd5 (pawn on the e-file captures the piece on d5). Ranks may be omitted if unambiguous, for example, exd (pawn on the e-file captures a piece somewhere on the d-file). A minority of publications use ":" to indicate a capture, and some omit the capture symbol altogether. In its most abbreviated form, exd5 may be rendered simply as ed. An en passant capture may optionally be marked with the notation "e.p."
For example, one variation of a simple trap known as the Scholar's mate (see animated diagram) can be recorded:
Variants of algebraic notation include long form algebraic, in which both the departure and destination square are indicated; abbreviated algebraic, in which capture signs, check signs, and ranks of pawn captures may be omitted; and Figurine Algebraic Notation, used in chess publications for universal readability regardless of language.
Portable Game Notation (PGN) is a text-based file format for recording chess games, based on short form English algebraic notation with a small amount of markup. PGN files (suffix .pgn) can be processed by most chess software, as well as being easily readable by humans.
Until about 1980, the majority of English language chess publications used descriptive notation, in which files are identified by the initial letter of the piece that occupies the first rank at the beginning of the game. In descriptive notation, the common opening move 1.e4 is rendered as "1.P-K4" ("pawn to king four"). Another system is ICCF numeric notation, recognized by the International Correspondence Chess Federation though its use is in decline.
In competitive games, players are normally required to keep a score (record of the game). For this purpose, only algebraic notation is recognized in FIDE-sanctioned events; game scores recorded in a different notation system may not be used as evidence in the event of a dispute.
Contemporary chess is an organized sport with structured international and national leagues, tournaments, and congresses. Thousands of chess tournaments, matches, and festivals are held around the world every year catering to players of all levels.
The term "match" refers not to an individual game, but to either a series of games between two players, or a team competition in which each player of one team plays one game against a player of the other team.
FIDE's most visible activity is organizing the World Chess Championship, a role it assumed in 1948. The current World Champion is Magnus Carlsen of Norway.[11] The reigning Women's World Champion is Ju Wenjun from China.[12]
Other competitions for individuals include the World Junior Chess Championship, the European Individual Chess Championship, the tournaments for the World Championship qualification cycle, and the various national championships. Invitation-only tournaments regularly attract the world's strongest players. Examples include Spain's Linares event, Monte Carlo's Melody Amber tournament, the Dortmund Sparkassen meeting, Sofia's M-tel Masters, and Wijk aan Zee's Tata Steel tournament.
Regular team chess events include the Chess Olympiad and the European Team Chess Championship.
The World Chess Solving Championship and World Correspondence Chess Championships include both team and individual events; these are held independently of FIDE.
In order to rank players, FIDE, ICCF, and most national chess organizations use the Elo rating system developed by Arpad Elo. An average club player has a rating of about 1500; the highest FIDE rating of all time, 2882, was achieved by Magnus Carlsen on the March 2014 FIDE rating list.[13]
The above titles are open to both men and women. There are also separate women-only titles; Woman Grandmaster (WGM), Woman International Master (WIM), Woman FIDE Master (WFM) and Woman Candidate Master (WCM). These require a performance level approximately 200 Elo rating points below the similarly named open titles, and their continued existence has sometimes been controversial. Beginning with Nona Gaprindashvili in 1978, a number of women have earned the open GM title.[note 2]
FIDE also awards titles for arbiters and trainers.[16][17] International titles are also awarded to composers and solvers of chess problems and to correspondence chess players (by the International Correspondence Chess Federation). National chess organizations may also award titles.
Chess has an extensive literature. In 1913, the chess historian H.J.R. Murray estimated the total number of books, magazines, and chess columns in newspapers to be about 5,000.[18] B.H. Wood estimated the number, as of 1949, to be about 20,000.[19] David Hooper and Kenneth Whyld write that, "Since then there has been a steady increase year by year of the number of new chess publications. No one knows how many have been printed."[19] Significant public chess libraries include the John G. White Chess and Checkers Collection at Cleveland Public Library, with over 32,000 chess books and over 6,000 bound volumes of chess periodicals;[20] and the Chess & Draughts collection at the National Library of the Netherlands, with about 30,000 books.[21]
Chess theory usually divides the game of chess into three phases with different sets of strategies: the opening, typically the first 10 to 20 moves, when players move their pieces to useful positions for the coming battle; the middlegame; and last the endgame, when most of the pieces are gone, kings typically take a more active part in the struggle, and pawn promotion is often decisive.
Opening theory is concerned with finding the best moves in the initial phase of the game. There are dozens of different openings, and hundreds of variants. The Oxford Companion to Chess lists 1,327 named openings and variants.[22]
Endgame theory is concerned with positions where there are only a few pieces left. Theoretics categorize these positions according to the pieces, for example "King and pawn endings" or "Rook versus a minor piece".
Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. This initially gives White the initiative.[25] Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.
Specific plans or strategic themes will often arise from particular groups of openings which result in a specific type of pawn structure. An example is the minority attack, which is the attack of queenside pawns against an opponent who has more pawns on the queenside. The study of openings is therefore connected to the preparation of plans that are typical of the resulting middlegames.[28]
Another important strategic question in the middlegame is whether and how to reduce material and transition into an endgame (i.e. simplify). Minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of a pawn, or sometimes even with a two-pawn advantage.[29]
Chess strategy is concerned with the evaluation of chess positions and with setting up goals and long-term plans for future play. During the evaluation, players must take into account numerous factors such as the value of the pieces on the board, control of the center and centralization, the pawn structure, king safety, and the control of key squares or groups of squares (for example, diagonals, open files, and dark or light squares).
The most basic step in evaluating a position is to count the total value of pieces of both sides.[34] The point values used for this purpose are based on experience; usually, pawns are considered worth one point, knights and bishops about three points each, rooks about five points (the value difference between a rook and a bishop or knight being known as the exchange), and queens about nine points. The king is more valuable than all of the other pieces combined, since its checkmate loses the game. But in practical terms, in the endgame, the king as a fighting piece is generally more powerful than a bishop or knight but less powerful than a rook.[35] These basic values are then modified by other factors like position of the piece (e.g. advanced pawns are usually more valuable than those on their initial squares), coordination between pieces (e.g. a pair of bishops usually coordinate better than a bishop and a knight), or the type of position (e.g. knights are generally better in closed positions with many pawns while bishops are more powerful in open positions).[36]
Another important factor in the evaluation of chess positions is pawn structure (sometimes known as the pawn skeleton): the configuration of pawns on the chessboard.[37] Since pawns are the least mobile of the pieces, pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in pawn structure include isolated, doubled, or backward pawns and holes; once created, they are often permanent. Care must therefore be taken to avoid these weaknesses unless they are compensated by another valuable asset (for example, by the possibility of developing an attack).[38]
The endgame (also end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:[39]
Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions in which one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides, and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to pieces on the board other than kings, such as "rook and pawn versus rook" endgames.
The earliest texts referring to the origins of chess date from the beginning of the 7th century. Three are written in Pahlavi (Middle Persian)[42] and one, the Harshacharita, is in Sanskrit.[43] One of these texts, the Chatrang-namak, represents one of the earliest written accounts of chess. The narrator Bozorgmehr explains that Chatrang, the Pahlavi word for chess, was introduced to Persia by 'Dewasarm, a great ruler of India' during the reign of Khosrow I.[44]
The game of chess was then played and known in all European countries. A famous 13th-century Spanish manuscript covering chess, backgammon, and dice is known as the Libro de los juegos, which is the earliest European treatise on chess as well as being the oldest document on European tables games. The rules were fundamentally similar to those of the Arabic shatranj. The differences were mostly in the use of a checkered board instead of a plain monochrome board used by Arabs and the habit of allowing some or all pawns to make an initial double step. In some regions, the Queen, which had replaced the Wazir, and/or the King could also make an initial two-square leap under some conditions.[63]
At the same time, the intellectual movement of romanticism had had a far-reaching impact on chess, with aesthetics and tactical beauty being held in higher regard than objective soundness and strategic planning. As a result, virtually all games began with the Open Game, and it was considered unsportsmanlike to decline gambits that invited tactical play such as the King's Gambit and the Evans Gambit.[75] This chess philosophy is known as Romantic chess, and a sharp, tactical style consistent with the principles of chess romanticism was predominant until the late 19th century.[76]
As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books, and chess journals appeared. There were correspondence matches between cities; for example, the London Chess Club played against the Edinburgh Chess Club in 1824.[79] Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling, and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.
The first modern chess tournament was organized by Howard Staunton, a leading English chess player, and was held in London in 1851. It was won by the German Adolf Anderssen, who was hailed as the leading chess master. His brilliant, energetic attacking style was typical for the time.[80][81] Sparkling games like Anderssen's Immortal Game and Evergreen Game or Morphy's "Opera Game" were regarded as the highest possible summit of the art of chess.[82]
Deeper insight into the nature of chess came with the American Paul Morphy, an extraordinary chess prodigy. Morphy won against all important competitors (except Staunton, who refused to play), including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks.[83]
Prague-born Wilhelm Steinitz laid the foundations for a scientific approach to the game, the art of breaking a position down into components[84] and preparing correct plans.[85] In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. This win marked a stylistic transition at the highest levels of chess from an attacking, tactical style predominant in the Romantic era to a more positional, strategic style introduced to the chess world by Steinitz. Steinitz lost his crown in 1894 to a much younger player, the German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of any world champion.[86]
After the end of the 19th century, the number of master tournaments and matches held annually quickly grew. The first Olympiad was held in Paris in 1924, and FIDE was founded initially for the purpose of organizing that event. In 1927, the Women's World Chess Championship was established; the first to hold the title was Czech-English master Vera Menchik.[87]
After the death of Alekhine, a new World Champion was sought. FIDE, which has controlled the title since then, ran a tournament of elite players. The winner of the 1948 tournament was Russian Mikhail Botvinnik.
In 1950 FIDE established a system of titles, conferring the titles of Grandmaster and International Master on 27 players. (Some sources state that in 1914 the title of chess Grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch, and Marshall, but this is a disputed claim.[note 5])
Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes.[99] In the 1984 World Chess Championship, Karpov faced his toughest challenge to date, the young Garry Kasparov from Baku, Soviet Azerbaijan. The match was aborted in controversial circumstances after 5 months and 48 games with Karpov leading by 5 wins to 3, but evidently exhausted; many commentators believed Kasparov, who had won the last two games, would have won the match had it continued. Kasparov won the 1985 rematch. Kasparov and Karpov contested three further closely fought matches in 1986, 1987 and 1990, Kasparov winning them all.[100] Kasparov became the dominant figure of world chess from the mid 1980s until his retirement from competition in 2005.
Chess-playing computer programs (later known as chess engines) began to appear in the 1960s. In 1970, the first major computer chess tournament, the North American Computer Chess Championship, was held, followed in 1974 by the first World Computer Chess Championship. In the late 1970s, dedicated home chess computers such as Fidelity Electronics' Chess Challenger became commercially available, as well as software to run on home computers. However, the overall standard of computer chess was low until the 1990s.
The first endgame tablebases, which provided perfect play for relatively simple endgames such as king and rook versus king and bishop, appeared in the late 1970s. This set a precedent to the complete six- and seven-piece tablebases that became available in the 2000s and 2010s respectively.[101]
The first commercial chess database, a collection of chess games searchable by move and position, was introduced by the German company ChessBase in 1987. Databases containing millions of chess games have since had a profound effect on opening theory and other areas of chess research.
Digital chess clocks were invented in 1973, though they did not become commonplace until the 1990s. Digital clocks allow for time controls involving increments and delays.
The Internet enabled online chess as a new medium of playing, with chess servers allowing users to play other people from different parts of the world in real time. The first such server, known as Internet Chess Server or ICS, was developed at the University of Utah in 1992. ICS formed the basis for the first commercial chess server, the Internet Chess Club, which was launched in 1995, and for other early chess servers such as FICS (Free Internet Chess Server). Since then, many other platforms have appeared, and online chess began to rival over-the-board chess in popularity.[102][103] During the 2020 COVID-19 pandemic, the isolation ensuing from quarantines imposed in many places around the world, combined with the success of the popular Netflix show The Queen's Gambit and other factors such as the popularity of online tournaments (notably PogChamps) and chess Twitch streamers, resulted in a surge of popularity not only for online chess, but for the game of chess in general; this phenomenon has been referred to in the media as the 2020 online chess boom.[104][105]
As endgame tablebases developed, they began to provide perfect play in endgame positions in which the game-theoretical outcome was previously unknown, such as positions with king, queen and pawn against king and queen. In 1991, Lewis Stiller published a tablebase for select six-piece endgames,[108][109] and by 2005, following the publication of Nalimov tablebases, all six-piece endgame positions were solved. In 2012, Lomonosov tablebases were published which solved all seven-piece endgame positions.[110] Use of tablebases enhances the performance of chess engines by providing definitive results in some branches of analysis.
Technological progress made in the 1990s and the 21st century has influenced the way that chess is studied at all levels, as well as the state of chess as a spectator sport.
Previously, preparation at the professional level required an extensive chess library and several subscriptions to publications such as Chess Informant to keep up with opening developments and study opponents' games. Today, preparation at the professional level involves the use of databases containing millions of games, and engines to analyze different opening variations and prepare novelties.[111] A number of online learning resources are also available for players of all levels, such as online courses, tactics trainers, and video lessons.[112]
Organized chess even for young children has become common. FIDE holds world championships for age levels down to 8 years old. The largest tournaments, in number of players, are those held for children.[116]
The number of grandmasters and other chess professionals has also grown in the modern era. Kenneth Regan and Guy Haworth conducted research involving comparison of move choices by players of different levels and from different periods with the analysis of strong chess engines; they concluded that the increase in the number of grandmasters and higher Elo ratings of the top players reflect an actual increase in the average standard of play, rather than "rating inflation" or "title inflation".[117]
In 1993, Garry Kasparov and Nigel Short broke ties with FIDE to organize their own match for the title and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Championships and respective World Champions: the PCA or "classical" champions extending the Steinitzian tradition in which the current champion plays a challenger in a series of games, and the other following FIDE's new format of many players competing in a large knockout tournament to determine the champion. Kasparov lost his PCA title in 2000 to Vladimir Kramnik of Russia.[118] Due to the complicated state of world chess politics and difficulties obtaining commercial sponsorships, Kasparov was never able to challenge for the title again. Despite this, he continued to dominate in top level tournaments and remained the world's highest rated player until his retirement from competitive chess in 2005.
The World Chess Championship 2006, in which Kramnik beat the FIDE World Champion Veselin Topalov, reunified the titles and made Kramnik the undisputed World Chess Champion.[119] In September 2007, he lost the title to Viswanathan Anand of India, who won the championship tournament in Mexico City. Anand defended his title in the revenge match of 2008,[120] 2010 and 2012. In 2013, Magnus Carlsen of Norway beat Anand in the 2013 World Chess Championship.[121] He defended his title 4 times since then and is the reigning world champion.
In the Middle Ages and during the Renaissance, chess was a part of noble culture; it was used to teach war strategy and was dubbed the "King's Game".[122] Gentlemen are "to be meanly seene in the play at Chestes", says the overview at the beginning of Baldassare Castiglione's The Book of the Courtier (1528, English 1561 by Sir Thomas Hoby), but chess should not be a gentleman's main passion. Castiglione explains it further:
And what say you to the game at chestes? It is truely an honest kynde of enterteynmente and wittie, quoth Syr Friderick. But me think it hath a fault, whiche is, that a man may be to couning at it, for who ever will be excellent in the playe of chestes, I beleave he must beestowe much tyme about it, and applie it with so much study, that a man may assoone learne some noble scyence, or compase any other matter of importaunce, and yet in the ende in beestowing all that laboure, he knoweth no more but a game. Therfore in this I beleave there happeneth a very rare thing, namely, that the meane is more commendable, then the excellency.[123]
Some of the elaborate chess sets used by the aristocracy at least partially survive, such as the Lewis chessmen.
The knyght ought to be made alle armed upon an hors in suche wyse that he haue an helme on his heed and a spere in his ryght hande/ and coueryd wyth his sheld/ a swerde and a mace on his lyft syde/ Cladd wyth an hawberk and plates to fore his breste/ legge harnoys on his legges/ Spores on his heelis on his handes his gauntelettes/ his hors well broken and taught and apte to bataylle and couerid with his armes/ whan the knyghtes ben maad they ben bayned or bathed/ that is the signe that they shold lede a newe lyf and newe maners/ also they wake alle the nyght in prayers and orysons vnto god that he wylle gyue hem grace that they may gete that thynge that they may not gete by nature/ The kynge or prynce gyrdeth a boute them a swerde in signe/ that they shold abyde and kepe hym of whom they take theyr dispenses and dignyte.[127]
During the Age of Enlightenment, chess was viewed as a means of self-improvement. Benjamin Franklin, in his article "The Morals of Chess" (1750), wrote:
The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:
I. Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...
III. Caution, not to make our moves too hastily ...[132]
Chess was occasionally criticized in the 19th century as a waste of time.[133][134]
Chess is taught to children in schools around the world today. Many schools host chess clubs, and there are many scholastic tournaments specifically for children. Tournaments are held regularly in many countries, hosted by organizations such as the United States Chess Federation and the National Scholastic Chess Foundation.[135]
Chess is many times depicted in the arts; significant works where chess plays a key role range from Thomas Middleton's A Game at Chess to Through the Looking-Glass by Lewis Carroll, to Vladimir Nabokov's The Defense, to The Royal Game by Stefan Zweig. Chess has also featured in film classics such as Ingmar Bergman's The Seventh Seal, Satyajit Ray's The Chess Players, and Powell and Pressburger's A Matter of Life and Death.
Chess is also present in contemporary popular culture. For example, the characters in Star Trek play a futuristic version of the game called "Federation Tri-Dimensional Chess"[136] and "Wizard's Chess" is played in J.K. Rowling's Harry Potter.[137]
The game structure and nature of chess are related to several branches of mathematics. Many combinatorical and topological problems connected to chess, such as the knight's tour and the eight queens puzzle, have been known for hundreds of years.
The number of legal positions in chess is estimated to be 4.59 (+/- 0.38) x1044 with a 95% confidence level,[138] with a game-tree complexity of approximately 10123. The game-tree complexity of chess was first calculated by Claude Shannon as 10120, a number known as the Shannon number.[139] An average position typically has thirty to forty possible moves, but there may be as few as zero (in the case of checkmate or stalemate) or (in a constructed position) as many as 218.[140]
In 1913, Ernst Zermelo used chess as a basis for his theory of game strategies, which is considered one of the predecessors of game theory.[141] Zermelo's theorem states that it is possible to solve chess, i.e. to determine with certainty the outcome of a perfectly played game (either White can force a win, or Black can force a win, or both sides can force at least a draw).[142] However, with 1043 legal positions in chess, it will take an impossibly long time to compute a perfect strategy with any feasible technology.[143]
There is an extensive scientific literature on chess psychology.[note 6][145][146][147][148] Alfred Binet and others showed that knowledge and verbal, rather than visuospatial, ability lies at the core of expertise.[149][150] In his doctoral thesis, Adriaan de Groot showed that chess masters can rapidly perceive the key features of a position.[151] According to de Groot, this perception, made possible by years of practice and study, is more important than the sheer ability to anticipate moves. De Groot showed that chess masters can memorize positions shown for a few seconds almost perfectly. The ability to memorize does not alone account for chess-playing skill, since masters and novices, when faced with random arrangements of chess pieces, had equivalent recall (about six positions in each case). Rather, it is the ability to recognize patterns, which are then memorized, which distinguished the skilled players from the novices. When the positions of the pieces were taken from an actual game, the masters had almost total positional recall.[152]
More recent research has focused on chess as mental training; the respective roles of knowledge and look-ahead search; brain imaging studies of chess masters and novices; blindfold chess; the role of personality and intelligence in chess skill; gender differences; and computational models of chess expertise. The role of practice and talent in the development of chess and other domains of expertise has led to much empirical investigation. Ericsson and colleagues have argued that deliberate practice is sufficient for reaching high levels of expertise in chess.[153] Recent research, however, fails to replicate their results and indicates that factors other than practice are also important.[154][155]
For example, Fernand Gobet and colleagues have shown that stronger players started playing chess at a young age and that experts born in the Northern Hemisphere are more likely to have been born in late winter and early spring. Compared to the general population, chess players are more likely to be non-right-handed, though they found no correlation between handedness and skill.[155]
A relationship between chess skill and intelligence has long been discussed in scientific literature as well as in popular culture. Academic studies that investigate the relationship date back at least to 1927.[156] Although one meta-analysis and most children studies find a positive correlation between general cognitive ability and chess skill, adult studies show mixed results.[157][158]
Chess composition is the art of creating chess problems (also called chess compositions). The creator is known as a chess composer.[160] There are many types of chess problems; the two most important are:
Fairy chess is a branch of chess problem composition involving altered rules, such as the use of unconventional pieces or boards, or unusual stipulations such as reflexmates.
Tournaments for composition and solving of chess problems are organized by the World Federation for Chess Composition, which works cooperatively with but independent of FIDE. The WFCC awards titles for composing and solving chess problems.[163]
Online chess is chess that is played over the internet, allowing players to play against each other in real time. This is done through the use of Internet chess servers, which pair up individual players based on their rating using an Elo or similar rating system. Online chess saw a spike in growth during the quarantines of the COVID-19 pandemic.[164][165] This can be attributed to both isolation and the popularity of Netflix miniseries The Queen's Gambit, which was released in October 2020.[164][165] Chess app downloads on the App Store and Google Play Store rose by 63% after the show debuted.[166] Chess.com saw more than twice as many account registrations in November as it had in previous months, and the number of games played monthly on Lichess doubled as well. There was also a demographic shift in players, with female registration on Chess.com shifting from 22% to 27% of new players.[167] Grandmaster Maurice Ashley said "A boom is taking place in chess like we have never seen maybe since the Bobby Fischer days," attributing the growth to an increased desire to do something constructive during the pandemic.[168] USCF Women's Program Director Jennifer Shahade stated that chess works well on the internet, since pieces do not need to be reset and matchmaking is virtually instant.[169]
The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require "thinking" for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of "thinking"; (4) the discrete structure of chess fits well into the digital nature of
modern computers.[173]
With huge databases of past games and high analytical ability, computers can help players to learn chess and prepare for matches. Internet Chess Servers allow people to find and play opponents worldwide. The presence of computers and modern communication tools have raised concerns regarding cheating during games.[182]
There are more than two thousand published chess variants, games with similar but different rules.[183] Most of them are of relatively recent origin.[184] They include:
In the context of chess variants, regular (i.e. FIDE) chess is commonly referred to as Western chess, international chess, orthodox chess, orthochess, and classic chess.[186][187]
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
To protect the wiki against automated account creation, we kindly ask you to enter the words that appear below in the box (more info):
To protect the wiki against automated account creation, we kindly ask you to enter the words that appear below in the box (more info):
Full help contents page
Training for students
A single-page guide to contributing
A training adventure game
Resources for new editors
People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using.
Many IP addresses change periodically, and are often shared by several people. You may create an account or log in to avoid future confusion with other logged out users. Creating an account also hides your IP address.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
Wikipedia is a compendium of the world's knowledge. If you know what you are looking for, type it into Wikipedia's search box. If, however, you need a bird's eye view of what Wikipedia has to offer, see its main contents pages below, which in turn list more specific pages.
Wikipedia's main contents systems are arranged into these subject classifications. Each subject is further divided into subtopics. 
Timelines list events chronologically, sometimes including links to articles with more detail.  There are several ways to find timelines:
You can help us keep Wikipedia up to date! The list below is for encyclopedia entries that describe and pertain to events happening on a current basis. 
Speaking of reference works, various third-party classification systems have been mapped to Wikipedia articles, which can be accessed from these pages:
Bibliographies list sources on a given topic, for verification or further reading outside Wikipedia:
Overview articles summarize in prose a broad topic like biology, and also have illustrations and links to subtopics like cell biology, biographies like Carl Linnaeus, and other related articles like Human Genome Project.
Outline pages have trees of topics in an outline format, which in turn are linked to further outlines and articles providing more detail.  Outlines show how important subtopics relate to each other based on how they are arranged in the tree, and they are useful as a more condensed, non-prose alternative to overview articles.
List pages enumerate items of a particular type, such as the List of sovereign states or List of South Africans.  Wikipedia has "lists of lists" when there are too many items to fit on a single page, when the items can be sorted in different ways, or as a way of navigating lists on a topic (for example Lists of countries and territories or Lists of people).  There are several ways to find lists:
Portals contain featured articles and images, news, categories, excerpts of key articles, links to related portals, and to-do lists for editors.  There are two ways to find portals:
Glossaries are lists of terms with definitions. Wikipedia includes hundreds of alphabetical glossaries; they can be found in two ways:
Wikipedia's collection of category pages is a classified index system.  It is automatically generated from category tags at the bottoms of articles and most other pages. Nearly all of the articles available so far on the website can be found through these subject indexes.
If you are simply looking to browse articles by topic, there are two top-level pages to choose from:
Category:Contents is technically at the top of the category hierarchy, but contains many categories useful to editors but not readers. Special:Categories lists every category alphabetically.
Vital articles are lists of subjects for which the English Wikipedia should have corresponding high-quality articles. They serve as centralized watchlists to track the quality status of Wikipedia's most important articles and to give editors guidance on which articles to prioritize for improvement.
Featured content represents the best of Wikipedia, and has undergone a thorough review process to ensure that it meets the highest encyclopedic standards. Presented by type:
Good articles are articles that meet a core set of editorial standards, the good article criteria, and successfully pass through the good article nomination process. They are well written, contain factually accurate and verifiable information, are broad in coverage, neutral in point of view, stable, and illustrated, where possible, by relevant images with suitable copyright licenses.
Growing collections of Wikipedia articles are starting to become available as spoken word recordings as well.
The Fowler House, also known as the Allen-Fowler House is a historic, two-story, modified L-plan house built in 1852 in Bastrop, Texas, United States. The house was added to the National Register of Historic Places on December 22, 1978,[1] and was designated a Recorded Texas Historic Landmark in 2008.
The house was built by Professor William J. Hancock of Aberdeen, Mississippi, in 1852 after he arrived in Bastrop to become headmaster at the Bastrop Academy, one of the leading schools in Texas at the time. The house was not only for his family and him,  but also for student boarders.
In 1857, Bastrop Academy became Bastrop Military Institute, which trained young men for service during the Civil War. Colonel Robert Thomas Pritchard Allen replaced Hancock as headmaster and Allen and his wife Julia moved into the house. They continued to board cadets who attended the institute. Sam Houston, a hero of the Texas Revolution, was a frequent guest of the Allens while his sons attended the institute.
John Preston Fowler and Maud Maynard Fowler bought the property in 1876 and added Victorian detailing and a projecting bay window to the structure. Fowler became mayor of Bastrop, county attorney, and a Texas state senator.
The current owner of the house is Geoff Connor, who purchased the house in 2006.
Wikipedia is a dynamic free online encyclopedia that anyone can edit in good faith, and tens of millions already have!
Wikipedia's purpose is to benefit readers by containing information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, whose articles also have numerous links to guide readers to more information.
Written collaboratively by largely anonymous volunteers, anyone with Internet access and not blocked, can write and make changes to Wikipedia articles (except in limited cases where editing is restricted to prevent disruption or vandalism). Since its creation on January 15, 2001, Wikipedia has grown into the world's largest reference website, attracting over a billion visitors monthly. It currently has more than sixty million articles in more than 300 languages, including 6,623,677 articles in English with 129,357 active contributors in the past month.
The fundamental principles of Wikipedia are summarized in its five pillars. The Wikipedia community has developed many policies and guidelines, but you do not need to be familiar with every one of them before contributing.
Anyone can edit Wikipedia's text, references, and images. What is written is more important than who writes it. The content must conform with Wikipedia's policies, including being verifiable by published sources. Editors' opinions, beliefs, personal experiences, unreviewed research, libelous material, and copyright violations will not remain. Wikipedia's software allows easy reversal of errors, and experienced editors watch and patrol bad edits.
Wikipedia differs from printed references in important ways. It is continually created and updated, and encyclopedic articles on new events appear within minutes rather than months or years. Because anyone can improve Wikipedia, it has become more comprehensive, clear, and balanced than any other encyclopedia. Its contributors improve the quality and quantity of the articles as well as remove misinformation, errors, and vandalism. Any reader can fix a mistake or add more information to articles (see Researching with Wikipedia). 
Wikipedia has tested the wisdom of the crowd since 2001 and found that it succeeds.
We could not find the above page on our servers.
Alternatively, you can visit the Main Page or read more information about this type of error.
This page provides help with the most common questions about Wikipedia.
You can also search Wikipedia's help pages using the search box below, or browse the Help menu or the Help directory.
The Readers' FAQ and our about page contain the most commonly sought information about Wikipedia.
There are other ways to browse and explore Wikipedia articles; many can be found at Wikipedia:Contents. See our disclaimer for cautions about Wikipedia's limitations.
For mobile access, press the mobile view link at the very bottom of every desktop view page.
Contributing is easy: see how to edit a page. For a quick summary on participating, see contributing to Wikipedia, and for a friendly tutorial, see our introduction. For a listing of introductions and tutorials by topic, see getting started. The Simplified Manual of Style and Cheatsheet can remind you of basic wiki markup.
The simple guide to vandalism cleanup can help you undo malicious edits.
If you're looking for places you can help out, the Task Center is the place to go, or check out what else is happening at the community portal. You can practice editing and experiment in a sandboxyour sandbox.
If there is a problem with an article about yourself, a family member, a friend or a colleague, please read Biographies of living persons/Help.
If you spot a problem with an article, you can fix it directly, by clicking on the "Edit" link at the top of that page. See the "edit an article" section of this page for more information.
If you don't feel ready to fix the article yourself, post a message on the article's talk page. This will bring the matter to the attention of others who work on that article. There is a "Talk" link at the beginning of every article page.
Check Your first article to see if your topic is appropriate, then the Article wizard will walk you through creating the article.
Once you have created an article, see Writing better articles for guidance on how to improve it and what to include (like reference citations).
For contributing images, audio or video files, see the Introduction to uploading images. Then the Upload wizard will guide you through that process.
Answers to common problems can be found at frequently asked questions.
Or check out where to ask questions or make comments.
New users having problems editing Wikipedia should ask at the Teahouse. More complex questions can be posed at the Help desk. Volunteers will respond as soon as they're able.
Or ask for help on your talk page and a volunteer will visit you there!
You can get live help with editing in the help chatroom.
For help with technical issues, ask at the Village pump.
If searching Wikipedia has not answered your question (for example, questions like "Which country has the world's largest fishing fleet?"), try the Reference Desk. Volunteers there will attempt to answer your questions on any topic, or point you toward the information you need.
Screen readers are a form of assistive technology for people with disabilities. A list of screen readers is available including a section, Software aids for people with reading difficulties.
Reader software examples include Spoken Web, JAWS, and NonVisual Desktop Access (NVDA). In addition, Fangs screen reader emulator is an open-source extension for the Pale Moon browser that simulates how a web page would look in JAWS.
Full help contents page
Training for students
A single-page guide to contributing
A training adventure game
Resources for new editors
This page provides a listing of current collaborations, tasks, and news about English Wikipedia. New to Wikipedia? See the contributing to Wikipedia page or our tutorial for everything you need to know to get started. For a listing of internal project pages of interest, see the department directory.
For a listing of ongoing discussions and current requests, see the Dashboard.
You can help improve the articles listed below! This list updates frequently, so check back here for more tasks to try. (See Wikipedia:Maintenance or the  Task Center for further information.)
Help counter systemic bias by creating new articles on important women.
Welcome to the community bulletin board, which is a page used for announcements from WikiProjects and other groups. Included here are coordinated efforts, events, projects, and other general announcements.
Also consider posting WikiProject, Task Force, and Collaboration news at The Signpost's WikiProject Report page.
Latest tech news from the Wikimedia technical community. Please tell other users about these changes. Not all changes will affect you. Translations are available.
Discussions in the following areas have requested wider attention via Requests for comment:
The School and university projects page collects information about Wikipedia editing projects for school and university classes, including an archive of many past class projects.
A list of current classes using Wikipedia can be found at current projects.
Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.
Uploads locally to Wikipedia; must comply with the non-free content criteria
Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript.
Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again.
Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. 
In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed.
The filename you chose seems to be very short, or overly generic. Please don't use:
If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
This should not be done, except in very rare exceptional cases.
Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.
Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the Wikimedia Commons.
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
Please note that by "entirely self-made" we really mean just that. 
Do not use this section for any of the following:
Editors who falsely declare such items as their "own work" will be blocked from editing.
Use this only if there is an explicit licensing statement in the source. 
The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
If the source website doesn't say so explicitly, please do not upload the file.
Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it.
 Please remember that you will need to demonstrate that:
Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually.
Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
If this is an article you are only planning to write, please write it first and upload the file afterwards.
The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
Please upload this file only if it is going to be used in an actual article.
If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
The page Example is not a real article, but a disambiguation page pointing to a number of other pages.
Please check and enter the exact title of the actual target article you meant.
If neither of these two statements applies, then please do not upload this image.
This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
In view of this, please explain how the use of this file will be minimal.
Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you.
This is the data that will be submitted to upload:
This might take a minute or two, depending on the size of the file and the speed of your internet connection.
Once uploading is completed, you will find your new file at this link:
Your file has been uploaded successfully and can now be found here:
Please follow the link and check that the image description page has all the information you meant to include.
If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
To insert this file into an article, you may want to use code similar to the following:
If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages.
Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page.
Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes.
Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.
Uploads locally to Wikipedia; must comply with the non-free content criteria
Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript.
Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again.
Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. 
In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed.
The filename you chose seems to be very short, or overly generic. Please don't use:
If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
This should not be done, except in very rare exceptional cases.
Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.
Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the Wikimedia Commons.
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
Please note that by "entirely self-made" we really mean just that. 
Do not use this section for any of the following:
Editors who falsely declare such items as their "own work" will be blocked from editing.
Use this only if there is an explicit licensing statement in the source. 
The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
If the source website doesn't say so explicitly, please do not upload the file.
Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it.
 Please remember that you will need to demonstrate that:
Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually.
Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
If this is an article you are only planning to write, please write it first and upload the file afterwards.
The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
Please upload this file only if it is going to be used in an actual article.
If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
The page Example is not a real article, but a disambiguation page pointing to a number of other pages.
Please check and enter the exact title of the actual target article you meant.
If neither of these two statements applies, then please do not upload this image.
This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
In view of this, please explain how the use of this file will be minimal.
Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you.
This is the data that will be submitted to upload:
This might take a minute or two, depending on the size of the file and the speed of your internet connection.
Once uploading is completed, you will find your new file at this link:
Your file has been uploaded successfully and can now be found here:
Please follow the link and check that the image description page has all the information you meant to include.
If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
To insert this file into an article, you may want to use code similar to the following:
If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages.
Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page.
This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators).
Chess is a board game between two players. It is sometimes called international chess or Western chess to distinguish it from related games, such as xiangqi (Chinese chess) and shogi (Japanese chess). The current form of the game emerged in Spain and the rest of Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin. Today, chess is one of the world's most popular games, played by millions of people worldwide.
Chess is an abstract strategy game and involves no hidden information. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. The player controlling the white pieces moves first, followed by the player controlling the black pieces. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in "check") and there is no way for it to escape. There are also several ways a game can end in a draw.
Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (the International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion. A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition, and chess in its turn influenced Western culture and art, and has connections with other fields such as mathematics, computer science, and psychology.
One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players and have deeply influenced the development of chess theory.
Chess pieces are divided into two different colored sets. While the sets might not be literally white and black (e.g. the light set may be a yellowish or off-white color, the dark set may be brown or red), they are always referred to as "white" and "black". The players of the sets are referred to as White and Black, respectively. Each set consists of sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. Chess sets come in a wide variety of styles; for competition, the Staunton pattern is preferred.
The game is played on a square board of eight rows (called ranks) and eight columns (called files). By convention, the 64 squares alternate in color and are referred to as light and dark squares; common colors for chessboards are white and brown, or white and dark green.
In competitive games, the piece colors are allocated to players by the organizers; in informal games, the colors are usually decided randomly, for example by a coin toss, or by one player concealing a white pawn in one hand and a black pawn in the other, and having the opponent choose. 
White moves first, after which players alternate turns, moving one piece per turn, except for castling, when two pieces are moved. A piece is moved to either an unoccupied square or one occupied by an opponent's piece, which is captured and removed from play. With the sole exception of en passant, all pieces capture by moving to the square that the opponent's piece occupies. Moving is compulsory; a player may not skip a turn, even when having to move is detrimental.
Each piece has its own way of moving. In the diagrams, the dots mark the squares to which the piece can move if there are no intervening piece(s) of either color (except the knight, which leaps over any intervening pieces). All pieces except the pawn can capture an enemy piece if it is located on a square to which they would be able to move if the square was unoccupied. The squares on which pawns can capture enemy pieces are marked in the diagram with black crosses.
When a king is under immediate attack, it is said to be in check. A move in response to a check is legal only if it results in a position where the king is no longer in check. This can involve capturing the checking piece; interposing a piece between the checking piece and the king (which is possible only if the attacking piece is a queen, rook, or bishop and there is a square between it and the king); or moving the king to a square where it is not under attack. Castling is not a permissible response to a check.[1]
The object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to get it out of check. It is never legal for a player to make a move that puts or leaves the player's own king in check. In casual games, it is common to announce "check" when putting the opponent's king in check, but this is not required by the rules of chess and is not usually done in tournaments.[2]
Once per game, each king can make a move known as castling. Castling consists of moving the king two squares toward a rook of the same color on the same rank, and then placing the rook on the square that the king crossed.
Castling is still permitted if the rook is under attack, or if the rook crosses an attacked square. 
When a pawn makes a two-step advance from its starting position and there is an opponent's pawn on a square next to the destination square on an adjacent file, then the opponent's pawn can capture it en passant ("in passing"), moving to the square the pawn passed over. This can be done only on the turn immediately following the enemy pawn's two-square advance; otherwise, the right to do so is forfeited. For example, in the animated diagram, the black pawn advances two squares from g7 to g5, and the white pawn on f5 can take it en passant on g6 (but only immediately after the black pawn's advance).
When a pawn advances to its eighth rank, as part of the move, it is promoted and must be exchanged for the player's choice of queen, rook, bishop, or knight of the same color. Usually, the pawn is chosen to be promoted to a queen, but in some cases, another piece is chosen; this is called underpromotion. In the animated diagram, the pawn on c7 can be advanced to the eighth rank and be promoted. There is no restriction on the piece promoted to, so it is possible to have more pieces of the same type than at the start of the game (e.g., two or more queens). If the required piece is not available (e.g. a second queen) an inverted rook is sometimes used as a substitute, but this is not recognized in FIDE-sanctioned games.
There are several ways a game can end in a draw:
Time is controlled using a chess clock that has two displays, one for each player's remaining time. Analog chess clocks have been largely replaced by digital clocks, which allow for time controls with increments.
Time controls are also enforced in correspondence chess competitions. A typical time control is 50 days for every 10 moves.
The pieces are identified by their initials. In English, these are K (king), Q (queen), R (rook), B (bishop), and N (knight; N is used to avoid confusion with king). For example, Qg5 means "queen moves to the g-file, 5th rank" (that is, to the square g5). Different initials may be used for other languages. In chess literature, figurine algebraic notation (FAN) is frequently used to aid understanding independent of language.
To resolve ambiguities, an additional letter or number is added to indicate the file or rank from which the piece moved (e.g. Ngf3 means "knight from the g-file moves to the square f3"; R1e2 means "rook on the first rank moves to e2"). For pawns, no letter initial is used; so e4 means "pawn moves to the square e4".
If the piece makes a capture, "x" is usually inserted before the destination square. Thus Bxf3 means "bishop captures on f3". When a pawn makes a capture, the file from which the pawn departed is used to identify the pawn making the capture, for example, exd5 (pawn on the e-file captures the piece on d5). Ranks may be omitted if unambiguous, for example, exd (pawn on the e-file captures a piece somewhere on the d-file). A minority of publications use ":" to indicate a capture, and some omit the capture symbol altogether. In its most abbreviated form, exd5 may be rendered simply as ed. An en passant capture may optionally be marked with the notation "e.p."
For example, one variation of a simple trap known as the Scholar's mate (see animated diagram) can be recorded:
Variants of algebraic notation include long form algebraic, in which both the departure and destination square are indicated; abbreviated algebraic, in which capture signs, check signs, and ranks of pawn captures may be omitted; and Figurine Algebraic Notation, used in chess publications for universal readability regardless of language.
Portable Game Notation (PGN) is a text-based file format for recording chess games, based on short form English algebraic notation with a small amount of markup. PGN files (suffix .pgn) can be processed by most chess software, as well as being easily readable by humans.
Until about 1980, the majority of English language chess publications used descriptive notation, in which files are identified by the initial letter of the piece that occupies the first rank at the beginning of the game. In descriptive notation, the common opening move 1.e4 is rendered as "1.P-K4" ("pawn to king four"). Another system is ICCF numeric notation, recognized by the International Correspondence Chess Federation though its use is in decline.
In competitive games, players are normally required to keep a score (record of the game). For this purpose, only algebraic notation is recognized in FIDE-sanctioned events; game scores recorded in a different notation system may not be used as evidence in the event of a dispute.
Contemporary chess is an organized sport with structured international and national leagues, tournaments, and congresses. Thousands of chess tournaments, matches, and festivals are held around the world every year catering to players of all levels.
The term "match" refers not to an individual game, but to either a series of games between two players, or a team competition in which each player of one team plays one game against a player of the other team.
FIDE's most visible activity is organizing the World Chess Championship, a role it assumed in 1948. The current World Champion is Magnus Carlsen of Norway.[11] The reigning Women's World Champion is Ju Wenjun from China.[12]
Other competitions for individuals include the World Junior Chess Championship, the European Individual Chess Championship, the tournaments for the World Championship qualification cycle, and the various national championships. Invitation-only tournaments regularly attract the world's strongest players. Examples include Spain's Linares event, Monte Carlo's Melody Amber tournament, the Dortmund Sparkassen meeting, Sofia's M-tel Masters, and Wijk aan Zee's Tata Steel tournament.
Regular team chess events include the Chess Olympiad and the European Team Chess Championship.
The World Chess Solving Championship and World Correspondence Chess Championships include both team and individual events; these are held independently of FIDE.
In order to rank players, FIDE, ICCF, and most national chess organizations use the Elo rating system developed by Arpad Elo. An average club player has a rating of about 1500; the highest FIDE rating of all time, 2882, was achieved by Magnus Carlsen on the March 2014 FIDE rating list.[13]
The above titles are open to both men and women. There are also separate women-only titles; Woman Grandmaster (WGM), Woman International Master (WIM), Woman FIDE Master (WFM) and Woman Candidate Master (WCM). These require a performance level approximately 200 Elo rating points below the similarly named open titles, and their continued existence has sometimes been controversial. Beginning with Nona Gaprindashvili in 1978, a number of women have earned the open GM title.[note 2]
FIDE also awards titles for arbiters and trainers.[16][17] International titles are also awarded to composers and solvers of chess problems and to correspondence chess players (by the International Correspondence Chess Federation). National chess organizations may also award titles.
Chess has an extensive literature. In 1913, the chess historian H.J.R. Murray estimated the total number of books, magazines, and chess columns in newspapers to be about 5,000.[18] B.H. Wood estimated the number, as of 1949, to be about 20,000.[19] David Hooper and Kenneth Whyld write that, "Since then there has been a steady increase year by year of the number of new chess publications. No one knows how many have been printed."[19] Significant public chess libraries include the John G. White Chess and Checkers Collection at Cleveland Public Library, with over 32,000 chess books and over 6,000 bound volumes of chess periodicals;[20] and the Chess & Draughts collection at the National Library of the Netherlands, with about 30,000 books.[21]
Chess theory usually divides the game of chess into three phases with different sets of strategies: the opening, typically the first 10 to 20 moves, when players move their pieces to useful positions for the coming battle; the middlegame; and last the endgame, when most of the pieces are gone, kings typically take a more active part in the struggle, and pawn promotion is often decisive.
Opening theory is concerned with finding the best moves in the initial phase of the game. There are dozens of different openings, and hundreds of variants. The Oxford Companion to Chess lists 1,327 named openings and variants.[22]
Endgame theory is concerned with positions where there are only a few pieces left. Theoretics categorize these positions according to the pieces, for example "King and pawn endings" or "Rook versus a minor piece".
Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. This initially gives White the initiative.[25] Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.
Specific plans or strategic themes will often arise from particular groups of openings which result in a specific type of pawn structure. An example is the minority attack, which is the attack of queenside pawns against an opponent who has more pawns on the queenside. The study of openings is therefore connected to the preparation of plans that are typical of the resulting middlegames.[28]
Another important strategic question in the middlegame is whether and how to reduce material and transition into an endgame (i.e. simplify). Minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of a pawn, or sometimes even with a two-pawn advantage.[29]
Chess strategy is concerned with the evaluation of chess positions and with setting up goals and long-term plans for future play. During the evaluation, players must take into account numerous factors such as the value of the pieces on the board, control of the center and centralization, the pawn structure, king safety, and the control of key squares or groups of squares (for example, diagonals, open files, and dark or light squares).
The most basic step in evaluating a position is to count the total value of pieces of both sides.[34] The point values used for this purpose are based on experience; usually, pawns are considered worth one point, knights and bishops about three points each, rooks about five points (the value difference between a rook and a bishop or knight being known as the exchange), and queens about nine points. The king is more valuable than all of the other pieces combined, since its checkmate loses the game. But in practical terms, in the endgame, the king as a fighting piece is generally more powerful than a bishop or knight but less powerful than a rook.[35] These basic values are then modified by other factors like position of the piece (e.g. advanced pawns are usually more valuable than those on their initial squares), coordination between pieces (e.g. a pair of bishops usually coordinate better than a bishop and a knight), or the type of position (e.g. knights are generally better in closed positions with many pawns while bishops are more powerful in open positions).[36]
Another important factor in the evaluation of chess positions is pawn structure (sometimes known as the pawn skeleton): the configuration of pawns on the chessboard.[37] Since pawns are the least mobile of the pieces, pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in pawn structure include isolated, doubled, or backward pawns and holes; once created, they are often permanent. Care must therefore be taken to avoid these weaknesses unless they are compensated by another valuable asset (for example, by the possibility of developing an attack).[38]
The endgame (also end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:[39]
Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions in which one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides, and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to pieces on the board other than kings, such as "rook and pawn versus rook" endgames.
The earliest texts referring to the origins of chess date from the beginning of the 7th century. Three are written in Pahlavi (Middle Persian)[42] and one, the Harshacharita, is in Sanskrit.[43] One of these texts, the Chatrang-namak, represents one of the earliest written accounts of chess. The narrator Bozorgmehr explains that Chatrang, the Pahlavi word for chess, was introduced to Persia by 'Dewasarm, a great ruler of India' during the reign of Khosrow I.[44]
The game of chess was then played and known in all European countries. A famous 13th-century Spanish manuscript covering chess, backgammon, and dice is known as the Libro de los juegos, which is the earliest European treatise on chess as well as being the oldest document on European tables games. The rules were fundamentally similar to those of the Arabic shatranj. The differences were mostly in the use of a checkered board instead of a plain monochrome board used by Arabs and the habit of allowing some or all pawns to make an initial double step. In some regions, the Queen, which had replaced the Wazir, and/or the King could also make an initial two-square leap under some conditions.[63]
At the same time, the intellectual movement of romanticism had had a far-reaching impact on chess, with aesthetics and tactical beauty being held in higher regard than objective soundness and strategic planning. As a result, virtually all games began with the Open Game, and it was considered unsportsmanlike to decline gambits that invited tactical play such as the King's Gambit and the Evans Gambit.[75] This chess philosophy is known as Romantic chess, and a sharp, tactical style consistent with the principles of chess romanticism was predominant until the late 19th century.[76]
As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books, and chess journals appeared. There were correspondence matches between cities; for example, the London Chess Club played against the Edinburgh Chess Club in 1824.[79] Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling, and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.
The first modern chess tournament was organized by Howard Staunton, a leading English chess player, and was held in London in 1851. It was won by the German Adolf Anderssen, who was hailed as the leading chess master. His brilliant, energetic attacking style was typical for the time.[80][81] Sparkling games like Anderssen's Immortal Game and Evergreen Game or Morphy's "Opera Game" were regarded as the highest possible summit of the art of chess.[82]
Deeper insight into the nature of chess came with the American Paul Morphy, an extraordinary chess prodigy. Morphy won against all important competitors (except Staunton, who refused to play), including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks.[83]
Prague-born Wilhelm Steinitz laid the foundations for a scientific approach to the game, the art of breaking a position down into components[84] and preparing correct plans.[85] In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. This win marked a stylistic transition at the highest levels of chess from an attacking, tactical style predominant in the Romantic era to a more positional, strategic style introduced to the chess world by Steinitz. Steinitz lost his crown in 1894 to a much younger player, the German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of any world champion.[86]
After the end of the 19th century, the number of master tournaments and matches held annually quickly grew. The first Olympiad was held in Paris in 1924, and FIDE was founded initially for the purpose of organizing that event. In 1927, the Women's World Chess Championship was established; the first to hold the title was Czech-English master Vera Menchik.[87]
After the death of Alekhine, a new World Champion was sought. FIDE, which has controlled the title since then, ran a tournament of elite players. The winner of the 1948 tournament was Russian Mikhail Botvinnik.
In 1950 FIDE established a system of titles, conferring the titles of Grandmaster and International Master on 27 players. (Some sources state that in 1914 the title of chess Grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch, and Marshall, but this is a disputed claim.[note 5])
Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes.[99] In the 1984 World Chess Championship, Karpov faced his toughest challenge to date, the young Garry Kasparov from Baku, Soviet Azerbaijan. The match was aborted in controversial circumstances after 5 months and 48 games with Karpov leading by 5 wins to 3, but evidently exhausted; many commentators believed Kasparov, who had won the last two games, would have won the match had it continued. Kasparov won the 1985 rematch. Kasparov and Karpov contested three further closely fought matches in 1986, 1987 and 1990, Kasparov winning them all.[100] Kasparov became the dominant figure of world chess from the mid 1980s until his retirement from competition in 2005.
Chess-playing computer programs (later known as chess engines) began to appear in the 1960s. In 1970, the first major computer chess tournament, the North American Computer Chess Championship, was held, followed in 1974 by the first World Computer Chess Championship. In the late 1970s, dedicated home chess computers such as Fidelity Electronics' Chess Challenger became commercially available, as well as software to run on home computers. However, the overall standard of computer chess was low until the 1990s.
The first endgame tablebases, which provided perfect play for relatively simple endgames such as king and rook versus king and bishop, appeared in the late 1970s. This set a precedent to the complete six- and seven-piece tablebases that became available in the 2000s and 2010s respectively.[101]
The first commercial chess database, a collection of chess games searchable by move and position, was introduced by the German company ChessBase in 1987. Databases containing millions of chess games have since had a profound effect on opening theory and other areas of chess research.
Digital chess clocks were invented in 1973, though they did not become commonplace until the 1990s. Digital clocks allow for time controls involving increments and delays.
The Internet enabled online chess as a new medium of playing, with chess servers allowing users to play other people from different parts of the world in real time. The first such server, known as Internet Chess Server or ICS, was developed at the University of Utah in 1992. ICS formed the basis for the first commercial chess server, the Internet Chess Club, which was launched in 1995, and for other early chess servers such as FICS (Free Internet Chess Server). Since then, many other platforms have appeared, and online chess began to rival over-the-board chess in popularity.[102][103] During the 2020 COVID-19 pandemic, the isolation ensuing from quarantines imposed in many places around the world, combined with the success of the popular Netflix show The Queen's Gambit and other factors such as the popularity of online tournaments (notably PogChamps) and chess Twitch streamers, resulted in a surge of popularity not only for online chess, but for the game of chess in general; this phenomenon has been referred to in the media as the 2020 online chess boom.[104][105]
As endgame tablebases developed, they began to provide perfect play in endgame positions in which the game-theoretical outcome was previously unknown, such as positions with king, queen and pawn against king and queen. In 1991, Lewis Stiller published a tablebase for select six-piece endgames,[108][109] and by 2005, following the publication of Nalimov tablebases, all six-piece endgame positions were solved. In 2012, Lomonosov tablebases were published which solved all seven-piece endgame positions.[110] Use of tablebases enhances the performance of chess engines by providing definitive results in some branches of analysis.
Technological progress made in the 1990s and the 21st century has influenced the way that chess is studied at all levels, as well as the state of chess as a spectator sport.
Previously, preparation at the professional level required an extensive chess library and several subscriptions to publications such as Chess Informant to keep up with opening developments and study opponents' games. Today, preparation at the professional level involves the use of databases containing millions of games, and engines to analyze different opening variations and prepare novelties.[111] A number of online learning resources are also available for players of all levels, such as online courses, tactics trainers, and video lessons.[112]
Organized chess even for young children has become common. FIDE holds world championships for age levels down to 8 years old. The largest tournaments, in number of players, are those held for children.[116]
The number of grandmasters and other chess professionals has also grown in the modern era. Kenneth Regan and Guy Haworth conducted research involving comparison of move choices by players of different levels and from different periods with the analysis of strong chess engines; they concluded that the increase in the number of grandmasters and higher Elo ratings of the top players reflect an actual increase in the average standard of play, rather than "rating inflation" or "title inflation".[117]
In 1993, Garry Kasparov and Nigel Short broke ties with FIDE to organize their own match for the title and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Championships and respective World Champions: the PCA or "classical" champions extending the Steinitzian tradition in which the current champion plays a challenger in a series of games, and the other following FIDE's new format of many players competing in a large knockout tournament to determine the champion. Kasparov lost his PCA title in 2000 to Vladimir Kramnik of Russia.[118] Due to the complicated state of world chess politics and difficulties obtaining commercial sponsorships, Kasparov was never able to challenge for the title again. Despite this, he continued to dominate in top level tournaments and remained the world's highest rated player until his retirement from competitive chess in 2005.
The World Chess Championship 2006, in which Kramnik beat the FIDE World Champion Veselin Topalov, reunified the titles and made Kramnik the undisputed World Chess Champion.[119] In September 2007, he lost the title to Viswanathan Anand of India, who won the championship tournament in Mexico City. Anand defended his title in the revenge match of 2008,[120] 2010 and 2012. In 2013, Magnus Carlsen of Norway beat Anand in the 2013 World Chess Championship.[121] He defended his title 4 times since then and is the reigning world champion.
In the Middle Ages and during the Renaissance, chess was a part of noble culture; it was used to teach war strategy and was dubbed the "King's Game".[122] Gentlemen are "to be meanly seene in the play at Chestes", says the overview at the beginning of Baldassare Castiglione's The Book of the Courtier (1528, English 1561 by Sir Thomas Hoby), but chess should not be a gentleman's main passion. Castiglione explains it further:
And what say you to the game at chestes? It is truely an honest kynde of enterteynmente and wittie, quoth Syr Friderick. But me think it hath a fault, whiche is, that a man may be to couning at it, for who ever will be excellent in the playe of chestes, I beleave he must beestowe much tyme about it, and applie it with so much study, that a man may assoone learne some noble scyence, or compase any other matter of importaunce, and yet in the ende in beestowing all that laboure, he knoweth no more but a game. Therfore in this I beleave there happeneth a very rare thing, namely, that the meane is more commendable, then the excellency.[123]
Some of the elaborate chess sets used by the aristocracy at least partially survive, such as the Lewis chessmen.
The knyght ought to be made alle armed upon an hors in suche wyse that he haue an helme on his heed and a spere in his ryght hande/ and coueryd wyth his sheld/ a swerde and a mace on his lyft syde/ Cladd wyth an hawberk and plates to fore his breste/ legge harnoys on his legges/ Spores on his heelis on his handes his gauntelettes/ his hors well broken and taught and apte to bataylle and couerid with his armes/ whan the knyghtes ben maad they ben bayned or bathed/ that is the signe that they shold lede a newe lyf and newe maners/ also they wake alle the nyght in prayers and orysons vnto god that he wylle gyue hem grace that they may gete that thynge that they may not gete by nature/ The kynge or prynce gyrdeth a boute them a swerde in signe/ that they shold abyde and kepe hym of whom they take theyr dispenses and dignyte.[127]
During the Age of Enlightenment, chess was viewed as a means of self-improvement. Benjamin Franklin, in his article "The Morals of Chess" (1750), wrote:
The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:
I. Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...
III. Caution, not to make our moves too hastily ...[132]
Chess was occasionally criticized in the 19th century as a waste of time.[133][134]
Chess is taught to children in schools around the world today. Many schools host chess clubs, and there are many scholastic tournaments specifically for children. Tournaments are held regularly in many countries, hosted by organizations such as the United States Chess Federation and the National Scholastic Chess Foundation.[135]
Chess is many times depicted in the arts; significant works where chess plays a key role range from Thomas Middleton's A Game at Chess to Through the Looking-Glass by Lewis Carroll, to Vladimir Nabokov's The Defense, to The Royal Game by Stefan Zweig. Chess has also featured in film classics such as Ingmar Bergman's The Seventh Seal, Satyajit Ray's The Chess Players, and Powell and Pressburger's A Matter of Life and Death.
Chess is also present in contemporary popular culture. For example, the characters in Star Trek play a futuristic version of the game called "Federation Tri-Dimensional Chess"[136] and "Wizard's Chess" is played in J.K. Rowling's Harry Potter.[137]
The game structure and nature of chess are related to several branches of mathematics. Many combinatorical and topological problems connected to chess, such as the knight's tour and the eight queens puzzle, have been known for hundreds of years.
The number of legal positions in chess is estimated to be 4.59 (+/- 0.38) x1044 with a 95% confidence level,[138] with a game-tree complexity of approximately 10123. The game-tree complexity of chess was first calculated by Claude Shannon as 10120, a number known as the Shannon number.[139] An average position typically has thirty to forty possible moves, but there may be as few as zero (in the case of checkmate or stalemate) or (in a constructed position) as many as 218.[140]
In 1913, Ernst Zermelo used chess as a basis for his theory of game strategies, which is considered one of the predecessors of game theory.[141] Zermelo's theorem states that it is possible to solve chess, i.e. to determine with certainty the outcome of a perfectly played game (either White can force a win, or Black can force a win, or both sides can force at least a draw).[142] However, with 1043 legal positions in chess, it will take an impossibly long time to compute a perfect strategy with any feasible technology.[143]
There is an extensive scientific literature on chess psychology.[note 6][145][146][147][148] Alfred Binet and others showed that knowledge and verbal, rather than visuospatial, ability lies at the core of expertise.[149][150] In his doctoral thesis, Adriaan de Groot showed that chess masters can rapidly perceive the key features of a position.[151] According to de Groot, this perception, made possible by years of practice and study, is more important than the sheer ability to anticipate moves. De Groot showed that chess masters can memorize positions shown for a few seconds almost perfectly. The ability to memorize does not alone account for chess-playing skill, since masters and novices, when faced with random arrangements of chess pieces, had equivalent recall (about six positions in each case). Rather, it is the ability to recognize patterns, which are then memorized, which distinguished the skilled players from the novices. When the positions of the pieces were taken from an actual game, the masters had almost total positional recall.[152]
More recent research has focused on chess as mental training; the respective roles of knowledge and look-ahead search; brain imaging studies of chess masters and novices; blindfold chess; the role of personality and intelligence in chess skill; gender differences; and computational models of chess expertise. The role of practice and talent in the development of chess and other domains of expertise has led to much empirical investigation. Ericsson and colleagues have argued that deliberate practice is sufficient for reaching high levels of expertise in chess.[153] Recent research, however, fails to replicate their results and indicates that factors other than practice are also important.[154][155]
For example, Fernand Gobet and colleagues have shown that stronger players started playing chess at a young age and that experts born in the Northern Hemisphere are more likely to have been born in late winter and early spring. Compared to the general population, chess players are more likely to be non-right-handed, though they found no correlation between handedness and skill.[155]
A relationship between chess skill and intelligence has long been discussed in scientific literature as well as in popular culture. Academic studies that investigate the relationship date back at least to 1927.[156] Although one meta-analysis and most children studies find a positive correlation between general cognitive ability and chess skill, adult studies show mixed results.[157][158]
Chess composition is the art of creating chess problems (also called chess compositions). The creator is known as a chess composer.[160] There are many types of chess problems; the two most important are:
Fairy chess is a branch of chess problem composition involving altered rules, such as the use of unconventional pieces or boards, or unusual stipulations such as reflexmates.
Tournaments for composition and solving of chess problems are organized by the World Federation for Chess Composition, which works cooperatively with but independent of FIDE. The WFCC awards titles for composing and solving chess problems.[163]
Online chess is chess that is played over the internet, allowing players to play against each other in real time. This is done through the use of Internet chess servers, which pair up individual players based on their rating using an Elo or similar rating system. Online chess saw a spike in growth during the quarantines of the COVID-19 pandemic.[164][165] This can be attributed to both isolation and the popularity of Netflix miniseries The Queen's Gambit, which was released in October 2020.[164][165] Chess app downloads on the App Store and Google Play Store rose by 63% after the show debuted.[166] Chess.com saw more than twice as many account registrations in November as it had in previous months, and the number of games played monthly on Lichess doubled as well. There was also a demographic shift in players, with female registration on Chess.com shifting from 22% to 27% of new players.[167] Grandmaster Maurice Ashley said "A boom is taking place in chess like we have never seen maybe since the Bobby Fischer days," attributing the growth to an increased desire to do something constructive during the pandemic.[168] USCF Women's Program Director Jennifer Shahade stated that chess works well on the internet, since pieces do not need to be reset and matchmaking is virtually instant.[169]
The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require "thinking" for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of "thinking"; (4) the discrete structure of chess fits well into the digital nature of
modern computers.[173]
With huge databases of past games and high analytical ability, computers can help players to learn chess and prepare for matches. Internet Chess Servers allow people to find and play opponents worldwide. The presence of computers and modern communication tools have raised concerns regarding cheating during games.[182]
There are more than two thousand published chess variants, games with similar but different rules.[183] Most of them are of relatively recent origin.[184] They include:
In the context of chess variants, regular (i.e. FIDE) chess is commonly referred to as Western chess, international chess, orthodox chess, orthochess, and classic chess.[186][187]
This page is a member of 25 hidden categories (help):
Pages transcluded onto the current version of this page (help):
Please remember to check your manual of style, standards guide or instructor's guidelines for the exact syntax to suit your needs.  For more detailed advice, see Citing Wikipedia.
Wikipedia contributors. (2023, February 21). Chess. In Wikipedia, The Free Encyclopedia. Retrieved 16:43, February 26, 2023, from https://en.wikipedia.org/w/index.php?title=Chess&oldid=1140681221
Wikipedia contributors. "Chess." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 21 Feb. 2023. Web. 26 Feb. 2023.
Wikipedia contributors, 'Chess',  Wikipedia, The Free Encyclopedia, 21 February 2023, 06:47 UTC, <https://en.wikipedia.org/w/index.php?title=Chess&oldid=1140681221> [accessed 26 February 2023]
Wikipedia contributors, "Chess,"  Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Chess&oldid=1140681221 (accessed February 26, 2023).
Wikipedia contributors. Chess [Internet].  Wikipedia, The Free Encyclopedia;  2023 Feb 21, 06:47 UTC [cited 2023 Feb 26].  Available from: 
https://en.wikipedia.org/w/index.php?title=Chess&oldid=1140681221.
Wikipedia contributors. Chess. Wikipedia, The Free Encyclopedia. February 21, 2023, 06:47 UTC. Available at: https://en.wikipedia.org/w/index.php?title=Chess&oldid=1140681221. Accessed February 26, 2023.
When using the LaTeX package url (\usepackage{url} somewhere in the preamble), which tends to give much more nicely formatted web addresses, the following may be preferred:
Chess is a board game between two players. It is sometimes called international chess or Western chess to distinguish it from related games, such as xiangqi (Chinese chess) and shogi (Japanese chess). The current form of the game emerged in Spain and the rest of Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin. Today, chess is one of the world's most popular games, played by millions of people worldwide.
Chess is an abstract strategy game and involves no hidden information. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. The player controlling the white pieces moves first, followed by the player controlling the black pieces. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in "check") and there is no way for it to escape. There are also several ways a game can end in a draw.
Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (the International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion. A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition, and chess in its turn influenced Western culture and art, and has connections with other fields such as mathematics, computer science, and psychology.
One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players and have deeply influenced the development of chess theory.
Chess pieces are divided into two different colored sets. While the sets might not be literally white and black (e.g. the light set may be a yellowish or off-white color, the dark set may be brown or red), they are always referred to as "white" and "black". The players of the sets are referred to as White and Black, respectively. Each set consists of sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. Chess sets come in a wide variety of styles; for competition, the Staunton pattern is preferred.
The game is played on a square board of eight rows (called ranks) and eight columns (called files). By convention, the 64 squares alternate in color and are referred to as light and dark squares; common colors for chessboards are white and brown, or white and dark green.
In competitive games, the piece colors are allocated to players by the organizers; in informal games, the colors are usually decided randomly, for example by a coin toss, or by one player concealing a white pawn in one hand and a black pawn in the other, and having the opponent choose. 
White moves first, after which players alternate turns, moving one piece per turn, except for castling, when two pieces are moved. A piece is moved to either an unoccupied square or one occupied by an opponent's piece, which is captured and removed from play. With the sole exception of en passant, all pieces capture by moving to the square that the opponent's piece occupies. Moving is compulsory; a player may not skip a turn, even when having to move is detrimental.
Each piece has its own way of moving. In the diagrams, the dots mark the squares to which the piece can move if there are no intervening piece(s) of either color (except the knight, which leaps over any intervening pieces). All pieces except the pawn can capture an enemy piece if it is located on a square to which they would be able to move if the square was unoccupied. The squares on which pawns can capture enemy pieces are marked in the diagram with black crosses.
When a king is under immediate attack, it is said to be in check. A move in response to a check is legal only if it results in a position where the king is no longer in check. This can involve capturing the checking piece; interposing a piece between the checking piece and the king (which is possible only if the attacking piece is a queen, rook, or bishop and there is a square between it and the king); or moving the king to a square where it is not under attack. Castling is not a permissible response to a check.[1]
The object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to get it out of check. It is never legal for a player to make a move that puts or leaves the player's own king in check. In casual games, it is common to announce "check" when putting the opponent's king in check, but this is not required by the rules of chess and is not usually done in tournaments.[2]
Once per game, each king can make a move known as castling. Castling consists of moving the king two squares toward a rook of the same color on the same rank, and then placing the rook on the square that the king crossed.
Castling is still permitted if the rook is under attack, or if the rook crosses an attacked square. 
When a pawn makes a two-step advance from its starting position and there is an opponent's pawn on a square next to the destination square on an adjacent file, then the opponent's pawn can capture it en passant ("in passing"), moving to the square the pawn passed over. This can be done only on the turn immediately following the enemy pawn's two-square advance; otherwise, the right to do so is forfeited. For example, in the animated diagram, the black pawn advances two squares from g7 to g5, and the white pawn on f5 can take it en passant on g6 (but only immediately after the black pawn's advance).
When a pawn advances to its eighth rank, as part of the move, it is promoted and must be exchanged for the player's choice of queen, rook, bishop, or knight of the same color. Usually, the pawn is chosen to be promoted to a queen, but in some cases, another piece is chosen; this is called underpromotion. In the animated diagram, the pawn on c7 can be advanced to the eighth rank and be promoted. There is no restriction on the piece promoted to, so it is possible to have more pieces of the same type than at the start of the game (e.g., two or more queens). If the required piece is not available (e.g. a second queen) an inverted rook is sometimes used as a substitute, but this is not recognized in FIDE-sanctioned games.
There are several ways a game can end in a draw:
Time is controlled using a chess clock that has two displays, one for each player's remaining time. Analog chess clocks have been largely replaced by digital clocks, which allow for time controls with increments.
Time controls are also enforced in correspondence chess competitions. A typical time control is 50 days for every 10 moves.
The pieces are identified by their initials. In English, these are K (king), Q (queen), R (rook), B (bishop), and N (knight; N is used to avoid confusion with king). For example, Qg5 means "queen moves to the g-file, 5th rank" (that is, to the square g5). Different initials may be used for other languages. In chess literature, figurine algebraic notation (FAN) is frequently used to aid understanding independent of language.
To resolve ambiguities, an additional letter or number is added to indicate the file or rank from which the piece moved (e.g. Ngf3 means "knight from the g-file moves to the square f3"; R1e2 means "rook on the first rank moves to e2"). For pawns, no letter initial is used; so e4 means "pawn moves to the square e4".
If the piece makes a capture, "x" is usually inserted before the destination square. Thus Bxf3 means "bishop captures on f3". When a pawn makes a capture, the file from which the pawn departed is used to identify the pawn making the capture, for example, exd5 (pawn on the e-file captures the piece on d5). Ranks may be omitted if unambiguous, for example, exd (pawn on the e-file captures a piece somewhere on the d-file). A minority of publications use ":" to indicate a capture, and some omit the capture symbol altogether. In its most abbreviated form, exd5 may be rendered simply as ed. An en passant capture may optionally be marked with the notation "e.p."
For example, one variation of a simple trap known as the Scholar's mate (see animated diagram) can be recorded:
Variants of algebraic notation include long form algebraic, in which both the departure and destination square are indicated; abbreviated algebraic, in which capture signs, check signs, and ranks of pawn captures may be omitted; and Figurine Algebraic Notation, used in chess publications for universal readability regardless of language.
Portable Game Notation (PGN) is a text-based file format for recording chess games, based on short form English algebraic notation with a small amount of markup. PGN files (suffix .pgn) can be processed by most chess software, as well as being easily readable by humans.
Until about 1980, the majority of English language chess publications used descriptive notation, in which files are identified by the initial letter of the piece that occupies the first rank at the beginning of the game. In descriptive notation, the common opening move 1.e4 is rendered as "1.P-K4" ("pawn to king four"). Another system is ICCF numeric notation, recognized by the International Correspondence Chess Federation though its use is in decline.
In competitive games, players are normally required to keep a score (record of the game). For this purpose, only algebraic notation is recognized in FIDE-sanctioned events; game scores recorded in a different notation system may not be used as evidence in the event of a dispute.
Contemporary chess is an organized sport with structured international and national leagues, tournaments, and congresses. Thousands of chess tournaments, matches, and festivals are held around the world every year catering to players of all levels.
The term "match" refers not to an individual game, but to either a series of games between two players, or a team competition in which each player of one team plays one game against a player of the other team.
FIDE's most visible activity is organizing the World Chess Championship, a role it assumed in 1948. The current World Champion is Magnus Carlsen of Norway.[11] The reigning Women's World Champion is Ju Wenjun from China.[12]
Other competitions for individuals include the World Junior Chess Championship, the European Individual Chess Championship, the tournaments for the World Championship qualification cycle, and the various national championships. Invitation-only tournaments regularly attract the world's strongest players. Examples include Spain's Linares event, Monte Carlo's Melody Amber tournament, the Dortmund Sparkassen meeting, Sofia's M-tel Masters, and Wijk aan Zee's Tata Steel tournament.
Regular team chess events include the Chess Olympiad and the European Team Chess Championship.
The World Chess Solving Championship and World Correspondence Chess Championships include both team and individual events; these are held independently of FIDE.
In order to rank players, FIDE, ICCF, and most national chess organizations use the Elo rating system developed by Arpad Elo. An average club player has a rating of about 1500; the highest FIDE rating of all time, 2882, was achieved by Magnus Carlsen on the March 2014 FIDE rating list.[13]
The above titles are open to both men and women. There are also separate women-only titles; Woman Grandmaster (WGM), Woman International Master (WIM), Woman FIDE Master (WFM) and Woman Candidate Master (WCM). These require a performance level approximately 200 Elo rating points below the similarly named open titles, and their continued existence has sometimes been controversial. Beginning with Nona Gaprindashvili in 1978, a number of women have earned the open GM title.[note 2]
FIDE also awards titles for arbiters and trainers.[16][17] International titles are also awarded to composers and solvers of chess problems and to correspondence chess players (by the International Correspondence Chess Federation). National chess organizations may also award titles.
Chess has an extensive literature. In 1913, the chess historian H.J.R. Murray estimated the total number of books, magazines, and chess columns in newspapers to be about 5,000.[18] B.H. Wood estimated the number, as of 1949, to be about 20,000.[19] David Hooper and Kenneth Whyld write that, "Since then there has been a steady increase year by year of the number of new chess publications. No one knows how many have been printed."[19] Significant public chess libraries include the John G. White Chess and Checkers Collection at Cleveland Public Library, with over 32,000 chess books and over 6,000 bound volumes of chess periodicals;[20] and the Chess & Draughts collection at the National Library of the Netherlands, with about 30,000 books.[21]
Chess theory usually divides the game of chess into three phases with different sets of strategies: the opening, typically the first 10 to 20 moves, when players move their pieces to useful positions for the coming battle; the middlegame; and last the endgame, when most of the pieces are gone, kings typically take a more active part in the struggle, and pawn promotion is often decisive.
Opening theory is concerned with finding the best moves in the initial phase of the game. There are dozens of different openings, and hundreds of variants. The Oxford Companion to Chess lists 1,327 named openings and variants.[22]
Endgame theory is concerned with positions where there are only a few pieces left. Theoretics categorize these positions according to the pieces, for example "King and pawn endings" or "Rook versus a minor piece".
Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. This initially gives White the initiative.[25] Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.
Specific plans or strategic themes will often arise from particular groups of openings which result in a specific type of pawn structure. An example is the minority attack, which is the attack of queenside pawns against an opponent who has more pawns on the queenside. The study of openings is therefore connected to the preparation of plans that are typical of the resulting middlegames.[28]
Another important strategic question in the middlegame is whether and how to reduce material and transition into an endgame (i.e. simplify). Minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of a pawn, or sometimes even with a two-pawn advantage.[29]
Chess strategy is concerned with the evaluation of chess positions and with setting up goals and long-term plans for future play. During the evaluation, players must take into account numerous factors such as the value of the pieces on the board, control of the center and centralization, the pawn structure, king safety, and the control of key squares or groups of squares (for example, diagonals, open files, and dark or light squares).
The most basic step in evaluating a position is to count the total value of pieces of both sides.[34] The point values used for this purpose are based on experience; usually, pawns are considered worth one point, knights and bishops about three points each, rooks about five points (the value difference between a rook and a bishop or knight being known as the exchange), and queens about nine points. The king is more valuable than all of the other pieces combined, since its checkmate loses the game. But in practical terms, in the endgame, the king as a fighting piece is generally more powerful than a bishop or knight but less powerful than a rook.[35] These basic values are then modified by other factors like position of the piece (e.g. advanced pawns are usually more valuable than those on their initial squares), coordination between pieces (e.g. a pair of bishops usually coordinate better than a bishop and a knight), or the type of position (e.g. knights are generally better in closed positions with many pawns while bishops are more powerful in open positions).[36]
Another important factor in the evaluation of chess positions is pawn structure (sometimes known as the pawn skeleton): the configuration of pawns on the chessboard.[37] Since pawns are the least mobile of the pieces, pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in pawn structure include isolated, doubled, or backward pawns and holes; once created, they are often permanent. Care must therefore be taken to avoid these weaknesses unless they are compensated by another valuable asset (for example, by the possibility of developing an attack).[38]
The endgame (also end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:[39]
Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions in which one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides, and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to pieces on the board other than kings, such as "rook and pawn versus rook" endgames.
The earliest texts referring to the origins of chess date from the beginning of the 7th century. Three are written in Pahlavi (Middle Persian)[42] and one, the Harshacharita, is in Sanskrit.[43] One of these texts, the Chatrang-namak, represents one of the earliest written accounts of chess. The narrator Bozorgmehr explains that Chatrang, the Pahlavi word for chess, was introduced to Persia by 'Dewasarm, a great ruler of India' during the reign of Khosrow I.[44]
The game of chess was then played and known in all European countries. A famous 13th-century Spanish manuscript covering chess, backgammon, and dice is known as the Libro de los juegos, which is the earliest European treatise on chess as well as being the oldest document on European tables games. The rules were fundamentally similar to those of the Arabic shatranj. The differences were mostly in the use of a checkered board instead of a plain monochrome board used by Arabs and the habit of allowing some or all pawns to make an initial double step. In some regions, the Queen, which had replaced the Wazir, and/or the King could also make an initial two-square leap under some conditions.[63]
At the same time, the intellectual movement of romanticism had had a far-reaching impact on chess, with aesthetics and tactical beauty being held in higher regard than objective soundness and strategic planning. As a result, virtually all games began with the Open Game, and it was considered unsportsmanlike to decline gambits that invited tactical play such as the King's Gambit and the Evans Gambit.[75] This chess philosophy is known as Romantic chess, and a sharp, tactical style consistent with the principles of chess romanticism was predominant until the late 19th century.[76]
As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books, and chess journals appeared. There were correspondence matches between cities; for example, the London Chess Club played against the Edinburgh Chess Club in 1824.[79] Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling, and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.
The first modern chess tournament was organized by Howard Staunton, a leading English chess player, and was held in London in 1851. It was won by the German Adolf Anderssen, who was hailed as the leading chess master. His brilliant, energetic attacking style was typical for the time.[80][81] Sparkling games like Anderssen's Immortal Game and Evergreen Game or Morphy's "Opera Game" were regarded as the highest possible summit of the art of chess.[82]
Deeper insight into the nature of chess came with the American Paul Morphy, an extraordinary chess prodigy. Morphy won against all important competitors (except Staunton, who refused to play), including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks.[83]
Prague-born Wilhelm Steinitz laid the foundations for a scientific approach to the game, the art of breaking a position down into components[84] and preparing correct plans.[85] In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. This win marked a stylistic transition at the highest levels of chess from an attacking, tactical style predominant in the Romantic era to a more positional, strategic style introduced to the chess world by Steinitz. Steinitz lost his crown in 1894 to a much younger player, the German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of any world champion.[86]
After the end of the 19th century, the number of master tournaments and matches held annually quickly grew. The first Olympiad was held in Paris in 1924, and FIDE was founded initially for the purpose of organizing that event. In 1927, the Women's World Chess Championship was established; the first to hold the title was Czech-English master Vera Menchik.[87]
After the death of Alekhine, a new World Champion was sought. FIDE, which has controlled the title since then, ran a tournament of elite players. The winner of the 1948 tournament was Russian Mikhail Botvinnik.
In 1950 FIDE established a system of titles, conferring the titles of Grandmaster and International Master on 27 players. (Some sources state that in 1914 the title of chess Grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch, and Marshall, but this is a disputed claim.[note 5])
Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes.[99] In the 1984 World Chess Championship, Karpov faced his toughest challenge to date, the young Garry Kasparov from Baku, Soviet Azerbaijan. The match was aborted in controversial circumstances after 5 months and 48 games with Karpov leading by 5 wins to 3, but evidently exhausted; many commentators believed Kasparov, who had won the last two games, would have won the match had it continued. Kasparov won the 1985 rematch. Kasparov and Karpov contested three further closely fought matches in 1986, 1987 and 1990, Kasparov winning them all.[100] Kasparov became the dominant figure of world chess from the mid 1980s until his retirement from competition in 2005.
Chess-playing computer programs (later known as chess engines) began to appear in the 1960s. In 1970, the first major computer chess tournament, the North American Computer Chess Championship, was held, followed in 1974 by the first World Computer Chess Championship. In the late 1970s, dedicated home chess computers such as Fidelity Electronics' Chess Challenger became commercially available, as well as software to run on home computers. However, the overall standard of computer chess was low until the 1990s.
The first endgame tablebases, which provided perfect play for relatively simple endgames such as king and rook versus king and bishop, appeared in the late 1970s. This set a precedent to the complete six- and seven-piece tablebases that became available in the 2000s and 2010s respectively.[101]
The first commercial chess database, a collection of chess games searchable by move and position, was introduced by the German company ChessBase in 1987. Databases containing millions of chess games have since had a profound effect on opening theory and other areas of chess research.
Digital chess clocks were invented in 1973, though they did not become commonplace until the 1990s. Digital clocks allow for time controls involving increments and delays.
The Internet enabled online chess as a new medium of playing, with chess servers allowing users to play other people from different parts of the world in real time. The first such server, known as Internet Chess Server or ICS, was developed at the University of Utah in 1992. ICS formed the basis for the first commercial chess server, the Internet Chess Club, which was launched in 1995, and for other early chess servers such as FICS (Free Internet Chess Server). Since then, many other platforms have appeared, and online chess began to rival over-the-board chess in popularity.[102][103] During the 2020 COVID-19 pandemic, the isolation ensuing from quarantines imposed in many places around the world, combined with the success of the popular Netflix show The Queen's Gambit and other factors such as the popularity of online tournaments (notably PogChamps) and chess Twitch streamers, resulted in a surge of popularity not only for online chess, but for the game of chess in general; this phenomenon has been referred to in the media as the 2020 online chess boom.[104][105]
As endgame tablebases developed, they began to provide perfect play in endgame positions in which the game-theoretical outcome was previously unknown, such as positions with king, queen and pawn against king and queen. In 1991, Lewis Stiller published a tablebase for select six-piece endgames,[108][109] and by 2005, following the publication of Nalimov tablebases, all six-piece endgame positions were solved. In 2012, Lomonosov tablebases were published which solved all seven-piece endgame positions.[110] Use of tablebases enhances the performance of chess engines by providing definitive results in some branches of analysis.
Technological progress made in the 1990s and the 21st century has influenced the way that chess is studied at all levels, as well as the state of chess as a spectator sport.
Previously, preparation at the professional level required an extensive chess library and several subscriptions to publications such as Chess Informant to keep up with opening developments and study opponents' games. Today, preparation at the professional level involves the use of databases containing millions of games, and engines to analyze different opening variations and prepare novelties.[111] A number of online learning resources are also available for players of all levels, such as online courses, tactics trainers, and video lessons.[112]
Organized chess even for young children has become common. FIDE holds world championships for age levels down to 8 years old. The largest tournaments, in number of players, are those held for children.[116]
The number of grandmasters and other chess professionals has also grown in the modern era. Kenneth Regan and Guy Haworth conducted research involving comparison of move choices by players of different levels and from different periods with the analysis of strong chess engines; they concluded that the increase in the number of grandmasters and higher Elo ratings of the top players reflect an actual increase in the average standard of play, rather than "rating inflation" or "title inflation".[117]
In 1993, Garry Kasparov and Nigel Short broke ties with FIDE to organize their own match for the title and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Championships and respective World Champions: the PCA or "classical" champions extending the Steinitzian tradition in which the current champion plays a challenger in a series of games, and the other following FIDE's new format of many players competing in a large knockout tournament to determine the champion. Kasparov lost his PCA title in 2000 to Vladimir Kramnik of Russia.[118] Due to the complicated state of world chess politics and difficulties obtaining commercial sponsorships, Kasparov was never able to challenge for the title again. Despite this, he continued to dominate in top level tournaments and remained the world's highest rated player until his retirement from competitive chess in 2005.
The World Chess Championship 2006, in which Kramnik beat the FIDE World Champion Veselin Topalov, reunified the titles and made Kramnik the undisputed World Chess Champion.[119] In September 2007, he lost the title to Viswanathan Anand of India, who won the championship tournament in Mexico City. Anand defended his title in the revenge match of 2008,[120] 2010 and 2012. In 2013, Magnus Carlsen of Norway beat Anand in the 2013 World Chess Championship.[121] He defended his title 4 times since then and is the reigning world champion.
In the Middle Ages and during the Renaissance, chess was a part of noble culture; it was used to teach war strategy and was dubbed the "King's Game".[122] Gentlemen are "to be meanly seene in the play at Chestes", says the overview at the beginning of Baldassare Castiglione's The Book of the Courtier (1528, English 1561 by Sir Thomas Hoby), but chess should not be a gentleman's main passion. Castiglione explains it further:
And what say you to the game at chestes? It is truely an honest kynde of enterteynmente and wittie, quoth Syr Friderick. But me think it hath a fault, whiche is, that a man may be to couning at it, for who ever will be excellent in the playe of chestes, I beleave he must beestowe much tyme about it, and applie it with so much study, that a man may assoone learne some noble scyence, or compase any other matter of importaunce, and yet in the ende in beestowing all that laboure, he knoweth no more but a game. Therfore in this I beleave there happeneth a very rare thing, namely, that the meane is more commendable, then the excellency.[123]
Some of the elaborate chess sets used by the aristocracy at least partially survive, such as the Lewis chessmen.
The knyght ought to be made alle armed upon an hors in suche wyse that he haue an helme on his heed and a spere in his ryght hande/ and coueryd wyth his sheld/ a swerde and a mace on his lyft syde/ Cladd wyth an hawberk and plates to fore his breste/ legge harnoys on his legges/ Spores on his heelis on his handes his gauntelettes/ his hors well broken and taught and apte to bataylle and couerid with his armes/ whan the knyghtes ben maad they ben bayned or bathed/ that is the signe that they shold lede a newe lyf and newe maners/ also they wake alle the nyght in prayers and orysons vnto god that he wylle gyue hem grace that they may gete that thynge that they may not gete by nature/ The kynge or prynce gyrdeth a boute them a swerde in signe/ that they shold abyde and kepe hym of whom they take theyr dispenses and dignyte.[127]
During the Age of Enlightenment, chess was viewed as a means of self-improvement. Benjamin Franklin, in his article "The Morals of Chess" (1750), wrote:
The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:
I. Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...
III. Caution, not to make our moves too hastily ...[132]
Chess was occasionally criticized in the 19th century as a waste of time.[133][134]
Chess is taught to children in schools around the world today. Many schools host chess clubs, and there are many scholastic tournaments specifically for children. Tournaments are held regularly in many countries, hosted by organizations such as the United States Chess Federation and the National Scholastic Chess Foundation.[135]
Chess is many times depicted in the arts; significant works where chess plays a key role range from Thomas Middleton's A Game at Chess to Through the Looking-Glass by Lewis Carroll, to Vladimir Nabokov's The Defense, to The Royal Game by Stefan Zweig. Chess has also featured in film classics such as Ingmar Bergman's The Seventh Seal, Satyajit Ray's The Chess Players, and Powell and Pressburger's A Matter of Life and Death.
Chess is also present in contemporary popular culture. For example, the characters in Star Trek play a futuristic version of the game called "Federation Tri-Dimensional Chess"[136] and "Wizard's Chess" is played in J.K. Rowling's Harry Potter.[137]
The game structure and nature of chess are related to several branches of mathematics. Many combinatorical and topological problems connected to chess, such as the knight's tour and the eight queens puzzle, have been known for hundreds of years.
The number of legal positions in chess is estimated to be 4.59 (+/- 0.38) x1044 with a 95% confidence level,[138] with a game-tree complexity of approximately 10123. The game-tree complexity of chess was first calculated by Claude Shannon as 10120, a number known as the Shannon number.[139] An average position typically has thirty to forty possible moves, but there may be as few as zero (in the case of checkmate or stalemate) or (in a constructed position) as many as 218.[140]
In 1913, Ernst Zermelo used chess as a basis for his theory of game strategies, which is considered one of the predecessors of game theory.[141] Zermelo's theorem states that it is possible to solve chess, i.e. to determine with certainty the outcome of a perfectly played game (either White can force a win, or Black can force a win, or both sides can force at least a draw).[142] However, with 1043 legal positions in chess, it will take an impossibly long time to compute a perfect strategy with any feasible technology.[143]
There is an extensive scientific literature on chess psychology.[note 6][145][146][147][148] Alfred Binet and others showed that knowledge and verbal, rather than visuospatial, ability lies at the core of expertise.[149][150] In his doctoral thesis, Adriaan de Groot showed that chess masters can rapidly perceive the key features of a position.[151] According to de Groot, this perception, made possible by years of practice and study, is more important than the sheer ability to anticipate moves. De Groot showed that chess masters can memorize positions shown for a few seconds almost perfectly. The ability to memorize does not alone account for chess-playing skill, since masters and novices, when faced with random arrangements of chess pieces, had equivalent recall (about six positions in each case). Rather, it is the ability to recognize patterns, which are then memorized, which distinguished the skilled players from the novices. When the positions of the pieces were taken from an actual game, the masters had almost total positional recall.[152]
More recent research has focused on chess as mental training; the respective roles of knowledge and look-ahead search; brain imaging studies of chess masters and novices; blindfold chess; the role of personality and intelligence in chess skill; gender differences; and computational models of chess expertise. The role of practice and talent in the development of chess and other domains of expertise has led to much empirical investigation. Ericsson and colleagues have argued that deliberate practice is sufficient for reaching high levels of expertise in chess.[153] Recent research, however, fails to replicate their results and indicates that factors other than practice are also important.[154][155]
For example, Fernand Gobet and colleagues have shown that stronger players started playing chess at a young age and that experts born in the Northern Hemisphere are more likely to have been born in late winter and early spring. Compared to the general population, chess players are more likely to be non-right-handed, though they found no correlation between handedness and skill.[155]
A relationship between chess skill and intelligence has long been discussed in scientific literature as well as in popular culture. Academic studies that investigate the relationship date back at least to 1927.[156] Although one meta-analysis and most children studies find a positive correlation between general cognitive ability and chess skill, adult studies show mixed results.[157][158]
Chess composition is the art of creating chess problems (also called chess compositions). The creator is known as a chess composer.[160] There are many types of chess problems; the two most important are:
Fairy chess is a branch of chess problem composition involving altered rules, such as the use of unconventional pieces or boards, or unusual stipulations such as reflexmates.
Tournaments for composition and solving of chess problems are organized by the World Federation for Chess Composition, which works cooperatively with but independent of FIDE. The WFCC awards titles for composing and solving chess problems.[163]
Online chess is chess that is played over the internet, allowing players to play against each other in real time. This is done through the use of Internet chess servers, which pair up individual players based on their rating using an Elo or similar rating system. Online chess saw a spike in growth during the quarantines of the COVID-19 pandemic.[164][165] This can be attributed to both isolation and the popularity of Netflix miniseries The Queen's Gambit, which was released in October 2020.[164][165] Chess app downloads on the App Store and Google Play Store rose by 63% after the show debuted.[166] Chess.com saw more than twice as many account registrations in November as it had in previous months, and the number of games played monthly on Lichess doubled as well. There was also a demographic shift in players, with female registration on Chess.com shifting from 22% to 27% of new players.[167] Grandmaster Maurice Ashley said "A boom is taking place in chess like we have never seen maybe since the Bobby Fischer days," attributing the growth to an increased desire to do something constructive during the pandemic.[168] USCF Women's Program Director Jennifer Shahade stated that chess works well on the internet, since pieces do not need to be reset and matchmaking is virtually instant.[169]
The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require "thinking" for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of "thinking"; (4) the discrete structure of chess fits well into the digital nature of
modern computers.[173]
With huge databases of past games and high analytical ability, computers can help players to learn chess and prepare for matches. Internet Chess Servers allow people to find and play opponents worldwide. The presence of computers and modern communication tools have raised concerns regarding cheating during games.[182]
There are more than two thousand published chess variants, games with similar but different rules.[183] Most of them are of relatively recent origin.[184] They include:
In the context of chess variants, regular (i.e. FIDE) chess is commonly referred to as Western chess, international chess, orthodox chess, orthochess, and classic chess.[186][187]
Chess is a board game between two players. It is sometimes called international chess or Western chess to distinguish it from related games, such as xiangqi (Chinese chess) and shogi (Japanese chess). The current form of the game emerged in Spain and the rest of Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin. Today, chess is one of the world's most popular games, played by millions of people worldwide.
Chess is an abstract strategy game and involves no hidden information. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. The player controlling the white pieces moves first, followed by the player controlling the black pieces. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in "check") and there is no way for it to escape. There are also several ways a game can end in a draw.
Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (the International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion. A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition, and chess in its turn influenced Western culture and art, and has connections with other fields such as mathematics, computer science, and psychology.
One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players and have deeply influenced the development of chess theory.
Chess pieces are divided into two different colored sets. While the sets might not be literally white and black (e.g. the light set may be a yellowish or off-white color, the dark set may be brown or red), they are always referred to as "white" and "black". The players of the sets are referred to as White and Black, respectively. Each set consists of sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. Chess sets come in a wide variety of styles; for competition, the Staunton pattern is preferred.
The game is played on a square board of eight rows (called ranks) and eight columns (called files). By convention, the 64 squares alternate in color and are referred to as light and dark squares; common colors for chessboards are white and brown, or white and dark green.
In competitive games, the piece colors are allocated to players by the organizers; in informal games, the colors are usually decided randomly, for example by a coin toss, or by one player concealing a white pawn in one hand and a black pawn in the other, and having the opponent choose. 
White moves first, after which players alternate turns, moving one piece per turn, except for castling, when two pieces are moved. A piece is moved to either an unoccupied square or one occupied by an opponent's piece, which is captured and removed from play. With the sole exception of en passant, all pieces capture by moving to the square that the opponent's piece occupies. Moving is compulsory; a player may not skip a turn, even when having to move is detrimental.
Each piece has its own way of moving. In the diagrams, the dots mark the squares to which the piece can move if there are no intervening piece(s) of either color (except the knight, which leaps over any intervening pieces). All pieces except the pawn can capture an enemy piece if it is located on a square to which they would be able to move if the square was unoccupied. The squares on which pawns can capture enemy pieces are marked in the diagram with black crosses.
When a king is under immediate attack, it is said to be in check. A move in response to a check is legal only if it results in a position where the king is no longer in check. This can involve capturing the checking piece; interposing a piece between the checking piece and the king (which is possible only if the attacking piece is a queen, rook, or bishop and there is a square between it and the king); or moving the king to a square where it is not under attack. Castling is not a permissible response to a check.[1]
The object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to get it out of check. It is never legal for a player to make a move that puts or leaves the player's own king in check. In casual games, it is common to announce "check" when putting the opponent's king in check, but this is not required by the rules of chess and is not usually done in tournaments.[2]
Once per game, each king can make a move known as castling. Castling consists of moving the king two squares toward a rook of the same color on the same rank, and then placing the rook on the square that the king crossed.
Castling is still permitted if the rook is under attack, or if the rook crosses an attacked square. 
When a pawn makes a two-step advance from its starting position and there is an opponent's pawn on a square next to the destination square on an adjacent file, then the opponent's pawn can capture it en passant ("in passing"), moving to the square the pawn passed over. This can be done only on the turn immediately following the enemy pawn's two-square advance; otherwise, the right to do so is forfeited. For example, in the animated diagram, the black pawn advances two squares from g7 to g5, and the white pawn on f5 can take it en passant on g6 (but only immediately after the black pawn's advance).
When a pawn advances to its eighth rank, as part of the move, it is promoted and must be exchanged for the player's choice of queen, rook, bishop, or knight of the same color. Usually, the pawn is chosen to be promoted to a queen, but in some cases, another piece is chosen; this is called underpromotion. In the animated diagram, the pawn on c7 can be advanced to the eighth rank and be promoted. There is no restriction on the piece promoted to, so it is possible to have more pieces of the same type than at the start of the game (e.g., two or more queens). If the required piece is not available (e.g. a second queen) an inverted rook is sometimes used as a substitute, but this is not recognized in FIDE-sanctioned games.
There are several ways a game can end in a draw:
Time is controlled using a chess clock that has two displays, one for each player's remaining time. Analog chess clocks have been largely replaced by digital clocks, which allow for time controls with increments.
Time controls are also enforced in correspondence chess competitions. A typical time control is 50 days for every 10 moves.
The pieces are identified by their initials. In English, these are K (king), Q (queen), R (rook), B (bishop), and N (knight; N is used to avoid confusion with king). For example, Qg5 means "queen moves to the g-file, 5th rank" (that is, to the square g5). Different initials may be used for other languages. In chess literature, figurine algebraic notation (FAN) is frequently used to aid understanding independent of language.
To resolve ambiguities, an additional letter or number is added to indicate the file or rank from which the piece moved (e.g. Ngf3 means "knight from the g-file moves to the square f3"; R1e2 means "rook on the first rank moves to e2"). For pawns, no letter initial is used; so e4 means "pawn moves to the square e4".
If the piece makes a capture, "x" is usually inserted before the destination square. Thus Bxf3 means "bishop captures on f3". When a pawn makes a capture, the file from which the pawn departed is used to identify the pawn making the capture, for example, exd5 (pawn on the e-file captures the piece on d5). Ranks may be omitted if unambiguous, for example, exd (pawn on the e-file captures a piece somewhere on the d-file). A minority of publications use ":" to indicate a capture, and some omit the capture symbol altogether. In its most abbreviated form, exd5 may be rendered simply as ed. An en passant capture may optionally be marked with the notation "e.p."
For example, one variation of a simple trap known as the Scholar's mate (see animated diagram) can be recorded:
Variants of algebraic notation include long form algebraic, in which both the departure and destination square are indicated; abbreviated algebraic, in which capture signs, check signs, and ranks of pawn captures may be omitted; and Figurine Algebraic Notation, used in chess publications for universal readability regardless of language.
Portable Game Notation (PGN) is a text-based file format for recording chess games, based on short form English algebraic notation with a small amount of markup. PGN files (suffix .pgn) can be processed by most chess software, as well as being easily readable by humans.
Until about 1980, the majority of English language chess publications used descriptive notation, in which files are identified by the initial letter of the piece that occupies the first rank at the beginning of the game. In descriptive notation, the common opening move 1.e4 is rendered as "1.P-K4" ("pawn to king four"). Another system is ICCF numeric notation, recognized by the International Correspondence Chess Federation though its use is in decline.
In competitive games, players are normally required to keep a score (record of the game). For this purpose, only algebraic notation is recognized in FIDE-sanctioned events; game scores recorded in a different notation system may not be used as evidence in the event of a dispute.
Contemporary chess is an organized sport with structured international and national leagues, tournaments, and congresses. Thousands of chess tournaments, matches, and festivals are held around the world every year catering to players of all levels.
The term "match" refers not to an individual game, but to either a series of games between two players, or a team competition in which each player of one team plays one game against a player of the other team.
FIDE's most visible activity is organizing the World Chess Championship, a role it assumed in 1948. The current World Champion is Magnus Carlsen of Norway.[11] The reigning Women's World Champion is Ju Wenjun from China.[12]
Other competitions for individuals include the World Junior Chess Championship, the European Individual Chess Championship, the tournaments for the World Championship qualification cycle, and the various national championships. Invitation-only tournaments regularly attract the world's strongest players. Examples include Spain's Linares event, Monte Carlo's Melody Amber tournament, the Dortmund Sparkassen meeting, Sofia's M-tel Masters, and Wijk aan Zee's Tata Steel tournament.
Regular team chess events include the Chess Olympiad and the European Team Chess Championship.
The World Chess Solving Championship and World Correspondence Chess Championships include both team and individual events; these are held independently of FIDE.
In order to rank players, FIDE, ICCF, and most national chess organizations use the Elo rating system developed by Arpad Elo. An average club player has a rating of about 1500; the highest FIDE rating of all time, 2882, was achieved by Magnus Carlsen on the March 2014 FIDE rating list.[13]
The above titles are open to both men and women. There are also separate women-only titles; Woman Grandmaster (WGM), Woman International Master (WIM), Woman FIDE Master (WFM) and Woman Candidate Master (WCM). These require a performance level approximately 200 Elo rating points below the similarly named open titles, and their continued existence has sometimes been controversial. Beginning with Nona Gaprindashvili in 1978, a number of women have earned the open GM title.[note 2]
FIDE also awards titles for arbiters and trainers.[16][17] International titles are also awarded to composers and solvers of chess problems and to correspondence chess players (by the International Correspondence Chess Federation). National chess organizations may also award titles.
Chess has an extensive literature. In 1913, the chess historian H.J.R. Murray estimated the total number of books, magazines, and chess columns in newspapers to be about 5,000.[18] B.H. Wood estimated the number, as of 1949, to be about 20,000.[19] David Hooper and Kenneth Whyld write that, "Since then there has been a steady increase year by year of the number of new chess publications. No one knows how many have been printed."[19] Significant public chess libraries include the John G. White Chess and Checkers Collection at Cleveland Public Library, with over 32,000 chess books and over 6,000 bound volumes of chess periodicals;[20] and the Chess & Draughts collection at the National Library of the Netherlands, with about 30,000 books.[21]
Chess theory usually divides the game of chess into three phases with different sets of strategies: the opening, typically the first 10 to 20 moves, when players move their pieces to useful positions for the coming battle; the middlegame; and last the endgame, when most of the pieces are gone, kings typically take a more active part in the struggle, and pawn promotion is often decisive.
Opening theory is concerned with finding the best moves in the initial phase of the game. There are dozens of different openings, and hundreds of variants. The Oxford Companion to Chess lists 1,327 named openings and variants.[22]
Endgame theory is concerned with positions where there are only a few pieces left. Theoretics categorize these positions according to the pieces, for example "King and pawn endings" or "Rook versus a minor piece".
Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. This initially gives White the initiative.[25] Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.
Specific plans or strategic themes will often arise from particular groups of openings which result in a specific type of pawn structure. An example is the minority attack, which is the attack of queenside pawns against an opponent who has more pawns on the queenside. The study of openings is therefore connected to the preparation of plans that are typical of the resulting middlegames.[28]
Another important strategic question in the middlegame is whether and how to reduce material and transition into an endgame (i.e. simplify). Minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of a pawn, or sometimes even with a two-pawn advantage.[29]
Chess strategy is concerned with the evaluation of chess positions and with setting up goals and long-term plans for future play. During the evaluation, players must take into account numerous factors such as the value of the pieces on the board, control of the center and centralization, the pawn structure, king safety, and the control of key squares or groups of squares (for example, diagonals, open files, and dark or light squares).
The most basic step in evaluating a position is to count the total value of pieces of both sides.[34] The point values used for this purpose are based on experience; usually, pawns are considered worth one point, knights and bishops about three points each, rooks about five points (the value difference between a rook and a bishop or knight being known as the exchange), and queens about nine points. The king is more valuable than all of the other pieces combined, since its checkmate loses the game. But in practical terms, in the endgame, the king as a fighting piece is generally more powerful than a bishop or knight but less powerful than a rook.[35] These basic values are then modified by other factors like position of the piece (e.g. advanced pawns are usually more valuable than those on their initial squares), coordination between pieces (e.g. a pair of bishops usually coordinate better than a bishop and a knight), or the type of position (e.g. knights are generally better in closed positions with many pawns while bishops are more powerful in open positions).[36]
Another important factor in the evaluation of chess positions is pawn structure (sometimes known as the pawn skeleton): the configuration of pawns on the chessboard.[37] Since pawns are the least mobile of the pieces, pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in pawn structure include isolated, doubled, or backward pawns and holes; once created, they are often permanent. Care must therefore be taken to avoid these weaknesses unless they are compensated by another valuable asset (for example, by the possibility of developing an attack).[38]
The endgame (also end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:[39]
Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions in which one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides, and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to pieces on the board other than kings, such as "rook and pawn versus rook" endgames.
The earliest texts referring to the origins of chess date from the beginning of the 7th century. Three are written in Pahlavi (Middle Persian)[42] and one, the Harshacharita, is in Sanskrit.[43] One of these texts, the Chatrang-namak, represents one of the earliest written accounts of chess. The narrator Bozorgmehr explains that Chatrang, the Pahlavi word for chess, was introduced to Persia by 'Dewasarm, a great ruler of India' during the reign of Khosrow I.[44]
The game of chess was then played and known in all European countries. A famous 13th-century Spanish manuscript covering chess, backgammon, and dice is known as the Libro de los juegos, which is the earliest European treatise on chess as well as being the oldest document on European tables games. The rules were fundamentally similar to those of the Arabic shatranj. The differences were mostly in the use of a checkered board instead of a plain monochrome board used by Arabs and the habit of allowing some or all pawns to make an initial double step. In some regions, the Queen, which had replaced the Wazir, and/or the King could also make an initial two-square leap under some conditions.[63]
At the same time, the intellectual movement of romanticism had had a far-reaching impact on chess, with aesthetics and tactical beauty being held in higher regard than objective soundness and strategic planning. As a result, virtually all games began with the Open Game, and it was considered unsportsmanlike to decline gambits that invited tactical play such as the King's Gambit and the Evans Gambit.[75] This chess philosophy is known as Romantic chess, and a sharp, tactical style consistent with the principles of chess romanticism was predominant until the late 19th century.[76]
As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books, and chess journals appeared. There were correspondence matches between cities; for example, the London Chess Club played against the Edinburgh Chess Club in 1824.[79] Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling, and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.
The first modern chess tournament was organized by Howard Staunton, a leading English chess player, and was held in London in 1851. It was won by the German Adolf Anderssen, who was hailed as the leading chess master. His brilliant, energetic attacking style was typical for the time.[80][81] Sparkling games like Anderssen's Immortal Game and Evergreen Game or Morphy's "Opera Game" were regarded as the highest possible summit of the art of chess.[82]
Deeper insight into the nature of chess came with the American Paul Morphy, an extraordinary chess prodigy. Morphy won against all important competitors (except Staunton, who refused to play), including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks.[83]
Prague-born Wilhelm Steinitz laid the foundations for a scientific approach to the game, the art of breaking a position down into components[84] and preparing correct plans.[85] In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. This win marked a stylistic transition at the highest levels of chess from an attacking, tactical style predominant in the Romantic era to a more positional, strategic style introduced to the chess world by Steinitz. Steinitz lost his crown in 1894 to a much younger player, the German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of any world champion.[86]
After the end of the 19th century, the number of master tournaments and matches held annually quickly grew. The first Olympiad was held in Paris in 1924, and FIDE was founded initially for the purpose of organizing that event. In 1927, the Women's World Chess Championship was established; the first to hold the title was Czech-English master Vera Menchik.[87]
After the death of Alekhine, a new World Champion was sought. FIDE, which has controlled the title since then, ran a tournament of elite players. The winner of the 1948 tournament was Russian Mikhail Botvinnik.
In 1950 FIDE established a system of titles, conferring the titles of Grandmaster and International Master on 27 players. (Some sources state that in 1914 the title of chess Grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch, and Marshall, but this is a disputed claim.[note 5])
Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes.[99] In the 1984 World Chess Championship, Karpov faced his toughest challenge to date, the young Garry Kasparov from Baku, Soviet Azerbaijan. The match was aborted in controversial circumstances after 5 months and 48 games with Karpov leading by 5 wins to 3, but evidently exhausted; many commentators believed Kasparov, who had won the last two games, would have won the match had it continued. Kasparov won the 1985 rematch. Kasparov and Karpov contested three further closely fought matches in 1986, 1987 and 1990, Kasparov winning them all.[100] Kasparov became the dominant figure of world chess from the mid 1980s until his retirement from competition in 2005.
Chess-playing computer programs (later known as chess engines) began to appear in the 1960s. In 1970, the first major computer chess tournament, the North American Computer Chess Championship, was held, followed in 1974 by the first World Computer Chess Championship. In the late 1970s, dedicated home chess computers such as Fidelity Electronics' Chess Challenger became commercially available, as well as software to run on home computers. However, the overall standard of computer chess was low until the 1990s.
The first endgame tablebases, which provided perfect play for relatively simple endgames such as king and rook versus king and bishop, appeared in the late 1970s. This set a precedent to the complete six- and seven-piece tablebases that became available in the 2000s and 2010s respectively.[101]
The first commercial chess database, a collection of chess games searchable by move and position, was introduced by the German company ChessBase in 1987. Databases containing millions of chess games have since had a profound effect on opening theory and other areas of chess research.
Digital chess clocks were invented in 1973, though they did not become commonplace until the 1990s. Digital clocks allow for time controls involving increments and delays.
The Internet enabled online chess as a new medium of playing, with chess servers allowing users to play other people from different parts of the world in real time. The first such server, known as Internet Chess Server or ICS, was developed at the University of Utah in 1992. ICS formed the basis for the first commercial chess server, the Internet Chess Club, which was launched in 1995, and for other early chess servers such as FICS (Free Internet Chess Server). Since then, many other platforms have appeared, and online chess began to rival over-the-board chess in popularity.[102][103] During the 2020 COVID-19 pandemic, the isolation ensuing from quarantines imposed in many places around the world, combined with the success of the popular Netflix show The Queen's Gambit and other factors such as the popularity of online tournaments (notably PogChamps) and chess Twitch streamers, resulted in a surge of popularity not only for online chess, but for the game of chess in general; this phenomenon has been referred to in the media as the 2020 online chess boom.[104][105]
As endgame tablebases developed, they began to provide perfect play in endgame positions in which the game-theoretical outcome was previously unknown, such as positions with king, queen and pawn against king and queen. In 1991, Lewis Stiller published a tablebase for select six-piece endgames,[108][109] and by 2005, following the publication of Nalimov tablebases, all six-piece endgame positions were solved. In 2012, Lomonosov tablebases were published which solved all seven-piece endgame positions.[110] Use of tablebases enhances the performance of chess engines by providing definitive results in some branches of analysis.
Technological progress made in the 1990s and the 21st century has influenced the way that chess is studied at all levels, as well as the state of chess as a spectator sport.
Previously, preparation at the professional level required an extensive chess library and several subscriptions to publications such as Chess Informant to keep up with opening developments and study opponents' games. Today, preparation at the professional level involves the use of databases containing millions of games, and engines to analyze different opening variations and prepare novelties.[111] A number of online learning resources are also available for players of all levels, such as online courses, tactics trainers, and video lessons.[112]
Organized chess even for young children has become common. FIDE holds world championships for age levels down to 8 years old. The largest tournaments, in number of players, are those held for children.[116]
The number of grandmasters and other chess professionals has also grown in the modern era. Kenneth Regan and Guy Haworth conducted research involving comparison of move choices by players of different levels and from different periods with the analysis of strong chess engines; they concluded that the increase in the number of grandmasters and higher Elo ratings of the top players reflect an actual increase in the average standard of play, rather than "rating inflation" or "title inflation".[117]
In 1993, Garry Kasparov and Nigel Short broke ties with FIDE to organize their own match for the title and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Championships and respective World Champions: the PCA or "classical" champions extending the Steinitzian tradition in which the current champion plays a challenger in a series of games, and the other following FIDE's new format of many players competing in a large knockout tournament to determine the champion. Kasparov lost his PCA title in 2000 to Vladimir Kramnik of Russia.[118] Due to the complicated state of world chess politics and difficulties obtaining commercial sponsorships, Kasparov was never able to challenge for the title again. Despite this, he continued to dominate in top level tournaments and remained the world's highest rated player until his retirement from competitive chess in 2005.
The World Chess Championship 2006, in which Kramnik beat the FIDE World Champion Veselin Topalov, reunified the titles and made Kramnik the undisputed World Chess Champion.[119] In September 2007, he lost the title to Viswanathan Anand of India, who won the championship tournament in Mexico City. Anand defended his title in the revenge match of 2008,[120] 2010 and 2012. In 2013, Magnus Carlsen of Norway beat Anand in the 2013 World Chess Championship.[121] He defended his title 4 times since then and is the reigning world champion.
In the Middle Ages and during the Renaissance, chess was a part of noble culture; it was used to teach war strategy and was dubbed the "King's Game".[122] Gentlemen are "to be meanly seene in the play at Chestes", says the overview at the beginning of Baldassare Castiglione's The Book of the Courtier (1528, English 1561 by Sir Thomas Hoby), but chess should not be a gentleman's main passion. Castiglione explains it further:
And what say you to the game at chestes? It is truely an honest kynde of enterteynmente and wittie, quoth Syr Friderick. But me think it hath a fault, whiche is, that a man may be to couning at it, for who ever will be excellent in the playe of chestes, I beleave he must beestowe much tyme about it, and applie it with so much study, that a man may assoone learne some noble scyence, or compase any other matter of importaunce, and yet in the ende in beestowing all that laboure, he knoweth no more but a game. Therfore in this I beleave there happeneth a very rare thing, namely, that the meane is more commendable, then the excellency.[123]
Some of the elaborate chess sets used by the aristocracy at least partially survive, such as the Lewis chessmen.
The knyght ought to be made alle armed upon an hors in suche wyse that he haue an helme on his heed and a spere in his ryght hande/ and coueryd wyth his sheld/ a swerde and a mace on his lyft syde/ Cladd wyth an hawberk and plates to fore his breste/ legge harnoys on his legges/ Spores on his heelis on his handes his gauntelettes/ his hors well broken and taught and apte to bataylle and couerid with his armes/ whan the knyghtes ben maad they ben bayned or bathed/ that is the signe that they shold lede a newe lyf and newe maners/ also they wake alle the nyght in prayers and orysons vnto god that he wylle gyue hem grace that they may gete that thynge that they may not gete by nature/ The kynge or prynce gyrdeth a boute them a swerde in signe/ that they shold abyde and kepe hym of whom they take theyr dispenses and dignyte.[127]
During the Age of Enlightenment, chess was viewed as a means of self-improvement. Benjamin Franklin, in his article "The Morals of Chess" (1750), wrote:
The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:
I. Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...
III. Caution, not to make our moves too hastily ...[132]
Chess was occasionally criticized in the 19th century as a waste of time.[133][134]
Chess is taught to children in schools around the world today. Many schools host chess clubs, and there are many scholastic tournaments specifically for children. Tournaments are held regularly in many countries, hosted by organizations such as the United States Chess Federation and the National Scholastic Chess Foundation.[135]
Chess is many times depicted in the arts; significant works where chess plays a key role range from Thomas Middleton's A Game at Chess to Through the Looking-Glass by Lewis Carroll, to Vladimir Nabokov's The Defense, to The Royal Game by Stefan Zweig. Chess has also featured in film classics such as Ingmar Bergman's The Seventh Seal, Satyajit Ray's The Chess Players, and Powell and Pressburger's A Matter of Life and Death.
Chess is also present in contemporary popular culture. For example, the characters in Star Trek play a futuristic version of the game called "Federation Tri-Dimensional Chess"[136] and "Wizard's Chess" is played in J.K. Rowling's Harry Potter.[137]
The game structure and nature of chess are related to several branches of mathematics. Many combinatorical and topological problems connected to chess, such as the knight's tour and the eight queens puzzle, have been known for hundreds of years.
The number of legal positions in chess is estimated to be 4.59 (+/- 0.38) x1044 with a 95% confidence level,[138] with a game-tree complexity of approximately 10123. The game-tree complexity of chess was first calculated by Claude Shannon as 10120, a number known as the Shannon number.[139] An average position typically has thirty to forty possible moves, but there may be as few as zero (in the case of checkmate or stalemate) or (in a constructed position) as many as 218.[140]
In 1913, Ernst Zermelo used chess as a basis for his theory of game strategies, which is considered one of the predecessors of game theory.[141] Zermelo's theorem states that it is possible to solve chess, i.e. to determine with certainty the outcome of a perfectly played game (either White can force a win, or Black can force a win, or both sides can force at least a draw).[142] However, with 1043 legal positions in chess, it will take an impossibly long time to compute a perfect strategy with any feasible technology.[143]
There is an extensive scientific literature on chess psychology.[note 6][145][146][147][148] Alfred Binet and others showed that knowledge and verbal, rather than visuospatial, ability lies at the core of expertise.[149][150] In his doctoral thesis, Adriaan de Groot showed that chess masters can rapidly perceive the key features of a position.[151] According to de Groot, this perception, made possible by years of practice and study, is more important than the sheer ability to anticipate moves. De Groot showed that chess masters can memorize positions shown for a few seconds almost perfectly. The ability to memorize does not alone account for chess-playing skill, since masters and novices, when faced with random arrangements of chess pieces, had equivalent recall (about six positions in each case). Rather, it is the ability to recognize patterns, which are then memorized, which distinguished the skilled players from the novices. When the positions of the pieces were taken from an actual game, the masters had almost total positional recall.[152]
More recent research has focused on chess as mental training; the respective roles of knowledge and look-ahead search; brain imaging studies of chess masters and novices; blindfold chess; the role of personality and intelligence in chess skill; gender differences; and computational models of chess expertise. The role of practice and talent in the development of chess and other domains of expertise has led to much empirical investigation. Ericsson and colleagues have argued that deliberate practice is sufficient for reaching high levels of expertise in chess.[153] Recent research, however, fails to replicate their results and indicates that factors other than practice are also important.[154][155]
For example, Fernand Gobet and colleagues have shown that stronger players started playing chess at a young age and that experts born in the Northern Hemisphere are more likely to have been born in late winter and early spring. Compared to the general population, chess players are more likely to be non-right-handed, though they found no correlation between handedness and skill.[155]
A relationship between chess skill and intelligence has long been discussed in scientific literature as well as in popular culture. Academic studies that investigate the relationship date back at least to 1927.[156] Although one meta-analysis and most children studies find a positive correlation between general cognitive ability and chess skill, adult studies show mixed results.[157][158]
Chess composition is the art of creating chess problems (also called chess compositions). The creator is known as a chess composer.[160] There are many types of chess problems; the two most important are:
Fairy chess is a branch of chess problem composition involving altered rules, such as the use of unconventional pieces or boards, or unusual stipulations such as reflexmates.
Tournaments for composition and solving of chess problems are organized by the World Federation for Chess Composition, which works cooperatively with but independent of FIDE. The WFCC awards titles for composing and solving chess problems.[163]
Online chess is chess that is played over the internet, allowing players to play against each other in real time. This is done through the use of Internet chess servers, which pair up individual players based on their rating using an Elo or similar rating system. Online chess saw a spike in growth during the quarantines of the COVID-19 pandemic.[164][165] This can be attributed to both isolation and the popularity of Netflix miniseries The Queen's Gambit, which was released in October 2020.[164][165] Chess app downloads on the App Store and Google Play Store rose by 63% after the show debuted.[166] Chess.com saw more than twice as many account registrations in November as it had in previous months, and the number of games played monthly on Lichess doubled as well. There was also a demographic shift in players, with female registration on Chess.com shifting from 22% to 27% of new players.[167] Grandmaster Maurice Ashley said "A boom is taking place in chess like we have never seen maybe since the Bobby Fischer days," attributing the growth to an increased desire to do something constructive during the pandemic.[168] USCF Women's Program Director Jennifer Shahade stated that chess works well on the internet, since pieces do not need to be reset and matchmaking is virtually instant.[169]
The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require "thinking" for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of "thinking"; (4) the discrete structure of chess fits well into the digital nature of
modern computers.[173]
With huge databases of past games and high analytical ability, computers can help players to learn chess and prepare for matches. Internet Chess Servers allow people to find and play opponents worldwide. The presence of computers and modern communication tools have raised concerns regarding cheating during games.[182]
There are more than two thousand published chess variants, games with similar but different rules.[183] Most of them are of relatively recent origin.[184] They include:
In the context of chess variants, regular (i.e. FIDE) chess is commonly referred to as Western chess, international chess, orthodox chess, orthochess, and classic chess.[186][187]
To fill out this checklist, please add the following code to the template call:
@Dark Looon and Wretchskull: As far as I know, there is nothing wrong with using the preposition "in" twice in this way, and indeed, it may even make the sentence easier to follow for the reader, who may otherwise suppose that the article "the" applies to both "literature" and "popular culture".  Bruce leverett (talk) 14:52, 13 May 2022 (UTC)Reply[reply]
Senterej ,old game in Ethiopia is one of the chess variants.
https://en.m.wikipedia.org/wiki/Senterej Mek2022 (talk) 06:12, 19 June 2022 (UTC)Reply[reply]
An editor has modified the article as if Calvo's theory of the Spanish origin of modern chess was generally accepted.  Is this warranted?
Comparable modifications have been made to History of Chess. Bruce leverett (tal) 17:59, 19 July 2022 (UTC)Reply[reply]
Dear all, I understand that the origins of chess are bit murky and that there is some desire among both Iranians and Indians to take the bulk of the credit for innovation of chess. 
I am just a layman when it comes to history, but the complete lack of any mention of the persian form of the game Shatranj in the introduction seems a bit politically motivated given the centrality of this form both in the documented history, development and transmission of chess between east and west. 
While there is no doubt that the most ancient predecessors of the game originate from India, it seems equally well established that the first complete descriptions of a game resembling modern chess are from persian/arabic sources (please correct me if I am wrong - again I am just a layman!). Also, from my understanding there is no doubt that the persian/arabic form of the game was the form that Europeans were initially exposed to when western chess was developed. 
So the current statement "The current form of the game emerged in Spain and the rest of Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin." seems to be positively misleading as Europans would not have been exposed to chaturanga at this time - rather something along the lines of would probably be more accurate "evolving from shatranj, a persian adaptation of chaturanga, a much older game of Indian origin." 
I hope that these issues can be addressed in way that reflects the importance and primacy of both the original Indian form of the game as well as the deep historial influence of the subsequent Persian adaptation.
Again, I am not a historian, and would very much appreciate any corrections to my understanding! 155.91.73.3 (talk) 10:34, 11 October 2022 (UTC)Reply[reply]
Are there any plans/interest to bring this article back to FA? It seems on first glance to have a solid enough foundation, and has interested editors working on it. Might be near GA level already Horsesizedduck (talk) 18:24, 19 January 2023 (UTC)Reply[reply]
Chess is a board game between two players. It is sometimes called international chess or Western chess to distinguish it from related games, such as xiangqi (Chinese chess) and shogi (Japanese chess). The current form of the game emerged in Spain and the rest of Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin. Today, chess is one of the world's most popular games, played by millions of people worldwide.
Chess is an abstract strategy game and involves no hidden information. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. The player controlling the white pieces moves first, followed by the player controlling the black pieces. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in "check") and there is no way for it to escape. There are also several ways a game can end in a draw.
Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (the International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion. A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition, and chess in its turn influenced Western culture and art, and has connections with other fields such as mathematics, computer science, and psychology.
One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players and have deeply influenced the development of chess theory.
Chess pieces are divided into two different colored sets. While the sets might not be literally white and black (e.g. the light set may be a yellowish or off-white color, the dark set may be brown or red), they are always referred to as "white" and "black". The players of the sets are referred to as White and Black, respectively. Each set consists of sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. Chess sets come in a wide variety of styles; for competition, the Staunton pattern is preferred.
The game is played on a square board of eight rows (called ranks) and eight columns (called files). By convention, the 64 squares alternate in color and are referred to as light and dark squares; common colors for chessboards are white and brown, or white and dark green.
In competitive games, the piece colors are allocated to players by the organizers; in informal games, the colors are usually decided randomly, for example by a coin toss, or by one player concealing a white pawn in one hand and a black pawn in the other, and having the opponent choose. 
White moves first, after which players alternate turns, moving one piece per turn, except for castling, when two pieces are moved. A piece is moved to either an unoccupied square or one occupied by an opponent's piece, which is captured and removed from play. With the sole exception of en passant, all pieces capture by moving to the square that the opponent's piece occupies. Moving is compulsory; a player may not skip a turn, even when having to move is detrimental.
Each piece has its own way of moving. In the diagrams, the dots mark the squares to which the piece can move if there are no intervening piece(s) of either color (except the knight, which leaps over any intervening pieces). All pieces except the pawn can capture an enemy piece if it is located on a square to which they would be able to move if the square was unoccupied. The squares on which pawns can capture enemy pieces are marked in the diagram with black crosses.
When a king is under immediate attack, it is said to be in check. A move in response to a check is legal only if it results in a position where the king is no longer in check. This can involve capturing the checking piece; interposing a piece between the checking piece and the king (which is possible only if the attacking piece is a queen, rook, or bishop and there is a square between it and the king); or moving the king to a square where it is not under attack. Castling is not a permissible response to a check.[1]
The object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to get it out of check. It is never legal for a player to make a move that puts or leaves the player's own king in check. In casual games, it is common to announce "check" when putting the opponent's king in check, but this is not required by the rules of chess and is not usually done in tournaments.[2]
Once per game, each king can make a move known as castling. Castling consists of moving the king two squares toward a rook of the same color on the same rank, and then placing the rook on the square that the king crossed.
Castling is still permitted if the rook is under attack, or if the rook crosses an attacked square. 
When a pawn makes a two-step advance from its starting position and there is an opponent's pawn on a square next to the destination square on an adjacent file, then the opponent's pawn can capture it en passant ("in passing"), moving to the square the pawn passed over. This can be done only on the turn immediately following the enemy pawn's two-square advance; otherwise, the right to do so is forfeited. For example, in the animated diagram, the black pawn advances two squares from g7 to g5, and the white pawn on f5 can take it en passant on g6 (but only immediately after the black pawn's advance).
When a pawn advances to its eighth rank, as part of the move, it is promoted and must be exchanged for the player's choice of queen, rook, bishop, or knight of the same color. Usually, the pawn is chosen to be promoted to a queen, but in some cases, another piece is chosen; this is called underpromotion. In the animated diagram, the pawn on c7 can be advanced to the eighth rank and be promoted. There is no restriction on the piece promoted to, so it is possible to have more pieces of the same type than at the start of the game (e.g., two or more queens). If the required piece is not available (e.g. a second queen) an inverted rook is sometimes used as a substitute, but this is not recognized in FIDE-sanctioned games.
There are several ways a game can end in a draw:
Time is controlled using a chess clock that has two displays, one for each player's remaining time. Analog chess clocks have been largely replaced by digital clocks, which allow for time controls with increments.
Time controls are also enforced in correspondence chess competitions. A typical time control is 50 days for every 10 moves.
The pieces are identified by their initials. In English, these are K (king), Q (queen), R (rook), B (bishop), and N (knight; N is used to avoid confusion with king). For example, Qg5 means "queen moves to the g-file, 5th rank" (that is, to the square g5). Different initials may be used for other languages. In chess literature, figurine algebraic notation (FAN) is frequently used to aid understanding independent of language.
To resolve ambiguities, an additional letter or number is added to indicate the file or rank from which the piece moved (e.g. Ngf3 means "knight from the g-file moves to the square f3"; R1e2 means "rook on the first rank moves to e2"). For pawns, no letter initial is used; so e4 means "pawn moves to the square e4".
If the piece makes a capture, "x" is usually inserted before the destination square. Thus Bxf3 means "bishop captures on f3". When a pawn makes a capture, the file from which the pawn departed is used to identify the pawn making the capture, for example, exd5 (pawn on the e-file captures the piece on d5). Ranks may be omitted if unambiguous, for example, exd (pawn on the e-file captures a piece somewhere on the d-file). A minority of publications use ":" to indicate a capture, and some omit the capture symbol altogether. In its most abbreviated form, exd5 may be rendered simply as ed. An en passant capture may optionally be marked with the notation "e.p."
For example, one variation of a simple trap known as the Scholar's mate (see animated diagram) can be recorded:
Variants of algebraic notation include long form algebraic, in which both the departure and destination square are indicated; abbreviated algebraic, in which capture signs, check signs, and ranks of pawn captures may be omitted; and Figurine Algebraic Notation, used in chess publications for universal readability regardless of language.
Portable Game Notation (PGN) is a text-based file format for recording chess games, based on short form English algebraic notation with a small amount of markup. PGN files (suffix .pgn) can be processed by most chess software, as well as being easily readable by humans.
Until about 1980, the majority of English language chess publications used descriptive notation, in which files are identified by the initial letter of the piece that occupies the first rank at the beginning of the game. In descriptive notation, the common opening move 1.e4 is rendered as "1.P-K4" ("pawn to king four"). Another system is ICCF numeric notation, recognized by the International Correspondence Chess Federation though its use is in decline.
In competitive games, players are normally required to keep a score (record of the game). For this purpose, only algebraic notation is recognized in FIDE-sanctioned events; game scores recorded in a different notation system may not be used as evidence in the event of a dispute.
Contemporary chess is an organized sport with structured international and national leagues, tournaments, and congresses. Thousands of chess tournaments, matches, and festivals are held around the world every year catering to players of all levels.
The term "match" refers not to an individual game, but to either a series of games between two players, or a team competition in which each player of one team plays one game against a player of the other team.
FIDE's most visible activity is organizing the World Chess Championship, a role it assumed in 1948. The current World Champion is Magnus Carlsen of Norway.[11] The reigning Women's World Champion is Ju Wenjun from China.[12]
Other competitions for individuals include the World Junior Chess Championship, the European Individual Chess Championship, the tournaments for the World Championship qualification cycle, and the various national championships. Invitation-only tournaments regularly attract the world's strongest players. Examples include Spain's Linares event, Monte Carlo's Melody Amber tournament, the Dortmund Sparkassen meeting, Sofia's M-tel Masters, and Wijk aan Zee's Tata Steel tournament.
Regular team chess events include the Chess Olympiad and the European Team Chess Championship.
The World Chess Solving Championship and World Correspondence Chess Championships include both team and individual events; these are held independently of FIDE.
In order to rank players, FIDE, ICCF, and most national chess organizations use the Elo rating system developed by Arpad Elo. An average club player has a rating of about 1500; the highest FIDE rating of all time, 2882, was achieved by Magnus Carlsen on the March 2014 FIDE rating list.[13]
The above titles are open to both men and women. There are also separate women-only titles; Woman Grandmaster (WGM), Woman International Master (WIM), Woman FIDE Master (WFM) and Woman Candidate Master (WCM). These require a performance level approximately 200 Elo rating points below the similarly named open titles, and their continued existence has sometimes been controversial. Beginning with Nona Gaprindashvili in 1978, a number of women have earned the open GM title.[note 2]
FIDE also awards titles for arbiters and trainers.[16][17] International titles are also awarded to composers and solvers of chess problems and to correspondence chess players (by the International Correspondence Chess Federation). National chess organizations may also award titles.
Chess has an extensive literature. In 1913, the chess historian H.J.R. Murray estimated the total number of books, magazines, and chess columns in newspapers to be about 5,000.[18] B.H. Wood estimated the number, as of 1949, to be about 20,000.[19] David Hooper and Kenneth Whyld write that, "Since then there has been a steady increase year by year of the number of new chess publications. No one knows how many have been printed."[19] Significant public chess libraries include the John G. White Chess and Checkers Collection at Cleveland Public Library, with over 32,000 chess books and over 6,000 bound volumes of chess periodicals;[20] and the Chess & Draughts collection at the National Library of the Netherlands, with about 30,000 books.[21]
Chess theory usually divides the game of chess into three phases with different sets of strategies: the opening, typically the first 10 to 20 moves, when players move their pieces to useful positions for the coming battle; the middlegame; and last the endgame, when most of the pieces are gone, kings typically take a more active part in the struggle, and pawn promotion is often decisive.
Opening theory is concerned with finding the best moves in the initial phase of the game. There are dozens of different openings, and hundreds of variants. The Oxford Companion to Chess lists 1,327 named openings and variants.[22]
Endgame theory is concerned with positions where there are only a few pieces left. Theoretics categorize these positions according to the pieces, for example "King and pawn endings" or "Rook versus a minor piece".
Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. This initially gives White the initiative.[25] Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.
Specific plans or strategic themes will often arise from particular groups of openings which result in a specific type of pawn structure. An example is the minority attack, which is the attack of queenside pawns against an opponent who has more pawns on the queenside. The study of openings is therefore connected to the preparation of plans that are typical of the resulting middlegames.[28]
Another important strategic question in the middlegame is whether and how to reduce material and transition into an endgame (i.e. simplify). Minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of a pawn, or sometimes even with a two-pawn advantage.[29]
Chess strategy is concerned with the evaluation of chess positions and with setting up goals and long-term plans for future play. During the evaluation, players must take into account numerous factors such as the value of the pieces on the board, control of the center and centralization, the pawn structure, king safety, and the control of key squares or groups of squares (for example, diagonals, open files, and dark or light squares).
The most basic step in evaluating a position is to count the total value of pieces of both sides.[34] The point values used for this purpose are based on experience; usually, pawns are considered worth one point, knights and bishops about three points each, rooks about five points (the value difference between a rook and a bishop or knight being known as the exchange), and queens about nine points. The king is more valuable than all of the other pieces combined, since its checkmate loses the game. But in practical terms, in the endgame, the king as a fighting piece is generally more powerful than a bishop or knight but less powerful than a rook.[35] These basic values are then modified by other factors like position of the piece (e.g. advanced pawns are usually more valuable than those on their initial squares), coordination between pieces (e.g. a pair of bishops usually coordinate better than a bishop and a knight), or the type of position (e.g. knights are generally better in closed positions with many pawns while bishops are more powerful in open positions).[36]
Another important factor in the evaluation of chess positions is pawn structure (sometimes known as the pawn skeleton): the configuration of pawns on the chessboard.[37] Since pawns are the least mobile of the pieces, pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in pawn structure include isolated, doubled, or backward pawns and holes; once created, they are often permanent. Care must therefore be taken to avoid these weaknesses unless they are compensated by another valuable asset (for example, by the possibility of developing an attack).[38]
The endgame (also end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:[39]
Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions in which one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides, and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to pieces on the board other than kings, such as "rook and pawn versus rook" endgames.
The earliest texts referring to the origins of chess date from the beginning of the 7th century. Three are written in Pahlavi (Middle Persian)[42] and one, the Harshacharita, is in Sanskrit.[43] One of these texts, the Chatrang-namak, represents one of the earliest written accounts of chess. The narrator Bozorgmehr explains that Chatrang, the Pahlavi word for chess, was introduced to Persia by 'Dewasarm, a great ruler of India' during the reign of Khosrow I.[44]
The game of chess was then played and known in all European countries. A famous 13th-century Spanish manuscript covering chess, backgammon, and dice is known as the Libro de los juegos, which is the earliest European treatise on chess as well as being the oldest document on European tables games. The rules were fundamentally similar to those of the Arabic shatranj. The differences were mostly in the use of a checkered board instead of a plain monochrome board used by Arabs and the habit of allowing some or all pawns to make an initial double step. In some regions, the Queen, which had replaced the Wazir, and/or the King could also make an initial two-square leap under some conditions.[63]
At the same time, the intellectual movement of romanticism had had a far-reaching impact on chess, with aesthetics and tactical beauty being held in higher regard than objective soundness and strategic planning. As a result, virtually all games began with the Open Game, and it was considered unsportsmanlike to decline gambits that invited tactical play such as the King's Gambit and the Evans Gambit.[75] This chess philosophy is known as Romantic chess, and a sharp, tactical style consistent with the principles of chess romanticism was predominant until the late 19th century.[76]
As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books, and chess journals appeared. There were correspondence matches between cities; for example, the London Chess Club played against the Edinburgh Chess Club in 1824.[79] Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling, and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.
The first modern chess tournament was organized by Howard Staunton, a leading English chess player, and was held in London in 1851. It was won by the German Adolf Anderssen, who was hailed as the leading chess master. His brilliant, energetic attacking style was typical for the time.[80][81] Sparkling games like Anderssen's Immortal Game and Evergreen Game or Morphy's "Opera Game" were regarded as the highest possible summit of the art of chess.[82]
Deeper insight into the nature of chess came with the American Paul Morphy, an extraordinary chess prodigy. Morphy won against all important competitors (except Staunton, who refused to play), including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks.[83]
Prague-born Wilhelm Steinitz laid the foundations for a scientific approach to the game, the art of breaking a position down into components[84] and preparing correct plans.[85] In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. This win marked a stylistic transition at the highest levels of chess from an attacking, tactical style predominant in the Romantic era to a more positional, strategic style introduced to the chess world by Steinitz. Steinitz lost his crown in 1894 to a much younger player, the German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of any world champion.[86]
After the end of the 19th century, the number of master tournaments and matches held annually quickly grew. The first Olympiad was held in Paris in 1924, and FIDE was founded initially for the purpose of organizing that event. In 1927, the Women's World Chess Championship was established; the first to hold the title was Czech-English master Vera Menchik.[87]
After the death of Alekhine, a new World Champion was sought. FIDE, which has controlled the title since then, ran a tournament of elite players. The winner of the 1948 tournament was Russian Mikhail Botvinnik.
In 1950 FIDE established a system of titles, conferring the titles of Grandmaster and International Master on 27 players. (Some sources state that in 1914 the title of chess Grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch, and Marshall, but this is a disputed claim.[note 5])
Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes.[99] In the 1984 World Chess Championship, Karpov faced his toughest challenge to date, the young Garry Kasparov from Baku, Soviet Azerbaijan. The match was aborted in controversial circumstances after 5 months and 48 games with Karpov leading by 5 wins to 3, but evidently exhausted; many commentators believed Kasparov, who had won the last two games, would have won the match had it continued. Kasparov won the 1985 rematch. Kasparov and Karpov contested three further closely fought matches in 1986, 1987 and 1990, Kasparov winning them all.[100] Kasparov became the dominant figure of world chess from the mid 1980s until his retirement from competition in 2005.
Chess-playing computer programs (later known as chess engines) began to appear in the 1960s. In 1970, the first major computer chess tournament, the North American Computer Chess Championship, was held, followed in 1974 by the first World Computer Chess Championship. In the late 1970s, dedicated home chess computers such as Fidelity Electronics' Chess Challenger became commercially available, as well as software to run on home computers. However, the overall standard of computer chess was low until the 1990s.
The first endgame tablebases, which provided perfect play for relatively simple endgames such as king and rook versus king and bishop, appeared in the late 1970s. This set a precedent to the complete six- and seven-piece tablebases that became available in the 2000s and 2010s respectively.[101]
The first commercial chess database, a collection of chess games searchable by move and position, was introduced by the German company ChessBase in 1987. Databases containing millions of chess games have since had a profound effect on opening theory and other areas of chess research.
Digital chess clocks were invented in 1973, though they did not become commonplace until the 1990s. Digital clocks allow for time controls involving increments and delays.
The Internet enabled online chess as a new medium of playing, with chess servers allowing users to play other people from different parts of the world in real time. The first such server, known as Internet Chess Server or ICS, was developed at the University of Utah in 1992. ICS formed the basis for the first commercial chess server, the Internet Chess Club, which was launched in 1995, and for other early chess servers such as FICS (Free Internet Chess Server). Since then, many other platforms have appeared, and online chess began to rival over-the-board chess in popularity.[102][103] During the 2020 COVID-19 pandemic, the isolation ensuing from quarantines imposed in many places around the world, combined with the success of the popular Netflix show The Queen's Gambit and other factors such as the popularity of online tournaments (notably PogChamps) and chess Twitch streamers, resulted in a surge of popularity not only for online chess, but for the game of chess in general; this phenomenon has been referred to in the media as the 2020 online chess boom.[104][105]
As endgame tablebases developed, they began to provide perfect play in endgame positions in which the game-theoretical outcome was previously unknown, such as positions with king, queen and pawn against king and queen. In 1991, Lewis Stiller published a tablebase for select six-piece endgames,[108][109] and by 2005, following the publication of Nalimov tablebases, all six-piece endgame positions were solved. In 2012, Lomonosov tablebases were published which solved all seven-piece endgame positions.[110] Use of tablebases enhances the performance of chess engines by providing definitive results in some branches of analysis.
Technological progress made in the 1990s and the 21st century has influenced the way that chess is studied at all levels, as well as the state of chess as a spectator sport.
Previously, preparation at the professional level required an extensive chess library and several subscriptions to publications such as Chess Informant to keep up with opening developments and study opponents' games. Today, preparation at the professional level involves the use of databases containing millions of games, and engines to analyze different opening variations and prepare novelties.[111] A number of online learning resources are also available for players of all levels, such as online courses, tactics trainers, and video lessons.[112]
Organized chess even for young children has become common. FIDE holds world championships for age levels down to 8 years old. The largest tournaments, in number of players, are those held for children.[116]
The number of grandmasters and other chess professionals has also grown in the modern era. Kenneth Regan and Guy Haworth conducted research involving comparison of move choices by players of different levels and from different periods with the analysis of strong chess engines; they concluded that the increase in the number of grandmasters and higher Elo ratings of the top players reflect an actual increase in the average standard of play, rather than "rating inflation" or "title inflation".[117]
In 1993, Garry Kasparov and Nigel Short broke ties with FIDE to organize their own match for the title and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Championships and respective World Champions: the PCA or "classical" champions extending the Steinitzian tradition in which the current champion plays a challenger in a series of games, and the other following FIDE's new format of many players competing in a large knockout tournament to determine the champion. Kasparov lost his PCA title in 2000 to Vladimir Kramnik of Russia.[118] Due to the complicated state of world chess politics and difficulties obtaining commercial sponsorships, Kasparov was never able to challenge for the title again. Despite this, he continued to dominate in top level tournaments and remained the world's highest rated player until his retirement from competitive chess in 2005.
The World Chess Championship 2006, in which Kramnik beat the FIDE World Champion Veselin Topalov, reunified the titles and made Kramnik the undisputed World Chess Champion.[119] In September 2007, he lost the title to Viswanathan Anand of India, who won the championship tournament in Mexico City. Anand defended his title in the revenge match of 2008,[120] 2010 and 2012. In 2013, Magnus Carlsen of Norway beat Anand in the 2013 World Chess Championship.[121] He defended his title 4 times since then and is the reigning world champion.
In the Middle Ages and during the Renaissance, chess was a part of noble culture; it was used to teach war strategy and was dubbed the "King's Game".[122] Gentlemen are "to be meanly seene in the play at Chestes", says the overview at the beginning of Baldassare Castiglione's The Book of the Courtier (1528, English 1561 by Sir Thomas Hoby), but chess should not be a gentleman's main passion. Castiglione explains it further:
And what say you to the game at chestes? It is truely an honest kynde of enterteynmente and wittie, quoth Syr Friderick. But me think it hath a fault, whiche is, that a man may be to couning at it, for who ever will be excellent in the playe of chestes, I beleave he must beestowe much tyme about it, and applie it with so much study, that a man may assoone learne some noble scyence, or compase any other matter of importaunce, and yet in the ende in beestowing all that laboure, he knoweth no more but a game. Therfore in this I beleave there happeneth a very rare thing, namely, that the meane is more commendable, then the excellency.[123]
Some of the elaborate chess sets used by the aristocracy at least partially survive, such as the Lewis chessmen.
The knyght ought to be made alle armed upon an hors in suche wyse that he haue an helme on his heed and a spere in his ryght hande/ and coueryd wyth his sheld/ a swerde and a mace on his lyft syde/ Cladd wyth an hawberk and plates to fore his breste/ legge harnoys on his legges/ Spores on his heelis on his handes his gauntelettes/ his hors well broken and taught and apte to bataylle and couerid with his armes/ whan the knyghtes ben maad they ben bayned or bathed/ that is the signe that they shold lede a newe lyf and newe maners/ also they wake alle the nyght in prayers and orysons vnto god that he wylle gyue hem grace that they may gete that thynge that they may not gete by nature/ The kynge or prynce gyrdeth a boute them a swerde in signe/ that they shold abyde and kepe hym of whom they take theyr dispenses and dignyte.[127]
During the Age of Enlightenment, chess was viewed as a means of self-improvement. Benjamin Franklin, in his article "The Morals of Chess" (1750), wrote:
The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:
I. Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...
III. Caution, not to make our moves too hastily ...[132]
Chess was occasionally criticized in the 19th century as a waste of time.[133][134]
Chess is taught to children in schools around the world today. Many schools host chess clubs, and there are many scholastic tournaments specifically for children. Tournaments are held regularly in many countries, hosted by organizations such as the United States Chess Federation and the National Scholastic Chess Foundation.[135]
Chess is many times depicted in the arts; significant works where chess plays a key role range from Thomas Middleton's A Game at Chess to Through the Looking-Glass by Lewis Carroll, to Vladimir Nabokov's The Defense, to The Royal Game by Stefan Zweig. Chess has also featured in film classics such as Ingmar Bergman's The Seventh Seal, Satyajit Ray's The Chess Players, and Powell and Pressburger's A Matter of Life and Death.
Chess is also present in contemporary popular culture. For example, the characters in Star Trek play a futuristic version of the game called "Federation Tri-Dimensional Chess"[136] and "Wizard's Chess" is played in J.K. Rowling's Harry Potter.[137]
The game structure and nature of chess are related to several branches of mathematics. Many combinatorical and topological problems connected to chess, such as the knight's tour and the eight queens puzzle, have been known for hundreds of years.
The number of legal positions in chess is estimated to be 4.59 (+/- 0.38) x1044 with a 95% confidence level,[138] with a game-tree complexity of approximately 10123. The game-tree complexity of chess was first calculated by Claude Shannon as 10120, a number known as the Shannon number.[139] An average position typically has thirty to forty possible moves, but there may be as few as zero (in the case of checkmate or stalemate) or (in a constructed position) as many as 218.[140]
In 1913, Ernst Zermelo used chess as a basis for his theory of game strategies, which is considered one of the predecessors of game theory.[141] Zermelo's theorem states that it is possible to solve chess, i.e. to determine with certainty the outcome of a perfectly played game (either White can force a win, or Black can force a win, or both sides can force at least a draw).[142] However, with 1043 legal positions in chess, it will take an impossibly long time to compute a perfect strategy with any feasible technology.[143]
There is an extensive scientific literature on chess psychology.[note 6][145][146][147][148] Alfred Binet and others showed that knowledge and verbal, rather than visuospatial, ability lies at the core of expertise.[149][150] In his doctoral thesis, Adriaan de Groot showed that chess masters can rapidly perceive the key features of a position.[151] According to de Groot, this perception, made possible by years of practice and study, is more important than the sheer ability to anticipate moves. De Groot showed that chess masters can memorize positions shown for a few seconds almost perfectly. The ability to memorize does not alone account for chess-playing skill, since masters and novices, when faced with random arrangements of chess pieces, had equivalent recall (about six positions in each case). Rather, it is the ability to recognize patterns, which are then memorized, which distinguished the skilled players from the novices. When the positions of the pieces were taken from an actual game, the masters had almost total positional recall.[152]
More recent research has focused on chess as mental training; the respective roles of knowledge and look-ahead search; brain imaging studies of chess masters and novices; blindfold chess; the role of personality and intelligence in chess skill; gender differences; and computational models of chess expertise. The role of practice and talent in the development of chess and other domains of expertise has led to much empirical investigation. Ericsson and colleagues have argued that deliberate practice is sufficient for reaching high levels of expertise in chess.[153] Recent research, however, fails to replicate their results and indicates that factors other than practice are also important.[154][155]
For example, Fernand Gobet and colleagues have shown that stronger players started playing chess at a young age and that experts born in the Northern Hemisphere are more likely to have been born in late winter and early spring. Compared to the general population, chess players are more likely to be non-right-handed, though they found no correlation between handedness and skill.[155]
A relationship between chess skill and intelligence has long been discussed in scientific literature as well as in popular culture. Academic studies that investigate the relationship date back at least to 1927.[156] Although one meta-analysis and most children studies find a positive correlation between general cognitive ability and chess skill, adult studies show mixed results.[157][158]
Chess composition is the art of creating chess problems (also called chess compositions). The creator is known as a chess composer.[160] There are many types of chess problems; the two most important are:
Fairy chess is a branch of chess problem composition involving altered rules, such as the use of unconventional pieces or boards, or unusual stipulations such as reflexmates.
Tournaments for composition and solving of chess problems are organized by the World Federation for Chess Composition, which works cooperatively with but independent of FIDE. The WFCC awards titles for composing and solving chess problems.[163]
Online chess is chess that is played over the internet, allowing players to play against each other in real time. This is done through the use of Internet chess servers, which pair up individual players based on their rating using an Elo or similar rating system. Online chess saw a spike in growth during the quarantines of the COVID-19 pandemic.[164][165] This can be attributed to both isolation and the popularity of Netflix miniseries The Queen's Gambit, which was released in October 2020.[164][165] Chess app downloads on the App Store and Google Play Store rose by 63% after the show debuted.[166] Chess.com saw more than twice as many account registrations in November as it had in previous months, and the number of games played monthly on Lichess doubled as well. There was also a demographic shift in players, with female registration on Chess.com shifting from 22% to 27% of new players.[167] Grandmaster Maurice Ashley said "A boom is taking place in chess like we have never seen maybe since the Bobby Fischer days," attributing the growth to an increased desire to do something constructive during the pandemic.[168] USCF Women's Program Director Jennifer Shahade stated that chess works well on the internet, since pieces do not need to be reset and matchmaking is virtually instant.[169]
The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require "thinking" for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of "thinking"; (4) the discrete structure of chess fits well into the digital nature of
modern computers.[173]
With huge databases of past games and high analytical ability, computers can help players to learn chess and prepare for matches. Internet Chess Servers allow people to find and play opponents worldwide. The presence of computers and modern communication tools have raised concerns regarding cheating during games.[182]
There are more than two thousand published chess variants, games with similar but different rules.[183] Most of them are of relatively recent origin.[184] They include:
In the context of chess variants, regular (i.e. FIDE) chess is commonly referred to as Western chess, international chess, orthodox chess, orthochess, and classic chess.[186][187]
You do not have permission to edit this page, for the following reason:
Pages transcluded onto the current version of this page (help):
Chess is a board game between two players. It is sometimes called international chess or Western chess to distinguish it from related games, such as xiangqi (Chinese chess) and shogi (Japanese chess). The current form of the game emerged in Spain and the rest of Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin. Today, chess is one of the world's most popular games, played by millions of people worldwide.
Chess is an abstract strategy game and involves no hidden information. It is played on a chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. The player controlling the white pieces moves first, followed by the player controlling the black pieces. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in "check") and there is no way for it to escape. There are also several ways a game can end in a draw.
Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (the International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion. A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition, and chess in its turn influenced Western culture and art, and has connections with other fields such as mathematics, computer science, and psychology.
One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players and have deeply influenced the development of chess theory.
Chess pieces are divided into two different colored sets. While the sets might not be literally white and black (e.g. the light set may be a yellowish or off-white color, the dark set may be brown or red), they are always referred to as "white" and "black". The players of the sets are referred to as White and Black, respectively. Each set consists of sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. Chess sets come in a wide variety of styles; for competition, the Staunton pattern is preferred.
The game is played on a square board of eight rows (called ranks) and eight columns (called files). By convention, the 64 squares alternate in color and are referred to as light and dark squares; common colors for chessboards are white and brown, or white and dark green.
In competitive games, the piece colors are allocated to players by the organizers; in informal games, the colors are usually decided randomly, for example by a coin toss, or by one player concealing a white pawn in one hand and a black pawn in the other, and having the opponent choose. 
White moves first, after which players alternate turns, moving one piece per turn, except for castling, when two pieces are moved. A piece is moved to either an unoccupied square or one occupied by an opponent's piece, which is captured and removed from play. With the sole exception of en passant, all pieces capture by moving to the square that the opponent's piece occupies. Moving is compulsory; a player may not skip a turn, even when having to move is detrimental.
Each piece has its own way of moving. In the diagrams, the dots mark the squares to which the piece can move if there are no intervening piece(s) of either color (except the knight, which leaps over any intervening pieces). All pieces except the pawn can capture an enemy piece if it is located on a square to which they would be able to move if the square was unoccupied. The squares on which pawns can capture enemy pieces are marked in the diagram with black crosses.
When a king is under immediate attack, it is said to be in check. A move in response to a check is legal only if it results in a position where the king is no longer in check. This can involve capturing the checking piece; interposing a piece between the checking piece and the king (which is possible only if the attacking piece is a queen, rook, or bishop and there is a square between it and the king); or moving the king to a square where it is not under attack. Castling is not a permissible response to a check.[1]
The object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to get it out of check. It is never legal for a player to make a move that puts or leaves the player's own king in check. In casual games, it is common to announce "check" when putting the opponent's king in check, but this is not required by the rules of chess and is not usually done in tournaments.[2]
Once per game, each king can make a move known as castling. Castling consists of moving the king two squares toward a rook of the same color on the same rank, and then placing the rook on the square that the king crossed.
Castling is still permitted if the rook is under attack, or if the rook crosses an attacked square. 
When a pawn makes a two-step advance from its starting position and there is an opponent's pawn on a square next to the destination square on an adjacent file, then the opponent's pawn can capture it en passant ("in passing"), moving to the square the pawn passed over. This can be done only on the turn immediately following the enemy pawn's two-square advance; otherwise, the right to do so is forfeited. For example, in the animated diagram, the black pawn advances two squares from g7 to g5, and the white pawn on f5 can take it en passant on g6 (but only immediately after the black pawn's advance).
When a pawn advances to its eighth rank, as part of the move, it is promoted and must be exchanged for the player's choice of queen, rook, bishop, or knight of the same color. Usually, the pawn is chosen to be promoted to a queen, but in some cases, another piece is chosen; this is called underpromotion. In the animated diagram, the pawn on c7 can be advanced to the eighth rank and be promoted. There is no restriction on the piece promoted to, so it is possible to have more pieces of the same type than at the start of the game (e.g., two or more queens). If the required piece is not available (e.g. a second queen) an inverted rook is sometimes used as a substitute, but this is not recognized in FIDE-sanctioned games.
There are several ways a game can end in a draw:
Time is controlled using a chess clock that has two displays, one for each player's remaining time. Analog chess clocks have been largely replaced by digital clocks, which allow for time controls with increments.
Time controls are also enforced in correspondence chess competitions. A typical time control is 50 days for every 10 moves.
The pieces are identified by their initials. In English, these are K (king), Q (queen), R (rook), B (bishop), and N (knight; N is used to avoid confusion with king). For example, Qg5 means "queen moves to the g-file, 5th rank" (that is, to the square g5). Different initials may be used for other languages. In chess literature, figurine algebraic notation (FAN) is frequently used to aid understanding independent of language.
To resolve ambiguities, an additional letter or number is added to indicate the file or rank from which the piece moved (e.g. Ngf3 means "knight from the g-file moves to the square f3"; R1e2 means "rook on the first rank moves to e2"). For pawns, no letter initial is used; so e4 means "pawn moves to the square e4".
If the piece makes a capture, "x" is usually inserted before the destination square. Thus Bxf3 means "bishop captures on f3". When a pawn makes a capture, the file from which the pawn departed is used to identify the pawn making the capture, for example, exd5 (pawn on the e-file captures the piece on d5). Ranks may be omitted if unambiguous, for example, exd (pawn on the e-file captures a piece somewhere on the d-file). A minority of publications use ":" to indicate a capture, and some omit the capture symbol altogether. In its most abbreviated form, exd5 may be rendered simply as ed. An en passant capture may optionally be marked with the notation "e.p."
For example, one variation of a simple trap known as the Scholar's mate (see animated diagram) can be recorded:
Variants of algebraic notation include long form algebraic, in which both the departure and destination square are indicated; abbreviated algebraic, in which capture signs, check signs, and ranks of pawn captures may be omitted; and Figurine Algebraic Notation, used in chess publications for universal readability regardless of language.
Portable Game Notation (PGN) is a text-based file format for recording chess games, based on short form English algebraic notation with a small amount of markup. PGN files (suffix .pgn) can be processed by most chess software, as well as being easily readable by humans.
Until about 1980, the majority of English language chess publications used descriptive notation, in which files are identified by the initial letter of the piece that occupies the first rank at the beginning of the game. In descriptive notation, the common opening move 1.e4 is rendered as "1.P-K4" ("pawn to king four"). Another system is ICCF numeric notation, recognized by the International Correspondence Chess Federation though its use is in decline.
In competitive games, players are normally required to keep a score (record of the game). For this purpose, only algebraic notation is recognized in FIDE-sanctioned events; game scores recorded in a different notation system may not be used as evidence in the event of a dispute.
Contemporary chess is an organized sport with structured international and national leagues, tournaments, and congresses. Thousands of chess tournaments, matches, and festivals are held around the world every year catering to players of all levels.
The term "match" refers not to an individual game, but to either a series of games between two players, or a team competition in which each player of one team plays one game against a player of the other team.
FIDE's most visible activity is organizing the World Chess Championship, a role it assumed in 1948. The current World Champion is Magnus Carlsen of Norway.[11] The reigning Women's World Champion is Ju Wenjun from China.[12]
Other competitions for individuals include the World Junior Chess Championship, the European Individual Chess Championship, the tournaments for the World Championship qualification cycle, and the various national championships. Invitation-only tournaments regularly attract the world's strongest players. Examples include Spain's Linares event, Monte Carlo's Melody Amber tournament, the Dortmund Sparkassen meeting, Sofia's M-tel Masters, and Wijk aan Zee's Tata Steel tournament.
Regular team chess events include the Chess Olympiad and the European Team Chess Championship.
The World Chess Solving Championship and World Correspondence Chess Championships include both team and individual events; these are held independently of FIDE.
In order to rank players, FIDE, ICCF, and most national chess organizations use the Elo rating system developed by Arpad Elo. An average club player has a rating of about 1500; the highest FIDE rating of all time, 2882, was achieved by Magnus Carlsen on the March 2014 FIDE rating list.[13]
The above titles are open to both men and women. There are also separate women-only titles; Woman Grandmaster (WGM), Woman International Master (WIM), Woman FIDE Master (WFM) and Woman Candidate Master (WCM). These require a performance level approximately 200 Elo rating points below the similarly named open titles, and their continued existence has sometimes been controversial. Beginning with Nona Gaprindashvili in 1978, a number of women have earned the open GM title.[note 2]
FIDE also awards titles for arbiters and trainers.[16][17] International titles are also awarded to composers and solvers of chess problems and to correspondence chess players (by the International Correspondence Chess Federation). National chess organizations may also award titles.
Chess has an extensive literature. In 1913, the chess historian H.J.R. Murray estimated the total number of books, magazines, and chess columns in newspapers to be about 5,000.[18] B.H. Wood estimated the number, as of 1949, to be about 20,000.[19] David Hooper and Kenneth Whyld write that, "Since then there has been a steady increase year by year of the number of new chess publications. No one knows how many have been printed."[19] Significant public chess libraries include the John G. White Chess and Checkers Collection at Cleveland Public Library, with over 32,000 chess books and over 6,000 bound volumes of chess periodicals;[20] and the Chess & Draughts collection at the National Library of the Netherlands, with about 30,000 books.[21]
Chess theory usually divides the game of chess into three phases with different sets of strategies: the opening, typically the first 10 to 20 moves, when players move their pieces to useful positions for the coming battle; the middlegame; and last the endgame, when most of the pieces are gone, kings typically take a more active part in the struggle, and pawn promotion is often decisive.
Opening theory is concerned with finding the best moves in the initial phase of the game. There are dozens of different openings, and hundreds of variants. The Oxford Companion to Chess lists 1,327 named openings and variants.[22]
Endgame theory is concerned with positions where there are only a few pieces left. Theoretics categorize these positions according to the pieces, for example "King and pawn endings" or "Rook versus a minor piece".
Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. This initially gives White the initiative.[25] Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.
Specific plans or strategic themes will often arise from particular groups of openings which result in a specific type of pawn structure. An example is the minority attack, which is the attack of queenside pawns against an opponent who has more pawns on the queenside. The study of openings is therefore connected to the preparation of plans that are typical of the resulting middlegames.[28]
Another important strategic question in the middlegame is whether and how to reduce material and transition into an endgame (i.e. simplify). Minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of a pawn, or sometimes even with a two-pawn advantage.[29]
Chess strategy is concerned with the evaluation of chess positions and with setting up goals and long-term plans for future play. During the evaluation, players must take into account numerous factors such as the value of the pieces on the board, control of the center and centralization, the pawn structure, king safety, and the control of key squares or groups of squares (for example, diagonals, open files, and dark or light squares).
The most basic step in evaluating a position is to count the total value of pieces of both sides.[34] The point values used for this purpose are based on experience; usually, pawns are considered worth one point, knights and bishops about three points each, rooks about five points (the value difference between a rook and a bishop or knight being known as the exchange), and queens about nine points. The king is more valuable than all of the other pieces combined, since its checkmate loses the game. But in practical terms, in the endgame, the king as a fighting piece is generally more powerful than a bishop or knight but less powerful than a rook.[35] These basic values are then modified by other factors like position of the piece (e.g. advanced pawns are usually more valuable than those on their initial squares), coordination between pieces (e.g. a pair of bishops usually coordinate better than a bishop and a knight), or the type of position (e.g. knights are generally better in closed positions with many pawns while bishops are more powerful in open positions).[36]
Another important factor in the evaluation of chess positions is pawn structure (sometimes known as the pawn skeleton): the configuration of pawns on the chessboard.[37] Since pawns are the least mobile of the pieces, pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in pawn structure include isolated, doubled, or backward pawns and holes; once created, they are often permanent. Care must therefore be taken to avoid these weaknesses unless they are compensated by another valuable asset (for example, by the possibility of developing an attack).[38]
The endgame (also end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:[39]
Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions in which one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides, and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to pieces on the board other than kings, such as "rook and pawn versus rook" endgames.
The earliest texts referring to the origins of chess date from the beginning of the 7th century. Three are written in Pahlavi (Middle Persian)[42] and one, the Harshacharita, is in Sanskrit.[43] One of these texts, the Chatrang-namak, represents one of the earliest written accounts of chess. The narrator Bozorgmehr explains that Chatrang, the Pahlavi word for chess, was introduced to Persia by 'Dewasarm, a great ruler of India' during the reign of Khosrow I.[44]
The game of chess was then played and known in all European countries. A famous 13th-century Spanish manuscript covering chess, backgammon, and dice is known as the Libro de los juegos, which is the earliest European treatise on chess as well as being the oldest document on European tables games. The rules were fundamentally similar to those of the Arabic shatranj. The differences were mostly in the use of a checkered board instead of a plain monochrome board used by Arabs and the habit of allowing some or all pawns to make an initial double step. In some regions, the Queen, which had replaced the Wazir, and/or the King could also make an initial two-square leap under some conditions.[63]
At the same time, the intellectual movement of romanticism had had a far-reaching impact on chess, with aesthetics and tactical beauty being held in higher regard than objective soundness and strategic planning. As a result, virtually all games began with the Open Game, and it was considered unsportsmanlike to decline gambits that invited tactical play such as the King's Gambit and the Evans Gambit.[75] This chess philosophy is known as Romantic chess, and a sharp, tactical style consistent with the principles of chess romanticism was predominant until the late 19th century.[76]
As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books, and chess journals appeared. There were correspondence matches between cities; for example, the London Chess Club played against the Edinburgh Chess Club in 1824.[79] Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling, and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.
The first modern chess tournament was organized by Howard Staunton, a leading English chess player, and was held in London in 1851. It was won by the German Adolf Anderssen, who was hailed as the leading chess master. His brilliant, energetic attacking style was typical for the time.[80][81] Sparkling games like Anderssen's Immortal Game and Evergreen Game or Morphy's "Opera Game" were regarded as the highest possible summit of the art of chess.[82]
Deeper insight into the nature of chess came with the American Paul Morphy, an extraordinary chess prodigy. Morphy won against all important competitors (except Staunton, who refused to play), including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks.[83]
Prague-born Wilhelm Steinitz laid the foundations for a scientific approach to the game, the art of breaking a position down into components[84] and preparing correct plans.[85] In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. This win marked a stylistic transition at the highest levels of chess from an attacking, tactical style predominant in the Romantic era to a more positional, strategic style introduced to the chess world by Steinitz. Steinitz lost his crown in 1894 to a much younger player, the German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of any world champion.[86]
After the end of the 19th century, the number of master tournaments and matches held annually quickly grew. The first Olympiad was held in Paris in 1924, and FIDE was founded initially for the purpose of organizing that event. In 1927, the Women's World Chess Championship was established; the first to hold the title was Czech-English master Vera Menchik.[87]
After the death of Alekhine, a new World Champion was sought. FIDE, which has controlled the title since then, ran a tournament of elite players. The winner of the 1948 tournament was Russian Mikhail Botvinnik.
In 1950 FIDE established a system of titles, conferring the titles of Grandmaster and International Master on 27 players. (Some sources state that in 1914 the title of chess Grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch, and Marshall, but this is a disputed claim.[note 5])
Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes.[99] In the 1984 World Chess Championship, Karpov faced his toughest challenge to date, the young Garry Kasparov from Baku, Soviet Azerbaijan. The match was aborted in controversial circumstances after 5 months and 48 games with Karpov leading by 5 wins to 3, but evidently exhausted; many commentators believed Kasparov, who had won the last two games, would have won the match had it continued. Kasparov won the 1985 rematch. Kasparov and Karpov contested three further closely fought matches in 1986, 1987 and 1990, Kasparov winning them all.[100] Kasparov became the dominant figure of world chess from the mid 1980s until his retirement from competition in 2005.
Chess-playing computer programs (later known as chess engines) began to appear in the 1960s. In 1970, the first major computer chess tournament, the North American Computer Chess Championship, was held, followed in 1974 by the first World Computer Chess Championship. In the late 1970s, dedicated home chess computers such as Fidelity Electronics' Chess Challenger became commercially available, as well as software to run on home computers. However, the overall standard of computer chess was low until the 1990s.
The first endgame tablebases, which provided perfect play for relatively simple endgames such as king and rook versus king and bishop, appeared in the late 1970s. This set a precedent to the complete six- and seven-piece tablebases that became available in the 2000s and 2010s respectively.[101]
The first commercial chess database, a collection of chess games searchable by move and position, was introduced by the German company ChessBase in 1987. Databases containing millions of chess games have since had a profound effect on opening theory and other areas of chess research.
Digital chess clocks were invented in 1973, though they did not become commonplace until the 1990s. Digital clocks allow for time controls involving increments and delays.
The Internet enabled online chess as a new medium of playing, with chess servers allowing users to play other people from different parts of the world in real time. The first such server, known as Internet Chess Server or ICS, was developed at the University of Utah in 1992. ICS formed the basis for the first commercial chess server, the Internet Chess Club, which was launched in 1995, and for other early chess servers such as FICS (Free Internet Chess Server). Since then, many other platforms have appeared, and online chess began to rival over-the-board chess in popularity.[102][103] During the 2020 COVID-19 pandemic, the isolation ensuing from quarantines imposed in many places around the world, combined with the success of the popular Netflix show The Queen's Gambit and other factors such as the popularity of online tournaments (notably PogChamps) and chess Twitch streamers, resulted in a surge of popularity not only for online chess, but for the game of chess in general; this phenomenon has been referred to in the media as the 2020 online chess boom.[104][105]
As endgame tablebases developed, they began to provide perfect play in endgame positions in which the game-theoretical outcome was previously unknown, such as positions with king, queen and pawn against king and queen. In 1991, Lewis Stiller published a tablebase for select six-piece endgames,[108][109] and by 2005, following the publication of Nalimov tablebases, all six-piece endgame positions were solved. In 2012, Lomonosov tablebases were published which solved all seven-piece endgame positions.[110] Use of tablebases enhances the performance of chess engines by providing definitive results in some branches of analysis.
Technological progress made in the 1990s and the 21st century has influenced the way that chess is studied at all levels, as well as the state of chess as a spectator sport.
Previously, preparation at the professional level required an extensive chess library and several subscriptions to publications such as Chess Informant to keep up with opening developments and study opponents' games. Today, preparation at the professional level involves the use of databases containing millions of games, and engines to analyze different opening variations and prepare novelties.[111] A number of online learning resources are also available for players of all levels, such as online courses, tactics trainers, and video lessons.[112]
Organized chess even for young children has become common. FIDE holds world championships for age levels down to 8 years old. The largest tournaments, in number of players, are those held for children.[116]
The number of grandmasters and other chess professionals has also grown in the modern era. Kenneth Regan and Guy Haworth conducted research involving comparison of move choices by players of different levels and from different periods with the analysis of strong chess engines; they concluded that the increase in the number of grandmasters and higher Elo ratings of the top players reflect an actual increase in the average standard of play, rather than "rating inflation" or "title inflation".[117]
In 1993, Garry Kasparov and Nigel Short broke ties with FIDE to organize their own match for the title and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Championships and respective World Champions: the PCA or "classical" champions extending the Steinitzian tradition in which the current champion plays a challenger in a series of games, and the other following FIDE's new format of many players competing in a large knockout tournament to determine the champion. Kasparov lost his PCA title in 2000 to Vladimir Kramnik of Russia.[118] Due to the complicated state of world chess politics and difficulties obtaining commercial sponsorships, Kasparov was never able to challenge for the title again. Despite this, he continued to dominate in top level tournaments and remained the world's highest rated player until his retirement from competitive chess in 2005.
The World Chess Championship 2006, in which Kramnik beat the FIDE World Champion Veselin Topalov, reunified the titles and made Kramnik the undisputed World Chess Champion.[119] In September 2007, he lost the title to Viswanathan Anand of India, who won the championship tournament in Mexico City. Anand defended his title in the revenge match of 2008,[120] 2010 and 2012. In 2013, Magnus Carlsen of Norway beat Anand in the 2013 World Chess Championship.[121] He defended his title 4 times since then and is the reigning world champion.
In the Middle Ages and during the Renaissance, chess was a part of noble culture; it was used to teach war strategy and was dubbed the "King's Game".[122] Gentlemen are "to be meanly seene in the play at Chestes", says the overview at the beginning of Baldassare Castiglione's The Book of the Courtier (1528, English 1561 by Sir Thomas Hoby), but chess should not be a gentleman's main passion. Castiglione explains it further:
And what say you to the game at chestes? It is truely an honest kynde of enterteynmente and wittie, quoth Syr Friderick. But me think it hath a fault, whiche is, that a man may be to couning at it, for who ever will be excellent in the playe of chestes, I beleave he must beestowe much tyme about it, and applie it with so much study, that a man may assoone learne some noble scyence, or compase any other matter of importaunce, and yet in the ende in beestowing all that laboure, he knoweth no more but a game. Therfore in this I beleave there happeneth a very rare thing, namely, that the meane is more commendable, then the excellency.[123]
Some of the elaborate chess sets used by the aristocracy at least partially survive, such as the Lewis chessmen.
The knyght ought to be made alle armed upon an hors in suche wyse that he haue an helme on his heed and a spere in his ryght hande/ and coueryd wyth his sheld/ a swerde and a mace on his lyft syde/ Cladd wyth an hawberk and plates to fore his breste/ legge harnoys on his legges/ Spores on his heelis on his handes his gauntelettes/ his hors well broken and taught and apte to bataylle and couerid with his armes/ whan the knyghtes ben maad they ben bayned or bathed/ that is the signe that they shold lede a newe lyf and newe maners/ also they wake alle the nyght in prayers and orysons vnto god that he wylle gyue hem grace that they may gete that thynge that they may not gete by nature/ The kynge or prynce gyrdeth a boute them a swerde in signe/ that they shold abyde and kepe hym of whom they take theyr dispenses and dignyte.[127]
During the Age of Enlightenment, chess was viewed as a means of self-improvement. Benjamin Franklin, in his article "The Morals of Chess" (1750), wrote:
The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:
I. Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...
III. Caution, not to make our moves too hastily ...[132]
Chess was occasionally criticized in the 19th century as a waste of time.[133][134]
Chess is taught to children in schools around the world today. Many schools host chess clubs, and there are many scholastic tournaments specifically for children. Tournaments are held regularly in many countries, hosted by organizations such as the United States Chess Federation and the National Scholastic Chess Foundation.[135]
Chess is many times depicted in the arts; significant works where chess plays a key role range from Thomas Middleton's A Game at Chess to Through the Looking-Glass by Lewis Carroll, to Vladimir Nabokov's The Defense, to The Royal Game by Stefan Zweig. Chess has also featured in film classics such as Ingmar Bergman's The Seventh Seal, Satyajit Ray's The Chess Players, and Powell and Pressburger's A Matter of Life and Death.
Chess is also present in contemporary popular culture. For example, the characters in Star Trek play a futuristic version of the game called "Federation Tri-Dimensional Chess"[136] and "Wizard's Chess" is played in J.K. Rowling's Harry Potter.[137]
The game structure and nature of chess are related to several branches of mathematics. Many combinatorical and topological problems connected to chess, such as the knight's tour and the eight queens puzzle, have been known for hundreds of years.
The number of legal positions in chess is estimated to be 4.59 (+/- 0.38) x1044 with a 95% confidence level,[138] with a game-tree complexity of approximately 10123. The game-tree complexity of chess was first calculated by Claude Shannon as 10120, a number known as the Shannon number.[139] An average position typically has thirty to forty possible moves, but there may be as few as zero (in the case of checkmate or stalemate) or (in a constructed position) as many as 218.[140]
In 1913, Ernst Zermelo used chess as a basis for his theory of game strategies, which is considered one of the predecessors of game theory.[141] Zermelo's theorem states that it is possible to solve chess, i.e. to determine with certainty the outcome of a perfectly played game (either White can force a win, or Black can force a win, or both sides can force at least a draw).[142] However, with 1043 legal positions in chess, it will take an impossibly long time to compute a perfect strategy with any feasible technology.[143]
There is an extensive scientific literature on chess psychology.[note 6][145][146][147][148] Alfred Binet and others showed that knowledge and verbal, rather than visuospatial, ability lies at the core of expertise.[149][150] In his doctoral thesis, Adriaan de Groot showed that chess masters can rapidly perceive the key features of a position.[151] According to de Groot, this perception, made possible by years of practice and study, is more important than the sheer ability to anticipate moves. De Groot showed that chess masters can memorize positions shown for a few seconds almost perfectly. The ability to memorize does not alone account for chess-playing skill, since masters and novices, when faced with random arrangements of chess pieces, had equivalent recall (about six positions in each case). Rather, it is the ability to recognize patterns, which are then memorized, which distinguished the skilled players from the novices. When the positions of the pieces were taken from an actual game, the masters had almost total positional recall.[152]
More recent research has focused on chess as mental training; the respective roles of knowledge and look-ahead search; brain imaging studies of chess masters and novices; blindfold chess; the role of personality and intelligence in chess skill; gender differences; and computational models of chess expertise. The role of practice and talent in the development of chess and other domains of expertise has led to much empirical investigation. Ericsson and colleagues have argued that deliberate practice is sufficient for reaching high levels of expertise in chess.[153] Recent research, however, fails to replicate their results and indicates that factors other than practice are also important.[154][155]
For example, Fernand Gobet and colleagues have shown that stronger players started playing chess at a young age and that experts born in the Northern Hemisphere are more likely to have been born in late winter and early spring. Compared to the general population, chess players are more likely to be non-right-handed, though they found no correlation between handedness and skill.[155]
A relationship between chess skill and intelligence has long been discussed in scientific literature as well as in popular culture. Academic studies that investigate the relationship date back at least to 1927.[156] Although one meta-analysis and most children studies find a positive correlation between general cognitive ability and chess skill, adult studies show mixed results.[157][158]
Chess composition is the art of creating chess problems (also called chess compositions). The creator is known as a chess composer.[160] There are many types of chess problems; the two most important are:
Fairy chess is a branch of chess problem composition involving altered rules, such as the use of unconventional pieces or boards, or unusual stipulations such as reflexmates.
Tournaments for composition and solving of chess problems are organized by the World Federation for Chess Composition, which works cooperatively with but independent of FIDE. The WFCC awards titles for composing and solving chess problems.[163]
Online chess is chess that is played over the internet, allowing players to play against each other in real time. This is done through the use of Internet chess servers, which pair up individual players based on their rating using an Elo or similar rating system. Online chess saw a spike in growth during the quarantines of the COVID-19 pandemic.[164][165] This can be attributed to both isolation and the popularity of Netflix miniseries The Queen's Gambit, which was released in October 2020.[164][165] Chess app downloads on the App Store and Google Play Store rose by 63% after the show debuted.[166] Chess.com saw more than twice as many account registrations in November as it had in previous months, and the number of games played monthly on Lichess doubled as well. There was also a demographic shift in players, with female registration on Chess.com shifting from 22% to 27% of new players.[167] Grandmaster Maurice Ashley said "A boom is taking place in chess like we have never seen maybe since the Bobby Fischer days," attributing the growth to an increased desire to do something constructive during the pandemic.[168] USCF Women's Program Director Jennifer Shahade stated that chess works well on the internet, since pieces do not need to be reset and matchmaking is virtually instant.[169]
The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require "thinking" for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of "thinking"; (4) the discrete structure of chess fits well into the digital nature of
modern computers.[173]
With huge databases of past games and high analytical ability, computers can help players to learn chess and prepare for matches. Internet Chess Servers allow people to find and play opponents worldwide. The presence of computers and modern communication tools have raised concerns regarding cheating during games.[182]
There are more than two thousand published chess variants, games with similar but different rules.[183] Most of them are of relatively recent origin.[184] They include:
In the context of chess variants, regular (i.e. FIDE) chess is commonly referred to as Western chess, international chess, orthodox chess, orthochess, and classic chess.[186][187]
You do not have permission to edit this page, for the following reason:
Pages transcluded onto the current version of this page (help):
In some circumstances, pages may need to be protected from modification by certain groups of editors. Pages are protected when a specific damaging event has been identified that can not be prevented through other means such as a block. Otherwise, Wikipedia is built on the principle that anyone can edit it, and it therefore aims to have as many of its pages as possible open for public editing so that anyone can add material and correct errors. This policy states in detail the protection types and procedures for page protection and unprotection and when each protection should and should not be applied.
Protection is a technical restriction applied only by administrators, although any user may request protection. Protection can be indefinite or expire after a specified time. The various levels of protection are detailed below, and they can be applied to the page edit, page move, page create, and file upload actions. Even when a page is protected from editing, the source code (wikitext) of the page can still be viewed and copied by anyone.
A protected page is marked at its top right by a padlock icon, usually added by the {{pp-protected}} template.
Applying page protection as a preemptive measure is contrary to the open nature of Wikipedia and is generally not allowed if applied solely for these reasons. However, brief periods of an appropriate and reasonable protection level are allowed in situations where blatant vandalism, disruption, or abuse is occurring by multiple users and at a level of frequency that requires its use in order to stop it. The duration of the protection should be set as short as possible, and the protection level should be set to the lowest restriction needed in order to stop the disruption while still allowing productive editors to make changes.
The following technical options are available to administrators for protecting different actions to pages:
The following technical options are available to administrators for adding protection levels to the different actions to pages:
Any type of protection (with the exception of cascading protection) may be requested at Wikipedia:Requests for page protection. Changes to a protected page should be proposed on the corresponding talk page, and then (if necessary) requested by adding an edit request. From there, if the requested changes are uncontroversial or if there is consensus for them, the changes can be carried out by a user who can edit the page.
Except in the case of office actions (see below), Arbitration Committee remedies, or pages in the MediaWiki namespace (see below), administrators may unprotect a page if the reason for its protection no longer applies, a reasonable period has elapsed, and there is no consensus that continued protection is necessary. Editors desiring the unprotection of a page should, in the first instance, ask the administrator who applied the protection unless the administrator is inactive or no longer an administrator; thereafter, requests may be made at Requests for unprotection. Note that such requests will normally be declined if the protecting administrator is active and was not consulted first. A log of protections and unprotections is available at Special:Log/protect.
Semi-protected pages like this page cannot be edited by unregistered users (IP addresses), as well as accounts that are not confirmed or autoconfirmed (accounts that are at least four days old and have made at least ten edits to Wikipedia). Semi-protection is useful when there is a significant amount of disruption or vandalism from new or unregistered users, or to prevent sockpuppets of blocked or banned users from editing, especially when it occurs on biographies of living persons who have had a recent high level of media interest. An alternative to semi-protection is pending changes, which is sometimes favored when an article is being vandalized regularly, but otherwise receives a low amount of editing.
Such users can request edits to a semi-protected page by proposing them on its talk page, using the {{Edit semi-protected}} template if necessary to gain attention. If the page in question and its talk page are both protected, the edit request should be made at Wikipedia:Requests for page protection instead. New users may also request the confirmed user right at Wikipedia:Requests for permissions/Confirmed.
Administrators may apply indefinite semi-protection to pages that are subject to heavy and persistent vandalism or violations of content policy (such as biographies of living persons, neutral point of view). Semi-protection should not be used as a preemptive measure against vandalism that has not yet occurred or to privilege registered users over unregistered users in (valid) content disputes.
In addition, administrators may apply temporary semi-protection on pages that are:
Today's featured article may be semi-protected just like any other article. However since the article is subject to sudden spurts of vandalism during certain times of day, administrators should semi-protect it for brief periods of time in most instances. For the former guideline, see Wikipedia:Main Page featured article protection.
When a page under pending changes protection is edited by an unregistered (IP addresses) editor or a new user, the edit is not directly visible to the majority of Wikipedia readers, until it is reviewed and accepted by an editor with the pending changes reviewer right. When a page under pending changes protection is edited by an autoconfirmed user, the edit will be immediately visible to Wikipedia readers, unless there are pending edits waiting to be reviewed.
Pending changes are visible in the page history, where they are marked as pending review. Readers that are not logged in (the vast majority of readers) are shown the latest accepted version of the page; logged-in users see the latest version of the page, with all changes (reviewed or not) applied. When editors who are not reviewers make changes to an article with unreviewed pending changes, their edits are also marked as pending and are not visible to most readers.
A user who clicks "edit this page" is always, at that point, shown the latest version of the page for editing regardless of whether the user is logged in or not.
Reviewing of pending changes should be resolved within reasonable time limits.
Pending changes protection should not be used as a preemptive measure against violations that have not yet occurred. Like semi-protection, PC protection should never be used in genuine content disputes, where there is a risk of placing a particular group of editors (unregistered users) at a disadvantage. Pending changes protection should not be used on articles with a very high edit rate, even if they meet the aforementioned criteria. Instead, semi-protection should be considered.
In addition, administrators may apply temporary pending changes protection on pages that are subject to significant but temporary vandalism or disruption (for example, due to media attention) when blocking individual users is not a feasible option. As with other forms of protection, the time frame of the protection should be proportional to the problem. Indefinite PC protection should be used only in cases of severe long-term disruption.
Removal of pending changes protection can be requested of any administrator, or at requests for unprotection.
The reviewing process is described in detail at Wikipedia:Reviewing pending changes.
Administrators can prevent the creation of pages. This type of protection is useful for pages that have been deleted but repeatedly recreated. Such protection is case-sensitive. There are several levels of creation protection that can be applied to pages, identical to the levels for edit protection. A list of protected titles may be found at Special:ProtectedTitles (see also historical lists).
Pre-emptive restrictions on new article titles are instituted through the title blacklist system, which allows for more flexible protection with support for substrings and regular expressions.
Pages that have been creation-protected are sometimes referred to as "salted". Editors wishing to re-create a salted title with appropriate content should either contact an administrator (preferably the protecting administrator), file a request at Wikipedia:Requests for page protection#Current requests for reduction in protection level, or use the deletion review process. To make a convincing case for re-creation, it is helpful to show a draft version of the intended article when filing a request.
While creation-protection is usually permanent, temporary creation protection may be applied if a page is repeatedly recreated by a single user (or sockpuppets of that user, if applicable).
Move-protected pages, or more technically, fully move-protected pages, cannot be moved to a new title except by an administrator. Move protection is commonly applied to:
As with full edit protection, protection because of edit warring should not be considered an endorsement of the current name. When move protection is applied during a requested move discussion, the page should be protected at the location it was at when the move request was started.
All files are implicitly move-protected; only file movers and administrators can rename files.
Upload-protected files, or more technically, fully upload-protected files, cannot be replaced with new versions except by an administrator. Upload protection does not protect file pages from editing. It may be applied by an administrator to:
As with full edit protection, administrators should avoid favoring one version over another, and protection should not be considered an endorsement of the current version. An exception to this rule is when they are protected due to upload vandalism.
Extended confirmed protection, also known as 30/500 protection, allows edits only by editors with the extended confirmed user access level, granted automatically to registered users with at least 30 days tenure and at least 500 edits.
Where semi-protection has proven to be ineffective, administrators may use extended confirmed protection to combat disruption (such as vandalism, abusive sockpuppetry, edit wars, etc.) on any topic.[2] Extended confirmed protection should not be used as a preemptive measure against disruption that has not yet occurred, nor should it be used to privilege extended confirmed users over unregistered/new users in valid content disputes (except as general sanction enforcement; see below).[1]
Two topic areas are under Arbitration Committee "extended confirmed restrictions" as a general sanction, in which only extended confirmed users may edit affected content; one is under a similar community general sanction. The extended confirmed restriction slightly differs from the earlier "30/500 restriction", which was independent of extended confirmed status. Administrators are authorized to enforce this restriction through extended confirmed protection or any other means.[3] It applies to:
When necessary to prevent disruption in designated contentious topic areas, administrators are authorized to make protections at any level. (This is distinct from the topic-wide restrictions discussed above.) Some community sanctions grant similar discretionary authorizations.
High-risk templates may be extended-confirmed protected at administrator discretion when template protection would be too restrictive and semi-protection would be ineffective to stop widespread disruption.[8]
Extended confirmed protection may be applied at the discretion of an administrator when creation-protecting a page.[1]
As of September 23, 2016, a bot posts a notification in a subsection of AN when this protection level is used.[9] Any protection made as arbitration enforcement must be logged at Wikipedia:Arbitration enforcement log. A full list of the 4188 pages under 30/500 protection can be found here.
Users can request edits to an extended confirmed-protected page by proposing them on its talk page, using the {{Edit extended-protected}} template if necessary to gain attention.
A template-protected page can be edited only by administrators or users in the Template editors group. This protection level should be used almost exclusively on high-risk templates and modules. In cases where pages in other namespaces become transcluded to a very high degree, this protection level is also valid.
Editors may request edits to a template-protected page by proposing them on its talk page, using the {{Edit template-protected}} template if necessary to gain attention.
A fully protected page cannot be edited or moved by anyone except administrators. The protection may be for a specified time or may be indefinite.
Modifications to a fully protected page can be proposed on its talk page (or at another appropriate forum) for discussion. Administrators can make changes to the protected article reflecting consensus. Placing the {{Edit fully-protected}} template on the talk page will draw the attention of administrators for implementing uncontroversial changes.
While content disputes and edit warring may be addressed with user blocks issued by uninvolved administrators, allowing normal page editing by other editors at the same time, the protection policy provides an alternative approach as administrators have the discretion to temporarily fully protect an article to end an ongoing edit war. This approach may better suit multi-party disputes and contentious content, as it makes talk page consensus a requirement for implementation of requested edits.
When protecting a page because of a content dispute, administrators have a duty to avoid protecting a version that contains policy-violating content, such as vandalism, copyright violations, defamation, or poor-quality coverage of living people. Administrators are deemed to remain uninvolved when exercising discretion on whether to apply protection to the current version of an article, or to an older, stable, or pre-edit-war version.
Fully protected pages may not be edited except to make changes that are uncontroversial or for which there is clear consensus. Editors convinced that the protected version of an article contains policy-violating content, or that protection has rewarded edit warring or disruption by establishing a contentious revision, may identify a stable version prior to the edit war and request reversion to that version. Before making such a request, editors should consider how independent editors might view the suggestion and recognize that continuing an edit war is grounds for being blocked.
Administrators who have made substantive content changes to an article are considered involved and must not use their advanced permissions to further their own positions. When involved in a dispute, it is almost always wisest to respect the editing policies that bind all editors and call for input from an uninvolved administrator, rather than to invite controversy by acting unilaterally.
If a deleted page is going through deletion review, only administrators are normally capable of viewing the former content of the page. If they feel it would benefit the discussion to allow other users to view the page content, administrators may restore the page, blank it or replace the contents with {{Temporarily undeleted}} template or a similar notice, and fully protect the page to prevent further editing. The previous contents of the page are then accessible to everyone via the page history.
Generic file names such as File:Photo.jpg, File:Example.jpg, File:Map.jpg, and File:Sound.wav are fully protected to prevent new versions from being uploaded. Furthermore, File:Map.jpg and File:Sound.wav are salted.
Cascading protection fully protects a page, and extends that full protection automatically to any page that is transcluded onto the protected page, whether directly or indirectly. This includes templates, images and other media that are hosted on the English Wikipedia. Files stored on Commons are not protected by any other wiki's cascading protection and, if they are to be protected, must be either temporarily uploaded to the English Wikipedia or explicitly protected at Commons (whether manually or through cascading protection there). When operational, KrinkleBot cascade-protects Commons files transcluded at Wikipedia:Main Page/Tomorrow, Wikipedia:Main Page/Commons media protection and Main Page. As the bot's response time varies, media should not be transcluded on the main page (or its constituent templates) until after it has been protected. (This is particularly relevant to Template:In the news, for which upcoming images are not queued at Wikipedia:Main Page/Tomorrow.) Cascading protection:
The list of cascading-protected pages can be found at Wikipedia:Cascade-protected items. Requests to add or remove cascading protection on a page should be made at Wikipedia talk:Cascade-protected items as an edit request.
Administrators cannot change or remove the protection for some areas on Wikipedia, which are permanently protected by the MediaWiki software:
Such protection is called permanent or indefinite protection, and interface protection in the case of CSS and JavaScript pages.
In addition to hard-coded protection, the following are usually fully protected for an indefinite period of time (though not necessarily with interface protection):
Superprotect was a level of protection, allowing editing only by Wikimedia Foundation employees who are in the Staff global group. It was implemented on August 10, 2014, and used the same day to override community consensus regarding the use of the Media Viewer on the German Wikipedia's primary site JavaScript, common.js. It was never used on the English Wikipedia. On November 5, 2015, the WMF decided to remove superprotect from all Wikimedia wikis.
The Gadget and Gadget definition namespaces have namespace-wide protection, and the permissions to edit them are only available to WMF Staff.  There is one page on the English Wikipedia in these namespaces. A request for local access to this namespace has been pending since 2019.
Cascading semi-protection was formerly possible, but it was disabled in 2007 after users noticed that non-administrators could fully protect any page by transcluding it onto the page to which cascading semi-protection had been applied by an administrator.
Modifications to a protected page can be proposed on its talk page (or at another appropriate forum) for discussion. Administrators can make changes to the protected article reflecting consensus. Placing the {{Edit protected}} template on the talk page will draw the attention of administrators for implementing uncontroversial changes.
Talk pages are not usually protected, and are semi-protected only for a limited duration in the most severe cases of vandalism.
User talk pages are rarely protected. However, protection may be applied if there is severe vandalism or abuse. Users whose talk pages are protected may wish to have an unprotected user talk subpage linked conspicuously from their main talk page to allow good-faith comments from users that the protection restricts editing from.
A user's request to have their own talk page protected is not a sufficient rationale by itself to protect the page, although requests may be considered if a reason is provided.
Blocked users' user talk pages should not ordinarily be protected, as this interferes with the user's ability to contest their block through the normal process. It also prevents others from being able to use the talk page to communicate with the blocked editor.
In extreme cases of abuse by the blocked user, such as abuse of the {{unblock}} template, re-blocking the user with talk page access removed should be preferred over applying protection to the page. If the user has been blocked and with the ability to edit their user talk page disabled, they should be informed of this in a block notice, subsequent notice, or message, and it should include information and instructions for appealing their block off-wiki, such as through the UTRS tool interface or, as a last recourse, the Arbitration Committee.
When required, protection should be implemented for only a brief period, not exceeding the duration of the block.
Confirmed socks of registered users should be dealt with in accordance with Wikipedia:Sockpuppetry; their pages are not normally protected.
Base user pages (for example, the page User:Example, and not User:Example/subpage or User talk:Example) are automatically protected from creation or editing by unconfirmed accounts and anonymous IP users. An exception to this includes an unconfirmed registered account attempting to create or edit their own user page. IP editors and unconfirmed accounts are also unable to create or edit user pages that do not belong to a currently-registered account. This protection is enforced by an edit filter.[13] Users may opt-out of this protection by placing {{unlocked userpage}} anywhere on their own user page.
User pages and subpages within their own user space may be protected upon a request from the user, as long as a need exists. Pages within the user space should not be automatically or pre-emptively protected without good reason or cause.[14][15] Requests for protection specifically at uncommon levels (such as template protection) may be granted if the user has expressed a genuine and realistic need.
When a filter is insufficient to stop user page vandalism, a user may choose to create a ".css" subpage (ex. User:Example/Userpage.css), copy all the contents of their user page onto the subpage, transclude the subpage by putting {{User:Example/Userpage.css}} on their user page, and then ask an administrator to fully protect their user page. Because user space pages that end in ".css", ".js", and ".json" are editable only by the user to which that user space belongs (and interface administrators), this will protect one's user page from further vandalism.
In the event of the confirmed death of a user, the user's user page (but not the user talk page) should be fully protected.
Protected templates should normally have the {{documentation}} template. It loads the unprotected /doc page, so that non-admins and IP-users can edit the documentation, categories and interwiki links. It also automatically adds {{pp-template}} to protected templates, which displays a small padlock in the top right corner and categorizes the template as protected. Only manually add {{pp-template}} to protected templates that don't use {{documentation}} (mostly the flag templates).
Cascading protection should generally not be applied directly to templates, as it will not protect transclusions inside <includeonly> tags or transclusions that depend on template parameters, but will protect the template's documentation subpage. Instead, consider any of the following:
Note: All editnotice templates (except those in userspace) are already protected via MediaWiki:Titleblacklist. They can be edited by admins, template editors and page movers only.
Sandboxes should not ordinarily be protected since their purpose is to let new users test and experiment with wiki syntax. Most sandboxes are automatically cleaned every 12 hours, although they are frequently overwritten by other testing users. The Wikipedia:Sandbox is cleaned every hour. Those who use sandboxes for malicious purposes, or to violate policies such as no personal attacks, civility, or copyrights, should instead be warned and/or blocked.
The following templates may be added at the very top of a page to indicate that it is protected:
On redirect pages, use the {{Redirect category shell}} template, which automatically categorizes by protection level, below the redirect line. A protection template may also be added below the redirect line, but it will serve only to categorize the page, as it will not be visible on the page, and it will have to be manually removed when protection is removed.
This image was copied from wikipedia:en. The original description was:
This is  a spoken word version of the Wikipedia article: ChessListen to this article (audio help)
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
This software is provided by The author and contributors "as is" and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. In no event shall The author and contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage.
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
Click on a date/time to view the file as it appeared at that time.
More than 100 pages use this file.
The following list shows the first 100 pages that use this file only.
A full list is available.
The Staunton chess set is the standard style of chess pieces,[1][2] recommended for use in competition by FIDE, the international chess governing body.[3]
One theory of the development of the set is that Cooke had used prestigious architectural concepts, familiar to an expanding class of educated and prosperous gentry. London architects, strongly influenced by the culture of Greece and the culture of ancient Rome, were designing prestigious buildings in the neoclassical style. The appearance of the new chessmen was based on this style. and the pieces were symbols of "respectable" Victorian society:
There were also practical innovations: A crown emblem was stamped onto a rook and knight of each side to identify their positioning on to the king's side of the board. This was because, in descriptive chess notation, the rooks and knights were often designated by being the "queen's knight", the "king's rook", etc.[10]
Another possibility is that Jaques, a master turner, had probably been experimenting with a design that not only would be accepted by players but also could be produced at a reasonable cost. In the end, he most likely borrowed and synthesized elements from sets already available to create a new design that used universally recognizable symbols atop conventional stems and bases: The resulting pieces were compact, well balanced, and weighted to provide an understandable, practical playing set.[10]
It may have been a combination of both theories with the synergy of Cooke the entrepreneur and Jaques the artisan.[10]
From 1820 on, diagrams in chess books began to use icons of chess pieces similar in many respects to the Staunton chessmen, including a change from arched crown to coronet for the queen. This shows that the Staunton design may have been taken from these diagrams, very likely created by a printer.[11]
A modified Staunton chess set, described in the FIDE Laws of Chess,[12] is used for the blind and visually impaired. In such a set, the black pieces are differentiated from the white pieces by a small point/spike atop the pieces, and the dark squares are raised above the light squares to allow the user to feel the board and pieces to understand the position. Furthermore, each square has a hole into which pegs on the bottom of each piece are inserted, allowing the user to feel the position of the pieces without moving them or knocking them over.
Cooke was the editor for the Illustrated London News, where Staunton published chess articles. He convinced the champion to endorse the chess set.
The Staunton set obtained the stamp of approval of FIDE, the World Chess Federation, when in 1924 it was selected as their choice of set, for use in all future international chess tournaments.
There are 17 recognized variants derived from the original 1849 Staunton chess set, classified as follows:[13]
Wooden Staunton chess sets were often turned on a lathe, then non-circular details were added by hand; the knights were made in two parts (head and base) which were stuck together with adhesive.
Even among sets of the standard Staunton pattern, the style of the pieces varies.  The knights vary considerably. Some examples are shown below.
In algebraic notation, the king is abbreviated by the letter K among English speakers. The white king starts the game on e1; the black king starts on e8. Unlike all other pieces, only one king per player can be on the board at any time, and the kings are never removed from the board during the game.
The white king starts on e1, on the first rank to the right of the queen from White's perspective. The black king starts on e8, directly across from the white king. Each king starts on a square opposite its own color.
A king can move one square horizontally, vertically, or diagonally unless the square is already occupied by a friendly piece or the move would place the king in check. If the square is occupied by an undefended enemy piece, the king may capture it, removing it from play. Opposing kings may never occupy adjacent squares (see opposition) to give check, as that would put the moving king in check as well. The king can give discovered check, however, by unblocking a bishop, rook, or queen.
The king can make a special move, in conjunction with a rook of the same color, called castling. When castling, the king moves two squares horizontally toward one of its rooks, and that rook is placed on the square over which the king crossed.
Castling with the h-file rook is known as castling kingside or short castling (denoted 0-0 in algebraic notation), while castling with the a-file rook is known as castling queenside or long castling (denoted 0-0-0).
A king that is under attack is said to be in check, and the player in check must immediately remedy the situation. There are three possible ways to remove the king from check:
If none of the three options are available, the player's king has been checkmated, and the player loses the game.
At amateur levels, when placing the opponent's king in check, it is common to announce "check", but this is not required by the rules of chess.
A stalemate occurs when a player, on their turn, has no legal moves, and the player's king is not in check.
If this happens, the king is said to have been stalemated, and the game ends in a draw. A player who has very little or no chance of winning will often, in order to avoid a loss, try to entice the opponent to inadvertently place the player's king in stalemate (see swindle).
The king's predecessor is the piece of the same name in shatranj. Like the modern king, it is the most important piece in the game and can move to any neighboring square. However, in shatranj, baring the king is a win unless the opponent can do the same immediately afterward; stalemating the king is a win; and castling does not exist.
In the opening and middlegame, the king will rarely play an active role in the development of an offensive or defensive position. Instead, a player will normally try to castle and seek safety on the edge of the board behind friendly pawns. In the endgame, however, the king emerges to play an active role as an offensive piece, and can assist in the promotion of the player's remaining pawns.
It is not meaningful to assign a value to the king relative to the other pieces, as it cannot be captured or exchanged and must be protected at all costs. In this sense, its value could be considered infinite. As an assessment of the king's capability as an offensive piece in the endgame, it is often considered to be slightly stronger than a bishop or knight. Emanuel Lasker gave it the value of a knight plus a pawn (i.e. four points on the scale of chess piece relative value),[1] though some other theorists evaluate it closer to three points. It is better at defending friendly pawns than the knight is, and it is better at attacking enemy pawns than the bishop is.[2]
The white rooks start on the squares a1 and h1, while the black rooks start on a8 and h8. The rook moves horizontally or vertically, through any number of unoccupied squares. The rook cannot jump over pieces. The rook may capture an enemy piece by moving to the square on which the enemy piece stands, removing it from play. The rook also participates with the king in a special move called castling, wherein it is transferred to the square crossed by the king after the king is shifted two squares toward the rook.
The rook is worth about five pawns. In general, rooks are stronger than bishops or knights  and are considered greater in value than either of those pieces by nearly two pawns, but less valuable than two minor pieces by approximately a pawn. Two rooks are generally considered to be worth slightly more than a queen (see chess piece relative value).[4] Winning a rook for a bishop or knight is referred to as winning the exchange. Rooks and queens are called major pieces or heavy pieces, as opposed to bishops and knights, the minor pieces.[5]
In the opening, the rooks are blocked in by other pieces and cannot immediately participate in the game, so it is usually desirable to connect one's rooks on the first rank by castling and then clearing all pieces except the king and rooks from the first rank. In that position, the rooks support each other and can more easily move to occupy and control the most favorable files.
A rook on the seventh rank (the opponent's second rank) is typically very powerful, as it threatens the opponent's unadvanced pawns and hems in the enemy king. A rook on the seventh rank is often considered sufficient compensation for a pawn.[6] In the diagrammed position from a game between Lev Polugaevsky and Larry Evans,[7] the rook on the seventh rank enables White to draw, despite being a pawn down.[8]
Two rooks on the seventh rank are often enough to force victory by the blind swine mate, or at least a draw by perpetual check.[9]
Rooks are most powerful towards the end of a game (i.e., the endgame), when they can move unobstructed by pawns and control large numbers of squares. They are somewhat clumsy at restraining enemy pawns from advancing towards promotion unless they can occupy the file behind the advancing pawn. As well, a rook best supports a friendly pawn towards promotion from behind it on the same file (see Tarrasch rule).
In a position with a rook and one or two minor pieces versus two rooks, generally in addition to pawns, and possibly other pieces, Lev Alburt advises that the player with the single rook should avoid exchanging the rook for one of his opponent's rooks.[10]
The rook is adept at delivering checkmate. Below are a few examples of rook checkmates that are easy to force. A single rook can force checkmate while a single minor piece cannot.
Persian war-chariots were heavily armored, carrying a driver and at least one ranged-weapon bearer, such as an archer. The sides of the chariot were built to resemble fortified stone work, giving the impression of small, mobile buildings, causing terror on the battlefield.[citation needed]
In Europe the castle or tower appears for the first time in the 16th century in  Vida's 1550 Ludus Scacchia, and then as a tower on the back of an elephant. In time, the elephant disappeared and only the tower was used as the piece.[13]
Rooks usually are similar in appearance to small castles; thus, a rook is sometimes called a "castle",[15] though modern chess literature rarely, if ever, uses this term.[16]
Peter Tyson suggests that there is a correlation between the name of the piece and the word rukh, a mythical giant bird of prey from Persian mythology.[21]
In Canadian heraldry, the chess rook is the cadency mark of a fifth daughter.
The queen can be moved any number of unoccupied squares in a straight line vertically, horizontally, or diagonally, thus combining the moves of the rook and bishop. The queen captures by moving to the square on which an enemy piece stands.
Although both players start with one queen each, a pawn can be promoted to any of several types of pieces, including a queen, when the pawn is moved to the player's furthest rank (the opponent's first rank). Such a queen created by promotion can be an additional queen or, if the player's queen has been captured, a replacement queen. The queen is by far the most common piece type a pawn is promoted to due to the relative power of a queen; promotion to a queen is colloquially called queening.
The queen is typically worth about nine pawns, which is slightly stronger than a rook and a bishop together, but slightly less strong than two rooks, though there are exceptions. It is almost always disadvantageous to exchange the queen for a single piece other than the enemy's queen.
The reason that the queen is stronger than a combination of a rook and bishop, even though they control the same number of squares, is twofold. First, the queen is more mobile than the rook and the bishop, as the entire power of the queen can be transferred to another location in one move, while transferring the entire firepower of a rook and bishop requires two moves, the bishop always being restricted to squares of one color. Second, unlike the bishop, the queen is not hampered by an inability to control squares of the opposite color to the square on which it stands. A factor in favor of the rook and bishop is that they can attack (or defend) a square twice, while a queen can only do so once. However, experience has shown that this factor is usually less significant than the points favoring the queen.[1]
The queen is strongest when the board is open, the enemy king is poorly defended, or there are loose (i.e. undefended) pieces in the enemy camp. Because of its long range and ability to move in multiple directions, the queen is well-equipped to execute forks. Compared to other long range pieces (i.e. rooks and bishops), the queen is less restricted and stronger in closed positions.
A player should generally delay developing the queen, as developing it too quickly can expose it to attacks by enemy pieces, causing the player to lose time removing the queen from danger. Despite this, beginners often develop the queen early in the game, hoping to plunder the enemy position and deliver an early checkmate, such as Scholar's mate.
Early queen attacks are rare in high-level chess, but there are some openings with early queen development that are used by high-level players. For example, the Scandinavian Defense (1.e4 d5), which often features queen moves by Black on the second and third moves, is considered sound and has been played at the world championship level. Some less common examples have also been observed in high-level games. The Danvers Opening (1.e4 e5 2.Qh5), which is widely characterized as a beginner's opening, has occasionally been played by the American grandmaster Hikaru Nakamura.[2]
A queen exchange often marks the beginning of the endgame, but there are queen endgames, and sometimes queens are exchanged in the opening, long before the endgame. A common goal in the endgame is to promote a pawn to a queen. As the queen has the largest range and mobility, queen and king vs. lone king is an easy win when compared to some other basic mates. Queen and king vs. rook and king is also a win for the player with the queen, but it is not easy.
The fers changed into the queen over time. The first surviving mention of this piece as a queen or similar is the Latin regina in the Einsiedeln Poem, a 98-line Medieval Latin poem written around 997 and preserved in a monastery at Einsiedeln in Switzerland. Some surviving early medieval pieces depict the piece as a queen. The word fers became grammatically feminized in several languages, such as alferza in Spanish and fierce or fierge in French.[3] The Carmina Burana also refer to the queen as femina (woman) and coniunx (spouse),[4] and the name Amazon has sometimes been seen.[5]
In Russian, the piece keeps its Persian name of ferz; koroleva (queen) is colloquial and is never used by professional chess players. However, the names korolevna (king's daughter), tsaritsa (tsar's wife), and baba (old woman) are attested as early as 1694.[7] In Arabic countries, the queen remains termed and, in some cases, depicted as a vizier.[8]
Historian Marilyn Yalom proposes several factors that might have been partly responsible for influencing the piece towards its identity as a queen and its power in modern chess: the prominence of medieval queens such as Eleanor of Aquitaine, Blanche of Castile, and more particularly Isabella I of Castile; the cult of the Virgin Mary;[4] the power ascribed to women in the troubadour tradition of courtly love; and the medieval popularity of chess as a game particularly suitable for women to play on equal terms with men.[9] She points to medieval poetry depicting the Virgin as the chess-queen of God or Fierce Dieu.[10]
At various times, the ability of pawns to be queened was restricted while the original queen was still on the board, so as not to cause scandal by providing the king with more than one queen. An early 12th-century Latin poem refers to a queened pawn as a ferzia, as opposed to the original queen or regina, to account for this.[17]
In Russia, for a long time, the queen could also move like a knight; some players disapproved of this ability to "gallop like the horse" (knight).[19][20] The book A History of Chess by H.J.R. Murray,[21] says that William Coxe, who was in Russia in 1772, saw chess played with the queen also moving like a knight. Such an augmented queen piece is now known as the fairy chess piece amazon.
Around 1230, the queen was also independently invented as a piece in Japan, where it formed part of the game of dai shogi. The piece was retained in the smaller and more popular chu shogi, but does not form a part of modern shogi.
Individual pawns are referred to by the file on which they stand. For example, one speaks of "White's f-pawn" or "Black's b-pawn". Alternatively, they can be referred to by the piece which stood on that file at the beginning of the game, e.g. "White's king bishop's pawn" or "Black's queen knight's pawn". It is also common to refer to a rook's pawn, meaning any pawn on the a- or h-files, a knight's pawn (on the b- or g-files), a bishop's pawn (on the c- or f-files), a queen's pawn (on the d-file), a king's pawn (on the e-file), and a central pawn (on the d- or e-files).
The pawn historically represents soldiers or infantry, or more particularly, armed peasants or pikemen.[1]
Each player begins the game with eight pawns placed along their second rank.
A pawn may move by vertically advancing to a vacant square ahead. The first time a pawn moves, it has the additional option of vertically advancing two squares, provided that both squares are vacant. Unlike other pieces, the pawn can only move forwards. In the second diagram, the pawn on c4 can move to c5; the pawn on e2 can move to either e3 or e4.
Unlike other pieces, the pawn does not capture in the same way that it moves. A pawn captures by moving diagonally forward one square to the left or right (see diagram), either replacing an enemy piece on its square or capturing en passant.
An en passant capture can occur after a pawn makes a move of two squares and the square it passes over is attacked by an enemy pawn. The enemy pawn is entitled to capture the moved pawn "in passing" as if the latter had advanced only one square. The capturing pawn moves to the square over which the moved pawn passed (see diagram), and the moved pawn is removed from the board. The option to capture the moved pawn en passant must be exercised on the move immediately following the double-step pawn advance, or it is lost for the remainder of the game. The en passant capture is the only capture in chess in which the capturing piece does not replace the captured piece on the same square.[2]
A pawn that advances to its last rank is promoted to a queen, rook, bishop, or knight of the same color. The pawn is replaced by the new piece on the same move. The choice of promotion is not limited to pieces that have been captured; thus, a player could, in theory, have as many as nine queens, ten rooks, ten bishops, or ten knights on the board. Promotion to a queen is also known as queening and to any other piece as underpromotion. Underpromotion is most often to a knight, typically to execute a checkmate or a fork to gain a significant material advantage, among other reasons. Underpromotion to rook or bishop is used to avoid or induce stalemate or for humorous reasons.
While some chess sets include an extra queen of each color, most standard sets do not come with additional pieces, so the physical piece used to replace a promoted pawn on the board is usually one that was previously captured. In informal games, when the correct piece is not available, an additional queen is often indicated by inverting a previously captured rook or by placing two pawns on the same square. In tournament games, however, this is not acceptable; in the former case, it may result in the arbiter ruling that the upturned piece is in fact a rook.[3]
The pawn structure, the configuration of pawns on the chessboard, mostly determines the strategic flavor of a game. While other pieces can usually be moved to more favorable positions if they are temporarily badly placed, a poorly positioned pawn is limited in its movement and often cannot be so relocated.
Because pawns capture diagonally and can be blocked from moving straight forward, opposing pawns can become locked in diagonal pawn chains of two or more pawns of each color, where each player controls squares of one color. In the diagram, Black and White have locked their d- and e-pawns.
Here, White has a long-term space advantage. White will have an easier time than Black in finding good squares for their pieces, particularly with an eye to the kingside. Black, in contrast, suffers from a bad bishop on c8, which is prevented by the black pawns from finding a good square or helping out on the kingside. On the other hand, White's central pawns are somewhat overextended and vulnerable to attack. Black can undermine the white pawn chain with an immediate ...c5 and perhaps a later ...f6.
Pawns on adjacent files can support each other in attack and defense. A pawn which has no friendly pawns in adjacent files is an isolated pawn. The square in front of an isolated pawn may become an enduring weakness. Any piece placed directly in front not only blocks the advance of that pawn but also cannot be driven away by other pawns.
In the diagram, Black has an isolated pawn on d5. If all the pieces except the kings and pawns were removed, the weakness of that pawn might prove fatal to Black in the endgame. In the middlegame, however, Black has slightly more freedom of movement than White and may be able to trade off the isolated pawn before an endgame ensues.
After a capture with a pawn, a player may end up with two pawns on the same file, called doubled pawns. Doubled pawns are substantially weaker than pawns which are side by side, because they cannot defend each other, they usually cannot both be defended by adjacent pawns, and the front pawn blocks the advance of the back one. In the diagram, Black is playing at a strategic disadvantage due to the doubled c-pawns.
There are situations where doubled pawns confer some advantage, typically when the guarding of consecutive squares in a file by the pawns prevents an invasion by the opponent's pieces.
Pawns which are both doubled and isolated are typically a tangible weakness. A single piece or pawn in front of doubled isolated pawns blocks both of them, and cannot be easily dislodged. It is rare for a player to have three pawns in a file, i.e. tripled pawns.
In chess endgames with a bishop, a rook pawn may be the wrong rook pawn, depending on the square-color of the bishop. This causes some positions to be draws that would otherwise be wins.
The pawn has its origins in the oldest version of chess, chaturanga, and it is present in all other significant versions of the game as well. In chaturanga, this piece could move one square directly forward and could capture one square diagonally forward.
In medieval chess, as an attempt to make the pieces more interesting, each pawn was given the name of a commoner's occupation:[5]
The most famous example of this is found in the second book ever printed in the English language, The Game and Playe of the Chesse. Purportedly, this book, printed by William Caxton,[8] was viewed to be as much a political commentary on society as a chess book.[7]
The ability to move two spaces and the related ability to capture en passant were introduced in 15th-century Europe;[9] the en passant capture spread to various regions throughout its history. The en passant capture intends to prevent a pawn on its initial square from safely bypassing a square controlled by an enemy pawn. The rule for promotion has changed throughout its history.
Outside of the game of chess, "pawn" is often taken to mean "one who is manipulated to serve another's purpose".[11][12] Because the pawn is the weakest piece, it is often used metaphorically to indicate unimportance or outright disposability, only having utility in the ability to be controlled; for example, "She's only a pawn in their game."
Compared to other chess pieces, the knight's movement is unique: it moves two squares vertically and one square horizontally, or two squares horizontally and one square vertically (with both forming the shape of an "L"). When moving, the knight can jump over pieces to reach its destination.[a][b][3] Knights capture in the same way, replacing the enemy piece on the square and removing it from the board. A knight can have up to eight available moves at once. Knights and pawns are the only pieces that can be moved in the chess starting position.[3]
Knights and bishops, also known as minor pieces, have a value of about three pawns.[4] Bishops utilize a longer range, but they can move only to squares of one color. The knight's value increases in closed positions since it can jump over blockades.[5] Knights and bishops are stronger when supported by other pieces (such as pawns) to create outposts and become more powerful when they advance, as long as they remain active.[3] Generally, knights are strongest in the center of the board, where they have up to eight moves, and weakest in a corner, where they have only two.
Compared to a bishop, a knight is often not as good in an endgame. A knight can exert control over only one part of the board at a time and often takes multiple moves to reposition to a new location, which often makes it less suitable in endgames with pawns on both sides of the board. This limitation is less important, however, in endgames with pawns on only one side of the board. Knights are superior to bishops in an endgame if all the pawns are on one side of the board. Furthermore, knights have the advantage of being able to control squares of either color, unlike a lone bishop. Nonetheless, a disadvantage of the knight (compared to the other pieces) is that by itself it cannot lose a move to put the opponent in zugzwang (see triangulation and tempo), while a bishop can. In the position pictured on the right, if the knight is on a white square and it is White's turn to move, White cannot win. Similarly, if the knight were on a black square and it were Black's turn to move, White cannot win. In the other two cases, White would win. If instead of the knight, White had a bishop on either color of square, White would win with either side to move.[7]
In an endgame where one side has only a king and a knight while the other side has only a king, the game is a draw since a checkmate is impossible. When a bare king faces a king and two knights, a checkmate can never be forced; checkmate can occur only if the opponent commits a blunder by moving their king to a square where it can be checkmated on the next move. Checkmate can be forced with a bishop and knight, however, or with two bishops, even though the bishop and knight are in general about equal in value. Paradoxically, checkmate with two knights sometimes can be forced if the weaker side has a single extra pawn, but this is a curiosity of little practical value (see two knights endgame). Pawnless endgames are a rarity, and if the stronger side has even a single pawn, an extra knight should give them an easy win. A bishop can trap (although it cannot then capture) a knight on the rim (see diagram), especially in the endgame.
In algebraic notation, the usual modern way of recording chess games, the letter N stands for the knight (K is reserved for the king); in descriptive chess notation, Kt is sometimes used instead, mainly in older literature. In chess problems and endgame studies, the letter S, standing for Springer, the German name for the piece, is often used (and in some variants of fairy chess, N is used for the nightrider, a popular fairy chess piece).
Pieces similar to the knight are found in almost all games of the chess family. The ma of xiangqi and janggi is slightly more restricted; conceptually, the piece is considered to pass through the adjacent orthogonal point, which must be unoccupied, rather than "jumping". Another related piece is the keima of shogi, which moves like a knight but can move only two squares forward followed by one square sideways, restricting its movement to two possible squares.
The knight is relevant in some mathematical problems. For example, the knight's tour problem is the problem of finding a series of moves by a knight on a chessboard in which every square is visited exactly once.
Even among sets of the standard Staunton pattern, the style of the pieces varies. The knights vary considerably. Here are some examples.
The king's bishop is placed between the king and the king's knight, f1 for White and f8 for Black; the queen's bishop is placed between the queen and the queen's knight, c1 for White and c8 for Black.
The bishop has no restrictions in distance for each move but is limited to diagonal movement. It cannot jump over other pieces. A bishop captures by occupying the square on which an enemy piece stands. As a consequence of its diagonal movement, each bishop always remains on one square color. Due to this, it is common to refer to a bishop as a light-squared or dark-squared bishop. 
A rook is generally worth about two pawns more than a bishop. The bishop has access to only half of the squares on the board, whereas all squares of the board are accessible to the rook. On an empty board, a rook always attacks fourteen squares regardless of position, whereas a bishop attacks no more than thirteen (one of four center squares) and sometimes as few as seven (sides and corners). Also, a king and rook can force checkmate against a lone king, while a king and bishop cannot.[1] However, a king and two bishops on opposite-colored squares can force mate.
Knights and bishops are each worth about three pawns. This means bishops are approximately equal in strength to knights, but depending on the game situation, either may have a distinct advantage.
Less experienced players tend to underrate the bishop compared to the knight because the knight can reach all squares and is more adept at forking. More experienced players understand the power of the bishop.[2]
Bishops usually gain in relative strength towards the endgame as more pieces are captured and more open lines become available on which they can operate. A bishop can easily influence both wings simultaneously, whereas a knight is less capable of doing so. In an open endgame, a pair of bishops is decidedly superior to either a bishop and a knight, or two knights. A player possessing a pair of bishops has a strategic weapon in the form of a long-term threat to trade down to an advantageous endgame.[1]
Two bishops on opposite-colored squares and king can force checkmate against a lone king, whereas two knights cannot. A bishop and knight can force mate, but with far greater difficulty than two bishops.
In certain positions a bishop can by itself lose a move (see triangulation and tempo), while a knight can never do so. The bishop is capable of skewering or pinning a piece, while the knight can do neither. A bishop can in some situations hinder a knight from moving. In these situations, the bishop is said to be "dominating" the knight.
On the other hand, in the opening and middlegame a bishop may be hemmed in by pawns of both players, and thus be inferior to a knight which can jump over them. A knight check cannot be blocked but a bishop check can. Furthermore, on a crowded board a knight has many tactical opportunities to fork two enemy pieces. A bishop can fork, but opportunities are more rare. One such example occurs in the position illustrated, which arises from the Ruy Lopez: 1.e4 e5 2.Nf3 Nc6 3.Bb5 a6 4.Ba4 Nf6 5.0-0 b5 6.Bb3 Be7 7.d4 d6 8.c3 Bg4 9.h3!? Bxf3 10.Qxf3 exd4 11.Qg3 g6 12.Bh6!
In the middlegame, a player with only one bishop should generally place friendly pawns on squares of the color that the bishop cannot move to. This allows the player to control squares of both colors, allows the bishop to move freely among the pawns, and helps fix enemy pawns on squares on which they can be attacked by the bishop. Such a bishop is often referred to as a "good" bishop.
Conversely, a bishop which is impeded by friendly pawns is often referred to as a "bad bishop" (or sometimes, disparagingly, a "tall pawn"). The black light-squared bishop in the French Defense is a notorious example of this concept. A "bad" bishop, however, need not always be a weakness, especially if it is outside its own pawn chains. In addition, having a "bad" bishop may be advantageous in an opposite-colored bishops endgame. Even if the bad bishop is passively placed, it may serve a useful defensive function; a well-known quip from GM Mihai Suba is that "Bad bishops protect good pawns."[3]
An endgame in which each player has only one bishop, one controlling the dark squares and the other the light, will often result in a draw even if one player has a pawn or sometimes two more than the other. The players tend to gain control of squares of opposite colors, and a deadlock results. In endgames with same-colored bishops, however, even a positional advantage may be enough to win.[6]
Endgames in which each player has only one bishop (and no other pieces besides the king) and the bishops are on opposite colors are often drawn, even when one side has an extra pawn or two. Many of these positions would be a win if the bishops were on the same color.
If two pawns are connected, they normally win if they reach their sixth rank, otherwise the game may be a draw (as above). If two pawns are separated by one file they usually draw, but win if they are farther apart.[8]
In an endgame with a bishop, in some cases the bishop is the "wrong bishop", meaning that it is on the wrong color of square for some purpose (usually promoting a pawn). For example, with just a bishop and a rook pawn, if the bishop cannot control the promotion square of the pawn, it is said to be the "wrong bishop" or the pawn is said to be the wrong rook pawn. This results in some positions being drawn (by setting up a fortress) which otherwise would be won.
The canonical chessmen date back to the Staunton chess set of 1849. The piece's deep groove symbolizes a bishop's (or abbot's) mitre. Some have written that the groove originated from the original form of the piece, an elephant[22][23] with the groove representing the elephant's tusks.[24] The English apparently chose to call the piece a bishop because the projections at the top resembled a mitre.[25] This groove was interpreted differently in different countries as the game moved to Europe; in France, for example, the groove was taken to be a jester's cap, hence in France the bishop is called fou, the "jester"[26] and in Romania the nebun (madman).[27]
In Mongolian and several Indian languages it is called the "camel".
In Lithuanian it is the rikis, a kind of military commander in medieval Lithuania.
In Latvia it is known as laidnis, a term for the wooden handle part of some firearms.[28]
Board games are tabletop games that typically use pieces. These pieces are moved or placed on a pre-marked board (playing surface) and often include elements of table, card, role-playing, and miniatures games as well.
Many board games feature a competition between two or more players. To show a few examples: in checkers (British English name 'draughts'), a player wins by capturing all opposing pieces, while Eurogames often end with a calculation of final scores. Pandemic is a cooperative game where players all win or lose as a team, and peg solitaire is a puzzle for one person.
There are many varieties of board games. Their representation of real-life situations can range from having no inherent theme, such as checkers, to having a specific theme and narrative, such as Cluedo. Rules can range from the very simple, such as in snakes and ladders; to deeply complex, as in Advanced Squad Leader. Play components now often include custom figures or shaped counters, and distinctively shaped player pieces commonly known as meeples as well as traditional cards and dice.
The time required to learn or master gameplay varies greatly from game to game, but is not necessarily related to the number or complexity of rules; for example, chess or Go possess relatively simple rulesets but have great strategic depth.[1]
Classical board games are divided into four categories: race games (such as pachisi), space games (such as noughts and crosses), chase games (such as hnefatafl), and games of displacement (such as chess).[2]
Hounds and jackals, another ancient Egyptian board game, appeared around 2000 BC.[10][11] The first complete set of this game was discovered from a Theban tomb that dates to the 13th dynasty.[12] This game was also popular in Mesopotamia and the Caucasus.[13]
 Men Playing Board Games, from The Sougandhika Parinaya Manuscript
Patolli game being watched by Macuilxochitl as depicted on page 048 of the Codex Magliabechiano
Han dynasty glazed pottery tomb figurines playing liubo, with six sticks laid out to the side of the game board
Board games have a long tradition in Europe. The oldest records of board gaming in Europe date back to Homer's Iliad (written in the 8th century BC), in which he mentions the Ancient Greek game of petteia.[16] This game of petteia would later evolve into the Roman ludus latrunculorum.[16] Board gaming in ancient Europe was not unique to the Greco-Roman world, with records estimating that the ancient Norse game of hnefatafl was developed sometime before 400AD.[17] In ancient Ireland, the game of fidchell or ficheall, is said to date back to at least 144 AD,[18] though this is likely an anachronism. A fidchell board dating from the 10th century has been uncovered in Co. Westmeath, Ireland.[19]
The association of dice and cards with gambling led to all dice games except backgammon being treated as lotteries by dice in the gaming acts of 1710 and 1845.[20] Early board game producers in the second half of the eighteenth century were mapmakers. The global popularization of Board Games, with special themes and branding, coincided with the formation of the global dominance of the British Empire.[21] John Wallis was an English board game publisher, bookseller, map/chart seller, printseller, music seller, and cartographer. With his sons John Wallis Jr. and Edward Wallis, he was one of the most prolific publishers of board games of the late 18th and early 19th centuries.[citation needed] John Betts' A Tour of the British Colonies and Foreign Possessions[22] and William Spooner's A Voyage of Discovery[23] were popular in the British empire. Kriegsspiel is a genre of wargaming developed in 19th century Prussia to teach battle tactics to officers.[24]
Achilles and Ajax playing a board game overseen by Athena, Attic black-figure neck amphora, ca. 510 BCE
An early games table desk (Germany, 1735) featuring chess/draughts (left) and nine men's morris (right)
In 17th- and 18th-century colonial America, the agrarian life of the country left little time for game playing,[citation needed] although draughts (checkers), bowling, and card games were not unknown. The Pilgrims and Puritans of New England frowned on game-playing, and they often viewed dice as instruments of the devil. When Governor William Bradford discovered a group of non-Puritans playing stool ball, pitching the bar, and pursuing other sports in the streets on Christmas Day, 1622, he confiscated their implements, reprimanded them, and told them their devotion for the day should be confined to their homes.
Almost all these pursuits of chance [i.e., of human industry] produce something useful to society. But there are some which produce nothing, and endanger the well-being of the individuals engaged in them or of others depending on them. Such are games with cards, dice, billiards, etc. And although the pursuit of them is a matter of natural right, yet society, perceiving the irresistible bent of some of its members to pursue them, and the ruin produced by them to the families depending on these individuals, consider it as a case of insanity, quoad hoc, step in to protect the family and the party himself, as in other cases of insanity, infancy, imbecility, etc., and suppress the pursuit altogether, and the natural right of following it. There are some other games of chance, useful on certain occasions, and injurious only when carried beyond their useful bounds. Such are insurances, lotteries, raffles, etc. These they do not suppress, but take their regulation under their own discretion.[25]
The board game Traveller's Tour Through the United States and its sister game Traveller's Tour Through Europe were published by New York City bookseller F. & R. Lockwood in 1822 and claim the distinction of being the first board games published in the United States.[15]
As the U.S. shifted from agrarian to urban living in the 19th century, greater leisure time and a rise in income became available to the middle class. The American home, once the center of economic production, became the locus of entertainment, enlightenment, and education under mothers' supervision. Children were encouraged to play board games that developed literacy skills and provided moral instruction.[26]
Commercially produced board games in the mid-19th century were monochrome prints laboriously hand-colored by teams of low-paid young factory women. Advances in papermaking and printmaking during the period enabled the commercial production of relatively inexpensive board games. The most significant advance was the development of chromolithography, a technological achievement that made bold, richly colored images available at affordable prices. Games cost as little as US$.25 for a small-boxed card game to $3.00 for more elaborate games.
American Protestants believed a virtuous life led to success, but the belief was challenged mid-century when the country embraced materialism and capitalism. In 1860, The Checkered Game of Life rewarded players for mundane activities such as attending college, marrying and getting rich. Daily life rather than eternal life became the focus of board games. The game was the first to focus on secular virtues rather than religious virtues;[26] it sold 40,000 copies in its first year.[28]
Game of the District Messenger Boy, or Merit Rewarded, published in 1886 by the New York City firm of McLoughlin Brothers, was one of the first board games based on materialism and capitalism published in the United States. The game is a typical roll-and-move track board game. Players move their tokens along the track at the spin of the arrow toward the goal at the track's end. Some spaces on the track will advance the player while others will send him back.
In the affluent 1880s, Americans witnessed the publication of Algeresque rags to riches games that permitted players to emulate the capitalist heroes of the age. One of the first such games, The Game of the District Messenger Boy, encouraged the idea that the lowliest messenger boy could ascend the corporate ladder to its topmost rung. Such games insinuated that the accumulation of wealth brought increased social status.[26] Competitive capitalistic games culminated in 1935 with Monopoly, the most commercially successful board game in U.S. history.[29]
McLoughlin Brothers published similar games based on the telegraph boy theme including Game of the Telegraph Boy, or Merit Rewarded (1888). Greg Downey notes in his essay, "Information Networks and Urban Spaces: The Case of the Telegraph Messenger Boy", that families who could afford the deluxe version of the game in its chromolithographed, the wood-sided box would not "have sent their sons out for such a rough apprenticeship in the working world."[30]
Outside of Europe and the U.S., many traditional board games are popular. In China, Go and many variations of chess are popular. In Africa and the Middle East, mancala is a popular board game archetype with many regional variations. In India, a community game called Carrom is popular.[31]
Some games, such as chess, depend completely on player skill, while many children's games such as Candy Land and snakes and ladders require no decisions by the players and are decided purely by luck.[39]
Many games require some level of both skill and luck. A player may be hampered by bad luck in backgammon, Monopoly, or Risk; but over many games, a skilled player will win more often.[40] The elements of luck can also make for more excitement at times, and allow for more diverse and multifaceted strategies, as concepts such as expected value and risk management must be considered.[citation needed]
Luck may be introduced into a game by several methods. The use of dice of various sorts goes back to the earliest board games. These can decide everything from how many steps a player moves their token, as in Monopoly, to how their forces fare in battle, as in Risk, or which resources a player gains, as in Catan. Other games such as Sorry! use a deck of special cards that, when shuffled, create randomness. Scrabble does something similar with randomly picked letters. Other games use spinners, timers of random length, or other sources of randomness. German-style board games are notable for often having fewer elements of luck than many North American board games.[41]
Another important aspect of some games is diplomacy, that is, players, making deals with one another. Negotiation generally features only in games with three or more players, cooperative games being the exception. An important facet of Catan, for example, is convincing players to trade with you rather than with opponents. In Risk, two or more players may team up against others. Easy diplomacy involves convincing other players that someone else is winning and should therefore be teamed up against. Advanced diplomacy (e.g., in the aptly named game Diplomacy) consists of making elaborate plans together, with the possibility of betrayal.[42]
In perfect information games, such as chess, each player has complete information on the state of the game, but in other games, such as Tigris and Euphrates or Stratego, some information is hidden from players. This makes finding the best move more difficult and may involve estimating probabilities by the opponents.[citation needed]
Many board games are now available as video games. These are aptly termed digital board games, and their distinguishing characteristic compared to traditional board games is they can now be played online against a computer or other players. Some websites (such as boardgamearena.com, yucata.de, etc.)[43] allow play in real time and immediately show the opponents' moves, while others use email to notify the players after each move.[44] The Internet and cheaper home printing has also influenced board games via print-and-play games that may be purchased and printed.[45] Some games use external media such as audio cassettes or DVDs in accompaniment to the game.[46][47]
There are also virtual tabletop programs that allow online players to play a variety of existing and new board games through tools needed to manipulate the game board but do not necessarily enforce the game's rules, leaving this up to the players. There are generalized programs such as Vassal, Tabletop Simulator and Tabletopia that can be used to play any board or card game, while programs like Roll20 and Fantasy Grounds that are more specialized for role-playing games.[48][49] Some of these virtual tabletops have worked with the license holders to allow for use of their game's assets within the program; for example, Fantasy Grounds has licenses for both Dungeons & Dragons and Pathfinder materials, while Tabletop Simulator allows game publishers to provide paid downloadable content for their games.[50][51] However, as these games offer the ability to add in the content through user modifications, there are also unlicensed uses of board game assets available through these programs.[52]
A dedicated field of research into gaming exists, known as game studies or ludology.[65]
While there has been a fair amount of scientific research on the psychology of older board games (e.g., chess, Go, mancala), less has been done on contemporary board games such as Monopoly, Scrabble, and Risk,[66] and especially modern board games such as Catan, Agricola, and Pandemic. Much research has been carried out on chess, partly because many tournament players are publicly ranked in national and international lists, which makes it possible to compare their levels of expertise. The works of Adriaan de Groot, William Chase, Herbert A. Simon, and Fernand Gobet have established that knowledge, more than the ability to anticipate moves, plays an essential role in chess-playing ability.[67]
Linearly arranged board games have improved children's spatial numerical understanding. This is because the game is similar to a number line in that they promote a linear understanding of numbers rather than the innate logarithmic one.[68]
Research studies show that board games such as Snakes and Ladders result in children showing significant improvements in aspects of basic number skills such as counting, recognizing numbers, numerical estimation, and number comprehension. They also practice fine motor skills each time they grasp a game piece.[69] Playing board games has also been tied to improving children's executive functions[70] and help reduce risks of dementia for the elderly.[71][72] Related to this is a growing academic interest in the topic of game accessibility, culminating in the development of guidelines for assessing the accessibility of modern tabletop games[73] and the extent to which they are playable for people with disabilities.[74]
Additionally, board games can be therapeutic. Bruce Halpenny, a games inventor said when interviewed about his game, The Great Train Robbery:
With crime you deal with every basic human emotion and also have enough elements to combine action with melodrama. The player's imagination is fired as they plan to rob the train. Because of the gamble, they take in the early stage of the game there is a build-up of tension, which is immediately released once the train is robbed. Release of tension is therapeutic and useful in our society because most jobs are boring and repetitive.[75]
Playing games has been suggested as a viable addition to the traditional educational curriculum if the content is appropriate and the gameplay informs students on the curriculum content.[76][77]
There are several ways in which board games can be classified, and considerable overlap may exist, so that a game belongs to several categories.[15]
H. J. R. Murray's A History of Board Games Other Than Chess (1952) has been called the first attempt to develop a "scheme for the classification of board games".[78] David Parlett's Oxford History of Board Games (1999) defines four primary categories: race games (where the goal is to be the first to move all one's pieces to the final destination), space games (in which the object is to arrange the pieces into some special configuration), chase games (asymmetrical games, where players start the game with different sets of pieces and objectives) and displace games (where the main objective is the capture the opponents' pieces). Parlett also distinguishes between abstract and thematic games, the latter having a specific theme or frame narrative (ex. regular chess versus, for example, Star Wars-themed chess).[78]
The following is a list of some of the most common game categories:
Although many board games have a jargon all their own, there is a generalized terminology to describe concepts applicable to basic game mechanics and attributes common to nearly all board games.
Abstract strategy games in contrast to strategy games in general usually have no or minimal narrative theme, outcomes determined only by player choice (with no randomness), and each players have perfect information about the game.[1] For example, Go is a pure abstract strategy game since it fulfills all three criteria; chess and related games are nearly so but feature a recognizable theme of ancient warfare; and Stratego is borderline since it is deterministic, loosely based on 19th-century Napoleonic warfare, and features concealed information.
Combinatorial games have no randomizers such as dice, no simultaneous movement, nor hidden information.  Some games that do have these elements are sometimes classified as abstract strategy games.  (Games such as Continuo, Octiles, Can't Stop, and Sequence, could be considered abstract strategy games, despite having a luck or bluffing element.) A smaller category of abstract strategy games manages to incorporate hidden information without using any random elements; the best known example is Stratego.
Some abstract strategy games have multiple starting positions of which it is required that one be randomly determined.  For a game to be one of skill, a starting position needs to be chosen by impartial means.  Some games, such as Arimaa and DVONN, have the players build the starting position in a separate initial phase which itself conforms strictly to combinatorial game principles.  Most players, however, would consider that although one is then starting each game from a different position, the game itself contains no luck element. Indeed, Bobby Fischer promoted randomization of the starting position in chess in order to increase player dependence on thinking at the board.[3]
As J. Mark Thompson wrote in his article "Defining the Abstract", play is sometimes said to resemble a series of puzzles the players pose to each other:[4][5] 
There is an intimate relationship between such games and puzzles: every board position presents the player with the puzzle, What is the best move?, which in theory could be solved by logic alone. A good abstract game can therefore be thought of as a "family" of potentially interesting logic puzzles, and the play consists of each player posing such a puzzle to the other. Good players are the ones who find the most difficult puzzles to present to their opponents.
Many abstract strategy games also happen to be "combinatorial"; i.e., there is no hidden information, no non-deterministic elements (such as shuffled cards or dice rolls), no simultaneous or hidden movement or setup, and (usually) two players or teams take a finite number of alternating turns.
Many games which are abstract in nature historically might have developed from thematic games, such as representation of military tactics.[6] In turn, it is common to see thematic version of such games; for example, chess is considered an abstract game, but many thematic versions, such as Star Wars-themed chess, exist.
Go was considered one of the four essential arts of the cultured aristocratic Chinese scholars in antiquity. The earliest written reference to the game is generally recognized as the historical annal Zuo Zhuan[17][18] (c. 4th century BC).[19]
Englishmen Lewis Waterman[20] and John W. Mollett both claim to have invented the game of Reversi in 1883, each denouncing the other as a fraud. The game gained considerable popularity in England at the end of the nineteenth century.[21] The game's first reliable mention is in 21 August 1886 edition of The Saturday Review.
As for the qualitative aspects, ranking abstract strategy games according to their interest, complexity, or strategy levels is a daunting task and subject to extreme subjectivity.  In terms of measuring how finite a mathematical field each of the three top contenders represents, it is estimated that checkers has a game-tree complexity of  1040 possible games, whereas chess has approximately 10123. As for Go, the possible legal game positions range in the magnitude of 10170.
The Mind Sports Olympiad first held the Abstract Games World Championship in 2008 to try to find the best abstract strategy games all-rounder.[2] The MSO event saw a change in format in 2011[22] restricting the competition to players' five best events, and was renamed to the Modern Abstract Games World Championship.
A mind sport, is a game of skill based on intellectual ability.
The first major use of the term was as a result of the Mind Sports Olympiad in 1997.[1] The phrase had been used prior to this event such as backgammon being described as a mind sport by Tony Buzan in 1996; Tony Buzan was also a co-founder of the Mind Sports Olympiad.[2] Bodies such as the World Memory Sports Council[3] use the term retrospectively.
It is a term that became fixed from games trying to obtain equal status to sports. For example, from 2002 British Minister for Sport, Richard Caborn said: 
...I believe we should have the same obligation to mental agility as we do to physical agility. Mind sports have to form UK national bodies and get together with the government to devise an acceptable amendment to the 1937 Act that clearly differentiates mind sports from parlour board games.[4][5]
Many of the games' official bodies which had come together for the Mind Sports Olympiad, formed larger organisations such as the Mind Sports Council and International Mind Sports Association (IMSA). With IMSA organising the World Mind Sports Games in Beijing 2008[6] for contract bridge, chess, go, draughts and xiangqi many other bodies have lobbied for inclusion such as the International Federation of Poker,[7] which won provisional membership at the annual congress of SportAccord in Dubai in 2009.[8]
The term also includes mental calculation or memory disciplines as presented in International competitions such as the Mental Calculation World Cup (held bi-annually since 2004) and the World Memory Championships (held annually since 1991)
As well as board and card games,[9] other disciplines that have been described as mind sports are speed reading, competitive programming[10][11] and cybersecurity wargames.[12][13][14][15][16] Other events that have been included where the physical element is comparable to the mental component such as when the official Mind Sports South Africa accepted speed-texting as a mind sport.[17]
Fast chess, also known as Speed chess, is a type of chess in which each player is given less time to consider their moves than normal tournament time controls allow. Fast chess is subdivided, by decreasing time controls, into rapid chess, blitz chess, and bullet chess. Armageddon chess is a particular variation of fast chess in which different rules apply for each of the two players.
As of February 2023, the top-ranked rapid chess player in the open section is Magnus Carlsen from Norway, who is also the top-ranked classical chess player and reigning world champion.[2] The top-ranked blitz chess player as of February 2023 is Alireza Firouzja.[3]
As of February 2023, the women's top-ranked blitz player is Tan Zhongyi from China. China's Hou Yifan is the top-ranked rapid player, who is also the top-ranked women's classical chess player.[4]
Players of fast and blitz chess are exempt from the requirement to record their moves onto a scoresheet (A.2). The arbiter or their assistant is responsible for the recording in competitions (A.3.1.2, B.3.1.2). Electronic recording is preferred.[7]
A fast chess game can be further divided into several categories, which are primarily distinguished by the selection of time controls. Games may be played with or without time increments per move.
Time controls for each player in a game of rapid chess are, according to FIDE, more than 10 minutes but less than 60 minutes.[6] Rapid chess can be played with or without time increments for each move. When time increments are used, a player can automatically gain, for instance, ten more seconds on the clock after each move. When time increments are used, the total time per player for a 60-move game must be more than 10 minutes but less than 60 minutes.[6] Rapid chess was called active chess by FIDE between 1987 and 1989.[8]
For the FIDE World Rapid Championship, each player has 15 minutes plus 10 seconds additional time per move starting from move 1.[9]
Time controls for each player in a game of blitz chess are, according to FIDE, 10 minutes or less per player.[6] This can be played with or without an increment or delay per move, made possible by the adoption of digital clocks. Three minutes with a two-second increment is preferred. In the case of time increments, the total time per player for a 60-move game must be 10 minutes or less (hence averaging 10 seconds or less per move).[6]
For the FIDE World Blitz Championship, each player has 3 minutes plus 2 additional seconds per move, starting from move 1.[9]
A variant of blitz chess, bullet chess games have less than three minutes per player, based on a 40-move game;[10][11] some chess servers rate one-minute-per-player games separately.[12] Lower time controls are called 'hyperbullet' and 'ultrabullet' for 30-second-per-player and 15-second-per-player games, respectively.[13][14] Other common time-control options for bullet games include two minutes with one-second increment, one minute with a two-second increment, or one minute with one-second increment. The term lightning can also be applied to this variant.[15] The use of increment in bullet chess is primarily to avoid issues with latency as well as so-called "dirty flagging".
Online bullet chess avoids practical problems associated with live bullet chess, particularly players accidentally knocking over the pieces. Playing online also allows premoving, or committing to a move before the opponent has taken their turn.[16]
A variant of blitz chess where a drawn game is counted as a win for Black. This guarantees the game ends decisively, so it can be used as a final tiebreaker game. It is used in tournaments such as Chess World Cup as a tiebreaker.[17]
To compensate for giving Black draw odds, White has more time on the clock. Common times are six minutes for White and five minutes for Black or five minutes for White and four minutes for Black. This can also be played with a small increment.[18]
Some tournaments utilise a bidding system for individual players of each match to decide how little time they would be willing to play with as black. The player with the lowest bid for each match receives the black pieces with draw odds. This system minimises the perceived unfairness of Armageddon time controls that are decided in advance before a tournament with colours randomly allocated.[19][20][21][22] Such an idea is reminiscent of the logical use case of fair cake-cutting.
Before the advent of digital clocks, five minutes per side was the standard for blitz or speed chess. Before the introduction of chess clocks, chess club "rapid transit" tournaments had referees who called out every ten seconds.[clarification needed] The Washington Divan (2445 15th St. NW) had regular weekly games and used a special clock that beeped every ten seconds to indicate the time to move. Players had to use their full ten seconds and move on the bell.[citation needed]
In 1988, Walter Browne formed the World Blitz Chess Association and its magazine Blitz Chess, which folded in 2003.[25]
In some chess tournaments and matches, the final standings of the contestants are decided by a series of games with ever-shortening control times as tie breaks. In this case, two games may be played with each time control, as playing with black or white pieces is not equally liked among players. The short time controls in fast chess reduce the amount of time available to consider each move, and may result in a frantic game, especially as time runs out. A player whose time runs out automatically loses, unless the opposing player has insufficient material to checkmate, in which case the game is a draw. "Losing on time" is possible at even the longer, traditional time controls, but is more common in blitz and rapid versions.
Play is governed by the FIDE Laws of Chess, except as modified by a specific tournament. However, in case of a dispute during a tournament, either player may stop the clock and call the arbiter to make a final and binding judgment.
Chess boxing uses a fast version for the chess component of the sport, granting 9 minutes for each side with no increment.[26]
The rules for fast chess differ between FIDE and the USCF.
With the USCF, a game with more than 10 minutes affects the Quick rating, and the upper bounds for this rating is capped at 65 minutes per player.[27] As 30-minute to 65-minute-per-player time controls are also under the Regular rating system, these games affect both the Quick and Regular ratings[27] and are known as dual-rated games. However, the K factor (a statistic used for ratings) is reduced by comparison, meaning that players will either lose or gain (or rarely both) fewer rating points compared to a solely Quick or Regular game. Any time control over 65 minutes counts under the Regular rating only.[27] All of these time controls include the delay added to the time control, such as a 60-minute game with a 5-second delay, which is still considered to be a 60-minute game, not a 65-minute game.
As of March 2013, the USCF has also added a separate Blitz class rating for any time control between 5 and 10 minutes per player.[27] It is not possible for a game to be dual rated as both Blitz and Quick. Unlike Quick chess, 5 minutes can also mean game 3+2 (three minutes with a two-second increment).
Both official and unofficial FIDE-sponsored world championships for fast chess have been held since the 1970s.
The 1988 victory by Anatoly Karpov in Mazatlan was officially called the World Active Championship, but FIDE changed the word 'active' to 'rapid' soon after.[8]
In 1992, FIDE held the Women's World Rapid and Blitz Championship in Budapest, Hungary. Both Rapid and Blitz Championships were won by Susan Polgar.[30]
The 2001 victory by Garry Kasparov in the FIDE World Cup of Rapid Chess (organized by the French Chess Federation in Cannes) was held contemporaneously to the Melody Amber rapids (thus splitting the top players between the two events),[31] and it is sometimes considered to be official, although it was never named as a "championship" but rather a "world cup".[32]
In 1992, FIDE held the Women's World Rapid and Blitz Championship in Budapest, Hungary. Both Rapid and Blitz Championships were won by Susan Polgar.[30]
In 2000, Anand won the Plus GSM World Blitz Chess Cup,[45] which has since been referred to as a world championship,[46][47] albeit inconsistently.
In 2009 and 2010, there was an event called the World Blitz Championship, held after the Tal Memorial in Moscow in November. It was won by Magnus Carlsen (in 2009)[50] and Levon Aronian (in 2010),[51] with the Women's Championship being won by Kateryna Lagno (in 2010).[52] There is no record of a 2009 blitz event in the FIDE Calendar for that year;[53] however, the October 2009 FIDE Congress discussed whether it should be a "proper" Championship (given the qualification scheme), and it left the decision to the corresponding internal Commission.[54] For 2010, it was organized in conjunction with FIDE from the beginning.[51] However, in neither case was an arbiter's report presented to the next FIDE Congress or General Assembly, as would be expected for a World Championship, and indeed occurred previously with the 2008 Blitz Championship.[55] The 2012 Arbiter's report refers to 7th World Blitz Championship thus seeming to imply that 2009 and 2010 events were indeed Championships;[56] although this report can be faulted for referring to the rapid championship of 2012 as being the 1st World Rapid Championship, which at the very least forgets Anand's official Rapid Championship in 2003. The balance of the evidence favors these Blitz Championships as being counted as official.
In 2011, there was no official blitz championship held, but FIDE was involved with the Sport Accord Mind Games blitz won by Maxime Vachier-Lagrave, with Hou Yifan winning the women's division.[57]
Since 2012, FIDE have held joint World Rapid and Blitz Championships most years, with some years Women's World Rapid and Blitz Championships also being held.
In 2012, the World Rapid and Blitz Championships were held at Batumi, Georgia and Astana, Kazakhstan (Women's Championships)[59] Sergey Karjakin won the Rapid Championship.[60] Alexander Grischuk won the Blitz Championship.[61] Antoaneta Stefanova won the Women's Rapid Championship.[62] Valentina Gunina won the Women's Blitz Championship.[62]
In 2014, the World Rapid and Blitz Championships were held at Dubai, UAE and Khanty-Mansiysk, Russia (Women's Championships).[59] Magnus Carlsen won both Rapid and Blitz Championships.[65][66] Kateryna Lagno won the Women's Rapid Championship.[67] Anna Muzychuk won the Women's Blitz Championship.[68]
In 2015, the World Rapid and Blitz Championships were held in Berlin, Germany. Magnus Carlsen won the Rapid Championship.[69] He also received the privilege of playing at a dedicated Board 1 the whole time, not having to move while others did. The given reason was that Norwegian television was sponsoring the event, and moving the heavy cameras around would be too much hassle.[70] After his first-round draw, he should not have been on Board 1 until Round 8 when he caught the leaders.[71] Carlsen himself later called this "weird" that Board 1 would be reserved for him.[72] Alexander Grischuk won the Blitz Championship.[73]
In 2015, FIDE did not receive the expected 80,000 euros from Agon's organization of the event, causing a budget shortfall of 55,000 euros.[74][75] It was later announced that approximately 200,000 euros were lost on the event.[76]
In 2016, the World Rapid Championships were held at the Ali Bin Hamad Al Attiya Arena in Doha, Qatar. Vassily Ivanchuk of Ukraine won the 2016 World Rapid Championship, while Carlsen, after defending his title with difficulty in 2015, came in third place. In the Blitz Championship, Sergey Karjakin of Russia and contender in the recently held World Chess Championship 2016 won the championship title albeit due to a better tiebreak over the second place Carlsen. Karjakin defeated Carlsen in their individual encounter. Carlsen was once again reserved board 1 for both championships. Anna Muzychuk also from Ukraine, won both the 2016 Women World Rapid and Blitz Championshipship.
At the FIDE Presidential Board meeting at the end of March 2016, they gave Agon six months to find an organizer for the 2017 event.[77] At the Baku General Assembly in September, it was announced they had extended this deadline until the end of 2016.[78] The issue of the non-payment of the players for the IMSA Mind Games was also brought up.[78]
Many top chess players do not take rapid, blitz, and bullet chess as seriously as chess with standard time controls. Some quotes from top chess players may serve to illustrate this:
Chess strategy is the aspect of chess play concerned with evaluation of chess positions and setting goals and long-term plans for future play. While evaluating a position strategically, a player must take into account such factors as the relative value of the pieces on the board, pawn structure, king safety, position of pieces, and control of key squares and groups of squares (e.g. diagonals, open files, and individual squares). Chess strategy is distinguished from chess tactics, which is the aspect of play concerned with the move-by-move setting up of threats and defenses. Some authors distinguish static strategic imbalances (e.g. having more valuable pieces or better pawn structure), which tend to persist for many moves, from dynamic imbalances (such as one player having an advantage in piece development), which are temporary.[1] This distinction affects the immediacy with which a sought-after plan should take effect. Until players reach the skill level of "master", chess tactics tend to ultimately decide the outcomes of games more often than strategy. Many chess coaches thus emphasize the study of tactics as the most efficient way to improve one's results in serious chess play.
The most basic way to evaluate one's position is to count the total value of pieces on both sides. The point values used for this purpose are based on experience. Usually pawns are considered to be worth one point, knights and bishops three points each, rooks five points, and queens nine points. The fighting value of the king in the endgame is approximately four points. These basic values are modified by other factors such as the position of the pieces (e.g. advanced pawns are usually more valuable than those on their starting squares), coordination between pieces (e.g. a bishop pair usually coordinates better than a bishop plus a knight), and the type of position (knights are generally better in closed positions with many pawns, while bishops are more powerful in open positions).
Another important factor in the evaluation of chess positions is the pawn structure or pawn skeleton. Since pawns are the most immobile and least valuable of the pieces, the pawn structure is relatively static and largely determines the strategic nature of the position. Weaknesses in the pawn structure, such as isolated, doubled, or backward pawns and holes, once created, are usually permanent. Care must therefore be taken to avoid them unless they are compensated by another valuable asset, such as the possibility to develop an attack.
A material advantage applies both strategically and tactically. Generally more pieces or an aggregate of more powerful pieces means greater chances of winning. A fundamental strategic and tactical rule is to capture opponent pieces while preserving one's own.
Bishops and knights are called minor pieces. A knight is about as valuable as a bishop, but less valuable than a rook. Rooks and the queen are called major pieces. Bishops are usually considered slightly better than knights in open positions, such as toward the end of the game when many of the pieces have been captured, whereas knights have an advantage in closed positions. Having two bishops (the bishop pair) is a particularly powerful weapon, especially if the opposing player lacks one or both of their bishops.
Three pawns are likely to be more useful than a knight in the endgame, but in the middlegame, a knight is often more powerful. Two minor pieces are stronger than a single rook, and two rooks are slightly stronger than a queen. The bishop on squares of the same color as the player is slightly more valuable in the opening as it can attack the vulnerable f7/f2-square. A rook is more valuable when doubled with another rook or queen; consequently, doubled rooks are worth more than two unconnected rooks.
Under a system like this, giving up a knight or bishop to win a rook ("winning the exchange") is advantageous and is worth about two pawns. This ignores complications such as the current position and freedom of the pieces involved, but it is a good starting point. In an open position, bishops are more valuable than knights (a bishop pair can easily be worth seven points or more in some situations); conversely, in a closed position, bishops are less valuable than knights. A knight in the center of the board that cannot be taken, however, is known as a knight outpost and threatens several fork instances. In such a case, a knight is worth far more than a bishop. Also, many pieces have a partner. By doubling up two knights, two rooks, rook and queen, or bishop and queen, the pieces can get stronger than the sum of the individual pieces alone. When pieces lose their partner, their values slightly decrease. The king is priceless since its capture results in the defeat of that player and ends that game. However, especially in the endgame, the king can also be a fighting piece, and is sometimes given a fighting value of three and a half points.
Other things being equal, the side that controls more space on the board has an advantage.[2] More space means more options, which can be exploited both tactically and strategically. A player who has all pieces developed and no tactical tricks or promising long-term plan should try to find a move that enlarges their influence, particularly in the center. In some openings, however, one player accepts less space for a time, to set up a counterattack in the middlegame. This is one of the concepts behind hypermodern play.
The easiest way to gain space is to push the pawn skeleton forward. One must be careful not to over stretch, however. If the opponent succeeds in getting a protected piece behind enemy lines, this piece can become such a serious problem that a piece with a higher value might have to be exchanged for it.
The strategy consists of placing pieces so that they attack the central four squares of the board. A piece being placed on a central square, however, does not necessarily mean it controls the center; e.g., a knight on a central square does not attack any central squares. Conversely, a piece does not have to be on a central square to control the center. For example, the bishop can control the center from afar.
Control of the center is generally considered important because tactical battles often take place around the central squares, from where pieces can access most of the board. Center control allows more movement and more possibility for attack and defense.
Chess openings try to control the center while developing pieces. Hypermodern openings are those that control the center with pieces from afar (usually the side, such as with a fianchetto); the older Classical (or Modern) openings control it with pawns.
The initiative belongs to the player who can make threats that cannot be ignored, such as checking the opponent's king. They thus put their opponent in the position of having to use their turns responding to threats rather than making their own, hindering the development of their pieces.[4] The player with the initiative is generally attacking and the other player is generally defending.
It is important to defend one's pieces even if they are not directly threatened. This helps stop possible future campaigns from the opponent. If a defender must be added at a later time, this may cost a tempo or even be impossible due to a fork or discovered attack. The approach of always defending one's pieces has an antecedent in the theory of Aron Nimzowitsch who referred to it as "overprotection." Similarly, if one spots undefended enemy pieces, one should immediately take advantage of those pieces' weakness.
Even a defended piece can be vulnerable. If the defending piece is also defending something else, it is called an overworked piece, and may not be able to fulfill its task. When there is more than one attacking piece, the number of defenders must also be increased, and their values taken into account. In addition to defending pieces, it is also often necessary to defend key squares, open files, and the back rank. These situations can easily occur if the pawn structure is weak.
To exchange pieces means to capture a hostile piece and then allow a piece of the same value to be captured. As a rule of thumb, exchanging pieces eases the task of the defender who typically has less room to operate in.
Exchanging pieces is usually desirable to a player with an existing advantage in material, since it brings the endgame closer and thereby leaves the opponent with less ability to recover ground. In the endgame even a single pawn advantage may be decisive. Exchanging also benefits the player who is being attacked, the player who controls less space, and the player with the better pawn structure.[citation needed]
When playing against stronger players, many beginners attempt to constantly exchange pieces "to simplify matters". However, stronger players are often relatively stronger in the endgame, whereas errors are more common during the more complicated middlegame.
Note that "the exchange" may also specifically mean a rook exchanged for a bishop or knight.  The phrase, "going up the exchange," means capturing a rook in exchange for a bishop or knight as that is a materially better trade.  Conversely, "going down an exchange," means losing a rook but capturing a bishop or knight, a materially worse trade.
In the endgame, passed pawns, unhindered by enemy pawns from promotion, are strong, especially if advanced or protected by another pawn. A passed pawn on the sixth rank is roughly as strong as a knight or bishop and often decides the game. (Also see isolated pawn, doubled pawns, backward pawn, connected pawns.)
A king and one knight is not sufficient material to checkmate an opposing lone king (see Two knights endgame). A king and two knights can checkmate a lone king but 
it cannot be forced.
A bishop always stays on squares of the color it started on, so once one of them is gone, the squares of that color become more difficult to control. When this happens, pawns moved to squares of the other color do not block the bishop, and enemy pawns directly facing them are stuck on the vulnerable color.
In general, a bishop is of roughly equal value to a knight. In certain circumstances, one can be more powerful than the other. If the game is "closed" with many interlocked pawn formations, the knight tends to be stronger, because it can hop over the pawns while they block the bishop. A bishop is also weak if it is restricted by its own pawns, especially if they are blocked and on the bishop's color. Once a bishop is lost, the remaining bishop is considered weaker since the opponent can now plan their moves to play a white or black color game.
In an open position with action on both sides of the board, the bishop tends to be stronger because of its long range. This is especially true in the endgame; if passed pawns race on opposite sides of the board, the player with a bishop usually has better winning chances than a player with a knight.
A king and a bishop is not sufficient material to checkmate an opposing lone king, but two bishops and a king checkmate an opposing lone king easily.
Rooks have more scope of movement on half-open files (ones with no pawns of one's own color). Rooks on the seventh rank can be very powerful as they attack pawns that can only be defended by other pieces, and they can restrict the enemy king to its back rank. A pair of rooks on the player's seventh rank is often a sign of a winning position.
In middlegames and endgames with a passed pawn, Tarrasch's rule states that rooks, both friend and foe of the pawn, are usually strongest behind the pawn rather than in front of it.
A king and a rook is sufficient material to checkmate an opposing lone king, although it's a little harder than checkmating with king and queen; thus the rook's distinction as a major piece above the knight and bishop.
Queens are the most powerful pieces. They have great mobility and can make many threats at once. They can act as a rook and as a bishop at the same time. For these reasons, checkmate attacks involving a queen are easier to achieve than those without one. Although powerful, the queen is also easily harassed. Thus, it is generally wise to wait to develop the queen until after the knights and bishops have been developed to prevent the queen from being attacked by minor pieces and losing tempo. When a pawn is promoted, most of the time it is promoted to a queen.
During the middlegame, the king is often best protected in a corner behind its pawns. Such a position for either of the players is often achieved by castling by that player. If the rooks and queen leave the first rank (commonly called that player's back rank), however, an enemy rook or queen can checkmate the king by invading the first rank, commonly called a back-rank checkmate. Moving one of the pawns in front of the king (making a luft) can allow it an escape square, but may weaken the king's overall safety otherwise. One must therefore wisely balance between these trade-offs.
Castling is often thought to help protect the king and often "connects" the player's two rooks together so the two rooks may protect each other. This can reduce a threat of a back-rank skewer in which the king can be skewered with capture of a rook behind it.
The king can become a strong piece in the endgame. With reduced material, a quick checkmate becomes less of a concern, and moving the king towards the center of the board gives it more opportunities to make threats and actively influence play.
Because of different strategic and tactical patterns, a game of chess is usually divided into three distinct phases: the opening, usually the first 10 to 25 moves, when players develop their armies and set up the stage for the coming battle; the middlegame, the developed phase of the game; and the endgame, when most of the pieces are gone and kings start to take an active part in the struggle.
A chess opening is the group of initial moves of a game (the "opening moves"). Recognized sequences of opening moves are referred to as openings and have been given names such as the Ruy Lopez or Sicilian Defence. They are catalogued in reference works such as the Encyclopaedia of Chess Openings. It is recommended for anyone but the chessmasters that when left with a choice to either invent a new variation or follow a standard opening, choose the latter.[citation needed]
During the opening, some pieces have a recognized optimum square they try to reach. Hence, an optimum deployment could be to push the king and queen pawn two squares, followed by moving the knights so they protect the center pawns and give additional control of the center. One can then deploy the bishops, protected by the knights, to pin the opponent's knights and pawns. The optimum opening is ended with  castling, moving the king to safety and deploying for a strong back rank and a rook along a center file.
Apart from these fundamentals, other strategic plans or tactical sequences may be employed in the opening.
Most players and theoreticians consider that White, by virtue of the first move, begins the game with a small advantage. Black usually strives to neutralize White's advantage and achieve equality, or to develop dynamic counterplay in an unbalanced position.
The middlegame is the part of the game when most pieces have been developed. Because the opening theory has ended, players have to assess the position to form plans based on the features of the positions, and at the same time take into account the tactical possibilities in the position.[9]
Another important strategical question in the middlegame is whether and how to reduce material and transform into an endgame (i.e. simplify). For example, minor material advantages can generally be transformed into victory only in an endgame, and therefore the stronger side must choose an appropriate way to achieve an ending. Not every reduction of material is good for this purpose; for example, if one side keeps a light-squared bishop and the opponent has a dark-squared one, the transformation into a bishops and pawns ending is usually advantageous for the weaker side only, because an endgame with bishops on opposite colors is likely to be a draw, even with an advantage of one or two pawns.
The endgame (or end game or ending) is the stage of the game when there are few pieces left on the board. There are three main strategic differences between earlier stages of the game and the endgame:[11]
Endgames can be classified according to the type of pieces remaining on the board. Basic checkmates are positions where one side has only a king and the other side has one or two pieces and can checkmate the opposing king, with the pieces working together with their king. For example, king and pawn endgames involve only kings and pawns on one or both sides and the task of the stronger side is to promote one of the pawns. Other more complicated endings are classified according to the pieces on the board other than kings, e.g. "rook and pawn versus rook endgame".
Tactics are usually contrasted with strategy, whereby the individual moves by themselves do not make indefensible threats, and the cumulative advantage of them takes longer to capitalise. The dichotomy can be summarised as tactics concerning short-term play and strategy concerning long-term play. Examples of strategic advantages are weaknesses in, compromised pawn structure in, and sustained pressure on, the opponent's position. Often, to dichotomize strategy and tactics, sequences of moves that make strategic instead of tactical threats or use tactical threats to obtain a strategic advantage are also classified as tactics.
Tactics usually follow one of a number of repeating patterns; these include forks, skewers, batteries, discovered attacks, undermining, overloading, deflection, pins, and interference.[1] The Encyclopedia of Chess Middlegames gives the following tactics categories: Annihilation of Defense, Blockade, Decoying, Deflection, Demolition of Pawns, Discovered Attack, Double Attack, Interception, Intermediate Move, Overloading, Passed Pawn, Pawns Breakthrough, Pin, Pursuit (perpetual attack), Space Clearance, and X-ray Attack. Often tactics of more than one type are conjoined in a combination. 
A piece is said to attack (or threaten) an opponent's piece if, on the next move, it could capture that piece. A piece is said to defend (or protect) a piece of the defender's color if, in case the defended piece were taken by the opponent, the defender could immediately recapture. Attacking a piece usually, but not always (see Sacrifice), forces the opponent to respond if the attacked piece is undefended, or if the attacking piece is of lower value than the one attacked.
When the piece attacked is a king, then a player has at most three options:
When the attacked piece is not a king, a player may have additional options, beyond the ones listed above:
A discovered attack is a move that allows an attack by another piece. A piece is moved away so as to allow the attack of a friendly bishop, rook or queen on an enemy piece. If the attacked piece is the king, the situation is referred to as a discovered check. Discovered attacks are powerful since the moved piece may be able to pose a second threat.
A special case of a discovered check is a double check, where both the piece being unmasked and the piece being moved attack the enemy king. A double check always forces the opponent to move the king, since it is impossible to defend attacks from two directions in any other way.
The queen is also an excellent forking piece, since she can move in eight different directions. However, a queen fork is only useful if both pieces are undefended, or if one is undefended and the other is the enemy's king. The queen is the most valuable attacking piece, so it is usually not profitable for her to capture a defended piece.
Fork attacks can be either relative (meaning the attacked pieces comprise pawn[s], knight[s], bishop[s], rook[s], or queen[s]), or absolute (one of the attacked pieces is the enemy king, in check). The targets of a fork do not have to be pieces, although this is known as a double attack. One or more of the targets can be a mate threat (for example, forking a loose knight and setting up a battery of queen and bishop that creates a mate threat as well) or implied threat (for example, a knight move that forks a loose bishop and also threatens to fork enemy queen and rook).
A pin is a move that inhibits an opponent piece from moving, because doing so would expose a more valuable (or vulnerable) piece behind it. Only bishops, rooks, and queens can perform a pin, since they can move more than one square in a straight line. If the pinned piece cannot move because doing so would produce check, the pin is called absolute. If moving the pinned piece would expose a non-king piece, the pin is called relative.
A skewer is a move that attacks two pieces in a line, similar to a pin, except that the enemy piece of greater value is in front of the piece of lesser value. After the more valuable piece moves away, the lesser piece can be captured. Like pins, only queens, rooks, and bishops can perform the skewer, and skewer attacks can be either absolute (the  more valuable piece in front is the king, in check) or relative (the piece in front is a non-king piece).
The pawn is the least valuable chess piece, so pawns are often used to capture defended pieces. A single pawn typically forces a more powerful piece, such as a rook or a knight, to retreat. The ability to fork two enemy  pieces by advancing a pawn is often a threat. Or a simple pawn move can reveal a discovered attack. When pawns are arranged on a diagonal, with each pawn guarded by the pawn behind it, they form a wall or pawn chain protecting any friendly pieces behind them. A weak pawn structure, with unprotected or isolated pawns ahead of more valuable pieces, can be a decisive weakness. A pawn that has advanced all the way to the opposite side of the board is promoted to any other piece except a king.
A sacrifice of some material is often necessary to throw the opponent's position out of balance, potentially gaining positional advantage. The sacrificed material is sometimes later offset with a consequent material gain. Pawn sacrifices in the opening are known as gambits; they are usually not intended for material gain, but rather to achieve a more active position.
Direct attacks against the enemy king are often started by sacrifices. A common example is sacrificing a bishop on h2 or h7, checking the king, who usually must take the bishop. This allows the queen and knight to develop a fulminant attack.
Zugzwang (German for "compulsion to move") occurs when a player is forced to make an undesirable move. The player is put at a disadvantage because they would prefer to pass and make no move, but a move has to be made, all of which weaken their position. Situations involving zugzwang occur uncommonly, but when they do occur, it is almost always in the endgame, where there are fewer choices of available moves.
Zwischenzug (German for "intermediate move") is a common tactic in which a player under threat, instead of directly countering or recapturing, introduces an even more devastating threat.  The tactic often involves a new attack against the opponent's queen or king. The opponent then may be forced to address the new threat, abandoning the earlier attack.
Board games are tabletop games that typically use pieces. These pieces are moved or placed on a pre-marked board (playing surface) and often include elements of table, card, role-playing, and miniatures games as well.
Many board games feature a competition between two or more players. To show a few examples: in checkers (British English name 'draughts'), a player wins by capturing all opposing pieces, while Eurogames often end with a calculation of final scores. Pandemic is a cooperative game where players all win or lose as a team, and peg solitaire is a puzzle for one person.
There are many varieties of board games. Their representation of real-life situations can range from having no inherent theme, such as checkers, to having a specific theme and narrative, such as Cluedo. Rules can range from the very simple, such as in snakes and ladders; to deeply complex, as in Advanced Squad Leader. Play components now often include custom figures or shaped counters, and distinctively shaped player pieces commonly known as meeples as well as traditional cards and dice.
The time required to learn or master gameplay varies greatly from game to game, but is not necessarily related to the number or complexity of rules; for example, chess or Go possess relatively simple rulesets but have great strategic depth.[1]
Classical board games are divided into four categories: race games (such as pachisi), space games (such as noughts and crosses), chase games (such as hnefatafl), and games of displacement (such as chess).[2]
Hounds and jackals, another ancient Egyptian board game, appeared around 2000 BC.[10][11] The first complete set of this game was discovered from a Theban tomb that dates to the 13th dynasty.[12] This game was also popular in Mesopotamia and the Caucasus.[13]
 Men Playing Board Games, from The Sougandhika Parinaya Manuscript
Patolli game being watched by Macuilxochitl as depicted on page 048 of the Codex Magliabechiano
Han dynasty glazed pottery tomb figurines playing liubo, with six sticks laid out to the side of the game board
Board games have a long tradition in Europe. The oldest records of board gaming in Europe date back to Homer's Iliad (written in the 8th century BC), in which he mentions the Ancient Greek game of petteia.[16] This game of petteia would later evolve into the Roman ludus latrunculorum.[16] Board gaming in ancient Europe was not unique to the Greco-Roman world, with records estimating that the ancient Norse game of hnefatafl was developed sometime before 400AD.[17] In ancient Ireland, the game of fidchell or ficheall, is said to date back to at least 144 AD,[18] though this is likely an anachronism. A fidchell board dating from the 10th century has been uncovered in Co. Westmeath, Ireland.[19]
The association of dice and cards with gambling led to all dice games except backgammon being treated as lotteries by dice in the gaming acts of 1710 and 1845.[20] Early board game producers in the second half of the eighteenth century were mapmakers. The global popularization of Board Games, with special themes and branding, coincided with the formation of the global dominance of the British Empire.[21] John Wallis was an English board game publisher, bookseller, map/chart seller, printseller, music seller, and cartographer. With his sons John Wallis Jr. and Edward Wallis, he was one of the most prolific publishers of board games of the late 18th and early 19th centuries.[citation needed] John Betts' A Tour of the British Colonies and Foreign Possessions[22] and William Spooner's A Voyage of Discovery[23] were popular in the British empire. Kriegsspiel is a genre of wargaming developed in 19th century Prussia to teach battle tactics to officers.[24]
Achilles and Ajax playing a board game overseen by Athena, Attic black-figure neck amphora, ca. 510 BCE
An early games table desk (Germany, 1735) featuring chess/draughts (left) and nine men's morris (right)
In 17th- and 18th-century colonial America, the agrarian life of the country left little time for game playing,[citation needed] although draughts (checkers), bowling, and card games were not unknown. The Pilgrims and Puritans of New England frowned on game-playing, and they often viewed dice as instruments of the devil. When Governor William Bradford discovered a group of non-Puritans playing stool ball, pitching the bar, and pursuing other sports in the streets on Christmas Day, 1622, he confiscated their implements, reprimanded them, and told them their devotion for the day should be confined to their homes.
Almost all these pursuits of chance [i.e., of human industry] produce something useful to society. But there are some which produce nothing, and endanger the well-being of the individuals engaged in them or of others depending on them. Such are games with cards, dice, billiards, etc. And although the pursuit of them is a matter of natural right, yet society, perceiving the irresistible bent of some of its members to pursue them, and the ruin produced by them to the families depending on these individuals, consider it as a case of insanity, quoad hoc, step in to protect the family and the party himself, as in other cases of insanity, infancy, imbecility, etc., and suppress the pursuit altogether, and the natural right of following it. There are some other games of chance, useful on certain occasions, and injurious only when carried beyond their useful bounds. Such are insurances, lotteries, raffles, etc. These they do not suppress, but take their regulation under their own discretion.[25]
The board game Traveller's Tour Through the United States and its sister game Traveller's Tour Through Europe were published by New York City bookseller F. & R. Lockwood in 1822 and claim the distinction of being the first board games published in the United States.[15]
As the U.S. shifted from agrarian to urban living in the 19th century, greater leisure time and a rise in income became available to the middle class. The American home, once the center of economic production, became the locus of entertainment, enlightenment, and education under mothers' supervision. Children were encouraged to play board games that developed literacy skills and provided moral instruction.[26]
Commercially produced board games in the mid-19th century were monochrome prints laboriously hand-colored by teams of low-paid young factory women. Advances in papermaking and printmaking during the period enabled the commercial production of relatively inexpensive board games. The most significant advance was the development of chromolithography, a technological achievement that made bold, richly colored images available at affordable prices. Games cost as little as US$.25 for a small-boxed card game to $3.00 for more elaborate games.
American Protestants believed a virtuous life led to success, but the belief was challenged mid-century when the country embraced materialism and capitalism. In 1860, The Checkered Game of Life rewarded players for mundane activities such as attending college, marrying and getting rich. Daily life rather than eternal life became the focus of board games. The game was the first to focus on secular virtues rather than religious virtues;[26] it sold 40,000 copies in its first year.[28]
Game of the District Messenger Boy, or Merit Rewarded, published in 1886 by the New York City firm of McLoughlin Brothers, was one of the first board games based on materialism and capitalism published in the United States. The game is a typical roll-and-move track board game. Players move their tokens along the track at the spin of the arrow toward the goal at the track's end. Some spaces on the track will advance the player while others will send him back.
In the affluent 1880s, Americans witnessed the publication of Algeresque rags to riches games that permitted players to emulate the capitalist heroes of the age. One of the first such games, The Game of the District Messenger Boy, encouraged the idea that the lowliest messenger boy could ascend the corporate ladder to its topmost rung. Such games insinuated that the accumulation of wealth brought increased social status.[26] Competitive capitalistic games culminated in 1935 with Monopoly, the most commercially successful board game in U.S. history.[29]
McLoughlin Brothers published similar games based on the telegraph boy theme including Game of the Telegraph Boy, or Merit Rewarded (1888). Greg Downey notes in his essay, "Information Networks and Urban Spaces: The Case of the Telegraph Messenger Boy", that families who could afford the deluxe version of the game in its chromolithographed, the wood-sided box would not "have sent their sons out for such a rough apprenticeship in the working world."[30]
Outside of Europe and the U.S., many traditional board games are popular. In China, Go and many variations of chess are popular. In Africa and the Middle East, mancala is a popular board game archetype with many regional variations. In India, a community game called Carrom is popular.[31]
Some games, such as chess, depend completely on player skill, while many children's games such as Candy Land and snakes and ladders require no decisions by the players and are decided purely by luck.[39]
Many games require some level of both skill and luck. A player may be hampered by bad luck in backgammon, Monopoly, or Risk; but over many games, a skilled player will win more often.[40] The elements of luck can also make for more excitement at times, and allow for more diverse and multifaceted strategies, as concepts such as expected value and risk management must be considered.[citation needed]
Luck may be introduced into a game by several methods. The use of dice of various sorts goes back to the earliest board games. These can decide everything from how many steps a player moves their token, as in Monopoly, to how their forces fare in battle, as in Risk, or which resources a player gains, as in Catan. Other games such as Sorry! use a deck of special cards that, when shuffled, create randomness. Scrabble does something similar with randomly picked letters. Other games use spinners, timers of random length, or other sources of randomness. German-style board games are notable for often having fewer elements of luck than many North American board games.[41]
Another important aspect of some games is diplomacy, that is, players, making deals with one another. Negotiation generally features only in games with three or more players, cooperative games being the exception. An important facet of Catan, for example, is convincing players to trade with you rather than with opponents. In Risk, two or more players may team up against others. Easy diplomacy involves convincing other players that someone else is winning and should therefore be teamed up against. Advanced diplomacy (e.g., in the aptly named game Diplomacy) consists of making elaborate plans together, with the possibility of betrayal.[42]
In perfect information games, such as chess, each player has complete information on the state of the game, but in other games, such as Tigris and Euphrates or Stratego, some information is hidden from players. This makes finding the best move more difficult and may involve estimating probabilities by the opponents.[citation needed]
Many board games are now available as video games. These are aptly termed digital board games, and their distinguishing characteristic compared to traditional board games is they can now be played online against a computer or other players. Some websites (such as boardgamearena.com, yucata.de, etc.)[43] allow play in real time and immediately show the opponents' moves, while others use email to notify the players after each move.[44] The Internet and cheaper home printing has also influenced board games via print-and-play games that may be purchased and printed.[45] Some games use external media such as audio cassettes or DVDs in accompaniment to the game.[46][47]
There are also virtual tabletop programs that allow online players to play a variety of existing and new board games through tools needed to manipulate the game board but do not necessarily enforce the game's rules, leaving this up to the players. There are generalized programs such as Vassal, Tabletop Simulator and Tabletopia that can be used to play any board or card game, while programs like Roll20 and Fantasy Grounds that are more specialized for role-playing games.[48][49] Some of these virtual tabletops have worked with the license holders to allow for use of their game's assets within the program; for example, Fantasy Grounds has licenses for both Dungeons & Dragons and Pathfinder materials, while Tabletop Simulator allows game publishers to provide paid downloadable content for their games.[50][51] However, as these games offer the ability to add in the content through user modifications, there are also unlicensed uses of board game assets available through these programs.[52]
A dedicated field of research into gaming exists, known as game studies or ludology.[65]
While there has been a fair amount of scientific research on the psychology of older board games (e.g., chess, Go, mancala), less has been done on contemporary board games such as Monopoly, Scrabble, and Risk,[66] and especially modern board games such as Catan, Agricola, and Pandemic. Much research has been carried out on chess, partly because many tournament players are publicly ranked in national and international lists, which makes it possible to compare their levels of expertise. The works of Adriaan de Groot, William Chase, Herbert A. Simon, and Fernand Gobet have established that knowledge, more than the ability to anticipate moves, plays an essential role in chess-playing ability.[67]
Linearly arranged board games have improved children's spatial numerical understanding. This is because the game is similar to a number line in that they promote a linear understanding of numbers rather than the innate logarithmic one.[68]
Research studies show that board games such as Snakes and Ladders result in children showing significant improvements in aspects of basic number skills such as counting, recognizing numbers, numerical estimation, and number comprehension. They also practice fine motor skills each time they grasp a game piece.[69] Playing board games has also been tied to improving children's executive functions[70] and help reduce risks of dementia for the elderly.[71][72] Related to this is a growing academic interest in the topic of game accessibility, culminating in the development of guidelines for assessing the accessibility of modern tabletop games[73] and the extent to which they are playable for people with disabilities.[74]
Additionally, board games can be therapeutic. Bruce Halpenny, a games inventor said when interviewed about his game, The Great Train Robbery:
With crime you deal with every basic human emotion and also have enough elements to combine action with melodrama. The player's imagination is fired as they plan to rob the train. Because of the gamble, they take in the early stage of the game there is a build-up of tension, which is immediately released once the train is robbed. Release of tension is therapeutic and useful in our society because most jobs are boring and repetitive.[75]
Playing games has been suggested as a viable addition to the traditional educational curriculum if the content is appropriate and the gameplay informs students on the curriculum content.[76][77]
There are several ways in which board games can be classified, and considerable overlap may exist, so that a game belongs to several categories.[15]
H. J. R. Murray's A History of Board Games Other Than Chess (1952) has been called the first attempt to develop a "scheme for the classification of board games".[78] David Parlett's Oxford History of Board Games (1999) defines four primary categories: race games (where the goal is to be the first to move all one's pieces to the final destination), space games (in which the object is to arrange the pieces into some special configuration), chase games (asymmetrical games, where players start the game with different sets of pieces and objectives) and displace games (where the main objective is the capture the opponents' pieces). Parlett also distinguishes between abstract and thematic games, the latter having a specific theme or frame narrative (ex. regular chess versus, for example, Star Wars-themed chess).[78]
The following is a list of some of the most common game categories:
Although many board games have a jargon all their own, there is a generalized terminology to describe concepts applicable to basic game mechanics and attributes common to nearly all board games.
Gameplay is the specific way in which players interact with a game,[1][2] and in particular with video games.[3][4] Gameplay is the pattern defined through the game rules,[2][5] connection between player and the game,[6] challenges[7] and overcoming them,[8] plot[9] and player's connection with it.[6] Video game gameplay is distinct from graphics[9][10] and audio elements.[9] In card games, the equivalent term is play.[a]
Arising alongside video game development in the 1980s, the term gameplay was used solely within the context of video games, though now its popularity has begun to see use in the description of other, more traditional, game forms. Generally, gameplay is considered the overall experience of playing a video game, excluding factors like graphics and sound. Game mechanics, on the other hand, is the sets of rules in a game that are intended to produce an enjoyable gaming experience. Academic discussions tend to favor game mechanics specifically to avoid gameplay since the latter is too vague.[13] The word is sometimes misapplied to card games where, however, the usual term is "play" and refers to the way the cards are played out in accordance with the rules (as opposed to other aspects such as dealing or bidding).
There are three components to gameplay: "Manipulation rules", defining what the player can do in the game, "Goal Rules", defining the goal of the game, and "Metarules", defining how a game can be tuned or modified.[14]  In video games gameplay can be divided into several types. For example, cooperative gameplay involves two or more players playing on a team. Another example is twitch gameplay which is based around testing a player's reaction times and precision, maybe in rhythm games or first-person shooters[citation needed].  Various gameplay types are listed below.
The term gameplay can be quite ambiguous to define; thus, it has been differently defined by different authors.
Playability is the ease by which the game can be played or the quantity or duration that a game can be played and is a common measure of the quality of gameplay.[20] Playability evaluative methods target games to improve design while player experience evaluative methods target players to improve gaming."[19]  This is not to be confused with the ability to control (or play) characters in multi-character games such as role playing games or fighting games, or factions in real-time strategy games.
Playability is defined as a set of properties that describe the Player Experience using a specific game system whose main objective is to provide enjoyment and entertainment by being credible and satisfying when the player plays alone or in the company of others. Playability is characterized by different attributes and properties to measure the video game player experience.[21]
The playability analysis is a very complex process due to the different point of view to analyze the different part of video game architecture. Each facet allows us to identify the different playability's attributes and properties affected by the different elements of video game architecture.[22] The playability's facets are:
Finally, a video game's "global" playability will be deduced through each attribute value in the different playability's facets. It is crucial to improve the playability in the different facets to guarantee the best player experience when the player plays the video game.
A chess variant is a game related to, derived from, or inspired by chess.[1] Such variants can differ from chess in many different ways.
"International" or "Western" chess itself is one of a family of games which have related origins and could be considered variants of each other. Chess developed from chaturanga, from which other members of this family, such as shatranj, Tamerlane chess,
shogi, and xiangqi also evolved.[2]
Many chess variants are designed to be played with the equipment of regular chess.[3] Most variants have a similar public-domain status as their parent game, but some have been made into commercial proprietary games. Just as in traditional chess, chess variants can be played over-the-board, by correspondence, or by computer. Some internet chess servers facilitate the play of some variants in addition to orthodox chess.
In the context of chess problems, chess variants are called heterodox chess or fairy chess.[4][5] Fairy chess variants tend to be created for problem composition rather than actual play.
The origins of the chess family of games can be traced to the game of chaturanga during the time of the Gupta Empire in India.[2] Over time, as the game spread geographically, modified versions of the rules became popular in different regions. In Sassanid Persia, a slightly modified form became known as shatranj. Modifications made to this game in Europe resulted in the modern game. Courier chess was a popular variant in medieval Europe, which had a significant impact on the "main" variant's development.[2]
Other games in the chess family, such as shogi (Japan), and xiangqi (China), are also developments from chaturanga made in other regions. These related games are considered chess variants[citation needed], though the majority of variants are, expressly, modifications of chess. The basic rules of chess were not standardized until the 19th century, and the history of chess before this involves many variants, with the most popular modifications spreading and eventually forming the modern game.
While some regional variants have historical origins comparable to or even older than chess, the majority of variants are express attempts by individuals or small groups to create new games with chess as a starting point. In most cases the creators are attempting to create new games of interest to chess enthusiasts or a wider audience. Variants normally have the same public domain status as chess, though a few (such as Knightmare Chess) are proprietary, and the materials for play are released as commercial products.
The variations from chess may be done to address a perceived issue with the standard game. For example, Fischer random chess, which randomises the starting positions, was invented by Bobby Fischer to combat what he perceived to be the detrimental dominance of opening preparation in chess.[8] Several variants introduce complications to the standard game, providing an additional challenge for experienced players, for example in Kriegspiel, where players cannot see the pieces of their opponent.
The table below details some, but not all, of the ways in which variants can differ from the orthodox game:
Variants can themselves be developed into further sub-variants, for example Horde chess is a variation upon Dunsany's Chess.[9]
Some variations are created for the purpose of composing interesting puzzles, rather than being intended for full games. This field of composition is known as fairy chess.
Fairy chess gave rise to the term "fairy chess piece" which is used more broadly across writings about chess variants to describe chess pieces with movement rules other than those of the standard chess pieces. Forms of standardised notation have been devised to systematically describe the movement of these. A distinguishing feature of several chess variants is the presence of one or more fairy pieces. Physical models of common fairy pieces are sold by major chess set suppliers.[10]
Individuals notable for creating multiple chess variants include V. R. Parton (best known for Alice chess), Ralph Betza, Philip M. Cohen and George R. Dekle Sr.
Some board game designers, notable for works across a wider range of board games, have created chess variants. These include Robert Abbott (Baroque chess) and Andy Looney (Martian chess).
While chess, shogi, and xiangqi have professional circuits as well as many organised tournaments for amateurs, play of chess variants is predominately on a casual basis.
Several internet chess servers facilitate live play of popular variants, including Chess.com,[12] Lichess,[13] and the Free Internet Chess Server.[14] The software packages Zillions of Games and Fairy-Max have been programmed to support many chess variants.[15][16]
Some chess engines are also able to play a handful of variants, for instance the version of Stockfish implemented on Lichess is able to play Crazyhouse, King-of-the-hill, Three-check chess, Atomic chess, Horde chess, and Racing Kings.[17] The AI included in Zillions of Games is able to play almost any variant correctly programmed within it to a reasonable standard.[16] Some variants, such as 5D Chess with Multiverse Time Travel, are implausible or even impossible to play physically and exist primarily as video games.
Various publications have been written regarding chess variants. Variant Chess magazine was published from 1990 to 2010, being an official publication of the British Chess Variants Society from 1997. This outlined and introduced multiple variants, as well as containing in-depth analyses.[18]
A leading figure in the field was David Pritchard, who authored several books on the topic. Most significantly, he compiled an encyclopedia of variants which outlined thousands of different games. Following Pritchard's death in 2005, the second edition of the encyclopedia was completed and published by John Beasley under the title The Classified Encyclopedia of Chess Variants.[19]
A recent overview of historical and some modern variants was published under the title of A World of Chess in 2017.[20]
The Chess Variant Pages website includes a constantly expanding catalogue of variants.
Chess variants have been invented in various fiction.[25] In The Chessmen of Mars author Edgar Rice Burroughs describes Jetan which depicts a war between two races of Martian. An appendix fully defines the rules of the game. More commonly specifics of fictional variants are not detailed in the original works, though several have been codified into playable games by fans. An example of this is Tri-Dimensional Chess from Star Trek. On-screen play was not conducted to any specific rules, but a comprehensive rulebook has been since developed.[2] Another well known example of fictional chess-like game are the Star Wars holochess, or dejarik.[26]
Fictional chess variants can involve fantastical or dangerous elements that cannot be implemented in real life. The Chessmen of Mars describes a form of Jetan where the pieces are human beings and captures are replaced by fights to the death between them. The Doctor Who episode "The Wedding of River Song" depicts "Live Chess", which introduces potentially lethal electric currents into the game.
The game represents a battle between two armies, with the primary object being to checkmate the enemy's general (king). Distinctive features of xiangqi include the cannon (pao), which must jump to capture; a rule prohibiting the generals from facing each other directly; areas on the board called the river and palace, which restrict the movement of some pieces but enhance that of others; and the placement of the pieces on the intersections of the board lines, rather than within the squares.
The pieces start in the position shown in the diagram above. Which player moves first has varied throughout history and from one part of China to another. Different xiangqi books advise either that the black or red side moves first.[citation needed] Some books refer to the two sides as north and south; which direction corresponds to which colour also varies from source to source. Generally, Red moves first in most modern tournaments.[1]
Different sets of rules set different limits on what is considered perpetual. For example, club xiangqi rules allow a player to check or chase six consecutive times using one piece, twelve times using two pieces, and eighteen times using three pieces before considering the action perpetual.[2]
The above rules to prevent perpetual checking and chasing, while popular, are not the only ones; there are numerous end game situations.[3]
Each player controls an army of 16 pieces; the armies are usually coloured red and black.[4] Pieces are flat circular disks labelled or engraved with a Chinese character identifying the piece type, and in a colour indicating which player has ownership. The black pieces are marked with somewhat different characters from the corresponding red pieces.
The general starts the game at the midpoint of the back edge, within the palace. The general may move and capture one point orthogonally and may not leave the palace, with the following exception.
The advisors start on either side of the general. They move and capture one point diagonally and may not leave the palace, which confines them to five points on the board.[6] The advisor is probably derived from the mantri in chaturanga, like the queen in Western chess.
Elephants may not cross the river to attack the enemy general, and serve as defensive pieces. Because an elephant's movement is restricted to just seven board positions, it can be easily trapped or threatened. The two elephants are often used to defend each other.
The Chinese characters for "minister" and "elephant" are homophones in Mandarin (Listen) and both have alternative meanings as "appearance" or "image". However, in English, both are referred to as elephants, and less commonly as "bishops", due to their similar movements.[8]
Since horses can be blocked, it is sometimes possible to trap the opponent's horse. It is possible for one player's horse to have an asymmetric attack advantage if an opponent's horse is blocked, as seen in the diagram on the right.
The horse is sometimes called the "knight" by English-speaking players, due to their similar movements.[8]
These approximate values[10] do not take into account the position of the piece in question (except the soldier in a general sense), the positions of other pieces on the board, or the number of pieces remaining.
There are several types of notation used to record xiangqi games. In each case the moves are numbered and written with the same general pattern.
It is clearer but not required to write each move pair on a separate line.
The book The Chess of China describes a move notation method in which the ranks of the board are numbered 1 to 10 from closest to farthest away, followed by a digit 1 to 9 for files from right to left.[11] Both values are relative to the moving player. Moves are then indicated as follows:
Thus, the most common opening in the game would be written as:
A notation system partially described in A Manual of Chinese Chess[12] and used by several computer software implementations describes moves in relative terms as follows:
[single-letter piece abbreviation][former file][operator indicating direction of movement][new file, or in the case of purely vertical movement, number of ranks traversed]
The file numbers are counted from each player's right to each player's left.
Thus, the most common opening in the game would be written as:
According to World Xiangqi Federation (WXF) notations for tandem pawns case when there are (tandem pawns) >=3 in a file, there is no need to specify the P for pawn. Instead, the position of the pawn in the tandem line is used as the first integer (with the front most pawn designated as 1). The second integer would be the file on which it was on. This would also solve the problem of two set of tandem pawns on two different files nicely.
Thus the notation to move the middle pawn (3 tandem pawns in a file) on the 5th file to the 4th file would be:
Thus, the most common opening in the game might be written as:
This system is unofficial and principally used by Western players.[citation needed] It is similar to algebraic notation for Western chess. Letters are used for files and numbers for ranks. File "a" is on Red's left and rank "1" is nearest to Red. A point's designation does not depend on which player moves; for both sides "a1" is the lowest left point from Red's side.
Pieces are abbreviated as in notation system 2, except that no letter is used for the soldier.
Former position is only indicated if necessary to distinguish between two identical pieces that could have made the move. If they share the same file, indicate which rank moves; if they share the same rank, indicate which file moves. If they share neither rank nor file, then the file is indicated.
Capture is indicated by "x". No symbol is used to indicate a non-capturing move.
Check is indicated by "+", double check by "++", triple check by "+++", and quadruple check by "++++". Checkmate is indicated by "#".
For analysis purposes, bad moves are indicated by "?" and good moves by "!". These can be combined if the analysis is uncertain ("!?" might be either but is probably good; "?!" is probably bad) or repeated for emphasis ("??" is a disaster).
Thus, the most common opening in the game would be written as:
An example of a brief game ("the early checkmate") is:
Because of the size of the board and the low number of long-range pieces, there is a tendency for the battle to focus on a particular area of the board.[clarification needed]
Xiangqi involves several tactics common to games in the chess family. Some common ones are briefly discussed here.
Usually, soldiers do not support each other until the endgame, because from the initial position it takes a minimum of five moves of a soldier to allow mutual protection between two of them, and they are often prone to capture by other pieces.
It is common to use cannons independently to control particular ranks and files. Using a cannon to control the middle file is often considered vital strategy, because it pins pieces such as the advisors and elephants. The two files adjacent to the middle file are also considered important and horses and chariots can be used to push for checkmate there.
Since the general is usually safest in his original position before the endgame phase, attacking the general commonly involves forcing the general out of his original position with check or with threats. Thus, specific points and formations are very important in xiangqi.
A common defensive configuration is to leave the general at its starting position, deploy one advisor and one elephant on the two points directly in front of the general, and to leave the other advisor and elephant in their starting positions, to the side of the general. In this setup, the advisor-elephant pairs support each other, and the general is immune from attacks by cannons. Losing any of the pieces makes the general vulnerable to cannon, and the setup may need to be abandoned. The defender may move advisors or elephants away from the general, or even sacrifice them intentionally, to ward off attack by a cannon.
Long sequences of checks leading to mate or gain of material are common both in chess compositions and in actual play. A skilled xiangqi player would often have to calculate several steps, or even tens of steps ahead for a forced sequence. In the diagram on the right, Black has an immediate mating threat which cannot be parried, forcing Red to check Black on every move. Although it requires 11 moves to mate, its general idea is clear: Induce a smothered check by sacrificing a chariot at the centre of the palace (e9), then force Black to open the centre file, enabling the Red general to assist the attack, and finally mate by facing generals.
Since the left and right flanks of the starting setup are symmetrical, it is customary to make the first move on the right flank. Starting on the left flank is considered needlessly confusing.[citation needed]
Alternative common first moves by Black are developing either cannons (1. ...C8.5/1. ...Che8, or 1. ...C2.5/1. ...Cbe8); note that after either of these moves, taking the central soldier with the cannon (2. C5+4 or 2. Cxe7+) is a beginner's trap that impedes development and coordination of Red's pieces if Black plays correctly (for example, 1. Che3 Che8 2. Cxe7+?? Ade9 3. Hg3 Hg8 4. Ce5 Rh10 when Black develops the rook first, and the loss of Black's middle pawn actually enabled Black's horses to occupy the centre on the next moves).
Other common first moves by Red include moving an elephant to the central column (1. Ege3), advancing the soldier on the third or seventh file (1. c5), moving a horse forward (1. Hg3), and moving either cannon to the 4th or 6th (d- or f-) file (1. Chd3 or 1. Chf3). Compared to the Central Cannon openings, these openings are generally less restricted by theory.
Xiangqi strategy shares common themes with chess, but have some differences:
Like in chess, xiangqi piece values depend highly on the position on the board. The following study from Volume 42 of the Elegant Pastime Manual, dating from the Ming Dynasty, illustrates this dramatically. It is Red to play and win.[13]
In this position, Red is up a soldier and cannon but Black threatens seemingly unstoppable mate with ...Rf1#, since 1.Ec5? Ad8! renews the mate threat. Note that the red chariot is nearly useless, having only two legal moves, in stark contrast to the very active black chariot. However, Red averts the checkmate by sacrificing both the cannon and chariot: 1.Ca10+!! Hxa10 2.Ea3! Rxa1 (otherwise the chariot is lost, since the red horse defends e2) 3.Eec1:
Despite the substantial sacrifice of material by Red, Black's chariot has now become useless as it is permanently immobilized by the red elephants and horse; the red general prevents the black soldier on g2 from moving laterally to free the chariot (for example 3...g1 4.Gf2). Black's horse similarly has no safe move due to the red soldier on c8. In addition, the black soldier on g6 is undefended and has no safe move, so Red can win it by pushing the soldier on c4 to c6 and moving it laterally to the g-file, after which the position is effectively an endgame of three soldiers against two advisers, an easy win for Red (see below) despite being down a chariot for three soldiers.
Though xiangqi endgames require remarkable skill to be played well, there are a number of widely known book wins and book draws. Without a counterpart to pawn promotion, xiangqi endgames instead focus more directly on forcing checkmate or stalemate, and in this regard resemble pawnless chess endgames. Since stalemate is a loss for the stalemated player instead of a draw, most book draws in xiangqi are due to fortresses, with a few draws due to insufficient material.
A general rule in xiangqi endgames for the advantageous side is that, when there's less material on the board, do not trade pieces easily, as with fewer attacking pieces on the board, defending is easier (in contrast to Western chess, where it is almost always advantageous to trade pieces when up on material). Hence, if a certain type of endgame can transpose, by trading pieces, into another type of endgame which is a book win, then this endgame itself is a book win.
Inducing zugzwang is a crucial theme in winning simple endgames, and almost exclusively in simple endgames. In the general + soldier vs general endgame shown on the right, Red's first main goal is to occupy the middle file. Red wins with 1. Gd1, a waiting move, and Black is in zugzwang. Black must proceed with 1. ...Ge8, as 1. ...Ge10 instantly loses after 2. f9#. After 1. ...Ge8 2. f9 Gf8 3. e9 Ge8 4. d9 Gf8 5. Ge1, Red's general successfully occupies the middle file. The game would conclude with 5. ...Gf9 6. e9+, and regardless of Black's reply, 7. Ge2# (stale)mates Black.
Reciprocal zugzwang is possible, but very rare and usually seen in endgame compositions. In this endgame shown on the right, whoever moves loses, since when either of the two generals moves to an open d- or f- file, it threatens unstoppable mate, while the player to move only helps the enemy general occupy one of the files. For instance, Red can only move his two soldiers if he is to move. Moving the f-(or d-)soldier allows the enemy general to occupy the f-file(d-file). Even if 1. fe9+ Gf10 2. d10, when Red threatens mate in 1, Black still mates immediately with either 2. ...fe2# or 2. ...f1#.
This type of endgame is considered one of the more complex endgames. Commonly known book wins and book draws are:
To support his argument, Murray quoted an old Chinese source that says that in the older xiangqi (which modern xiangqi may have taken some of its rules from) the game pieces could be shuffled, which does not happen in the modern chess-style xiangqi.[15] Murray also wrote that in ancient China there was more than one game called xiangqi.[16] Murray also even supposed that chaturanga from India influenced the formation of present-day xiangqi.[17]
Xiangqi is the same as it is today from Southern Song dynasty.
With the popularization of xiangqi, many different schools of circles and players came into prominence, many books and manuals on the techniques of playing the game were also published, they played an important role in popularizing xiangqi and improving the techniques of play in modern times. With the economic and cultural development during the Qing dynasty, xiangqi entered a new stage. A Western-style Encyclopedia of Chinese Chess Openings was written in 2004.[20]
Although xiangqi has its origin in Asia, there are xiangqi leagues and clubs all over the world. Each European nation generally has its own governing league; for example, in Britain, xiangqi is regulated by the United Kingdom Chinese Chess Association. Asian countries also have nationwide leagues, such as the Malaysia Chinese Chess Association.[citation needed]
In addition, there are several international federations and tournaments. The Chinese Xiangqi Association hosts several tournaments every year, including the Yin Li and Ram Cup Tournaments.[21] Other organizations include the Asian Xiangqi Federation[22] and a World Xiangqi Federation,[23] which hosts tournaments and competitions bi-annually, with most limited to players from member nations.
The Asian Xiangqi Federation (AXF) and its corresponding member associations rank players in a format similar to the Elo rating system of chess. According to the XiangQi DataBase, the top-ranking female and male players in China, as of June 2012, were Tang Dan and Jiang Chuan, with ratings of 2529 and 2667, respectively.[27][28] Other strong players include Zhao GuanFang (female), Xu Yinchuan (male), Lu Qin (male), and Wang LinNa (female).[citation needed]
The Asian Xiangqi Federation also bestows the title of grandmaster to select individuals around the world who have excelled at xiangqi or made special contributions to the game. There are no specific criteria for becoming a grandmaster and there are only approximately 100 grandmasters as of 2020.[29] The titles of grandmaster is bestowed by bodies such as the AXF and the Chinese Xiangqi Association (CXA).[30]
The game-tree complexity of xiangqi is approximately 10150; in 2004 it was projected that a human top player would be defeated before 2010.[31] Xiangqi is one of the more popular computer-versus-computer competitions at the Computer Olympiads.[32]
Computer programs for playing xiangqi show the same development trend as has occurred for international chess: they are usually console applications (called engines) which communicate their moves in text form through some standard protocol. For displaying the board graphically, they then rely on a separate graphical user interface (GUI). Through such standardization, many different engines can be used through the same GUI, which can also be used for automated play of different engines against each other. Popular protocols are UCI (Universal Chess Interface), UCCI (Universal Chinese Chess Interface), Qianhong (QH) protocol, and WinBoard/XBoard (WB) protocol (the latter two named after the GUIs that implemented them). There now exist many dozens of xiangqi engines supporting one or more of these protocols, including some commercial engines.[citation needed]
Manchu chessInvented during the Manchu-led Qing dynasty. Red horses, cannons, and one of the chariots are absent, but the remaining chariot can be played as horses and cannons as well.
Supply chess Similar to the Western chess variant Bughouse chess, this variant features the ability to re-deploy captured pieces, similar to a rule in shogi. Four players play as two-person teams in two side-by-side games. One teammate plays Black and other plays Red. Any piece obtained by capturing the opponent's piece is given to the teammate for use in the other game. These pieces can be deployed by the teammate to give him an advantage over the other player, so long as the piece starts on the player's own side of the board and does not cause the opponent to be in check.
Formation Similar to Fischer Random Chess, one player's pieces are placed randomly on one side of the river, except for the generals and advisors, which must be at their usual positions, and the elephants, which must start at two of the seven points they can normally reach. The other player's pieces are set up to mirror the first's. All other rules are the same.
Banqi This variation is more well known in Hong Kong than in mainland China. It uses the xiangqi pieces and board, but does not follow any of its rules, bearing more of a resemblance to the Western game Stratego as well as the Chinese game Luzhanqi.
There are many versions of three-player xiangqi, or san xiangqui, all played on special boards.
Sanrenqi "Three Men Chess" is a riverless, commercial variant played on a cross-shaped board with some special rules, including a fourth, neutral country called Han. Han has three Chariots, one Cannon, and one General named "Emperor Xian of Han", but these pieces do not move and do not belong to any of the players until a certain point in the game when two players team up against the third player. At that point the third player gets to also control Han.[34]
Si Guo Qi "Four Kingdoms Chess" is also played on a riverless, cross-shaped board, but with four players. Because there are no rivers, elephants may move about the board freely.[34]
Qi Guo Xiang Qi "Game of the Seven Kingdoms" is based symbolically on the Warring States Period.
Xiangqi pieces were added to the Unicode Standard in June 2018 with the release of version 11.0.
The game appears in the early 17th-century novel Jin Ping Mei.[35]
In Season 1, Episode 21 of Person of Interest, the protagonist John Reese plays the game with an older Chinese man.[36]
Shogi was the earliest chess-related historical game to allow captured pieces to be returned to the board by the capturing player.[2] This drop rule is speculated to have been invented in the 15th century and possibly connected to the practice of 15th century mercenaries switching loyalties when captured instead of being killed.[3]
Several of these names were chosen to correspond to their rough equivalents in international chess, and not as literal translations of the Japanese names.
Each piece has its name written on its surface in the form of two kanji (Chinese characters used in Japanese), usually in black ink. On the reverse side of each piece, other than the king and gold general, are one or two other characters, in amateur sets often in a different color (usually red); this side is turned face up during play to indicate that the piece has been promoted.
Following is a table of the pieces with their Japanese representations and English equivalents. The abbreviations are used for game notation and often when referring to the pieces in speech in Japanese.
English speakers sometimes refer to promoted bishops as horses and promoted rooks as dragons, after their Japanese names, and generally use the Japanese term tokin for promoted pawns. Silver generals and gold generals are commonly referred to simply as silvers and golds, respectively.
The suggestion that the Japanese characters have deterred Western players from learning shogi has led to "Westernized" or "international" pieces which use iconic symbols instead of characters. Most players soon learn to recognize the characters, however, partially because the traditional pieces are already iconic by size, with more powerful pieces being larger. As a result, Westernized pieces have never become popular. Bilingual pieces with both Japanese characters and English captions have been developed as have pieces with animal cartoons.
Each player sets up friendly pieces facing forward (toward the opponent).
After the piece toss furigoma, the game proceeds. If multiple games are played, then players alternate turns for who goes first in subsequent games. (The terms "Black" and "White" are used to differentiate sides although there is no difference in the color of the pieces.) For each turn, a player may either move a piece that is currently on the board (and potentially promote it, capture an opposing piece, or both) or else drop a piece that has been previously captured onto a square of the board. These options are explained below.
The usual goal of a game is for one player to checkmate the other player's king, winning the game.
Most shogi pieces can move only to an adjacent square. A few may move across the board, and one jumps over intervening pieces.
The lance, bishop, and rook are ranging pieces: They can move any number of squares along a straight line limited only by intervening pieces and the edge of the board. If an opposing piece intervenes, it may be captured by removing it from the board and replacing it with the moving piece. If a friendly piece intervenes, the moving piece must stop short of that square; if the friendly piece is adjacent, the moving piece may not move in that direction at all.
All pieces but the knight move either horizontally, vertically, or diagonally. These directions cannot be combined in a single move; one direction must be chosen.
Every piece blocks the movement of all other non-jumping pieces through the square it occupies.
If a piece occupies a legal destination for an opposing piece, it may be captured by removing it from the board and replacing it with the opposing piece. The capturing piece may not continue beyond that square on that turn. Shogi pieces capture the same as they move.
Normally, when moving a piece, a player snaps it to the board with the ends of the fingers of the same hand. This makes a sudden sound effect, bringing the piece to the attention of the opponent. This is also true for capturing and dropping pieces. On a traditional shogi-ban, the pitch of the snap is deeper, delivering a subtler effect.
Promoting a piece is usually not compulsory; however, if a pawn or lance is moved to the furthest rank, or a knight is moved to either of the two furthest ranks, that piece must promote (otherwise, it would have no legal move on subsequent turns). A silver general is never required to promote, and it is often advantageous to keep a silver general unpromoted (it is easier, for example, to extract an unpromoted silver from behind enemy lines: a promoted silver, with only one line of retreat, can be easily blocked.) Rooks, bishops and pawns are almost always promoted, as these pieces do not lose any of their powers upon promotion.
Promoting a piece changes the way it moves. The various pieces promote as follows:
When captured, a piece loses its promoted status. Otherwise promotion is permanent.
A drop cannot capture a piece, nor does dropping within the promotion zone result in immediate promotion. Capture and/or promotion may occur normally, however, on subsequent moves of the piece.
Restrictions. There are three restrictions on dropping pieces; the last two of these apply only to pawns.
A corollary of the second restriction is that a player with an unpromoted pawn on every file is unable to drop a pawn anywhere. For this reason, it is common to sacrifice a pawn in order to gain flexibility for drops.
It is common for players to swap bishops, which oppose each other across the board, early in the game. This leaves each player with a bishop in hand to be dropped later. The ability for drops in shogi gives the game tactical richness and complexity. The fact that no piece ever goes entirely out of play accounts for the rarity of draws.
When a player's move threatens to capture the opposing king on the next turn, the move is said to give check to the king and the king is said to be in check. If a player's king is in check, that player's responding move must remove the check if possible.[7] Ways to remove a check include moving the king away from the threat, capturing the threatening piece, or placing another interposing piece between the king and the threatening piece.
Unlike western chess, there is no tradition of offering a mutual draw by agreement.
In professional and serious (tournament) amateur games, a player who makes an illegal move loses immediately.[c] The loss stands even if play continued and the move was discovered later in game. However, if neither the opponent nor a third party points out the illegal move and the opponent later resigned, the resignation stands as the result.
In friendly amateur games, this rule is sometimes relaxed, and the player may be able to take back the illegal move and replay a new legal move.[12][13]
In professional shogi, a repetition draw outcome is not a final result as draws essentially do not count. Each game can only end in either a win or loss.[d] In the case of a repetition draw, professional shogi players will have to immediately play a subsequent game (or as many games as necessary) with sides reversed in order to obtain a true win outcome. (That is, the player who was White becomes Black, and vice versa.) Also, depending on the tournament, professional players play the subsequent game in the remainder of the allowed game time.
However, in amateur shogi, there are different practices most of which force a win resolution to the Impasse in order to avoid a draw result.
In the adjacent diagram example, although White's king is in a strong Bear-in-the-hole castle, Black's king has entered White's territory making it very difficult to mate. Therefore, this position favors Black.[18]
For amateur games, there are various guidances with little standardization. Fairbairn reports a practice in the 1980s (considered a rule by the now defunct Shogi Association for The West) where the dispute is resolved by either player moving all friendly pieces into the promotion zone and then the game ends with points tallied.[22]
Shogi has a handicap system (like go) in which games between players of disparate strengths are adjusted so that the stronger player is put in a more disadvantageous position in order to compensate for the difference in playing levels. In a handicap game, one or more of White's pieces are removed from the setup, and instead White plays first.
There are two common systems used to notate piece movements in shogi game records. One is used in Japanese language texts while a second was created for western players by George Hodges and Glyndon Townhill in the English language. This system was updated by Hosking to be closer to the Japanese standard (two numerals).[32][33] Other systems are used to notate shogi board positions.  Unlike chess, the origin (11 square) is at the top right of a printed position rather than the bottom left.
Although not strictly part of the notational calculus for games, game results are indicated in Japanese newspapers, websites, etc. with wins indicated by a white circle and losses indicated by a black circle.
Shogi is similar to chess but has a much larger game tree complexity because of the use of drops, greater number of pieces, and larger board size.[34] In comparison, shogi games average about 140 (half-)moves per game (or 70 chess move-pairs) whereas  chess games average about 80 moves per game (or 40 chess move-pairs) and minishogi averages about 40 moves per game (or 20 chess move-pairs).[35][e]
Like chess, however, the game can be divided into the opening, middle game and endgame, each requiring a different strategy. The opening consists of arranging one's defenses usually in a castle and positioning for attack; the mid game consists of attempting to break through the opposing defenses while maintaining one's own; and the endgame starts when one side's defenses have been compromised.
Shogi players are expected to follow etiquette in addition to rules explicitly described. Commonly accepted etiquette include the following:
The higher-ranked (or older) player also sits facing the door of the room and is the person who takes the pieces out of the piece box.[36]
Shogi does not have a touch-move rule as in western chess tournament play or chu shogi. However, in professional games, a piece is considered to be moved when the piece has been let go of. In both amateur and professional play, any piece may be touched in order to adjust its centralization within its square (to look tidy).[37]
Professional players are required to follow several ritualistic etiquette prescriptions such as kneeling exactly 15 centimeters from the shogi board, sitting in the formal seiza position, etc.[38]
 The world's first chess variant, chaturanga arose in India in approximately the seventh century AD. From there it migrated both westward and northward, mutating along the way. The western branch became shatranj in Arabia and Orthodox Chess in Europe. The northern branch became xiangqi in China and janggi in Korea. Sometime in the tenth to twelfth centuries, 'chess' crossed the channel to Japan where it spawned a number of interesting variants. One of these was called 'Small Shogi'. Eventually, Small Shogi (though it went through many forms) won out over the larger variants and is now referred to simply as 'Shogi'. It is certain that Shogi in its present form was played in Japan as early as the 16th century.
Around the 13th century the game of dai shogi developed, created by increasing the number of pieces in Heian shogi, as was sho shogi, which added the rook, bishop, and drunken elephant from dai shogi to Heian shogi. The drunken elephant steps one square in any direction except directly backward, and promotes to the prince, which acts as a second king and must also be captured along with the original king for the other player to win. Around the 15th century, the rules of dai shogi were simplified, creating the game of chu shogi. Chu shogi, like its parent dai shogi, contains many distinct pieces, such as the queen (identical with Western chess) and the lion (which moves like a king, but twice per turn, potentially being able to capture twice, among other idiosyncrasies). The popularity of dai shogi soon waned in favour of chu shogi, until it stopped being played commonly. Chu shogi rivalled sho shogi in popularity until the introduction of drops in the latter, upon which standard shogi became ascendant, although chu shogi was still commonly played until about World War II, especially in Kyoto.
It is thought that the rules of standard shogi were fixed in the 16th century, when the drunken elephant was removed from the set of pieces present in sho shogi. There is no clear record of when drops were introduced, however.
After the Second World War, SCAP (occupational government mainly led by US) tried to eliminate all "feudal" factors from Japanese society and shogi was included in the possible list of items to be banned along with Bushido (philosophy of samurai) and other things. The reason for banning shogi for SCAP was its exceptional character as a board game seen in the usage of captured pieces. SCAP insisted that this could lead to the idea of prisoner abuse. But Kozo Masuda, then one of the top professional shogi players, when summoned to the SCAP headquarters for an investigation, criticized such understanding of shogi and insisted that it is not shogi but western chess that potentially contains the idea of prisoner abuse because it just kills the pieces of the opponent while shogi is rather democratic for giving prisoners the chance to get back into the game. Masuda also said that chess contradicts the ideal of gender equality in western society because the king shields itself behind the queen and runs away. Masuda's assertion is said to have eventually led to the exemption of shogi from the list of items to be banned.[42]
The JSA is the only body which can organize tournaments for professionals, e.g., the eight major tournaments in the titleholder system and other professional tournaments. In 1996, Yoshiharu Habu became the only kishi to hold seven major titles at the same time. For female professionals, both the JSA and LPSA organize tournaments, either jointly or separately. Tournaments for amateurs may be organized by the JSA and LPSA as well as local clubs, newspapers, private corporations, educational institutions or municipal governments for cities or prefectures under the guidance of the JSA or LPSA.
As of November 2017[update], in Europe there are currently over 1,200 active players.[47]
Shogi has the highest game complexity of all popular chess variants. Computers have steadily improved in playing shogi since the 1970s. In 2007, champion Yoshiharu Habu estimated the strength of the 2006 world computer shogi champion Bonanza at the level of two-dan shoreikai.
On October 12, 2010, after some 35 years of development, a computer finally beat a professional player, when the top ranked female champion Ichiyo Shimizu was beaten by the Akara2010 system in a game lasting just over 6 hours.[49]
On July 24, 2011, computer shogi programs Bonanza and Akara crushed the amateur team of Kosaku and Shinoda in two games. The allotted time for the amateurs was one hour and then three minutes per move. The allotted time for the computer was 25 minutes and then 10 seconds per move.[50]
On April 20, 2013, GPS Shogi defeated 8-dan professional shogi player Hiroyuki Miura in a 102-move game which lasted over 8 hours.[51]
On December 13, 2015, the highest rated player on Shogi Club 24 was computer program Ponanza, rated 3455.[52]
On April 10, 2016, Ponanza defeated Takayuki Yamasaki, 8-dan in 85 moves. Takayuki used 7 hours 9 minutes.[53]
In October 2017, DeepMind claimed that its program AlphaZero, after a full nine hours of training, defeated elmo in a 100-game match, winning 90, losing 8, and drawing two.[54][55]
From a computational complexity point of view, generalized shogi is EXPTIME-complete.[56]
Hundreds of video games were released exclusively in Japan for several consoles.
Clubhouse Games: 51 Worldwide Classics was released internationally by Nintendo in 2020 for the Nintendo Switch console, offering both Shogi and mini Shogi variants using either traditional or bilingual pieces.
According to professional player Yoshiharu Habu, in Japan shogi is viewed as not merely a game as entertainment or a mind sport but is instead an art that is a part of traditional Japanese culture along with haiku, tanka, noh, ikebana, and the Japanese tea ceremony. Its elevated status was established by the iemoto system supported by the historical shogunate.[57][58]
In Japan, Shogi is one of the most popular board games, with a player population that some say exceeds 10 million. Go has 3 million players, and Western chess has 20,000 players.
Ningen Shogi held in Himeji City with real people (2015)
In the manga and anime series Naruto, shogi plays an essential part in Shikamaru Nara's character development. He often plays it with his sensei, Asuma Sarutobi, apparently always beating him. When Asuma is fatally injured in battle, he reminds Shikamaru that the shogi king must always be protected, and draws a parallel between the king in shogi and the children who would grow up to take care of the Hidden Leaf (Konoha) in the future, as well as his yet-unborn daughter, Mirai, whom he wanted Shikamaru to guide.
In the manga and anime Durarara!!, the information broker Izaya Orihara plays a twisted version of chess, go and shogi, where he mixes all three games into one as a representation of the battles in Ikebukuro.
In the video game Persona 5, the Star confidant, a girl named Hifumi Togo, is a high school shogi player looking to break into the ranks of the professionals. The player character will gain a knowledge stat when spending time with the confidant, supposedly from learning to play shogi. The abilities learned from ranking up the confidant comes from Japanese shogi terms.
In the manga and anime When Will Ayumu Make His Move?, second-year high school student Urushi Yaotome is the president of her school's shogi club, though the club is considered illegitimate due to not having enough members, the only other member being first-year student Ayumu Tanaka.
The history of chess can be traced back nearly 1500 years to its earliest known predecessor, called chaturanga, in India; its prehistory is the subject of speculation. From India it spread to Persia. Following the Arab invasion and conquest of Persia, chess was taken up by the Muslim world and subsequently spread to Spain and the rest of Southern Europe. The game evolved roughly into its current form by about 1500 CE.
The Mongols call the game shatar, and in Ethiopia it is called senterej, both evidently derived from shatranj.
The game reached Western Europe and Russia by at least three routes, the earliest being in the 9th century. By the year 1000 it had spread throughout Europe.[7] Introduced into the Iberian Peninsula by the Moors in the 10th century, it was described in a famous 13th-century Spanish manuscript covering shatranj, backgammon and dice named the Libro de los juegos, which is the earliest European treatise on chess as well as being the oldest document on European tables games.
Chess spread throughout the world and many variants of the game soon began taking shape.[8] Buddhist pilgrims, Silk Road traders and others carried it to the Far East where it was transformed and assimilated into a game often played on the intersection of the lines of the board rather than within the squares.[9][10] Chaturanga reached Europe through Persia, the Byzantine empire and the expanding Arabian empire.[11] Muslims carried chess to North Africa, Sicily, and Iberia by the 10th century.[12]
The game was developed extensively in Europe.  By the late 15th century, it had survived a series of prohibitions and Christian Church sanctions to almost take the shape of the modern game.[13] Modern history saw reliable reference works,[14] competitive chess tournaments,[15] and exciting new variants.  These factors added to the game's popularity,[15] further bolstered by reliable timing mechanisms (first introduced in 1861), effective rules,[15] and charismatic players.[16]
The Cox-Forbes theory, proposed in the late 18th century by Hiram Cox, and later developed by Duncan Forbes, asserted that the four-handed game chaturaji was the original form of chaturanga.[22]  The theory is no longer considered tenable.[23]
Some people formerly played chess using a die to decide which piece to move. There was an unproven theory that chess started as this dice-chess and that the gambling and dice aspects of the game were removed because of Hindu religious objections.[25]
In some variants, a win was by checkmate, or by stalemate, or by "bare king" (taking all of an opponent's pieces except the king).
In some parts of India the pieces in the places of the rook, knight and bishop were renamed by words meaning (in this order) Boat, Horse, and Elephant, or Elephant, Horse, and Camel, but keeping the same moves.[27]
In early chess the moves of the pieces were:
Two Arab travelers each recorded a severe Indian chess rule against stalemate:[28]
Iranian shatranj set, glazed fritware, 12th century, New York Metropolitan Museum of Art[29]
Persian manuscript from the 14th century describing how an ambassador from India brought chess to the Persian court
The Karnamak-i Ardeshir-i Papakan, a Pahlavi epical treatise about the founder of the Sassanid Persian Empire, mentions the game of chatrang as one of the accomplishments of the legendary hero, Ardashir I, founder of the Empire.[30] The oldest recorded game in chess history is a 10th-century game played between a historian from Baghdad and a pupil.[11][non-tertiary source needed]
A manuscript explaining the rules of the game called "Matikan-i-chatrang" (the book of chess) in Middle Persian or Pahlavi still exists.[31]
In the 11th-century Shahnameh, Ferdowsi describes a Raja visiting from India who re-enacts the past battles on the chessboard.[26] A translation in English, based on the manuscripts in the British Museum, is given below:[30]
The philosopher and theologian Al-Ghazali mentions chess in The Alchemy of Happiness (c. 1100). He uses it as a specific example of a habit that may cloud a person's good disposition:[34]
Indeed, a person who has become habituated to gaming with pigeons, playing chess, or gambling, so that it becomes second-nature to him, will give all the comforts of the world and all that he has for those (pursuits) and cannot keep away from them.
As a strategy board game played in China, chess is believed to have been derived from the Indian chaturanga.[36] Chaturanga was transformed into the game xiangqi where the pieces are placed on the intersection of the lines of the board rather than within the squares.[37] The object of the Chinese variation is similar to chaturanga, i.e. to render helpless the opponent's king, known as "general" on one side and "governor" on the other.[38] Chinese chess also borrows elements from the game of Go, which was played in China since at least the 6th century BC. Owing to the influence of Go, Chinese chess is played on the intersections of the lines on the board, rather than in the squares. The game of Xianqi is also unique in that the middle rank represents a river, and is not divided into squares.[39] Chinese chess pieces are usually flat and resemble those used in checkers, with pieces differentiated by writing their names on the flat surface.[37]
A prominent variant of chess in East Asia is the game of shogi, transmitted from India to China and Korea before finally reaching Japan.[42] The three distinguishing features of shogi are:
Chu shogi declined in popularity after the addition of drops to sho shogi and the removal of the drunk elephant in the 16th century, becoming moribund around the late 20th century. These changes to sho shogi created what is essentially the modern game of shogi.
The Thai variant of chess, makruk is a close living relative to chaturanga, retaining the vizier, non-checkered board, limited promotion, offset kings, and elephant-like bishop move.[44]
Chess is recorded from Mongolian-inhabited areas, where the pieces are now called:
Western chess is now the prevalent form of the game in Mongolia.
Chess was also recorded from the Yakuts, Tunguses, and Yukaghirs; but only as a children's game among the Chukchi. Chessmen have been collected from the Yakutat people in Alaska, having no resemblance to European chessmen, and thus likely part of a chess tradition coming from Siberia.[46]
Chess passed from Persia to the Arab world, where its name changed to Arabic shatranj. From there it passed to Western Europe, probably via Spain.
Over the centuries, features of European chess (e.g. the modern moves of queen and bishop, and castling) found their way via trade into Islamic areas. Murray's sources found the old moves of queen and bishop still current in Ethiopia.[47] The game became so popular it was used in writing at that time, played by nobility and regular people. The poet al-Katib once said, "The skilled player places his pieces in such a way as to discover consequences that the ignorant man never sees... thus, he serves the Sultan's interests, by showing how to foresee disaster."[6]
Chess has 1000 years of history in Russia. Chess was probably brought to Old Russia in 9th century via the Volga-Caspian trade route. From the 10th century cultural connections with the Byzantine Empire and the Vikings also influenced the history of chess in Russia. The vocabulary in Russian chess has various foreign-language elements and testifies to different influences in the evolution of chess in Russia. Chess is mentioned in folk poems as a popular game and is documented in the Old Russian byliny. Numerous archeological finds of the chess game have already been found in the regions of Old Russia. From 1262 on chess was called in Russia shakhmaty. Various foreign travellers commented that in the 16th century, chess was popular among all classes in Russia. Ivan IV the Terrible, who ruled Russia from 1530 to 1584, is said to have died while playing chess.[48] In 1791 the popular chess book Morals of Chess by Benjamin Franklin was translated into Russian and published in the country. Chess enjoys a very high status in Russia and was gradually introduced as a school subject in all primary schools since 2017.[49][50][51]
Shatranj made its way via the expanding Islamic Arabian empire to Europe.[11] It also spread to the Byzantine empire, where it was called zatrikion. Chess appeared in Southern Europe during the end of the first millennium, often introduced to new lands by conquering armies, such as the Norman Conquest of England.[13] Previously little known, chess became popular in Northern Europe when figure pieces were introduced.[13]
In the 14th century, Timur played an enlarged variation of the game which is commonly referred to as Tamerlane chess. This complex game involved each pawn having a particular purpose, as well as additional pieces.[52]
The sides are conventionally called White and Black. But, in earlier European chess writings, the sides were often called Red and Black because those were the commonly available colours of ink when handwriting drawing a chess game layout. In such layouts, each piece was represented by its name, often abbreviated (e.g. "ch'r" for French "chevalier" = "knight").
The practice of playing chess for money became so widespread during the 13th century that Louis IX of France issued an ordinance against gambling in 1254.[53] This ordinance turned out to be unenforceable and was largely neglected by the common public, and even the courtly society, which continued to enjoy the now-prohibited chess tournaments uninterrupted.[53]
Otto IV of Brandenburg playing chess with a woman, 1305 to 1340
The pieces, which had been nonrepresentational in Islamic countries (see piece values in shantranj), changed shape in Christian cultures. Carved images of men and animals reappeared. The shape of the rook, originally a rectangular block with a V-shaped cut in the top, changed; the two top parts separated by the split tended to get long and hang over, and in some old pictures look like horses' heads. The split top of the piece now called the bishop was interpreted as a bishop's mitre or a fool's cap.
By the mid-12th century, the pieces of the chess set were depicted as kings, queens, bishops, knights and men at arms.[58] Chessmen made of ivory began to appear in North-West Europe, and ornate pieces of traditional knight warriors were used as early as the mid 13th century.[59] The initially nondescript pawn had now found association with the pedes, pedinus, or the footman, which symbolized both infantry and loyal domestic service.[58]
The following table provides a glimpse of the changes in names and character of chess pieces as they crossed from India through Persia to Europe:[60][61]
The game, as played during the early Middle Ages, was slow, with many games lasting days.[13] Some variations in rules began to change the shape of the game by the year 1300. A notable, but initially unpopular, change was the ability of the pawn to move two places in the first move instead of one.[62]
In Europe some of the pieces gradually received new names:
Attempts to make the start of the game run faster to get the opposing pieces in contact sooner included:
The queen and bishop remained relatively weak until between 1475 AD and 1500 AD, in Spain (in the Kingdom of Valencia), the queen's and bishop's modern moves started and spread, making chess close to its modern form.[13] The first document showing the Queen (or Dama) moving this way is the allegorical poem Scachs d'amor, written in Catalan in Valencia in 1475.[66][67] This form of chess got such names as "Queen's Chess" or "Mad Queen Chess" (Italian alla rabiosa = "with the madwoman").[68] This led to much more value being attached to the previously minor tactic of pawn promotion.[68] Checkmate became easier and games could now be won in fewer moves.[69][70] These new rules quickly spread in Spain and throughout the rest of Western Europe,[71][72] with the exception of the rules about stalemate, which were finalized in the early 19th century.[73] The modern move of the queen may have started as an extension of its older ability to once move two squares with jump, diagonally or straight. Marilyn Yalom says that the new move of the queen started in Spain: see history of the queen.
In some areas (e.g. Russia), the queen could also move like a knight.
An Italian player, Gioacchino Greco, regarded as one of the first true professionals of the game, authored an analysis of a number of composed games that illustrated two differing approaches to chess.[14][non-tertiary source needed] His work was influential in popularizing chess, and demonstrated many theories regarding game play and tactics.[14][non-tertiary source needed]
A woodcut drawn from Caxton's chess book printed in England in 1474
As the 19th century progressed, chess organization developed quickly. Many chess clubs, chess books and chess journals appeared. There were correspondence matches between cities; for example the London Chess Club played against the Edinburgh Chess Club in 1824.[77] Chess problems became a regular part of 19th-century newspapers; Bernhard Horwitz, Josef Kling and Samuel Loyd composed some of the most influential problems. In 1843, von der Lasa published his and Bilguer's Handbuch des Schachspiels (Handbook of Chess), the first comprehensive manual of chess theory.
Competitive chess became visible in 1834 with the La Bourdonnais-McDonnell matches, and the 1851 London Chess tournament raised concerns about the time taken by the players to deliberate their moves. On recording time it was found that players often took hours to analyze moves, and one player took as much as two hours and 20 minutes to think over a single move at the London tournament. The following years saw the development of speed chess, five-minute chess and the most popular variant, a version allowing a bank of time to each player in which to play a previously agreed number of moves, e.g. two hours for 30 moves. In the final variant, the player who made the predetermined number of moves in the agreed time received additional time budget for his next moves. Penalties for exceeding a time limit came in form of fines and forfeiture. Since fines were easy to bear for professional players, forfeiture became the only effective penalty; this added "lost on time" to the traditional means of losing such as checkmate and resigning.[15][non-tertiary source needed]
In 1861 the first time limits, using sandglasses, were employed in a tournament match at Bristol, England. The sandglasses were later replaced by pendulums. Modern clocks, consisting of two parallel timers with a small button for a player to press after completing a move, were later employed to aid the players. A tiny latch called a flag further helped settle arguments over players exceeding time limit at the turn of the 19th century.[15][non-tertiary source needed]
A Russian composer, Vladimir Korolkov, authored a work entitled "Excelsior" in 1958 in which the White side wins only by making six consecutive captures by a pawn.[78][non-tertiary source needed] Position analysis became particularly popular in the 19th century.[78][non-tertiary source needed] Many leading players were also accomplished analysts, including Max Euwe, Mikhail Botvinnik, Vasily Smyslov and Jan Timman.[78][non-tertiary source needed] Digital clocks appeared in the 1980s.[15][non-tertiary source needed]
Another problem that arose in competitive chess was when adjourning a game for a meal break or overnight. The player who moved last before adjournment would be at a disadvantage, as the other player would have a long period to analyze before having to make a reply when the game was resumed. Preventing access to a chess set to work out moves during the adjournment would not stop him from analyzing the position in his head. Various strange ideas were attempted, but the eventual solution was the "sealed move". The final move before adjournment is not made on the board but instead is written on a piece of paper which the referee seals in an envelope and keeps safe. When the game is continued after adjournment, the referee makes the sealed move and the players resume.
The first modern chess tournament was held in London in 1851 and won, surprisingly, by German Adolf Anderssen, who was relatively unknown at the time. Anderssen was hailed as the leading chess master, and his brilliant, energetic attacking style became typical for the time, although it was retrospectively regarded as strategically shallow.[79][80] Sparkling games like Anderssen's Immortal game and Evergreen Game or Morphy's Opera game were regarded as the highest possible summit of the chess art.[81]
Deeper insight into the nature of chess came with two younger players. American Paul Morphy, an extraordinary chess prodigy, won against all important competitors, including Anderssen, during his short chess career between 1857 and 1863. Morphy's success stemmed from a combination of brilliant attacks and sound strategy; he intuitively knew how to prepare attacks.[82] Prague-born Wilhelm Steinitz later described how to avoid weaknesses in one's own position and how to create and exploit such weaknesses in the opponent's position.[83] In addition to his theoretical achievements, Steinitz founded an important tradition: his triumph over the leading Polish-German master Johannes Zukertort in 1886 is regarded as the first official World Chess Championship. Steinitz lost his crown in 1894 to a much younger German mathematician Emanuel Lasker, who maintained this title for 27 years, the longest tenure of all World Champions.[84]
Since the end of 19th century, the number of annually held master tournaments and matches quickly grew. Some sources state that in 1914 the title of chess grandmaster was first formally conferred by Tsar Nicholas II of Russia to Lasker, Capablanca, Alekhine, Tarrasch and Marshall, but this is a disputed claim.[87] The tradition of awarding such titles was continued by the World Chess Federation (FIDE), founded in 1924 in Paris. In 1927, Women's World Chess Championship was established; the first to hold it was Czech-English master Vera Menchik.[88]
During World War II, many prominent chess players died or were killed, including: Isaak Appel, Zoltan Balla, Sergey Belavenets, Henryk Friedman, Achilles Frydman, Eduard Gerstenfeld, Alexander Ilyin-Genevsky, Mikhail Kogan, Jakub Kolski, Leon Kremer, Arvid Kubbel, Leonid Kubbel, Salo Landau, Moishe Lowtzky, Vera Menchik, Vladimir Petrov, David Przepiorka, Ilya Rabinovich, Vsevolod Rauzer, Nikolai Riumin, Endre Steiner, Mark Stolberg, Abram Szpiro, Karel Treybal, Alexey Troitzky, Samuil Vainshtein, Heinrich Wolf, and Lazar Zalkind.[89]
In the previous informal system, the World Champion decided which challenger he would play for the title and the challenger was forced to seek sponsors for the match.[91] FIDE set up a new system of qualifying tournaments and matches. The world's strongest players were seeded into "Interzonal tournaments", where they were joined by players who had qualified from "Zonal tournaments". The leading finishers in these Interzonals would go on the "Candidates" stage, which was initially a tournament, later a series of knock-out matches. The winner of the Candidates would then play the reigning champion for the title. A champion defeated in a match had a right to play a rematch a year later. This system worked on a three-year cycle.[91]
Botvinnik participated in championship matches over a period of fifteen years. He won the world championship tournament in 1948 and retained the title in tied matches in 1951 and 1954. In 1957, he lost to Vasily Smyslov, but regained the title in a rematch in 1958. In 1960, he lost the title to the Latvian prodigy Mikhail Tal, an accomplished tactician and attacking player. Botvinnik again regained the title in a rematch in 1961.
The next championship saw the first non-Soviet challenger since World War II, Bobby Fischer, who defeated his Candidates opponents by unheard-of margins and won the world championship match. In 1975, however, Fischer refused to defend his title against Soviet Anatoly Karpov when FIDE refused to meet his demands, and Karpov obtained the title by default. Karpov defended his title twice against Viktor Korchnoi and dominated the 1970s and early 1980s with a string of tournament successes.[93]
Karpov's reign finally ended in 1985 at the hands of another Russian player, Garry Kasparov. Kasparov and Karpov contested five world title matches between 1984 and 1990; Karpov never won his title back.[94]
In 1993, Garry Kasparov and Nigel Short cut ties with FIDE to organize their own match for the title and formed a competing Professional Chess Association (PCA). From then until 2006, there were two simultaneous World Champions and World Championships: the PCA or Classical champion extending the Steinitzian tradition in which the current champion plays a challenger in a series of many games; the other following FIDE's new format of many players competing in a tournament to determine the champion. Kasparov lost his Classical title in 2000 to Vladimir Kramnik of Russia.
Earlier in 1999, Kasparov as the reigning world champion played a game online against the world team composed of more than 50,000 participants from more than 75 countries. The moves of the world team were decided by plurality vote, and after 62 moves played over four months Kasparov won the game.[95]
The FIDE World Chess Championship 2006 reunified the titles, when Kramnik beat the FIDE World Champion Veselin Topalov and became the undisputed World Chess Champion.[96] In September 2007, Viswanathan Anand from India became the next champion by winning a championship tournament.[97] In October 2008, Anand retained his title, decisively winning the rematch against Kramnik.[98] Anand retained his title until 2013, when he lost it to Magnus Carlsen from Norway, the current World Chess Champion. Carlsen defended his title four times. However, he declined to defend his title in the 2023 championship, and thus will relinquish it to the winner of that match.[99]
Stalemate was originally considered an inferior form of victory; at various times it has been considered a win, a draw, or even a loss for the player delivering it. Since the 18th century, it has been considered a draw.
The convention that White moves first was established in the 19th century; previously either White or Black could move first.
Castling rules have varied, variations persisting in Italy until the late 19th century.
Rules concerning draws by repetition and the fifty-move rule have been refined and now require a formal claim. Perpetual check is no longer included in the rules of chess.
There have been no recent changes to the moves of the pieces, but the wording of some rules has been changed for the purposes of clarity.
The London 1883 chess tournament introduced chess clocks, creating a new rule for loss on time.
In line with the rule against receiving outside assistance, if a player's mobile phone or other electronic device generates sound, the player is immediately forfeited.[100] In amateur tournaments players are asked to hand their phones to the tournament director; in professional tournaments they may be required to go through a metal detector.
Southern Europe is the southern region of Europe.[1] It is also known as Mediterranean Europe, as its geography is essentially marked by the Mediterranean Sea. Definitions of Southern Europe include some or all of these countries and regions: Albania, Andorra, Bosnia and Herzegovina, Bulgaria,[note 1] Croatia, Cyprus,[note 2] Turkey (East Thrace),[note 3] Gibraltar,[note 4] Greece,[note 5] Italy,[note 6] Kosovo,[note 7] Malta,[note 8] Moldova,[note 9] Monaco,[note 10] Montenegro, North Macedonia, Portugal, Romania,[note 11] San Marino, Serbia, Slovenia, Southern France,[note 12] Spain, and Vatican City (the Holy See).[4][5][6][7][8][9][10][11]
Southern Europe is focused on the three peninsulas located in the extreme south of the European continent. These are the Iberian Peninsula, the Apennine Peninsula, and the Balkan Peninsula.[12][13] These three peninsulas are separated from the rest of Europe by towering mountain ranges, respectively by the Pyrenees, the Alps and the Balkan Mountains. The location of these peninsulas in the heart of the Mediterranean Sea, as well as their mountainous reliefs, provide them with very different types of climates (mainly subtropical Mediterranean) from the rest of the continent. So, the Sirocco hot wind that originates in the heart of the Sahara blows over Italy, going up to the interior of the Alpine arc (Po Valley). The Alps prevent the Sirocco from spreading to the rest of Europe. And, conversely, the Alps and the Pyrenees protect the Italian and Iberian peninsulas from the rains and icy winds from the south of France such as the Mistral and the Tramontane. When the Mistral and the Tramontane are blowing, this provokes an "upwelling" phenomenon on the French coast. They push the surface waters out to sea and bring deeper, cooler waters up to the seaside. Consequently, the temperature of the waters of the French coasts are therefore very cool even in summer, and not representative of the rest of the Mediterranean.[14][15][16]
This same kind of phenomenon takes place between the two slopes of the Balkan Mountain Range. These mountains have, moreover, been a serious handicap to population displacement, focusing southern Europe mainly on the Mediterranean world. The climate and cultures are therefore very specific.
Geographically, Southern Europe is the southern portion of the European continent. This definition is relative, although largely based on history, culture, climate, and flora, which is shared across the region. Southern Europe can be subdivided into three subregions:
The major islands in Southern Europe generally include the Balearic Islands, Corsica, Crete, Cyprus,[note 2] Sardinia, and Sicily, as well as the island country of Malta.[note 8]
Map representing the geography of Europe, with the mountain ranges separating Southern Europe.
Southern Europe's most emblematic climate is the Mediterranean climate, influenced by the large subtropical semi-permanent centre of high atmospheric pressure found, not in the Mediterranean itself, but in the Atlantic Ocean, the Azores High. The Mediterranean climate covers Portugal, Spain, Italy, the southern coast of France, coastal Croatia, coastal Slovenia, southern Bosnia and Herzegovina, Montenegro, Albania, and Greece, as well as the Mediterranean islands. Those areas of Mediterranean climate present similar vegetations and landscapes throughout, including dry hills, small plains, pine forests and olive trees.
Cooler climates can be found in certain parts of Southern European countries, for example within the mountain ranges of Spain and Italy. Additionally, the north coast of Spain experiences a wetter Atlantic climate. In the highest regions of the Alps, which border Southern Europe, even ice cap climate can be found.[note 19]
Some parts of Southern Europe have humid subtropical climates with warm and wet summers, unlike typical Mediterranean climates. This climate is mainly found in Italy and Croatia around the Adriatic Sea in cities such as Venice and Trieste, but also further north, near the Alpine foothills, in cities such as Como and Lugano.
Southern Europe's flora is mainly characterized by Mediterranean forests, woodlands, and scrub, but also temperate broadleaf and mixed forests. The Mediterranean and Submediterranean climate regions in Europe are found in much of Southern Europe, mainly Portugal, Spain, Italy, Malta, Albania, Greece, Cyprus and all the mediterranean islands, but also in southeast France, the Balkan Mediterranean coast and part of Macedonia.[19][20]
In the Mediterranean coastal areas, olive groves, maquis shrubland, and steppes are very common. At higher elevations, or latitudes, they are replaced by chestnut and (often coppiced) mixed forests.[21]
Oak savanna of Alentejo, Portugal (Q. suber and Q. rotundifolia)
The period known as classical antiquity began with the rise of the city-states of Ancient Greece. Greek influence reached its zenith under the expansive empire of Alexander the Great, spreading throughout Asia. The Roman Empire came to dominate the entire Mediterranean Basin in a vast empire based on Roman law and Roman legions. It promoted trade, tolerance, and Greek culture. By 300 AD the Roman Empire was divided into the Western Roman Empire based in Rome, and the Eastern Roman Empire based in Constantinople. The attacks of the Goths led to the fall of the Western Roman Empire in 476 AD, a date which traditionally marks the end of the classical period and the start of the Middle Ages. During the Middle Ages, the Eastern Roman Empire survived, though modern historians refer to this state as the Byzantine Empire. In Western Europe, Germanic peoples moved into positions of power in the remnants of the former Western Roman Empire and established kingdoms and empires of their own.
The period known as the Crusades, a series of religiously motivated military expeditions originally intended to bring the Levant back into Christian rule, began. Several Crusader states were founded in the eastern Mediterranean. These were all short-lived. The Crusaders would have a profound impact on many parts of Europe. Their sack of Constantinople in 1204 brought an abrupt end to the Byzantine Empire. Though it would later be re-established, it would never recover its former glory. The Crusaders would establish trade routes that would develop into the Silk Road and open the way for the merchant republics of Genoa and Venice to become major economic powers. The Reconquista, a related movement, worked to reconquer Iberia for Christendom. The late Middle Ages represented a period of upheaval in Europe. The epidemic known as the Black Death and an associated famine caused demographic catastrophe in Europe as the population plummeted. Dynastic struggles and wars of conquest kept many of the states of Europe at war for much of the period. In the Balkans, the Ottoman Empire, a Turkish state originating in Anatolia, encroached steadily on former Byzantine lands, culminating in the fall of Constantinople in 1453.
European overseas expansion led to the rise of colonial empires, producing the Columbian Exchange.[25] The combination of resource inflows from the New World and the Industrial Revolution of Great Britain, allowed a new economy based on manufacturing instead of subsistence agriculture.[26] The period between 1815 and 1871 saw a large number of revolutionary attempts and independence wars. Balkan nations began to regain independence from the Ottoman Empire. Italy unified into a nation state. The capture of Rome in 1870 ended the Papal temporal power.
The Gotthard, a major and direct transport axis between Northern and Southern Europe, was completed in 2016 with the Gotthard Base Tunnel.[29] The Gotthard inscribes itself in a long history of transit across the Alps, which saw them progressively changing from an obstacle to a corridor between the North Sea and the Mediterranean Sea.[30]
The most widely spoken family of languages in Southern Europe are the Romance languages, the heirs of Latin, which have spread from the Italian peninsula, and are emblematic of Southwestern Europe. (See the Latin Arch.) By far the most common Romance languages in Southern Europe are Italian (spoken by over 50 million people in Italy, southern Switzerland, Malta, San Marino, and Vatican City) and Spanish, which is spoken by over 40 million people in Spain, Andorra and Gibraltar. Other common Romance languages include Portuguese (spoken in Portugal and Andorra), French (spoken in France, Monaco, and the Aosta Valley in Italy), Catalan (spoken in eastern Spain, Andorra, Southwestern France, and the Sardinian town of Alghero in Italy), Galician (spoken in northwestern Spain), Mirandese (spoken in northeast Portugal), and Occitan, which is spoken in the Val d'Aran in Catalonia, in the Occitan Valleys in Italy and in southern France.[citation needed]
The Hellenic languages or Greek language are widely spoken in Greece and Cyprus. Additionally, other varieties of Greek are spoken in small communities in parts of other European countries.[citation needed]
English is used as a second language in parts of Southern Europe. As a primary language, however, English has only a small presence in Southern Europe, only in Gibraltar (alongside Spanish) and Malta (secondary to Maltese). English is also widely spoken in Cyprus.[citation needed]
In the CIA World Factbook, the description of each country includes information about "Location" under the heading "Geography", where the country is classified into a region. The following countries are included in their classification "Southern Europe":[33]
In addition, Andorra, Gibraltar, Portugal and Spain are classified as "Southwestern Europe", while Albania, Bosnia and Herzegovina, Bulgaria, Croatia, Montenegro, North Macedonia, Romania, Serbia and Turkey (the part west of the Bosporus) are described as located in "Southeastern Europe".
EuroVoc is a multilingual thesaurus maintained by the Publications Office of the European Union, giving definitions of terms for official use. In the definition of "Southern Europe", the following countries are included:[34]
The United Nations geoscheme is a system devised by the United Nations Statistics Division (UNSD) which divides the countries of the world into regional and subregional groups, based on the M49 coding classification. The partition is for statistical convenience and does not imply any assumption regarding political or other affiliation of countries or territories.[35]
In the UN geoscheme, the following countries are classified as Southern Europe:[35]
European Travel Commission divides the European region on the basis of Tourism Decision Metrics (TDM) model.
Countries which belong to the Southern/Mediterranean Europe in this classification are:[36]
Albania, Bosnia and Herzegovina, Croatia, East Thrace (Turkey), Greece, Italy, Malta, Montenegro, North Macedonia, Portugal, Serbia, Slovenia, and Spain.
Chaturanga is first known from the Gupta Empire in India around the 6th century CE. In the 7th century, it was adopted as chatrang (shatranj) in Sassanid Persia, which in turn was the form of chess brought to late-medieval Europe.[2] Archeological remains from 2000 to 3000 BC have been found from the city of Lothal (of the Indus Valley civilisation) of pieces on a board that resemble chess.[3] According to Stewart Culin, chaturanga was first described in the Hindu text Bhavishya Purana.[4] The Bhavishya Purana is known to include modern additions and interpolations, however, even mentioning British rule of India.[5]
The exact rules of chaturanga are unknown. Chess historians suppose that the game had similar rules to those of its successor, shatranj. In particular, there is uncertainty as to the moves of the Gaja (elephant).[6]
The Sanskrit word chaturanga means "four-limbed" or "four arms", referring to ancient army divisions of infantry, cavalry, elephantry, and chariotry. The origin of chaturanga has been a puzzle for centuries. It has its origins in the Gupta Empire, with the earliest clear reference dating from the sixth century of the common era, and from north India.  The first substantial argument that chaturanga is much older than this is the fact that the chariot is the most powerful piece on the board, although chariots appear to have been obsolete in warfare for at least five or six centuries.[citation needed]  The counter-argument is that they remained prominent in literature.
An early reference to an ancient Indian board game is sometimes attributed to Subandhu in his Vasavadatta, dated between the 5th and 7th centuries AD: 
The time of the rains played its game with frogs for pieces [nayadyutair] yellow and green in colour, as if mottled by lac, leapt up on the black field squares.
The colours are not those of the two camps, but mean that the frogs have two colours, yellow and green.
Banabhatta's Harsha Charitha (c. AD 625) contains the earliest reference to the name chaturanga: 
The game was first introduced to the West in Thomas Hyde's De ludis orientalibus libri duo, published in 1694. Subsequently, translations of Sanskrit accounts of the game were published by Sir William Jones.[10]
In Arabic, most of the terminology of chess is derived directly from chaturanga: Modern chess itself is called shatranj in Arabic, and the bishop is called the elephant.[11] The Tamerlane chess was also introduced in Iran later.[citation needed]
The initial position is as shown. White moves first. The objective in chaturanga is to checkmate the opponent's Raja (king) or reducing the opposition to just the Raja.[12]
King: moves the same as the king in chess. There is no castling in Chaturanga.
Ferz: moves step diagonally in any direction, like the fers in shatranj.
Chariot: moves the same as a rook in chess, whereby the rook moves horizontally or vertically, through any number of unoccupied squares. The same move is used for the boat in Chaturaji.
Elephant: 
According to consensus in modern genetics, anatomically modern humans first arrived on the Indian subcontinent from Africa between 73,000 and 55,000 years ago.[1] However, the earliest known human remains in South Asia date to 30,000 years ago. Sedentariness, which involves the transition from foraging to farming and pastoralism, began in South Asia around 7000 BCE. At the site of Mehrgarh, presence can be documented, with evidence of domestication of wheat and barley, rapidly followed by that of goats, sheep, and cattle.[2] By 4500 BCE, such settled life had spread more[2] and began to gradually evolve into the Indus Valley civilisation, which was contemporaneous with Ancient Egypt and Mesopotamia. This civilisation flourished between 2500 BCE and 1900 BCE in what today is Pakistan and north-western India, and was noted for its urban planning, baked brick houses, elaborate drainage, and water supply.[3]
Most of the Indian subcontinent was conquered by the Maurya Empire during the 4th and 3rd centuries BCE. From the 3rd century BCE onwards, Prakrit and Pali literature in the north and Tamil Sangam literature in southern India started to flourish.[6][7] Wootz steel originated in south India in the 3rd century BCE and was exported.[8][9][10] The Maurya Empire would collapse in 185 BCE, on the assassination of the then-Emperor Brihadratha by his General Pushyamitra Shunga. Shunga would go on to form the Shunga Empire in the North and Northeast of the subcontinent, while the Greco-Bactrian Kingdom would claim the Northwest, and found the Indo-Greek Kingdom. Various parts of India were ruled by numerous dynasties, including the Gupta Empire in the 4-6th centuries CE. 
This period, witnessing a Hindu religious and intellectual resurgence, is known as the Classical or Golden Age of India. During this time, aspects of Indian civilisation, administration, culture, and religion spread to much of Asia. Kingdoms in southern India had maritime business links with the Middle East and the Mediterranean. Indian cultural influence spread over many parts of Southeast Asia, which led to the establishment of Indianised kingdoms in the region, forming the Greater India.[11][12]
The most significant event between the 7th and 11th century was the Tripartite struggle centred on Kannauj that lasted for more than two centuries between the Pala Empire, Rashtrakuta Empire, and Gurjara-Pratihara Empire. Southern India saw the rise of multiple imperial powers from the middle of the fifth century, most notably the Chalukya, Chola, Pallava, Chera, Pandyan, and Western Chalukya Empires. The Chola dynasty conquered southern India and successfully invaded parts of Southeast Asia, Sri Lanka, the Maldives, and Bengal[13] in the 11th century.[14][15] In the early medieval period Indian mathematics, including Hindu numerals, influenced the development of mathematics and astronomy in the Arab world.[16]
The early modern period began in the 16th century, when the Mughal Empire conquered most of the Indian subcontinent,[22] signalling the proto-industrialization, becoming the biggest global economy and manufacturing power,[23] with a nominal GDP that valued a quarter of world GDP, superior than the combination of Europe's GDP.[24][25] The Mughals suffered a gradual decline in the early 18th century, which provided opportunities for the Marathas, Sikhs, Mysoreans, Nizams, and Nawabs of Bengal to exercise control over large regions of the Indian subcontinent.[26][27]
From the mid-18th century to the mid-19th century, large regions of India were gradually annexed by the East India Company, a chartered company acting as a sovereign power on behalf of the British government. Dissatisfaction with company rule in India led to the Indian Rebellion of 1857, which rocked parts of north and central India, and led to the dissolution of the company. India was afterwards ruled directly by the British Crown, in the British Raj. After World War I, a nationwide struggle for independence was launched by the Indian National Congress, led by Mahatma Gandhi, notable for nonviolence. Later, the All-India Muslim League would advocate for a separate Muslim-majority nation state. The British Indian Empire was partitioned in August 1947 into the Dominion of India and Dominion of Pakistan, each gaining its independence.
Hominin expansion from Africa is estimated to have reached the Indian subcontinent approximately two million years ago, and possibly as early as 2.2 million years before the present.[32][33][34] This dating is based on the known presence of Homo erectus in Indonesia by 1.8 million years before the present and in East Asia by 1.36 million years before present, as well as the discovery of stone tools at Riwat in the Soan River valley of the Pabbi Hills region, Pakistan.[33][35] Although some older discoveries have been claimed, the suggested dates, based on the dating of fluvial sediments, have not been independently verified.[34][36]
The oldest hominin fossil remains in the Indian subcontinent are those of Homo erectus or Homo heidelbergensis, from the Narmada Valley in central India, and are dated to approximately half a million years ago.[33][36] Older fossil finds have been claimed, but are considered unreliable.[36] Reviews of archaeological evidence have suggested that occupation of the Indian subcontinent by hominins was sporadic until approximately 700,000 years ago, and was geographically widespread by approximately 250,000 years before the present, from which point onward, archaeological evidence of proto-human presence is widely mentioned.[36][34]
According to a historical demographer of South Asia, Tim Dyson: 
And according to historian of South Asia, Michael H. Fisher: 
Scholars estimate that the first successful expansion of the Homo sapiens range beyond Africa and across the Arabian Peninsula occurred from as early as 80,000 years ago to as late as 40,000 years ago, although there may have been prior unsuccessful emigrations. Some of their descendants extended the human range ever further in each generation, spreading into each habitable land they encountered. One human channel was along the warm and productive coastal lands of the Persian Gulf and northern Indian Ocean. Eventually, various bands entered India between 75,000 years ago and 35,000 years ago.[39]
Settled life emerged on the subcontinent in the western margins of the Indus River alluvium approximately 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE.[2][44] According to Tim Dyson: "By 7,000 years ago agriculture was firmly established in Baluchistan. And, over the next 2,000 years, the practice of farming slowly spread eastwards into the Indus valley." And according to Michael Fisher:[45] 
 "The earliest discovered instance ... of well-established, settled agricultural society is at Mehrgarh in the hills between the Bolan Pass and the Indus plain (today in Pakistan) (see Map 3.1). From as early as 7000 BCE, communities there started investing increased labor in preparing the land and selecting, planting, tending, and harvesting particular grain-producing plants. They also domesticated animals, including sheep, goats, pigs, and oxen (both humped zebu [Bos indicus] and unhumped [Bos taurus]). Castrating oxen, for instance, turned them from mainly meat sources into domesticated draft-animals as well."[45]
Inhabitants of the ancient Indus river valley, the Harappans, developed new techniques in metallurgy and handicraft (carneol[further explanation needed] products, seal carving), and produced copper, bronze, lead, and tin. The civilisation is noted for its cities built of brick, roadside drainage system, and multi-storeyed houses and is thought to have had some kind of municipal organisation. Civilization also developed a Indus script, which is presently undeciphered.[49] This is the reason why Harappan language is not directly attested, and its affiliation uncertain. [50] A relationship or membership of the Dravidian or Elamo-Dravidian language family is proposed by some scholars.[51][52]
After the collapse of Indus Valley civilisation, the inhabitants of the Indus Valley civilisation migrated from the river valleys of Indus and Ghaggar-Hakra, towards the Himalayan foothills of Ganga-Yamuna basin.[53]
During 2nd millennium BCE, Ochre Coloured Pottery culture was in Ganga Yamuna Doab region. These were rural settlement with agriculture and hunting. They were using copper tools such as axes, spears, arrows, and swords. The people had domesticated cattle, goats, sheep, horses, pigs and dogs.[55] The site gained attention for its Bronze Age solid-disk wheel carts, found in 2018,[56] which were interpreted by some as horse-pulled "chariots".[57][58][note 1]
Starting ca. 1900 BCE, Indo-Aryan tribes moved into the Punjab from Central Asia in several waves of migration.[60][61] The Vedic period is the period when the Vedas were composed, the liturgical hymns from the Indo-Aryan people. The Vedic culture was located in part of north-west India, while other parts of India had a distinct cultural identity during this period. Many regions of the Indian subcontinent transitioned from the Chalcolithic to the Iron Age in this period.[62]
Historians have analysed the Vedas to posit a Vedic culture in the Punjab region and the upper Gangetic Plain.[62] The peepal tree and cow were sanctified by the time of the Atharva Veda.[67] Many of the concepts of Indian philosophy espoused later, like dharma, trace their roots to Vedic antecedents.[68]
Early Vedic society is described in the Rigveda, the oldest Vedic text, believed to have been compiled during 2nd millennium BCE,[69][70] in the northwestern region of the Indian subcontinent.[71] At this time, Aryan society consisted of largely tribal and pastoral groups, distinct from the Harappan urbanisation which had been abandoned.[72] The early Indo-Aryan presence probably corresponds, in part, to the Ochre Coloured Pottery culture in archaeological contexts.[73][74]
At the end of the Rigvedic period, the Aryan society began to expand from the northwestern region of the Indian subcontinent, into the western Ganges plain. It became increasingly agricultural and was socially organised around the hierarchy of the four varnas, or social classes. This social structure was characterized both by syncretising with the native cultures of northern India,[75] but also eventually by the excluding of some indigenous peoples by labeling their occupations impure.[76] During this period, many of the previous small tribal units and chiefdoms began to coalesce into Janapadas (monarchical, state-level polities).[77]
This period corresponds in an archaeological context to the Northern Black Polished Ware culture. Especially focused in the Central Ganges plain but also spreading across vast areas of the northern and central Indian subcontinent, this culture is characterized by the emergence of large cities with massive fortifications, significant population growth, increased social stratification, wide-ranging trade networks, construction of public architecture and water channels, specialized craft industries (e.g., ivory and carnelian carving), a system of weights, punch-marked coins, and the introduction of writing in the form of Brahmi and Kharosthi scripts.[108][109] The language of the gentry at that time was Sanskrit, while the languages of the general population of northern India are referred to as Prakrits.
Magadha formed one of the sixteen Mahajanapadas (Sanskrit: "Great Realms") or kingdoms in ancient India. The core of the kingdom was the area of Bihar south of the Ganges; its first capital was Rajagriha (modern Rajgir) then Pataliputra (modern Patna). Magadha expanded to include most of Bihar and Bengal with the conquest of Licchavi and Anga respectively,[110] followed by much of eastern Uttar Pradesh and Orissa. The ancient kingdom of Magadha is heavily mentioned in Jain and Buddhist texts. It is also mentioned in the Ramayana, Mahabharata and Puranas.[111] The earliest reference to the Magadha people occurs in the Atharva-Veda where they are found listed along with the Angas, Gandharis, and Mujavats. Magadha played an important role in the development of Jainism and Buddhism. The Magadha kingdom included republican communities such as the community of Rajakumara. Villages had their own assemblies under their local chiefs called Gramakas. Their administrations were divided into executive, judicial, and military functions.
Under Chandragupta Maurya and his successors, internal and external trade, agriculture, and economic activities all thrived and expanded across India thanks to the creation of a single efficient system of finance, administration, and security. The Mauryans built the Grand Trunk Road, one of Asia's oldest and longest major roads connecting the Indian subcontinent with Central Asia.[123] After the Kalinga War, the Empire experienced nearly half a century of peace and security under Ashoka. Mauryan India also enjoyed an era of social harmony, religious transformation, and expansion of the sciences and of knowledge. Chandragupta Maurya's embrace of Jainism increased social and religious renewal and reform across his society, while Ashoka's embrace of Buddhism has been said to have been the foundation of the reign of social and political peace and non-violence across all of India. Ashoka sponsored the spreading of Buddhist missionaries into Sri Lanka, Southeast Asia, West Asia, North Africa, and Mediterranean Europe.[124]
The Arthashastra wrote by Chanakya and the Edicts of Ashoka are the primary written records of the Mauryan times. Archaeologically, this period falls into the era of Northern Black Polished Ware. The Mauryan Empire was based on a modern and efficient economy and society. However, the sale of merchandise was closely regulated by the government.[125] Although there was no banking in the Mauryan society, usury was customary. A significant amount of written records on slavery are found, suggesting a prevalence thereof.[126] During this period, a high-quality steel called Wootz steel was developed in south India and was later exported to China and Arabia.[8]
During the Sangam period Tamil literature flourished from the 3rd century BCE to the 4th century CE. During this period, three Tamil dynasties, collectively known as the Three Crowned Kings of Tamilakam: Chera dynasty, Chola dynasty, and the Pandya dynasty ruled parts of southern India.[128]
The Sangam literature deals with the history, politics, wars, and culture of the Tamil people of this period.[129] The scholars of the Sangam period rose from among the common people who sought the patronage of the Tamil Kings, but who mainly wrote about the common people and their concerns.[130] Unlike Sanskrit writers who were mostly Brahmins, Sangam writers came from diverse classes and social backgrounds and were mostly non-Brahmins. They belonged to different faiths and professions such as farmers, artisans, merchants, monks, and priests, including also royalty and women.[130]
Ancient India during the rise of the Shungas from the North, Satavahanas from the Deccan, and Pandyas and Cholas from the southern tip of India.
The Great Chaitya in the Karla Caves. The shrines were developed over the period from 2nd century BCE to the 5th century CE.
Udayagiri and Khandagiri Caves is home to the Hathigumpha inscription, which was inscribed under Kharavela, the then Emperor of Kalinga of the Mahameghavahana dynasty.
Relief of a multi-storied temple, 2nd century CE, Ghantasala Stupa.[135][136]
Pushyamitra Shunga ruled for 36 years and was succeeded by his son Agnimitra. There were ten Shunga rulers. However, after the death of Agnimitra, the empire rapidly disintegrated;[141] inscriptions and coins indicate that much of northern and central India consisted of small kingdoms and city-states that were independent of any Shunga hegemony.[142] The empire is noted for its numerous wars with both foreign and indigenous powers. They fought battles with the Mahameghavahana dynasty of Kalinga, Satavahana dynasty of Deccan, the Indo-Greeks, and possibly the Panchalas and Mitras of Mathura.
Art, education, philosophy, and other forms of learning flowered during this period including small terracotta images, larger stone sculptures, and architectural monuments such as the Stupa at Bharhut, and the renowned Great Stupa at Sanchi. The Shunga rulers helped to establish the tradition of royal sponsorship of learning and art. The script used by the empire was a variant of Brahmi and was used to write the Sanskrit language. The Shunga Empire played an imperative role in patronising Indian culture at a time when some of the most important developments in Hindu thought were taking place. This helped the empire flourish and gain power.
The Kushan Empire expanded out of what is now Afghanistan into the northwest of the Indian subcontinent under the leadership of their first emperor, Kujula Kadphises, about the middle of the 1st century CE. The Kushans were possibly of Tocharian speaking tribe;[150] one of five branches of the Yuezhi confederation.[151][152] By the time of his grandson, Kanishka the Great, the empire spread to encompass much of Afghanistan,[153] and then the northern parts of the Indian subcontinent at least as far as Saketa and Sarnath near Varanasi (Banaras).[154]
Emperor Kanishka was a great patron of Buddhism; however, as Kushans expanded southward, the deities of their later coinage came to reflect its new Hindu majority.[155][156] They played an important role in the establishment of Buddhism in India and its spread to Central Asia and China.
He played the part of a second Ashoka in the history of Buddhism.[157]
The empire linked the Indian Ocean maritime trade with the commerce of the Silk Road through the Indus valley, encouraging long-distance trade, particularly between China and Rome. The Kushans brought new trends to the budding and blossoming Gandhara art and Mathura art, which reached its peak during Kushan rule.[158]
The Kushan period is a fitting prelude to the Age of the Guptas.[159]
By the 3rd century, their empire in India was disintegrating and their last known great emperor was Vasudeva I.[160][161]
The latter Guptas successfully resisted the northwestern kingdoms until the arrival of the Alchon Huns, who established themselves in Afghanistan by the first half of the 5th century CE, with their capital at Bamiyan.[166] However, much of the southern India including Deccan were largely unaffected by these events in the north.[167][168]
The Vakatakas are noted for having been patrons of the arts, architecture and literature. They led public works and their monuments are a visible legacy. The rock-cut Buddhist viharas and chaityas of Ajanta Caves (a UNESCO World Heritage Site) were built under the patronage of Vakataka emperor, Harishena.[169][170]
The Ajanta Caves are 30 rock-cut Buddhist cave monument built under the Vakatakas.
Buddhist monks praying in front of the Dagoba of Chaitya Cave 26 of the Ajanta Caves.
Buddhist "Chaitya Griha" or prayer hall, with a seated Buddha, Cave 26 of the Ajanta Caves.
Many foreign ambassadors, representatives, and travelers are included as devotees attending the Buddha's descent from Trayastrimsa Heaven; painting from Cave 17 of the Ajanta Caves.
Samudragupta's 4th-century Allahabad pillar inscription mentions Kamarupa (Western Assam)[171] and Davaka (Central Assam)[172] as frontier kingdoms of the Gupta Empire. Davaka was later absorbed by Kamarupa, which grew into a large kingdom that spanned from Karatoya river to near present Sadiya and covered the entire Brahmaputra valley, North Bengal, parts of Bangladesh and, at times Purnea and parts of West Bengal.[173]
The Pallavas, during the 4th to 9th centuries were, alongside the Guptas of the North, great patronisers of Sanskrit development in the South of the Indian subcontinent. The Pallava reign saw the first Sanskrit inscriptions in a script called Grantha.[177] Early Pallavas had different connexions to Southeast Asian countries. The Pallavas used Dravidian architecture to build some very important Hindu temples and academies in Mamallapuram, Kanchipuram and other places; their rule saw the rise of great poets. The practice of dedicating temples to different deities came into vogue followed by fine artistic temple architecture and sculpture style of Vastu Shastra.[178]
Harsha ruled northern India from 606 to 647 CE. He was the son of Prabhakarvardhana and the younger brother of Rajyavardhana, who were members of the Vardhana dynasty and ruled Thanesar, in present-day Haryana.
After the downfall of the prior Gupta Empire in the middle of the 6th century, North India reverted to smaller republics and monarchical states. The power vacuum resulted in the rise of the Vardhanas of Thanesar, who began uniting the republics and monarchies from the Punjab to central India. After the death of Harsha's father and brother, representatives of the empire crowned Harsha emperor at an assembly in April 606 CE, giving him the title of Maharaja when he was merely 16 years old.[181] At the height of his power, his Empire covered much of North and Northwestern India, extended East until Kamarupa, and South until Narmada River; and eventually made Kannauj (in present Uttar Pradesh state) his capital, and ruled until 647 CE.[182]
The peace and prosperity that prevailed made his court a centre of cosmopolitanism, attracting scholars, artists and religious visitors from far and wide.[182] During this time, Harsha converted to Buddhism from Surya worship.[183] The Chinese traveller Xuanzang visited the court of Harsha and wrote a very favourable account of him, praising his justice and generosity.[182] His biography Harshacharita ("Deeds of Harsha") written by Sanskrit poet Banabhatta, describes his association with Thanesar, besides mentioning the defence wall, a moat and the palace with a two-storied Dhavalagriha (White Mansion).[184][185]
Early medieval India began after the end of the Gupta Empire in the 6th century CE.[137] This period also covers the "Late Classical Age" of Hinduism,[186] which began after the end of the Gupta Empire,[186] and the collapse of the Empire of Harsha in the 7th century CE;[186] the beginning of Imperial Kannauj, leading to the Tripartite struggle; and ended in the 13th century with the rise of the Delhi Sultanate in Northern India[187] and the end of the Later Cholas with the death of Rajendra Chola III in 1279 in Southern India; however some aspects of the Classical period continued until the fall of the Vijayanagara Empire in the south around the 17th century.
The Hindu Shahi dynasty ruled portions of eastern Afghanistan, northern Pakistan, and Kashmir from the mid-7th century to the early 11th century. While in Odisha, the Eastern Ganga Empire rose to power; noted for the advancement of Hindu architecture, most notable being Jagannath Temple and Konark Sun Temple, as well as being patrons of art and literature.
Martand Sun Temple Central shrine, dedicated to the deity Surya, and built by the third ruler of the Karkota dynasty, Lalitaditya Muktapida, in the 8th century CE.
Kandariya Mahadeva Temple in the Khajuraho complex was built by the Chandelas.
Jagannath Temple at Puri, built by Anantavarman Chodaganga Deva of the Eastern Ganga dynasty.
Galaganatha Temple at Pattadakal complex (UNESCO World Heritage) is an example of Badami Chalukya architecture.
Bhutanatha temple complex at Badami, next to a waterfall, during the monsoon.
Vishnu image inside the Badami Cave Temple Complex. The complex is an example of Indian rock-cut architecture.
8th century Durga temple exterior view at Aihole complex. Aihole complex includes Hindu, Buddhist and Jain temples and monuments.
Founded by Dantidurga around 753,[206] the Rashtrakuta Empire ruled from its capital at Manyakheta for almost two centuries.[207] At its peak, the Rashtrakutas ruled from the Ganges River and Yamuna River doab in the north to Cape Comorin in the south, a fruitful time of political expansion, architectural achievements and famous literary contributions.[208][209]
The early rulers of this dynasty were Hindu, but the later rulers were strongly influenced by Jainism.[210] Govinda III and Amoghavarsha were the most famous of the long line of able administrators produced by the dynasty. Amoghavarsha, who ruled for 64 years, was also an author and wrote Kavirajamarga, the earliest known Kannada work on poetics.[207][211] Architecture reached a milestone in the Dravidian style, the finest example of which is seen in the Kailasanath Temple at Ellora. Other important contributions are the Kashivishvanatha temple and the Jain Narayana temple at Pattadakal in Karnataka.
Kailasa temple, is one of the largest rock-cut ancient Hindu temples located in Ellora.
Statue of the Buddha seated. A part of the Carpenter's cave (Buddhist Cave 10).
Jain Tirthankara Mahavira with Yaksha Matanga and Yakshi Siddhaiki at Ellora Caves.
By the 10th century, several feudatories of the empire took advantage of the temporary weakness of the Gurjara-Pratiharas to declare their independence, notably the Paramaras of Malwa, the Chandelas of Bundelkhand, the Kalachuris of Mahakoshal, the Tomaras of Haryana, and the Chauhans of Rajputana.
One of the four entrances of the Teli ka Mandir. This Hindu temple was built by the Pratihara emperor Mihira Bhoja.[216]
Jainism-related cave monuments and statues carved into the rock face inside Siddhachal Caves, Gwalior Fort.
Ghateshwara Mahadeva temple at Baroli Temples complex. The complex of eight temples, built by the Gurjara-Pratiharas, is situated within a walled enclosure.
Gahadavala dynasty ruled parts of the present-day Indian states of Uttar Pradesh and Bihar, during 11th and 12th centuries. Their capital was located at Varanasi in the Gangetic plains.[217]
The Khayaravala dynasty, ruled parts of the present-day Indian states of Bihar and Jharkhand, during 11th and 12th centuries. Their capital was located at Khayaragarh in Shahabad district. Pratapdhavala and Shri Pratapa were king of the dynasty according to inscription of Rohtas.[218]
The Pala Empire was founded by Gopala I.[219][220][221] It was ruled by a Buddhist dynasty from Bengal in the eastern region of the Indian subcontinent. The Palas reunified Bengal after the fall of Shashanka's Gauda Kingdom.[222]
The Palas were followers of the Mahayana and Tantric schools of Buddhism,[223] they also patronised Shaivism and Vaishnavism.[224] The morpheme Pala, meaning "protector", was used as an ending for the names of all the Pala monarchs. The empire reached its peak under Dharmapala and Devapala. Dharmapala is believed to have conquered Kanauj and extended his sway up to the farthest limits of India in the northwest.[224]
The Pala Empire can be considered as the golden era of Bengal in many ways.[225] Dharmapala founded the Vikramashila and revived Nalanda,[224] considered one of the first great universities in recorded history. Nalanda reached its height under the patronage of the Pala Empire.[225][226] The Palas also built many viharas. They maintained close cultural and commercial ties with countries of Southeast Asia and Tibet. Sea trade added greatly to the prosperity of the Pala Empire. The Arab merchant Suleiman notes the enormity of the Pala army in his memoirs.[224]
Medieval Cholas rose to prominence during the middle of the 9th century CE and established the greatest empire South India had seen.[227] They successfully united the South India under their rule and through their naval strength extended their influence in the Southeast Asian countries such as Srivijaya.[202] Under Rajaraja Chola I and his successors Rajendra Chola I, Rajadhiraja Chola, Virarajendra Chola and Kulothunga Chola I the dynasty became a military, economic and cultural power in South Asia and South-East Asia.[228][229] Rajendra Chola I's navies went even further, occupying the sea coasts from Burma to Vietnam,[230] the Andaman and Nicobar Islands, the Lakshadweep (Laccadive) islands, Sumatra, and the Malay Peninsula in Southeast Asia and the Pegu islands. The power of the new empire was proclaimed to the eastern world by the expedition to the Ganges which Rajendra Chola I undertook and by the occupation of cities of the maritime empire of Srivijaya in Southeast Asia, as well as by the repeated embassies to China.[231]
They dominated the political affairs of Sri Lanka for over two centuries through repeated invasions and occupation. They also had continuing trade contacts with the Arabs in the west and with the Chinese empire in the east.[232] Rajaraja Chola I and his equally distinguished son Rajendra Chola I gave political unity to the whole of Southern India and established the Chola Empire as a respected sea power.[233] Under the Cholas, the South India reached new heights of excellence in art, religion and literature. In all of these spheres, the Chola period marked the culmination of movements that had begun in an earlier age under the Pallavas. Monumental architecture in the form of majestic temples and sculpture in stone and bronze reached a finesse never before achieved in India.[234]
Chariot detail at Airavatesvara Temple built by Rajaraja Chola II in the 12th century CE.
The Western Chalukya Empire ruled most of the western Deccan, South India, between the 10th and 12th centuries.[235] Vast areas between the Narmada River in the north and Kaveri River in the south came under Chalukya control.[235] During this period the other major ruling families of the Deccan, the Hoysalas, the Seuna Yadavas of Devagiri, the Kakatiya dynasty and the Southern Kalachuris, were subordinates of the Western Chalukyas and gained their independence only when the power of the Chalukya waned during the latter half of the 12th century.[236]
Shrine outer wall and Dravida style superstructure (shikhara) at Siddhesvara Temple at Haveri.
Ornate entrance to the closed hall from the south at Kalleshvara Temple at Bagali.
Shrine wall relief, molding frieze and miniature decorative tower in Mallikarjuna Temple at Kuruvatti.
Rear view showing lateral entrances of the Mahadeva Temple at Itagi.
The late medieval period is marked by repeated invasions of the Muslim Central Asian nomadic clans,[240][241] the rule of the Delhi sultanate, and by the growth of other dynasties and empires, built upon military technology of the Sultanate.[242]
The Delhi Sultanate was a series of successive Islamic states based in Delhi, ruled by several dynasties of Turkic, Turko-Indian[244] and Pashtun origins.[245] It ruled large parts of the Indian subcontinent from the 13th century to the early 16th century.[246] In the 12th and 13th centuries, Central Asian Turks invaded parts of northern India and established the Delhi Sultanate in the former Hindu holdings.[247] The subsequent Mamluk dynasty of Delhi managed to conquer large areas of northern India, while the Khalji dynasty conquered most of central India while forcing the principal Hindu kingdoms of South India to become vassal states.[246]
During the Delhi Sultanate, there was a synthesis between Indian civilisation and Islamic civilisation[citation needed]. The latter was a cosmopolitan civilisation, with a multicultural and pluralistic society, and wide-ranging international networks, including social and economic networks, spanning large parts of Afro-Eurasia, leading to escalating circulation of goods, peoples, technologies and ideas. While initially disruptive due to the passing of power from native Indian elites to Turkic Muslim elites, the Delhi Sultanate was responsible for integrating the Indian subcontinent into a growing world system, drawing India into a wider international network, which had a significant impact on Indian culture and society.[248] However, the Delhi Sultanate also caused large-scale destruction and desecration of temples in the Indian subcontinent.[249]
The Mongol invasions of India were successfully repelled by the Delhi Sultanate during the rule of Alauddin Khalji. A major factor in their success was their Turkic Mamluk slave army, who were highly skilled in the same style of nomadic cavalry warfare as the Mongols, as a result of having similar nomadic Central Asian roots. It is possible that the Mongol Empire may have expanded into India were it not for the Delhi Sultanate's role in repelling them.[250] By repeatedly repulsing the Mongol raiders, the sultanate saved India from the devastation visited on West and Central Asia, setting the scene for centuries of migration of fleeing soldiers, learned men, mystics, traders, artists, and artisans from that region into the subcontinent, thereby creating a syncretic Indo-Islamic culture in the north.[251][250]
A Turco-Mongol conqueror in Central Asia, Timur (Tamerlane), attacked the reigning Sultan Nasir-u Din Mehmud of the Tughlaq dynasty in the north Indian city of Delhi.[252] The Sultan's army was defeated on 17 December 1398. Timur entered Delhi and the city was sacked, destroyed, and left in ruins after Timur's army had killed and plundered for three days and nights. He ordered the whole city to be sacked except for the sayyids, scholars, and the "other Muslims" (artists); 100,000 war prisoners were put to death in one day.[253] The Sultanate suffered significantly from the sacking of Delhi. Though revived briefly under the Lodi dynasty, it was but a shadow of the former.
Qutb Minar, a UNESCO World Heritage Site, whose construction was begun by Qutb ud-Din Aibak, the first Sultan of Delhi.
Dargahs of Sufi-saint Nizamuddin Auliya, and poet and musician Amir Khusro in Delhi.
The grave of Razia, the Sultana of Delhi, from 1236 CE to 1240 CE, the only female ruler of a major realm on the Indian subcontinent until modern times.[citation needed]
The Vijayanagara Empire was established in 1336 by Harihara I and his brother Bukka Raya I of Sangama Dynasty,[254] which originated as a political heir of the Hoysala Empire, Kakatiya Empire,[255] and the Pandyan Empire.[256] The empire rose to prominence as a culmination of attempts by the south Indian powers to ward off Islamic invasions by the end of the 13th century. It lasted until 1646, although its power declined after a major military defeat in 1565 by the combined armies of the Deccan sultanates. The empire is named after its capital city of Vijayanagara, whose ruins surround present day Hampi, now a World Heritage Site in Karnataka, India.[257]
In the first two decades after the founding of the empire, Harihara I gained control over most of the area south of the Tungabhadra river and earned the title of Purvapaschima Samudradhishavara ("master of the eastern and western seas"). By 1374 Bukka Raya I, successor to Harihara I, had defeated the chiefdom of Arcot, the Reddys of Kondavidu, and the Sultan of Madurai and had gained control over Goa in the west and the Tungabhadra-Krishna River doab in the north.[258][259]
With the Vijayanagara Kingdom now imperial in stature, Harihara II, the second son of Bukka Raya I, further consolidated the kingdom beyond the Krishna River and brought the whole of South India under the Vijayanagara umbrella.[260] The next ruler, Deva Raya I, emerged successful against the Gajapatis of Odisha and undertook important works of fortification and irrigation.[261] Italian traveler Niccolo de Conti wrote of him as the most powerful ruler of India.[262] Deva Raya II (called Gajabetekara)[263] succeeded to the throne in 1424 and was possibly the most capable of the Sangama Dynasty rulers.[264] He quelled rebelling feudal lords as well as the Zamorin of Calicut and Quilon in the south. He invaded the island of Sri Lanka and became overlord of the kings of Burma at Pegu and Tanasserim.[265][266][267]
The Vijayanagara Emperors were tolerant of all religions and sects, as writings by foreign visitors show.[268] The kings used titles such as Gobrahamana Pratipalanacharya (literally, "protector of cows and Brahmins") and Hindurayasuratrana (lit, "upholder of Hindu faith") that testified to their intention of protecting Hinduism and yet were at the same time staunchly Islamicate in their court ceremonials and dress.[269] The empire's founders, Harihara I and Bukka Raya I, were devout Shaivas (worshippers of Shiva), but made grants to the Vaishnava order of Sringeri with Vidyaranya as their patron saint, and designated Varaha (the boar, an Avatar of Vishnu) as their emblem.[270] Excavations have found several "Islamic quarters" in the capital.[271] Nobles from Central Asia's Timurid kingdoms also came to Vijayanagara.[272] The later Saluva and Tuluva kings were Vaishnava by faith, but worshipped at the feet of Lord Virupaksha (Shiva) at Hampi as well as Lord Venkateshwara (Vishnu) at Tirupati.[273] A Sanskrit work, Jambavati Kalyanam by King Krishnadevaraya, called Lord Virupaksha Karnata Rajya Raksha Mani ("protective jewel of Karnata Empire").[274] The kings patronised the saints of the dvaita order (philosophy of dualism) of Madhvacharya at Udupi.[275]
An 1868 photograph of the ruins of the Vijayanagara Empire at Hampi, now a UNESCO World Heritage Site[276]
Gajashaala or elephant's stable, built by the Vijayanagar rulers for their war elephants.[277]
Vijayanagara marketplace at Hampi, along with the sacred tank located on the side of Krishna temple.
Chinese manuscript Tribute Giraffe with Attendant, depicting a giraffe presented by Bengali envoys in the name of Sultan Saifuddin Hamza Shah of Bengal to the Yongle Emperor of Ming China.
Mahmud Gawan Madrasa was built by Mahmud Gawan, the Wazir of the Bahmani Sultanate as the centre of religious as well as secular education.
15th century copper plate grant of Gajapati king Purushottama Deva
For two and a half centuries from the mid-13th century, politics in Northern India was dominated by the Delhi Sultanate, and in Southern India by the Vijayanagar Empire. However, there were other regional powers present as well. After fall of Pala Empire, the Chero dynasty ruled much of Eastern Uttar Pradesh, Bihar and Jharkhand from 12th CE to 18th CE.[286][287][288] The Reddy dynasty successfully defeated the Delhi Sultanate; and extended their rule from Cuttack in the north to Kanchi in the south, eventually being absorbed into the expanding Vijayanagara Empire.[289]
In the south, the Bahmani Sultanate, which was established either by a Brahman convert or patronised by a Brahman and from that source it was given the name Bahmani,[293] was the chief rival of the Vijayanagara, and frequently created difficulties for the Vijayanagara.[294] In the early 16th century Krishnadevaraya of the Vijayanagar Empire defeated the last remnant of Bahmani Sultanate power. After which, the Bahmani Sultanate collapsed,[295] resulting it being split into five small Deccan sultanates.[296] In 1490, Ahmadnagar declared independence, followed by Bijapur and Berar in the same year; Golkonda became independent in 1518 and Bidar in 1528.[297] Although generally rivals, they did ally against the Vijayanagara Empire in 1565, permanently weakening Vijayanagar in the Battle of Talikota.
In the East, the Gajapati Kingdom remained a strong regional power to reckon with, associated with a high point in the growth of regional culture and architecture. Under Kapilendradeva, Gajapatis became an empire stretching from the lower Ganga in the north to the Kaveri in the south.[298] In Northeast India, the Ahom Kingdom was a major power for six centuries;[299][300] led by Lachit Borphukan, the Ahoms decisively defeated the Mughal army at the Battle of Saraighat during the Ahom-Mughal conflicts.[301] Further east in Northeastern India was the Kingdom of Manipur, which ruled from their seat of power at Kangla Fort and developed a sophisticated Hindu Gaudiya Vaishnavite culture.[302][303][304]
The Bhakti movement refers to the theistic devotional trend that emerged in medieval Hinduism[305] and later revolutionised in Sikhism.[306] It originated in the seventh-century south India (now parts of Tamil Nadu and Kerala), and spread northwards.[305] It swept over east and north India from the 15th century onwards, reaching its zenith between the 15th and 17th century CE.[307]
Rang Ghar, built by Pramatta Singha in Ahom kingdom's capital Rangpur, is one of the earliest pavilions of outdoor stadia in the Indian subcontinent.
Chittor Fort is the largest fort on the Indian subcontinent; it is one of the six Hill Forts of Rajasthan.
Ranakpur Jain temple was built in the 15th century with the support of the Rajput state of Mewar.
Gol Gumbaz built by the Bijapur Sultanate, has the second largest pre-modern dome in the world after the Byzantine Hagia Sophia.
The early modern period of Indian history is dated from 1526 CE to 1858 CE, corresponding to the rise and fall of the Mughal Empire, which inherited from the Timurid Renaissance. During this age India's economy expanded, relative peace was maintained and arts were patronized. This period witnessed the further development of Indo-Islamic architecture;[317][318] the growth of Mahrattas and Sikhs enabled them to rule significant regions of India in the waning days of the Mughal empire, which formally came to an end when the British Raj was founded.[22] With the discovery of the Cape route in the 1500s, the first Europeans to arrive by sea and establish themselves, were the Portuguese in Goa and Bombay.[319]
It was one of the largest empires to have existed in the Indian subcontinent,[323] and surpassed China to become the world's largest economic power, controlling 24.4% of the world economy,[324] and the world leader in manufacturing,[325] producing 25% of global industrial output.[326] The economic and demographic upsurge was stimulated by Mughal agrarian reforms that intensified agricultural production,[327] and a relatively high degree of urbanisation for its time.[328]
The Agra Fort showing the river Yamuna and the Taj Mahal in the background
Fatehpur Sikri, near Agra, showing Buland Darwaza, the complex built by Akbar, the third Mughal emperor.
The Red Fort, Delhi, its construction begun in 1639 CE, and ended in 1648 CE.
The empire went into decline thereafter. The Mughals suffered several blows due to invasions from Marathas, Rajputs, Jats and Afghans. In 1737, the Maratha general Bajirao of the Maratha Empire invaded and plundered Delhi. Under the general Amir Khan Umrao Al Udat, the Mughal Emperor sent 8,000 troops to drive away the 5,000 Maratha cavalry soldiers. Baji Rao, however, easily routed the novice Mughal general and the rest of the imperial Mughal army fled. In 1737, in the final defeat of Mughal Empire, the commander-in-chief of the Mughal Army, Nizam-ul-mulk, was routed at Bhopal by the Maratha army. This essentially brought an end to the Mughal Empire. While Bharatpur State under Jat ruler Suraj Mal, overran the Mughal garrison at Agra and plundered the city taking with them the two great silver doors of the entrance of the famous Taj Mahal; which were then melted down by Suraj Mal in 1761.[336] In 1739, Nader Shah, emperor of Iran, defeated the Mughal army at the Battle of Karnal.[337] After this victory, Nader captured and sacked Delhi, carrying away many treasures, including the Peacock Throne.[338] Mughal rule was further weakened by constant native Indian resistance; Banda Singh Bahadur led the Sikh Khalsa against Mughal religious oppression; Hindu Rajas of Bengal, Pratapaditya and Raja Sitaram Ray revolted; and Maharaja Chhatrasal, of Bundela Rajputs, fought the Mughals and established the Panna State.[339] The Mughal dynasty was reduced to puppet rulers by 1757. Vadda Ghalughara took place under the Muslim provincial government based at Lahore to wipe out the Sikhs, with 30,000 Sikhs being killed, an offensive that had begun with the Mughals, with the Chhota Ghallughara,[340] and lasted several decades under its Muslim successor states.[341]
The Maratha kingdom was founded and consolidated by Chatrapati Shivaji, a Maratha aristocrat of the Bhonsle clan.[342] However, the credit for making the Marathas formidable power nationally goes to Peshwa (chief minister) Bajirao I. Historian K.K. Datta wrote that Bajirao I "may very well be regarded as the second founder of the Maratha Empire".[343]
In the early 18th century, under the Peshwas, the Marathas consolidated and ruled over much of South Asia. The Marathas are credited to a large extent for ending Mughal rule in India.[344][345][346] In 1737, the Marathas defeated a Mughal army in their capital, in the Battle of Delhi. The Marathas continued their military campaigns against the Mughals, Nizam, Nawab of Bengal and the Durrani Empire to further extend their boundaries. By 1760, the domain of the Marathas stretched across most of the Indian subcontinent.[citation needed] The Marathas even attempted to capture Delhi and discussed putting Vishwasrao Peshwa on the throne there in place of the Mughal emperor.[347]
The Maratha empire at its peak stretched from Tamil Nadu in the south,[348] to Peshawar (modern-day Khyber Pakhtunkhwa, Pakistan[349] [note 3]) in the north, and Bengal in the east. The Northwestern expansion of the Marathas was stopped after the Third Battle of Panipat (1761). However, the Maratha authority in the north was re-established within a decade under Peshwa Madhavrao I.[351]
Maharaja Ranjit Singh consolidated many parts of northern India into an empire. He primarily used his Sikh Khalsa Army that he trained in European military techniques and equipped with modern military technologies. Ranjit Singh proved himself to be a master strategist and selected well-qualified generals for his army. He continuously defeated the Afghan armies and successfully ended the Afghan-Sikh Wars. In stages, he added central Punjab, the provinces of Multan and Kashmir, and the Peshawar Valley to his empire.[354][355]
At its peak, in the 19th century, the empire extended from the Khyber Pass in the west, to Kashmir in the north, to Sindh in the south, running along Sutlej river to Himachal in the east. After the death of Ranjit Singh, the empire weakened, leading to conflict with the British East India Company. The hard-fought First Anglo-Sikh War and Second Anglo-Sikh War marked the downfall of the Sikh Empire, making it among the last areas of the Indian subcontinent to be conquered by the British.
Hyderabad was founded by the Qutb Shahi dynasty of Golconda in 1591. Following a brief Mughal rule, Asif Jah, a Mughal official, seized control of Hyderabad and declared himself Nizam-al-Mulk of Hyderabad in 1724. The Nizams lost considerable territory and paid tribute to the Maratha Empire after being routed in multiple battles, such as the Battle of Palkhed.[356] However, the Nizams maintained their sovereignty from 1724 until 1948 through paying tributes to the Marathas, and later, being vassels of the British. Hyderabad State became a princely state in British India in 1798.
The Nawabs of Bengal had become the de facto rulers of Bengal following the decline of Mughal Empire. However, their rule was interrupted by Marathas who carried out six expeditions in Bengal from 1741 to 1748, as a result of which Bengal became a tributary state of Marathas. On 23 June 1757, Siraj ud-Daulah, the last independent Nawab of Bengal was betrayed in the Battle of Plassey by Mir Jafar. He lost to the British, who took over the charge of Bengal in 1757, installed Mir Jafar on the Masnad (throne) and established itself to a political power in Bengal.[357] In 1765 the system of Dual Government was established, in which the Nawabs ruled on behalf of the British and were mere puppets to the British. In 1772 the system was abolished and Bengal was brought under the direct control of the British. In 1793, when the Nizamat (governorship) of the Nawab was also taken away from them, they remained as the mere pensioners of the British East India Company.[358][359]
In the 18th century, the whole of Rajputana was virtually subdued by the Marathas. The Second Anglo-Maratha War distracted the Marathas from 1807 to 1809, but afterward Maratha domination of Rajputana resumed. In 1817, the British went to war with the Pindaris, raiders who were fled in Maratha territory, which quickly became the Third Anglo-Maratha War, and the British government offered its protection to the Rajput rulers from the Pindaris and the Marathas. By the end of 1818 similar treaties had been executed between the other Rajput states and Britain. The Maratha Sindhia ruler of Gwalior gave up the district of Ajmer-Merwara to the British, and Maratha influence in Rajasthan came to an end.[360] Most of the Rajput princes remained loyal to Britain in the Revolt of 1857, and few political changes were made in Rajputana until Indian independence in 1947. The Rajputana Agency contained more than 20 princely states, most notable being Udaipur State, Jaipur State, Bikaner State and Jodhpur State.
After the fall of the Maratha Empire, many Maratha dynasties and states became vassals in a subsidiary alliance with the British, to form the largest bloc of princely states in the British Raj, in terms of territory and population.[citation needed] With the decline of the Sikh Empire, after the First Anglo-Sikh War in 1846, under the terms of the Treaty of Amritsar, the British government sold Kashmir to Maharaja Gulab Singh and the princely state of Jammu and Kashmir, the second-largest princely state in British India, was created by the Dogra dynasty.[361][362] While in Eastern and Northeastern India, the Hindu and Buddhist states of Cooch Behar Kingdom, Twipra Kingdom and Kingdom of Sikkim were annexed by the British and made vassal princely state.
After the fall of the Vijayanagara Empire, Polygar states emerged in Southern India; and managed to weather invasions and flourished until the Polygar Wars, where they were defeated by the British East India Company forces.[363] Around the 18th century, the Kingdom of Nepal was formed by Rajput rulers.[364]
The next to arrive were the Dutch, with their main base in Ceylon. They established ports in Malabar. However, their expansion into India was halted after their defeat in the Battle of Colachel by the Kingdom of Travancore during the Travancore-Dutch War. The Dutch never recovered from the defeat and no longer posed a large colonial threat to India.[367][368]
The English East India Company was founded in 1600 as The Company of Merchants of London Trading into the East Indies. It gained a foothold in India with the establishment of a factory in Masulipatnam on the Eastern coast of India in 1611 and a grant of rights by the Mughal emperor Jahangir to establish a factory in Surat in 1612. In 1640, after receiving similar permission from the Vijayanagara ruler farther south, a second factory was established in Madras on the southeastern coast. The islet of Bom Bahia in present-day Mumbai (Bombay), was a Portuguese outpost not far from Surat, it was presented to Charles II of England as dowry, in his marriage to Catherine of Braganza, Charles in turn leased Bombay to the Company in 1668. Two decades later, the company established a trade post in the River Ganges delta, when a factory was set up in Calcutta (Kolkata). During this time other companies established by the Portuguese, Dutch, French, and Danish were similarly expanding in the sub-continent.
Warren Hastings, the first governor-general of Fort William (Bengal) who oversaw the company's territories in India.
Gold coin, minted 1835, with obverse showing the bust of William IV, king of United Kingdom from 26 June 1830 to 20 June 1837, and reverse marked "Two mohurs" in English (do ashrafi in Urdu) issued during Company rule in India
Photograph (1855) showing the construction of the Bhor Ghaut incline bridge, Bombay; the incline was conceived by George Clark, the Chief Engineer in the East India Company's Government of Bombay.
Watercolor (1863) titled, The Ganges Canal, Roorkee, Saharanpur District (U.P.). The canal was the brainchild of Sir Proby Cautley; construction began in 1840, and the canal was opened by Governor-General Lord Dalhousie in April 1854
The second form of asserting power involved treaties in which Indian rulers acknowledged the company's hegemony in return for limited internal autonomy. Since the company operated under financial constraints, it had to set up political underpinnings for its rule.[374] The most important such support came from the subsidiary alliances with Indian princes during the first 75 years of Company rule.[374] In the early 19th century, the territories of these princes accounted for two-thirds of India.[374] When an Indian ruler who was able to secure his territory wanted to enter such an alliance, the company welcomed it as an economical method of indirect rule that did not involve the economic costs of direct administration or the political costs of gaining the support of alien subjects.[375]
In return, the company undertook the "defense of these subordinate allies and treated them with traditional respect and marks of honor."[375] Subsidiary alliances created the Princely States of the Hindu maharajas and the Muslim nawabs. Prominent among the princely states were Cochin (1791), Jaipur (1794), Travancore (1795), Hyderabad (1798), Mysore (1799), Cis-Sutlej Hill States (1815), Central India Agency (1819), Cutch and Gujarat Gaikwad territories (1819), Rajputana (1818) and Bahawalpur (1833).[373]
Lord Dalhousie, the Governor-General of India from 1848 to 1856, who devised the Doctrine of Lapse.
Lakshmibai, the Rani of Jhansi, one of the principal leaders of the rebellion who earlier had lost her kingdom as a result of the Doctrine of lapse.
Bahadur Shah Zafar the last Mughal Emperor, crowned Emperor of India by the rebels, he was deposed by the British, and died in exile in Burma
The Indian rebellion of 1857 was a large-scale rebellion by soldiers employed by the British East India Company in northern and central India against the company's rule. The spark that led to the mutiny was the issue of new gunpowder cartridges for the Enfield rifle, which was insensitive to local religious prohibition. The key mutineer was Mangal Pandey.[376] In addition, the underlying grievances over British taxation, the ethnic gulf between the British officers and their Indian troops and land annexations played a significant role in the rebellion. Within weeks after Pandey's mutiny, dozens of units of the Indian army joined peasant armies in widespread rebellion. The rebel soldiers were later joined by Indian nobility, many of whom had lost titles and domains under the Doctrine of Lapse and felt that the company had interfered with a traditional system of inheritance. Rebel leaders such as Nana Sahib and the Rani of Jhansi belonged to this group.[377]
After the outbreak of the mutiny in Meerut, the rebels very quickly reached Delhi. The rebels had also captured large tracts of the North-Western Provinces and Awadh (Oudh). Most notably, in Awadh, the rebellion took on the attributes of a patriotic revolt against British presence.[378] However, the British East India Company mobilised rapidly with the assistance of friendly Princely states, but it took the British the remainder of 1857 and the better part of 1858 to suppress the rebellion. Due to the rebels being poorly equipped and having no outside support or funding, they were brutally subdued by the British.[379]
After 1857, the colonial government strengthened and expanded its infrastructure via the court system, legal procedures, and statutes. The Indian Penal Code came into being.[381] In education, Thomas Babington Macaulay had made schooling a priority for the Raj in his famous minute of February 1835 and succeeded in implementing the use of English as the medium of instruction. By 1890 some 60,000 Indians had matriculated.[382] The Indian economy grew at about 1% per year from 1880 to 1920, and the population also grew at 1%. However, from 1910s Indian private industry began to grow significantly. India built a modern railway system in the late 19th century which was the fourth largest in the world.[383] The British Raj invested heavily in infrastructure, including canals and irrigation systems in addition to railways, telegraphy, roads and ports.[citation needed] However, historians have been bitterly divided on issues of economic history, with the Nationalist school arguing that India was poorer at the end of British rule than at the beginning and that impoverishment occurred because of the British.[384]
In 1905, Lord Curzon split the large province of Bengal into a largely Hindu western half and "Eastern Bengal and Assam", a largely Muslim eastern half. The British goal was said to be for efficient administration but the people of Bengal were outraged at the apparent "divide and rule" strategy. It also marked the beginning of the organised anti-colonial movement. When the Liberal party in Britain came to power in 1906, he was removed. Bengal was reunified in 1911. The new Viceroy Gilbert Minto and the new Secretary of State for India John Morley consulted with Congress leaders on political reforms. The Morley-Minto reforms of 1909 provided for Indian membership of the provincial executive councils as well as the Viceroy's executive council. The Imperial Legislative Council was enlarged from 25 to 60 members and separate communal representation for Muslims was established in a dramatic step towards representative and responsible government.[385] Several socio-religious organisations came into being at that time. Muslims set up the All India Muslim League in 1906. It was not a mass party but was designed to protect the interests of the aristocratic Muslims. It was internally divided by conflicting loyalties to Islam, the British, and India, and by distrust of Hindus.[citation needed] The Hindu Mahasabha and Rashtriya Swayamsevak Sangh (RSS) sought to represent Hindu interests though the latter always claimed it to be a "cultural" organisation.[386] Sikhs founded the Shiromani Akali Dal in 1920.[387] However, the largest and oldest political party Indian National Congress, founded in 1885, attempted to keep a distance from the socio-religious movements and identity politics.[388]
Two silver rupee coins issued by the British Raj in 1862 and 1886 respectively, the first in obverse showing a bust of Victoria, Queen, the second of Victoria, Empress.  Victoria became Empress of India in 1876.
Ronald Ross, left, at Cunningham's laboratory of Presidency Hospital in Calcutta, where the transmission of malaria by mosquitoes was discovered, winning Ross the second Nobel Prize for Physiology or Medicine in 1902.
A Darjeeling Himalayan Railway train shown in 1870.  The railway became a UNESCO World Heritage Site in 1999.
A second-day cancellation of the stamps issued in February 1931 to commemorate the inauguration of New Delhi as the capital of the British Indian Empire. Between 1858 and 1911, Calcutta had been the capital of the Raj
During this period, Bengal witnessed an intellectual awakening that is in some way similar to the Renaissance. This movement questioned existing orthodoxies, particularly with respect to women, marriage, the dowry system, the caste system, and religion. One of the earliest social movements that emerged during this time was the Young Bengal movement, which espoused rationalism and atheism as the common denominators of civil conduct among upper caste educated Hindus.[391] It played an important role in reawakening Indian minds and intellect across the Indian subcontinent.
A picture of orphans who survived the Bengal famine of 1943
Indian cavalry from the Deccan Horse during the Battle of Bazentin Ridge in 1916.
Indian Army gunners (probably 39th Battery) with 3.7-inch mountain howitzers, Jerusalem 1917.
During World War I, over 800,000 volunteered for the army, and more than 400,000 volunteered for non-combat roles, compared with the pre-war annual recruitment of about 15,000 men.[400] The Army saw action on the Western Front within a month of the start of the war at the First Battle of Ypres. After a year of front-line duty, sickness and casualties had reduced the Indian Corps to the point where it had to be withdrawn. Nearly 700,000 Indians fought the Turks in the Mesopotamian campaign. Indian formations were also sent to East Africa, Egypt, and Gallipoli.[401]
Indian Army and Imperial Service Troops fought during the Sinai and Palestine Campaign's defence of the Suez Canal in 1915, at Romani in 1916 and to Jerusalem in 1917. India units occupied the Jordan Valley and after the German spring offensive they became the major force in the Egyptian Expeditionary Force during the Battle of Megiddo and in the Desert Mounted Corps' advance to Damascus and on to Aleppo. Other divisions remained in India guarding the North-West Frontier and fulfilling internal security obligations.
One million Indian troops served abroad during the war. In total, 74,187 died,[402] and another 67,000 were wounded.[403] The roughly 90,000 soldiers who died fighting in World War I and the Afghan Wars are commemorated by the India Gate.
General Claude Auchinleck (right), Commander-in-Chief of the Indian Army, with the then Viceroy Wavell (centre) and General Montgomery (left)
Indian women training for Air Raid Precautions (ARP) duties in Bombay, 1942
Indian infantrymen of the 7th Rajput Regiment about to go on patrol on the Arakan front in Burma, 1944.
The stamp series "Victory" issued by the Government of British India to commemorate allied victory in World War II.
British India officially declared war on Nazi Germany in September 1939.[404] The British Raj, as part of the Allied Nations, sent over two and a half million volunteer soldiers to fight under British command against the Axis powers. Additionally, several Indian Princely States provided large donations to support the Allied campaign during the War. India also provided the base for American operations in support of China in the China Burma India Theatre.
Indians fought with distinction throughout the world, including in the European theatre against Germany, in North Africa against Germany and Italy, against the Italians in East Africa, in the Middle East against the Vichy French, in the South Asian region defending India against the Japanese and fighting the Japanese in Burma. Indians also aided in liberating British colonies such as Singapore and Hong Kong after the Japanese surrender in August 1945. Over 87,000 soldiers from the subcontinent died in World War II.
The Indian National Congress, denounced Nazi Germany but would not fight it or anyone else until India was independent. Congress launched the Quit India Movement in August 1942, refusing to co-operate in any way with the government until independence was granted. The government was ready for this move. It immediately arrested over 60,000 national and local Congress leaders. The Muslim League rejected the Quit India movement and worked closely with the Raj authorities.
Subhas Chandra Bose (also called Netaji) broke with Congress and tried to form a military alliance with Germany or Japan to gain independence. The Germans assisted Bose in the formation of the Indian Legion;[405] however, it was Japan that helped him revamp the Indian National Army (INA), after the First Indian National Army under Mohan Singh was dissolved. The INA fought under Japanese direction, mostly in Burma.[406] Bose also headed the Provisional Government of Free India (or Azad Hind), a government-in-exile based in Singapore.[407][408] The government of Azad Hind had its own currency, court, and civil code; and in the eyes of some Indians its existence gave a greater legitimacy to the independence struggle against the British.[citation needed]
By 1942, neighbouring Burma was invaded by Japan, which by then had already captured the Indian territory of Andaman and Nicobar Islands. Japan gave nominal control of the islands to the Provisional Government of Free India on 21 October 1943, and in the following March, the Indian National Army with the help of Japan crossed into India and advanced as far as Kohima in Nagaland. This advance on the mainland of the Indian subcontinent reached its farthest point on Indian territory, retreating from the Battle of Kohima in June and from that of Imphal on 3 July 1944.
The first session of the Indian National Congress in 1885.  A. O. Hume, the founder, is shown in the middle (third row from the front).  The Congress was the first modern nationalist movement to emerge in the British Empire in Asia and Africa.[411]
Surya Sen, leader of the Chittagong armoury raid, a raid on 18 April 1930 on the armoury of police and auxiliary forces in Chittagong, Bengal, now Bangladesh
Front page of the Tribune (25 March 1931), reporting the execution of Bhagat Singh, Rajguru and Sukhdev by the British for the murder of 21-year-old police officer J. P. Saunders. Bhagat Singh quickly became a folk hero of the Indian independence movement.
From the late 19th century, and especially after 1920, under the leadership of Mahatma Gandhi (right), the Congress became the principal leader of the Indian independence movement.[412]  Gandhi is shown here with Jawaharlal Nehru, later the first prime minister of India.
The numbers of British in India were small,[413] yet they were able to rule 52% of the Indian subcontinent directly and exercise considerable leverage over the princely states that accounted for 48% of the area.[414]
One of the most important events of the 19th century was the rise of Indian nationalism,[415] leading Indians to seek first "self-rule" and later "complete independence". However, historians are divided over the causes of its rise. Probable reasons include a "clash of interests of the Indian people with British interests",[415] "racial discriminations",[416] and "the revelation of India's past".[417]
The first step toward Indian self-rule was the appointment of councillors to advise the British viceroy in 1861 and the first Indian was appointed in 1909. Provincial Councils with Indian members were also set up. The councillors' participation was subsequently widened into legislative councils. The British built a large British Indian Army, with the senior officers all British and many of the troops from small minority groups such as Gurkhas from Nepal and Sikhs.[418] The civil service was increasingly filled with natives at the lower levels, with the British holding the more senior positions.[419]
The partition of Bengal in 1905 further increased the revolutionary movement for Indian independence. The disenfranchisement lead some to take violent action.
From 1920 leaders such as Mahatma Gandhi began highly popular mass movements to campaign against the British Raj using largely peaceful methods. The Gandhi-led independence movement opposed the British rule using non-violent methods like non-co-operation, civil disobedience and economic resistance. However, revolutionary activities against the British rule took place throughout the Indian subcontinent and some others adopted a militant approach like the Hindustan Republican Association, founded by Chandrasekhar Azad, Bhagat Singh, Sukhdev Thapar and others, that sought to overthrow British rule by armed struggle.
The All India Azad Muslim Conference gathered in Delhi in April 1940 to voice its support for an independent and united India.[424] Its members included several Islamic organisations in India, as well as 1400 nationalist Muslim delegates.[425][426][427] The pro-separatist All-India Muslim League worked to try to silence those nationalist Muslims who stood against the partition of India, often using "intimidation and coercion".[426][427] The murder of the All India Azad Muslim Conference leader Allah Bakhsh Soomro also made it easier for the pro-separatist All-India Muslim League to demand the creation of a Pakistan.[427]
"A moment comes, which comes but rarely in history, when we step out from the old to the new; when an age ends; and when the soul of a nation long suppressed finds utterance."
In January 1946, several mutinies broke out in the armed services, starting with that of RAF servicemen frustrated with their slow repatriation to Britain. The mutinies came to a head with mutiny of the Royal Indian Navy in Bombay in February 1946, followed by others in Calcutta, Madras, and Karachi. The mutinies were rapidly suppressed. Also in early 1946, new elections were called and Congress candidates won in eight of the eleven provinces.
Late in 1946, the Labour government decided to end British rule of India, and in early 1947 it announced its intention of transferring power no later than June 1948 and participating in the formation of an interim government.
Along with the desire for independence, tensions between Hindus and Muslims had also been developing over the years. The Muslims had always been a minority within the Indian subcontinent, and the prospect of an exclusively Hindu government made them wary of independence; they were as inclined to mistrust Hindu rule as they were to resist the foreign Raj.
Muslim League leader Muhammad Ali Jinnah proclaimed 16 August 1946 as Direct Action Day, with the stated goal of highlighting, peacefully, the demand for a Muslim homeland in British India, which resulted in the outbreak of the cycle of violence that would be later called the "Great Calcutta Killing of August 1946". The communal violence spread to Bihar (where Muslims were attacked by Hindus), to Noakhali in Bengal (where Hindus were targeted by Muslims), in Garhmukteshwar in the United Provinces (where Muslims were attacked by Hindus), and on to Rawalpindi in March 1947 in which Hindus were attacked or driven out by Muslims.
A map of the prevailing religions of the British Indian empire based on district-wise majorities based on the Indian census of 1909, and published in the Imperial Gazetteer of India.  The partition of the Punjab and Bengal was based on such majorities.
Gandhi touring Bela, Bihar, a village struck by religious rioting in March 1947. On the right is Khan Abdul Gaffar Khan.
Jawaharlal Nehru being sworn in as the first prime minister of independent India by viceroy Lord Louis Mountbatten at 8:30 AM 15 August 1947.
In recent decades there have been four main schools of historiography in how historians study India: Cambridge, Nationalist, Marxist, and subaltern. The once common "Orientalist" approach, with its image of a sensuous, inscrutable, and wholly spiritual India, has died out in serious scholarship.[433]
The "Cambridge School", led by Anil Seal,[434] Gordon Johnson,[435] Richard Gordon, and David A. Washbrook,[436] downplays ideology.[437] However, this school of historiography is criticised for western bias or Eurocentrism.[438]
The Nationalist school has focused on Congress, Gandhi, Nehru and high level politics. It highlighted the Mutiny of 1857 as a war of liberation, and Gandhi's 'Quit India' begun in 1942, as defining historical events. This school of historiography has received criticism for elitism.[439]
The Marxists have focused on studies of economic development, landownership, and class conflict in precolonial India and of deindustrialisation during the colonial period. The Marxists portrayed Gandhi's movement as a device of the bourgeois elite to harness popular, potentially revolutionary forces for its own ends. Again, the Marxists are accused of being "too much" ideologically influenced.[440]
The "subaltern school", was begun in the 1980s by Ranajit Guha and Gyan Prakash.[441] It focuses attention away from the elites and politicians to "history from below", looking at the peasants using folklore, poetry, riddles, proverbs, songs, oral history and methods inspired by anthropology. It focuses on the colonial era before 1947 and typically emphasises caste and downplays class, to the annoyance of the Marxist school.[442]
More recently, Hindu nationalists have created a version of history to support their demands for Hindutva ('Hinduness') in Indian society. This school of thought is still in the process of development.[443] In March 2012, Diana L. Eck, professor of Comparative Religion and Indian Studies at Harvard University, authored in her book India: A Sacred Geography, that the idea of India dates to a much earlier time than the British or the Mughals; it was not just a cluster of regional identities and it was not ethnic or racial.[444][445][446][447]
Abstract strategy games in contrast to strategy games in general usually have no or minimal narrative theme, outcomes determined only by player choice (with no randomness), and each players have perfect information about the game.[1] For example, Go is a pure abstract strategy game since it fulfills all three criteria; chess and related games are nearly so but feature a recognizable theme of ancient warfare; and Stratego is borderline since it is deterministic, loosely based on 19th-century Napoleonic warfare, and features concealed information.
Combinatorial games have no randomizers such as dice, no simultaneous movement, nor hidden information.  Some games that do have these elements are sometimes classified as abstract strategy games.  (Games such as Continuo, Octiles, Can't Stop, and Sequence, could be considered abstract strategy games, despite having a luck or bluffing element.) A smaller category of abstract strategy games manages to incorporate hidden information without using any random elements; the best known example is Stratego.
Some abstract strategy games have multiple starting positions of which it is required that one be randomly determined.  For a game to be one of skill, a starting position needs to be chosen by impartial means.  Some games, such as Arimaa and DVONN, have the players build the starting position in a separate initial phase which itself conforms strictly to combinatorial game principles.  Most players, however, would consider that although one is then starting each game from a different position, the game itself contains no luck element. Indeed, Bobby Fischer promoted randomization of the starting position in chess in order to increase player dependence on thinking at the board.[3]
As J. Mark Thompson wrote in his article "Defining the Abstract", play is sometimes said to resemble a series of puzzles the players pose to each other:[4][5] 
There is an intimate relationship between such games and puzzles: every board position presents the player with the puzzle, What is the best move?, which in theory could be solved by logic alone. A good abstract game can therefore be thought of as a "family" of potentially interesting logic puzzles, and the play consists of each player posing such a puzzle to the other. Good players are the ones who find the most difficult puzzles to present to their opponents.
Many abstract strategy games also happen to be "combinatorial"; i.e., there is no hidden information, no non-deterministic elements (such as shuffled cards or dice rolls), no simultaneous or hidden movement or setup, and (usually) two players or teams take a finite number of alternating turns.
Many games which are abstract in nature historically might have developed from thematic games, such as representation of military tactics.[6] In turn, it is common to see thematic version of such games; for example, chess is considered an abstract game, but many thematic versions, such as Star Wars-themed chess, exist.
Go was considered one of the four essential arts of the cultured aristocratic Chinese scholars in antiquity. The earliest written reference to the game is generally recognized as the historical annal Zuo Zhuan[17][18] (c. 4th century BC).[19]
Englishmen Lewis Waterman[20] and John W. Mollett both claim to have invented the game of Reversi in 1883, each denouncing the other as a fraud. The game gained considerable popularity in England at the end of the nineteenth century.[21] The game's first reliable mention is in 21 August 1886 edition of The Saturday Review.
As for the qualitative aspects, ranking abstract strategy games according to their interest, complexity, or strategy levels is a daunting task and subject to extreme subjectivity.  In terms of measuring how finite a mathematical field each of the three top contenders represents, it is estimated that checkers has a game-tree complexity of  1040 possible games, whereas chess has approximately 10123. As for Go, the possible legal game positions range in the magnitude of 10170.
The Mind Sports Olympiad first held the Abstract Games World Championship in 2008 to try to find the best abstract strategy games all-rounder.[2] The MSO event saw a change in format in 2011[22] restricting the competition to players' five best events, and was renamed to the Modern Abstract Games World Championship.
In economics, perfect information (sometimes referred to as "no hidden information") is a feature of perfect competition. With perfect information in a market, all consumers and producers have complete and instantaneous knowledge of all market prices, their own utility, and own cost functions.
In game theory, a sequential game has perfect information if each player, when making any decision, is perfectly informed of all the events that have previously occurred, including the "initialization event" of the game (e.g. the starting hands of each player in a card game).[1][2][3][4]
Perfect information is importantly different from complete information, which implies common knowledge of each player's utility functions, payoffs, strategies and "types". A game with perfect information may or may not have complete information.
Chess is an example of a game with perfect information, as each player can see all the pieces on the board at all times.[2] Other games with perfect information include tic-tac-toe, Reversi, checkers, and Go.[3]
Academic literature has not produced consensus on a standard definition of perfect information which defines whether games with chance, but no secret information, and games with simultaneous moves are games of perfect information.[4][7][8][9][10]
Games which are sequential (players alternate in moving) and which have chance events (with known probabilities to all players) but no secret information, are sometimes considered games of perfect information. This includes games such as backgammon and Monopoly. But there are some academic papers which do not regard such games as games of perfect information because the results of chance themselves are unknown prior to them occurring.[4][7][8][9][10]
Games with simultaneous moves are generally not considered games of perfect information. This is because each player holds information which is secret, and must play a move without knowing the opponent's secret information. Nevertheless, some such games are symmetrical, and fair. An example of a game in this category includes rock paper scissors.[4][7][8][9][10]
A chessboard is a gameboard used to play chess. It consists of 64 squares, 8 rows by 8 columns, on which the chess pieces are placed. It is square in shape and uses two colours of squares, one light and one dark, in a chequered pattern. During play, the board is oriented such that each player's near-right corner square is a light square.
The columns of a chessboard are known as files, the rows are known as ranks, and the lines of adjoining same-coloured squares (each running from one edge of the board to an adjacent edge) are known as diagonals. Each square of the board is named using algebraic, descriptive, or numeric chess notation; algebraic notation is the FIDE standard. In algebraic notation, using White's perspective, files are labeled a through h from left to right, and ranks are labeled 1 through 8 from bottom to top; each square is identified by the file and rank which it occupies. The a- through d-files comprise the queenside, while the e- through h-files comprise the kingside.
Chessboards have been made from numerous materials over the years, such as ebony, ivory, marble, metal, glass, and plastic. They can also be found as decorative elements in plazas, gardens, and living rooms.
High-level games generally use wooden boards, while vinyl, plastic, and cardboard are common for less important tournaments and matches, as well as for home use. Additionally, some very large chessboards are built into or drawn on the ground. Rarely, decorative glass and marble boards are permitted for games conducted by national or international chess federations.
Wooden boards are traditionally made of unstained woods that are light brown and dark brown in colour. To reduce cost, some boards are made with veneers of more expensive woods glued to an inner piece of plywood or chipboard. A variety of colour combinations are used for plastic, vinyl, and silicone boards. Common dark-light combinations are black and white, as well as brown, green or blue with buff or cream.
For international or continental championships, FIDE's regulations state that wooden boards should be used. For other FIDE tournaments, wood, plastic, or cardboard boards may be used, and the board should be rigid in all instances. The board may also be made of marble, as long as there is an appropriate contrast between the light and dark squares. The finishing should be neutral or frosted but never shiny. The squares should be from 5 to 6 cm in length, at least twice the diameter of a pawn's base. If the table and the board are two separate pieces, the latter must be fixed so it stays in place.[8]
There are various systems for recording moves and referring to the squares of the chessboard; the standard contemporary system is algebraic notation. In algebraic notation, the files are identified by the letters a to h, from left to right from the white player's point of view, and the ranks by the numbers 1 to 8, with 1 being closest to the white player. Each square on the board is identified by a unique coordinate pairing, from a1 to h8.[9]
In the older descriptive notation, the files are labelled by the piece originally occupying its first rank (e.g. queen, king's rook, queen's bishop), and ranks by the numbers 1 to 8 from each player's point of view, depending on the move being described. This method is no longer commonly used. FIDE stopped using descriptive notation in 1981.
ICCF numeric notation assigns numbers to both files and ranks, with rank 1 being the one closest to the player with the white pieces. The file leftmost to the white player (a in algebraic notation and QR in descriptive notation) is file one and the rightmost to them (h in algebraic notation and KR in descriptive notation) is file eight.
Some chess variants use more than a single board per match. Bughouse chess, for example, involves four players playing two simultaneous matches on separate boards.[17] Alice Chess is a popular variant which is usually played on two boards to facilitate the movement of pieces between the boards.[18] Three-dimensional boards are often represented by multiple two-dimensional boards. Variants may use anywhere from two to eight boards.[19] For example, Raumschach utilises five boards of twenty-five squares each, totaling 125 squares.[19] Another noteworthy variant, Star Trek Chess, utilises a board of sixty-four squares divided into seven levels.[20] Star Trek Chess uses a board with movable parts divided into seven levels. In the initial position, each player occupies two of the movable four-square attack boards. The white pieces start in the lower level, using attack boards connected to this level and the first two rows of the board, while the black pieces start at the top, using the attack boards and first two rows of the third level.[20]
The game of chess has been represented in the arts since its creation. Chess sets usually had considerable artistic value; they were made of noble materials, such as ebony and ivory, and in large sizes. Many of the pieces in these sets were offered to churches as relics. The book Liber miraculorum sancte Fidis tells a story in which a nobleman, after miraculously escaping from prison, is forced to carry a chessboard until a sanctuary as gesture of gratitude. However, more frequently, there are stories in which the chessboard is used as a weapon. The French tale of Ogier the Dane reports how the son of Charlemagne brutally kills one of Ogier's sons with a chessboard after losing a match, although there is no evidence confirming the veracity of the story.[21]
In 1250, a sermon called Quaedam moralitas de scaccario per Innocentium papum (The Innocent Morality) showed the world as being represented by a chessboard. The white and black squares represented the two conditions of life and death or praise and censure; over these, the pieces, representing humanity, would confront each other in the adversities of the game, which symbolised life.
Due to its simple geometry, the chessboard is often used in mathematical puzzles or problems unrelated to chess, such as the wheat and chessboard problem and the mutilated chessboard problem. The term infinite chessboard is sometimes used to refer to a grid.
A chessboard is often painted or engraved on a chess table
DGT Electronic Chessboard that detects moves and interfaces to chess clock and computers
A portable green and white mousepad style board designed to lie perfectly flat
Social play on a vinyl board in a park in Kyiv
A board of circular chess, one of the many variants of traditional chess
A giant outdoor chessboard, with pieces about three feet (91 cm) tall.
A chess piece, or chessman, is a game piece that is placed on a chessboard to play the game of chess. It can be either white or black, and it can be one of six types: king, queen, rook, bishop, knight, or pawn.
Chess sets generally come with sixteen pieces of each color. Additional pieces, usually an extra queen per color, may be provided for use in promotion.
Each player begins with sixteen pieces (but see the subsection below for other usage of the term piece).  The pieces that belong to each player are distinguished by color: the lighter colored pieces are referred to as "white" and the player that owns them as "White", whereas the darker colored pieces are referred to as "black" and the player that owns them as "Black".
In a standard game, each of the two players begins with the following sixteen pieces:
The word "piece" has three meanings, depending on the context.[1]
The rules of chess prescribe the moves each type of chess piece can make. During play, the players take turns moving their own chess pieces.
Pieces other than pawns capture in the same way that they move. A capturing piece replaces the opponent piece on its square, except for an en passant capture by a pawn. Captured pieces are immediately removed from the game. A square may hold only one piece at any given time. Except for castling and the knight's move, no piece may jump over another piece.[7]
The value assigned to a piece attempts to represent the potential strength of the piece in the game. As the game develops, the relative values of the pieces will also change. For example, in an open game, bishops are relatively more valuable; they can be positioned to control long, open diagonal spaces.  In a closed game with lines of protected pawns blocking bishops, however, knights usually become relatively more potent. Similar ideas apply to placing rooks on open files and knights on active, central squares. The standard valuation is one point for a pawn, three points for a knight or bishop, five points for a rook, and nine points for a queen.[8] These values are reliable in endgames, particularly with a limited number of pieces. But these values can change depending on the position or the phase of the game (opening, middle game, or ending). A bishop pair for example, is worth an additional half-pawn on average.[9] In specific circumstances, the values may be quite different: sometimes a knight can be more valuable than a queen if a particular angle is required for a mating attack, such as certain smothered mates.  The humble pawn becomes more and more valuable the closer it is to securing a queen promotion for another example.
Chess evolved over time from its earliest versions in India and Persia to variants that spread both West and East.  Pieces changed names and rules as well; the most notable changes was the Vizir (or Firz) becoming the Queen, and the Elephant becoming the Bishop in European versions of chess.  The movement patterns for Queens and Bishops also changed, with the earliest rules restricting elephants to just two squares along a diagonal, but allowing them to "jump" (seen in the fairy chess piece the alfil); and the earliest versions of queens could only move a single square diagonally (the fairy chess piece Ferz).  The modern bishop's movement was popularized in the 14th and 15th centuries, and the modern queen was popularized in the 15th and 16th centuries, with versions with the more powerful modern queen eclipsing older variants.
A knight made around 1250 in London, England.  The knight is battling an evil dragon.
A 12th century warder (modern rook) made of whale ivory of Scandinavian origin, similar to the famous Lewis chessmen.
A 13th century Queen astride a horse with attendants, of Scandinavian origin.  The Queen replaced the Persian Vizier in European chess.
The characters implied by pieces' names vary between languages. For example, in many languages, the piece known in English as the "knight" frequently translates as "horse", and the English "bishop" frequently translates as "elephant" in language areas that adapted the modern bishop's movement pattern, but not its new name.[10]
In algebraic notation, the king is abbreviated by the letter K among English speakers. The white king starts the game on e1; the black king starts on e8. Unlike all other pieces, only one king per player can be on the board at any time, and the kings are never removed from the board during the game.
The white king starts on e1, on the first rank to the right of the queen from White's perspective. The black king starts on e8, directly across from the white king. Each king starts on a square opposite its own color.
A king can move one square horizontally, vertically, or diagonally unless the square is already occupied by a friendly piece or the move would place the king in check. If the square is occupied by an undefended enemy piece, the king may capture it, removing it from play. Opposing kings may never occupy adjacent squares (see opposition) to give check, as that would put the moving king in check as well. The king can give discovered check, however, by unblocking a bishop, rook, or queen.
The king can make a special move, in conjunction with a rook of the same color, called castling. When castling, the king moves two squares horizontally toward one of its rooks, and that rook is placed on the square over which the king crossed.
Castling with the h-file rook is known as castling kingside or short castling (denoted 0-0 in algebraic notation), while castling with the a-file rook is known as castling queenside or long castling (denoted 0-0-0).
A king that is under attack is said to be in check, and the player in check must immediately remedy the situation. There are three possible ways to remove the king from check:
If none of the three options are available, the player's king has been checkmated, and the player loses the game.
At amateur levels, when placing the opponent's king in check, it is common to announce "check", but this is not required by the rules of chess.
A stalemate occurs when a player, on their turn, has no legal moves, and the player's king is not in check.
If this happens, the king is said to have been stalemated, and the game ends in a draw. A player who has very little or no chance of winning will often, in order to avoid a loss, try to entice the opponent to inadvertently place the player's king in stalemate (see swindle).
The king's predecessor is the piece of the same name in shatranj. Like the modern king, it is the most important piece in the game and can move to any neighboring square. However, in shatranj, baring the king is a win unless the opponent can do the same immediately afterward; stalemating the king is a win; and castling does not exist.
In the opening and middlegame, the king will rarely play an active role in the development of an offensive or defensive position. Instead, a player will normally try to castle and seek safety on the edge of the board behind friendly pawns. In the endgame, however, the king emerges to play an active role as an offensive piece, and can assist in the promotion of the player's remaining pawns.
It is not meaningful to assign a value to the king relative to the other pieces, as it cannot be captured or exchanged and must be protected at all costs. In this sense, its value could be considered infinite. As an assessment of the king's capability as an offensive piece in the endgame, it is often considered to be slightly stronger than a bishop or knight. Emanuel Lasker gave it the value of a knight plus a pawn (i.e. four points on the scale of chess piece relative value),[1] though some other theorists evaluate it closer to three points. It is better at defending friendly pawns than the knight is, and it is better at attacking enemy pawns than the bishop is.[2]
The queen can be moved any number of unoccupied squares in a straight line vertically, horizontally, or diagonally, thus combining the moves of the rook and bishop. The queen captures by moving to the square on which an enemy piece stands.
Although both players start with one queen each, a pawn can be promoted to any of several types of pieces, including a queen, when the pawn is moved to the player's furthest rank (the opponent's first rank). Such a queen created by promotion can be an additional queen or, if the player's queen has been captured, a replacement queen. The queen is by far the most common piece type a pawn is promoted to due to the relative power of a queen; promotion to a queen is colloquially called queening.
The queen is typically worth about nine pawns, which is slightly stronger than a rook and a bishop together, but slightly less strong than two rooks, though there are exceptions. It is almost always disadvantageous to exchange the queen for a single piece other than the enemy's queen.
The reason that the queen is stronger than a combination of a rook and bishop, even though they control the same number of squares, is twofold. First, the queen is more mobile than the rook and the bishop, as the entire power of the queen can be transferred to another location in one move, while transferring the entire firepower of a rook and bishop requires two moves, the bishop always being restricted to squares of one color. Second, unlike the bishop, the queen is not hampered by an inability to control squares of the opposite color to the square on which it stands. A factor in favor of the rook and bishop is that they can attack (or defend) a square twice, while a queen can only do so once. However, experience has shown that this factor is usually less significant than the points favoring the queen.[1]
The queen is strongest when the board is open, the enemy king is poorly defended, or there are loose (i.e. undefended) pieces in the enemy camp. Because of its long range and ability to move in multiple directions, the queen is well-equipped to execute forks. Compared to other long range pieces (i.e. rooks and bishops), the queen is less restricted and stronger in closed positions.
A player should generally delay developing the queen, as developing it too quickly can expose it to attacks by enemy pieces, causing the player to lose time removing the queen from danger. Despite this, beginners often develop the queen early in the game, hoping to plunder the enemy position and deliver an early checkmate, such as Scholar's mate.
Early queen attacks are rare in high-level chess, but there are some openings with early queen development that are used by high-level players. For example, the Scandinavian Defense (1.e4 d5), which often features queen moves by Black on the second and third moves, is considered sound and has been played at the world championship level. Some less common examples have also been observed in high-level games. The Danvers Opening (1.e4 e5 2.Qh5), which is widely characterized as a beginner's opening, has occasionally been played by the American grandmaster Hikaru Nakamura.[2]
A queen exchange often marks the beginning of the endgame, but there are queen endgames, and sometimes queens are exchanged in the opening, long before the endgame. A common goal in the endgame is to promote a pawn to a queen. As the queen has the largest range and mobility, queen and king vs. lone king is an easy win when compared to some other basic mates. Queen and king vs. rook and king is also a win for the player with the queen, but it is not easy.
The fers changed into the queen over time. The first surviving mention of this piece as a queen or similar is the Latin regina in the Einsiedeln Poem, a 98-line Medieval Latin poem written around 997 and preserved in a monastery at Einsiedeln in Switzerland. Some surviving early medieval pieces depict the piece as a queen. The word fers became grammatically feminized in several languages, such as alferza in Spanish and fierce or fierge in French.[3] The Carmina Burana also refer to the queen as femina (woman) and coniunx (spouse),[4] and the name Amazon has sometimes been seen.[5]
In Russian, the piece keeps its Persian name of ferz; koroleva (queen) is colloquial and is never used by professional chess players. However, the names korolevna (king's daughter), tsaritsa (tsar's wife), and baba (old woman) are attested as early as 1694.[7] In Arabic countries, the queen remains termed and, in some cases, depicted as a vizier.[8]
Historian Marilyn Yalom proposes several factors that might have been partly responsible for influencing the piece towards its identity as a queen and its power in modern chess: the prominence of medieval queens such as Eleanor of Aquitaine, Blanche of Castile, and more particularly Isabella I of Castile; the cult of the Virgin Mary;[4] the power ascribed to women in the troubadour tradition of courtly love; and the medieval popularity of chess as a game particularly suitable for women to play on equal terms with men.[9] She points to medieval poetry depicting the Virgin as the chess-queen of God or Fierce Dieu.[10]
At various times, the ability of pawns to be queened was restricted while the original queen was still on the board, so as not to cause scandal by providing the king with more than one queen. An early 12th-century Latin poem refers to a queened pawn as a ferzia, as opposed to the original queen or regina, to account for this.[17]
In Russia, for a long time, the queen could also move like a knight; some players disapproved of this ability to "gallop like the horse" (knight).[19][20] The book A History of Chess by H.J.R. Murray,[21] says that William Coxe, who was in Russia in 1772, saw chess played with the queen also moving like a knight. Such an augmented queen piece is now known as the fairy chess piece amazon.
Around 1230, the queen was also independently invented as a piece in Japan, where it formed part of the game of dai shogi. The piece was retained in the smaller and more popular chu shogi, but does not form a part of modern shogi.
The white rooks start on the squares a1 and h1, while the black rooks start on a8 and h8. The rook moves horizontally or vertically, through any number of unoccupied squares. The rook cannot jump over pieces. The rook may capture an enemy piece by moving to the square on which the enemy piece stands, removing it from play. The rook also participates with the king in a special move called castling, wherein it is transferred to the square crossed by the king after the king is shifted two squares toward the rook.
The rook is worth about five pawns. In general, rooks are stronger than bishops or knights  and are considered greater in value than either of those pieces by nearly two pawns, but less valuable than two minor pieces by approximately a pawn. Two rooks are generally considered to be worth slightly more than a queen (see chess piece relative value).[4] Winning a rook for a bishop or knight is referred to as winning the exchange. Rooks and queens are called major pieces or heavy pieces, as opposed to bishops and knights, the minor pieces.[5]
In the opening, the rooks are blocked in by other pieces and cannot immediately participate in the game, so it is usually desirable to connect one's rooks on the first rank by castling and then clearing all pieces except the king and rooks from the first rank. In that position, the rooks support each other and can more easily move to occupy and control the most favorable files.
A rook on the seventh rank (the opponent's second rank) is typically very powerful, as it threatens the opponent's unadvanced pawns and hems in the enemy king. A rook on the seventh rank is often considered sufficient compensation for a pawn.[6] In the diagrammed position from a game between Lev Polugaevsky and Larry Evans,[7] the rook on the seventh rank enables White to draw, despite being a pawn down.[8]
Two rooks on the seventh rank are often enough to force victory by the blind swine mate, or at least a draw by perpetual check.[9]
Rooks are most powerful towards the end of a game (i.e., the endgame), when they can move unobstructed by pawns and control large numbers of squares. They are somewhat clumsy at restraining enemy pawns from advancing towards promotion unless they can occupy the file behind the advancing pawn. As well, a rook best supports a friendly pawn towards promotion from behind it on the same file (see Tarrasch rule).
In a position with a rook and one or two minor pieces versus two rooks, generally in addition to pawns, and possibly other pieces, Lev Alburt advises that the player with the single rook should avoid exchanging the rook for one of his opponent's rooks.[10]
The rook is adept at delivering checkmate. Below are a few examples of rook checkmates that are easy to force. A single rook can force checkmate while a single minor piece cannot.
Persian war-chariots were heavily armored, carrying a driver and at least one ranged-weapon bearer, such as an archer. The sides of the chariot were built to resemble fortified stone work, giving the impression of small, mobile buildings, causing terror on the battlefield.[citation needed]
In Europe the castle or tower appears for the first time in the 16th century in  Vida's 1550 Ludus Scacchia, and then as a tower on the back of an elephant. In time, the elephant disappeared and only the tower was used as the piece.[13]
Rooks usually are similar in appearance to small castles; thus, a rook is sometimes called a "castle",[15] though modern chess literature rarely, if ever, uses this term.[16]
Peter Tyson suggests that there is a correlation between the name of the piece and the word rukh, a mythical giant bird of prey from Persian mythology.[21]
In Canadian heraldry, the chess rook is the cadency mark of a fifth daughter.
The king's bishop is placed between the king and the king's knight, f1 for White and f8 for Black; the queen's bishop is placed between the queen and the queen's knight, c1 for White and c8 for Black.
The bishop has no restrictions in distance for each move but is limited to diagonal movement. It cannot jump over other pieces. A bishop captures by occupying the square on which an enemy piece stands. As a consequence of its diagonal movement, each bishop always remains on one square color. Due to this, it is common to refer to a bishop as a light-squared or dark-squared bishop. 
A rook is generally worth about two pawns more than a bishop. The bishop has access to only half of the squares on the board, whereas all squares of the board are accessible to the rook. On an empty board, a rook always attacks fourteen squares regardless of position, whereas a bishop attacks no more than thirteen (one of four center squares) and sometimes as few as seven (sides and corners). Also, a king and rook can force checkmate against a lone king, while a king and bishop cannot.[1] However, a king and two bishops on opposite-colored squares can force mate.
Knights and bishops are each worth about three pawns. This means bishops are approximately equal in strength to knights, but depending on the game situation, either may have a distinct advantage.
Less experienced players tend to underrate the bishop compared to the knight because the knight can reach all squares and is more adept at forking. More experienced players understand the power of the bishop.[2]
Bishops usually gain in relative strength towards the endgame as more pieces are captured and more open lines become available on which they can operate. A bishop can easily influence both wings simultaneously, whereas a knight is less capable of doing so. In an open endgame, a pair of bishops is decidedly superior to either a bishop and a knight, or two knights. A player possessing a pair of bishops has a strategic weapon in the form of a long-term threat to trade down to an advantageous endgame.[1]
Two bishops on opposite-colored squares and king can force checkmate against a lone king, whereas two knights cannot. A bishop and knight can force mate, but with far greater difficulty than two bishops.
In certain positions a bishop can by itself lose a move (see triangulation and tempo), while a knight can never do so. The bishop is capable of skewering or pinning a piece, while the knight can do neither. A bishop can in some situations hinder a knight from moving. In these situations, the bishop is said to be "dominating" the knight.
On the other hand, in the opening and middlegame a bishop may be hemmed in by pawns of both players, and thus be inferior to a knight which can jump over them. A knight check cannot be blocked but a bishop check can. Furthermore, on a crowded board a knight has many tactical opportunities to fork two enemy pieces. A bishop can fork, but opportunities are more rare. One such example occurs in the position illustrated, which arises from the Ruy Lopez: 1.e4 e5 2.Nf3 Nc6 3.Bb5 a6 4.Ba4 Nf6 5.0-0 b5 6.Bb3 Be7 7.d4 d6 8.c3 Bg4 9.h3!? Bxf3 10.Qxf3 exd4 11.Qg3 g6 12.Bh6!
In the middlegame, a player with only one bishop should generally place friendly pawns on squares of the color that the bishop cannot move to. This allows the player to control squares of both colors, allows the bishop to move freely among the pawns, and helps fix enemy pawns on squares on which they can be attacked by the bishop. Such a bishop is often referred to as a "good" bishop.
Conversely, a bishop which is impeded by friendly pawns is often referred to as a "bad bishop" (or sometimes, disparagingly, a "tall pawn"). The black light-squared bishop in the French Defense is a notorious example of this concept. A "bad" bishop, however, need not always be a weakness, especially if it is outside its own pawn chains. In addition, having a "bad" bishop may be advantageous in an opposite-colored bishops endgame. Even if the bad bishop is passively placed, it may serve a useful defensive function; a well-known quip from GM Mihai Suba is that "Bad bishops protect good pawns."[3]
An endgame in which each player has only one bishop, one controlling the dark squares and the other the light, will often result in a draw even if one player has a pawn or sometimes two more than the other. The players tend to gain control of squares of opposite colors, and a deadlock results. In endgames with same-colored bishops, however, even a positional advantage may be enough to win.[6]
Endgames in which each player has only one bishop (and no other pieces besides the king) and the bishops are on opposite colors are often drawn, even when one side has an extra pawn or two. Many of these positions would be a win if the bishops were on the same color.
If two pawns are connected, they normally win if they reach their sixth rank, otherwise the game may be a draw (as above). If two pawns are separated by one file they usually draw, but win if they are farther apart.[8]
In an endgame with a bishop, in some cases the bishop is the "wrong bishop", meaning that it is on the wrong color of square for some purpose (usually promoting a pawn). For example, with just a bishop and a rook pawn, if the bishop cannot control the promotion square of the pawn, it is said to be the "wrong bishop" or the pawn is said to be the wrong rook pawn. This results in some positions being drawn (by setting up a fortress) which otherwise would be won.
The canonical chessmen date back to the Staunton chess set of 1849. The piece's deep groove symbolizes a bishop's (or abbot's) mitre. Some have written that the groove originated from the original form of the piece, an elephant[22][23] with the groove representing the elephant's tusks.[24] The English apparently chose to call the piece a bishop because the projections at the top resembled a mitre.[25] This groove was interpreted differently in different countries as the game moved to Europe; in France, for example, the groove was taken to be a jester's cap, hence in France the bishop is called fou, the "jester"[26] and in Romania the nebun (madman).[27]
In Mongolian and several Indian languages it is called the "camel".
In Lithuanian it is the rikis, a kind of military commander in medieval Lithuania.
In Latvia it is known as laidnis, a term for the wooden handle part of some firearms.[28]
Compared to other chess pieces, the knight's movement is unique: it moves two squares vertically and one square horizontally, or two squares horizontally and one square vertically (with both forming the shape of an "L"). When moving, the knight can jump over pieces to reach its destination.[a][b][3] Knights capture in the same way, replacing the enemy piece on the square and removing it from the board. A knight can have up to eight available moves at once. Knights and pawns are the only pieces that can be moved in the chess starting position.[3]
Knights and bishops, also known as minor pieces, have a value of about three pawns.[4] Bishops utilize a longer range, but they can move only to squares of one color. The knight's value increases in closed positions since it can jump over blockades.[5] Knights and bishops are stronger when supported by other pieces (such as pawns) to create outposts and become more powerful when they advance, as long as they remain active.[3] Generally, knights are strongest in the center of the board, where they have up to eight moves, and weakest in a corner, where they have only two.
Compared to a bishop, a knight is often not as good in an endgame. A knight can exert control over only one part of the board at a time and often takes multiple moves to reposition to a new location, which often makes it less suitable in endgames with pawns on both sides of the board. This limitation is less important, however, in endgames with pawns on only one side of the board. Knights are superior to bishops in an endgame if all the pawns are on one side of the board. Furthermore, knights have the advantage of being able to control squares of either color, unlike a lone bishop. Nonetheless, a disadvantage of the knight (compared to the other pieces) is that by itself it cannot lose a move to put the opponent in zugzwang (see triangulation and tempo), while a bishop can. In the position pictured on the right, if the knight is on a white square and it is White's turn to move, White cannot win. Similarly, if the knight were on a black square and it were Black's turn to move, White cannot win. In the other two cases, White would win. If instead of the knight, White had a bishop on either color of square, White would win with either side to move.[7]
In an endgame where one side has only a king and a knight while the other side has only a king, the game is a draw since a checkmate is impossible. When a bare king faces a king and two knights, a checkmate can never be forced; checkmate can occur only if the opponent commits a blunder by moving their king to a square where it can be checkmated on the next move. Checkmate can be forced with a bishop and knight, however, or with two bishops, even though the bishop and knight are in general about equal in value. Paradoxically, checkmate with two knights sometimes can be forced if the weaker side has a single extra pawn, but this is a curiosity of little practical value (see two knights endgame). Pawnless endgames are a rarity, and if the stronger side has even a single pawn, an extra knight should give them an easy win. A bishop can trap (although it cannot then capture) a knight on the rim (see diagram), especially in the endgame.
In algebraic notation, the usual modern way of recording chess games, the letter N stands for the knight (K is reserved for the king); in descriptive chess notation, Kt is sometimes used instead, mainly in older literature. In chess problems and endgame studies, the letter S, standing for Springer, the German name for the piece, is often used (and in some variants of fairy chess, N is used for the nightrider, a popular fairy chess piece).
Pieces similar to the knight are found in almost all games of the chess family. The ma of xiangqi and janggi is slightly more restricted; conceptually, the piece is considered to pass through the adjacent orthogonal point, which must be unoccupied, rather than "jumping". Another related piece is the keima of shogi, which moves like a knight but can move only two squares forward followed by one square sideways, restricting its movement to two possible squares.
The knight is relevant in some mathematical problems. For example, the knight's tour problem is the problem of finding a series of moves by a knight on a chessboard in which every square is visited exactly once.
Even among sets of the standard Staunton pattern, the style of the pieces varies. The knights vary considerably. Here are some examples.
Individual pawns are referred to by the file on which they stand. For example, one speaks of "White's f-pawn" or "Black's b-pawn". Alternatively, they can be referred to by the piece which stood on that file at the beginning of the game, e.g. "White's king bishop's pawn" or "Black's queen knight's pawn". It is also common to refer to a rook's pawn, meaning any pawn on the a- or h-files, a knight's pawn (on the b- or g-files), a bishop's pawn (on the c- or f-files), a queen's pawn (on the d-file), a king's pawn (on the e-file), and a central pawn (on the d- or e-files).
The pawn historically represents soldiers or infantry, or more particularly, armed peasants or pikemen.[1]
Each player begins the game with eight pawns placed along their second rank.
A pawn may move by vertically advancing to a vacant square ahead. The first time a pawn moves, it has the additional option of vertically advancing two squares, provided that both squares are vacant. Unlike other pieces, the pawn can only move forwards. In the second diagram, the pawn on c4 can move to c5; the pawn on e2 can move to either e3 or e4.
Unlike other pieces, the pawn does not capture in the same way that it moves. A pawn captures by moving diagonally forward one square to the left or right (see diagram), either replacing an enemy piece on its square or capturing en passant.
An en passant capture can occur after a pawn makes a move of two squares and the square it passes over is attacked by an enemy pawn. The enemy pawn is entitled to capture the moved pawn "in passing" as if the latter had advanced only one square. The capturing pawn moves to the square over which the moved pawn passed (see diagram), and the moved pawn is removed from the board. The option to capture the moved pawn en passant must be exercised on the move immediately following the double-step pawn advance, or it is lost for the remainder of the game. The en passant capture is the only capture in chess in which the capturing piece does not replace the captured piece on the same square.[2]
A pawn that advances to its last rank is promoted to a queen, rook, bishop, or knight of the same color. The pawn is replaced by the new piece on the same move. The choice of promotion is not limited to pieces that have been captured; thus, a player could, in theory, have as many as nine queens, ten rooks, ten bishops, or ten knights on the board. Promotion to a queen is also known as queening and to any other piece as underpromotion. Underpromotion is most often to a knight, typically to execute a checkmate or a fork to gain a significant material advantage, among other reasons. Underpromotion to rook or bishop is used to avoid or induce stalemate or for humorous reasons.
While some chess sets include an extra queen of each color, most standard sets do not come with additional pieces, so the physical piece used to replace a promoted pawn on the board is usually one that was previously captured. In informal games, when the correct piece is not available, an additional queen is often indicated by inverting a previously captured rook or by placing two pawns on the same square. In tournament games, however, this is not acceptable; in the former case, it may result in the arbiter ruling that the upturned piece is in fact a rook.[3]
The pawn structure, the configuration of pawns on the chessboard, mostly determines the strategic flavor of a game. While other pieces can usually be moved to more favorable positions if they are temporarily badly placed, a poorly positioned pawn is limited in its movement and often cannot be so relocated.
Because pawns capture diagonally and can be blocked from moving straight forward, opposing pawns can become locked in diagonal pawn chains of two or more pawns of each color, where each player controls squares of one color. In the diagram, Black and White have locked their d- and e-pawns.
Here, White has a long-term space advantage. White will have an easier time than Black in finding good squares for their pieces, particularly with an eye to the kingside. Black, in contrast, suffers from a bad bishop on c8, which is prevented by the black pawns from finding a good square or helping out on the kingside. On the other hand, White's central pawns are somewhat overextended and vulnerable to attack. Black can undermine the white pawn chain with an immediate ...c5 and perhaps a later ...f6.
Pawns on adjacent files can support each other in attack and defense. A pawn which has no friendly pawns in adjacent files is an isolated pawn. The square in front of an isolated pawn may become an enduring weakness. Any piece placed directly in front not only blocks the advance of that pawn but also cannot be driven away by other pawns.
In the diagram, Black has an isolated pawn on d5. If all the pieces except the kings and pawns were removed, the weakness of that pawn might prove fatal to Black in the endgame. In the middlegame, however, Black has slightly more freedom of movement than White and may be able to trade off the isolated pawn before an endgame ensues.
After a capture with a pawn, a player may end up with two pawns on the same file, called doubled pawns. Doubled pawns are substantially weaker than pawns which are side by side, because they cannot defend each other, they usually cannot both be defended by adjacent pawns, and the front pawn blocks the advance of the back one. In the diagram, Black is playing at a strategic disadvantage due to the doubled c-pawns.
There are situations where doubled pawns confer some advantage, typically when the guarding of consecutive squares in a file by the pawns prevents an invasion by the opponent's pieces.
Pawns which are both doubled and isolated are typically a tangible weakness. A single piece or pawn in front of doubled isolated pawns blocks both of them, and cannot be easily dislodged. It is rare for a player to have three pawns in a file, i.e. tripled pawns.
In chess endgames with a bishop, a rook pawn may be the wrong rook pawn, depending on the square-color of the bishop. This causes some positions to be draws that would otherwise be wins.
The pawn has its origins in the oldest version of chess, chaturanga, and it is present in all other significant versions of the game as well. In chaturanga, this piece could move one square directly forward and could capture one square diagonally forward.
In medieval chess, as an attempt to make the pieces more interesting, each pawn was given the name of a commoner's occupation:[5]
The most famous example of this is found in the second book ever printed in the English language, The Game and Playe of the Chesse. Purportedly, this book, printed by William Caxton,[8] was viewed to be as much a political commentary on society as a chess book.[7]
The ability to move two spaces and the related ability to capture en passant were introduced in 15th-century Europe;[9] the en passant capture spread to various regions throughout its history. The en passant capture intends to prevent a pawn on its initial square from safely bypassing a square controlled by an enemy pawn. The rule for promotion has changed throughout its history.
Outside of the game of chess, "pawn" is often taken to mean "one who is manipulated to serve another's purpose".[11][12] Because the pawn is the weakest piece, it is often used metaphorically to indicate unimportance or outright disposability, only having utility in the ability to be controlled; for example, "She's only a pawn in their game."
In chess, the player who moves first is referred to as "White" and the player who moves second is referred to as "Black". Similarly, the pieces that each conducts are called, respectively, "the white pieces" and "the black pieces". The pieces are often not literally white and black, but some other colors (usually a light color and a dark color, respectively). The 64 squares of the chessboard, which is colored in a checkered pattern, are likewise referred to as "white squares" or "light squares", and "black squares" or "dark squares", though usually the squares are of contrasting light and dark color rather than literally white and black. For example, the squares on vinyl boards may be off-white ("buff") and green, while those on wood boards are often light brown and dark brown.[1]
white: 1. There are 16 light-colored pieces and 32 squares called white. 2. When capitalized, the word refers to the player of the white pieces.
An entry in the Glossary of terms in the Laws of Chess at the end of the current FIDE laws[2] appears for black, too.
In old chess writings, the sides are often called Red and Black, because those were the two colors of ink then commonly available when hand-drawing or printing chess position diagrams.
As Howard Staunton observed, "In the earlier ages of chess, the board was simply divided into sixty-four squares, without any difference of colour".[3] The checkering of the squares was a European innovation, introduced in the thirteenth century.[4]
He preferred to have Black, as first player as well as second ... this was a common fad in his day, which persisted with a great number of players, as a study of the Chess Players' Chronicle and other magazines shows.
As late as the mid-to-late 19th century, the practice of White moving first had not yet become standard. George Walker in his popular treatise The Art of Chess-Play: A New Treatise on the Game of Chess (4th edition 1846), set forth the rules of London's St George's Chess Club in June, 1841.[10] "Law III" provided that the player who moved first had the choice of color; if the players played more games at the same sitting, the first move would alternate, but each player would continue to use the same colored pieces as he had in the first game.[11] Staunton observed in 1871 that "many players still cultivate the foolish habit of playing exclusively with one colour."[12]
Chess historian Robert John McCrary writes that the earliest rule he has found requiring that White move first is Rule 9 given on page 126 of the New York, 1880 tournament book, which specified, "In each round the players shall have the first move alternately; in the first game it shall be determined by lot. The one having the move, in every case, is to play with the white pieces." McCrary observes:[16]
Prior to that, it had gradually become conventional, over a number of years, to have White move first in published analysis, and by about 1862 to have White move first in all published games. But it was evident that players could in many cases choose Black when they had the first move, even if the published game-score indicated that White had moved first.
Three years after the example cited by McCrary, the "Revised International Chess Code" issued at the London 1883 tournament (one of the strongest in history)[17] provided that the player who won by lot the right to move first had the choice of color.[18]
In 1889, Wilhelm Steinitz, the first World Champion, wrote that "In all international and public Chess matches and tournaments ... it is the rule for the first player to have the white men".[19] Emanuel Lasker, the second World Champion, stated in Lasker's Manual of Chess (first published in 1927)[20] that "White makes the first move".[21]
There has been a debate among chess players since at least 1846 about whether playing first gives White a significant advantage. Statistical analysis shows that White scores between 52 and 56 percent at most levels of play, with White's margin increasing as the standard of play improves.[22]
Checkmate (often shortened to mate) is any game position in chess and other chess-like games in which a player's king is in check (threatened with capture) and there is no possible escape. Checkmating the opponent wins the game.
If a player is not in check but has no legal move, then it is stalemate, and the game immediately ends in a draw. A checkmating move is recorded in algebraic notation using the hash symbol "#", for example: 34.Qg3#. 
A checkmate may occur in as few as two moves on one side with all of the pieces still on the board (as in Fool's mate, in the opening phase of the game), in a middlegame position (as in the 1956 game called the Game of the Century between Donald Byrne and Bobby Fischer),[3] or after many moves with as few as three pieces in an endgame position.
In modern Persian, the word mate depicts a person who is frozen, open-mouthed, staring, confused and unresponsive.  The words "stupefied" or "stunned" bear close correlation.  So a possible alternative would be to interpret mate as "unable to respond".  A king being in mate (shah-mat) then means a king is unable to respond, which would correspond to there being no response that a player's king can make to the opponent's final move. This interpretation is much closer to the original intent of the game being not to kill a king but to leave him with no viable response other than surrender, which better matches the origin story detailed in the Shahnameh.
In modern parlance, the term checkmate is a metaphor for an irrefutable and strategic victory.[13]
Before about 1600, the game could also be won by capturing all of the opponent's pieces, leaving just a bare king. This style of play is now called annihilation or robado.[16] In Medieval times, players began to consider it nobler to win by checkmate, so annihilation became a half-win for a while, until it was abandoned.[15]
Two major pieces (queens or rooks) can easily force checkmate on the edge of the board using a technique known as the ladder checkmate.[17] The process is to put the two pieces on adjacent ranks or files and force the king to the side of the board by using one piece to check the king and the other to cut it off from going up the board.[18] In the illustration, White checkmates by forcing the Black king to the edge, one row at a time. The ladder checkmate can be used to checkmate with two rooks, two queens, or a rook and a queen.[18]
There are four fundamental checkmates when one side has only their king and the other side has only the minimum material needed to force checkmate, i.e. (1) one queen, (2) one rook, (3) two bishops on opposite-colored squares, or (4) a bishop and a knight. The king must help in accomplishing all of these checkmates.[20]  If the winning side has more material, checkmates are easier.
The checkmate with the queen is the most common, and easiest to achieve. It often occurs after a pawn has queened. A checkmate with the rook is also common, but a checkmate with two bishops or with a bishop and knight occurs infrequently. The two-bishop checkmate is fairly easy to accomplish, but the bishop and knight checkmate is difficult and requires precision.
The first two diagrams show representatives of the basic checkmate positions with a queen, which can occur on any edge of the board. Naturally, the exact position can vary from the diagram.
In the first of the checkmate positions, the queen is directly in front of the opposing king and the white king is protecting its queen. In the second checkmate position, the kings are in opposition and the queen mates on the rank (or file) of the king.
With the side with the queen to move, checkmate can be forced in at most ten moves from any starting position, with optimal play by both sides, but usually fewer moves are required.[23][24] In positions in which a pawn has just promoted to a queen, at most nine moves are required.[25]
In the position diagrammed, White checkmates easily by confining the black king to a rectangle and shrinking the rectangle to force the king to the edge of the board:
The winning side must be careful to not stalemate the opposing king, whereas the defender would like to get into such a position. There are two general types of stalemate positions that can occur, which the stronger side must avoid.[27]
The first diagram shows the basic checkmate position with a rook, which can occur on any edge of the board.  The black king can be on any square on the edge of the board, the white king is in opposition to it, and the rook can check from any square on the rank or file (assuming that it cannot be captured). The second diagram shows a slightly different position where the kings are not in opposition but the defending king must be in a corner.
In the third diagram position, White checkmates by confining the black king to a rectangle and shrinking the rectangle to force the king to the edge of the board:
Here are the two basic checkmate positions with two bishops (on opposite-colored squares), which can occur in any corner. (Two or more bishops of the same color, which could occur because of pawn underpromotion, cannot checkmate.)
The first is a checkmate in the corner. The second position is a checkmate on a side square next to the corner square (this position can theoretically occur anywhere along an edge, but can only be forced adjacent to a corner). With the side with the bishops to move, checkmate can be forced in at most nineteen moves,[31] except in some very rare positions (0.03% of the possible positions).[32]
It is not too difficult for two bishops to force checkmate, with the aid of their king. Two principles apply:
In the position from Seirawan, White wins by first forcing the black king to the side of the board, then to a corner, and then checkmates. It can be any side of the board and any corner. The process is:
One example of a stalemate is this position, where 1. Kb6 (marked with the x) would be stalemate.[34]
Of the basic checkmates, this is the most difficult one to force, because these two pieces cannot form a linear barrier to the enemy king from a distance. Also, the checkmate can be forced only in a corner that the bishop controls.[32][35]
Two basic checkmate positions are shown with a bishop and a knight, or the bishop and knight checkmate.[36] The first position is a checkmate by the bishop, with the black king in the corner. The bishop can be on other squares along the diagonal, the white king and knight have to be on squares that attack g8 and h7. The second position is a checkmate by the knight, with the black king on a side square next to the corner. The knight can be on other squares that check the black king. The white king must be on a square to protect the bishop and cover a square not covered by the knight.
With the side with the bishop and knight to move, checkmate can be forced in at most thirty-three moves from any starting position,[37] except those in which the defending king is initially forking the bishop and knight and it is not possible to defend both. However, the mating process requires accurate play, since a few errors could result in a draw either by the fifty-move rule or stalemate.
Opinions differ as to whether or not a player should learn this checkmate procedure.  James Howell omits the checkmate with two bishops in his book because it rarely occurs but includes the bishop and knight checkmate.  Howell says that he has had it three times (always on the defending side) and that it occurs more often than the checkmate with two bishops.[38]  On the other hand, Jeremy Silman includes the checkmate with two bishops but not the bishop plus knight checkmate because he has had it only once and his friend John Watson has never had it.[39] Silman says: "... mastering it would take a significant chunk of time.  Should the chess hopeful really spend many of his precious hours he's put aside for chess study learning an endgame he will achieve (at most) only once or twice in his lifetime?"
This position is an example of a stalemate, from the end of a 1966 endgame study by A. H. Branton. White has just moved 1. Na3+?  If Black moves 1... Kc1! then White must move his bishop to save it because if the bishop is captured, the position is a draw because of the insufficient material rule.  But after any bishop move, the position is a stalemate.[40]
A back-rank checkmate is a checkmate delivered by a rook or queen along a back rank (that is, the row on which the pieces [not pawns] stand at the start of the game) in which the mated king is unable to move up the board because the king is blocked by friendly pieces (usually pawns) on the second rank.[41] An example of a back-rank checkmate is shown in the diagram. It is also known as the corridor mate.
Scholar's Mate (also known as the four-move checkmate) is the checkmate achieved by the moves:
The moves might be played in a different order or in slight variation, but the basic idea is the same: the queen and bishop combine in a simple mating attack on f7 (or f2 if Black is performing the mate).[42] There are also other ways to checkmate in four moves.
Fool's Mate, also known as the "Two-Move Checkmate", is the quickest possible checkmate. A prime example consists of the moves:
resulting in the position shown.[43] (The pattern can have slight variations, for example White might play f4 instead of f3 or move the g-pawn first, and Black might play ...e6 instead of ...e5.)
A smothered mate is a checkmate delivered by a knight in which the mated king is unable to move because it is surrounded (or smothered) by its own pieces.[45]
The mate is usually seen in a corner of the board, since fewer pieces are needed to surround the king there. The most common form of smothered mate is seen in the adjacent diagram. The knight on f7 delivers mate to the king on h8 which is prevented from escaping the check by the rook on g8 and the pawns on g7 and h7. Similarly, White can be mated with the white king on h1 and the knight on f2. Analogous mates on a1 and a8 are rarer, because kingside castling is more common as it safely places the king closer to the corner compared to queenside castling.
In some rare positions it is possible to force checkmate with a king and knight versus a king and pawn.
In the diagram showing Stamma's mate (named for Philipp Stamma), White to move wins:[46]
Reaching the position in the first diagram, with Black to move.
A similar position with the knight on d2 is more than 500 years old, identified as "Partito n. 23" by Luca Pacioli, in his MS De ludo scachorum (Latin for "The game of chess"), dated 1498 and recently reprinted (Gli scacchi) by Aboca Museum Edizioni.
There are also positions in which a king and a knight can checkmate a king and a bishop, knight, or rook; or a king and a bishop can checkmate a king with a bishop on the other color of squares or with a knight, but the checkmate cannot be forced if there is no other material on the board (see the diagrams for some examples).[49] Nevertheless, it keeps these material combinations from being ruled a draw because of "insufficient mating material" or "impossibility of checkmate" under the FIDE rules of chess. The U.S. Chess Federation rules are different.  In a typical position with a minor piece versus a minor piece, a player would be able to claim a draw if they have a limited amount of time left.[50]
In the third diagram, one knight is guarding c1, leaving the other knight to try to checkmate.  After 1. Ndc3+ Ka1, White needs to get the knight on e2 to c2.  But if White plays 2. Nd4, Black is stalemated.[52]
Under some circumstances, two knights and a king can force checkmate against a king and pawn (or rarely more pawns). The winning plan, quite difficult to execute in practice, is to blockade the enemy pawn(s) with one of the knights, maneuver the enemy king into a stalemated position, then bring the other knight over to checkmate.[52] (See Two knights endgame.)
Three knights and a king can force checkmate against a lone king within twenty moves (assuming that the lone king cannot quickly win a knight).[53]  These situations are generally only seen in chess problems, since at least one of the knights must be a promoted piece, and there is rarely a reason to promote a pawn to a piece other than a queen (see Underpromotion).
In chess and similar games, check is a condition that occurs when a player's king is under threat of capture on the opponent's next turn. A king so threatened is said to be in check. A player must get out of check if possible by moving the king to an unattacked square, interposing a piece between the threatening piece and the king, or capturing the threatening piece. If the player cannot remove the check by any of these options, the game ends in checkmate and the player loses. Players cannot make any move that puts their own king in check.
Many variations of chess feature check, such as the traditional chess games shogi, xiangqi, and janggi.
A check is the result of a move that places the opposing king under an immediate threat of capture by one (or occasionally two) of the player's pieces. Making a move that checks is sometimes called "giving check". Even if a piece is pinned against the player's own king, it may still give check. For example, in the diagrammed position, White has just played Be4+, simultaneously giving check and blocking the check from Black's rook. Black must now address the check; the fact that the bishop cannot legally move is irrelevant. If the king is in check and the checked player has no legal move to get out of check, the king is checkmated and the player loses.
Under the standard rules of chess, a player may not make any move that places or leaves their king in check. A player may move the king, capture the threatening piece, or block the check with another piece.[1] A king cannot itself directly check the opposing king, since this would place the first king in check as well. A move of the king could expose the opposing king to a discovered check by another piece, however.
In fast chess, depending on the rules in effect, placing or leaving one's king in check may result in immediate loss of the game.
There may be up to three ways to get a king out of a single check:
In the position in the diagram, White can get out of check by any of three methods:
If a king is placed in double check, the king must get out of both checks on the following move. Since it is impossible to capture both checking pieces or block both lines of attack in a single move, a double check can be escaped only by moving the king.[3] The king itself, however, can capture one of the checking pieces or another enemy piece if it brings the king into safety. If none of these possibilities can remove the check, then the king is checkmated and the game is lost.
Sometimes a given check is part of a chess tactic such as a fork, a skewer, or a discovered attack on another piece. In some cases, a check can be used to defend against such tactics.
There are also a few more special types of check:
The idea of warning that the king was under attack (announcing "check" in modern terminology) is present in the earliest descriptions of chess rules, in Persian/Arabian manuscripts.[6]  This was done to avoid the early and accidental end of a game.  Later the Persians added the additional rule that a king could not be moved into check or left in check.  As a result, the king could not be captured (Davidson 1949:22).
Less commonly (and obsolete), the warning garde can be said when a player directly attacks the opponent's queen in a similar way. This was mostly abandoned in the 19th century (Hooper & Whyld 1992:74). A move can be both check and garde simultaneously.  Before the queen acquired its current move (about 1495) the rook was the most powerful piece. At that time the term check-rook was used for a move that checked the king and attacked a rook at the same time (Hooper & Whyld 1992:75).
Until the early 20th century a player was expected to announce "check" when making a checking move, and some sources of rules even allowed a player to ignore an unannounced check (Hooper & Whyld 1992:74).
In informal games, most players still announce "check"; however, this is no longer required under the rules of chess and is not encouraged in formal games (Just & Burg 2003:28). In the FIDE rules for rapid chess, if a player leaves or places their king in check or commits any other illegal move, their opponent can claim a win.[8]
In algebraic chess notation, a "+" is normally written after a checking move. A minority of publications, most notably ECO, omit any mention of check.
Sometimes checking an opponent provides no benefit to the checking player. This is called a "useless check" and it may even provide the checked opponent with a tempo (move opportunity) to move the king into a safer position (Hooper & Whyld 1992:437). For example, 1.e4 e6 2.d4 Bb4+? does nothing for Black and in fact causes him to lose a tempo after 3.c3! A check given with the sole intention of delaying an inevitable defeat by one move is referred to as a "spite check", and may be considered somewhat unsporting (Eade 2005:65).
There are many instances, however, when checking the opponent's king may be a useful tactic or part of a tactic, either in attacking or in defense. Checking is often used in combinations with many other tactics or simply to force an opponent into a position where their king can be checkmated, otherwise taken advantage of, or is otherwise worse for the opponent. Some attacks involve numerous checks to force an opponent into a losing position, especially when the king is exposed. 
In chess, there are a number of ways that a game can end in a draw, neither player winning. Draws are codified by various rules of chess including stalemate (when the player to move is not in check but has no legal move), threefold repetition (when the same position occurs three times with the same player to move), and the fifty-move rule (when the last fifty successive moves made by both players contain no capture or pawn move). Under the standard FIDE rules, a draw also occurs in a dead position (when no sequence of legal moves can lead to checkmate), most commonly when neither player has sufficient material to checkmate the opponent.
Unless specific tournament rules forbid it, players may agree to a draw at any time. Ethical considerations may make a draw uncustomary in situations where at least one player has a reasonable chance of winning. For example, a draw could be called after a move or two, but this would likely be thought unsporting.
In the 19th century, some tournaments, notably London 1883, required that drawn games be replayed; however, this was found to cause organizational problems due to the backlog. It is now standard practice to score a decisive game as one point to the winner, and a draw as a half point to each player.
The rules allow for several types of draws: stalemate, threefold or fivefold repetition of a position, if there has been no capture or a pawn being moved in the last fifty or seventy five moves, if checkmate is impossible, or if the players agree to a draw. In games played under time control, a draw may result under additional conditions.[1] A stalemate is an automatic draw, as is a draw due to impossibility of checkmate. A draw by threefold repetition or the fifty-move rule may be claimed by one of the players with the arbiter (normally using his score sheet), and claiming it is optional. The draw by fivefold repetition or the seventy-five-move rule is mandatory by the arbiter.
A claim of a draw first counts as an offer of a draw, and the opponent may accept the draw without the arbiter examining the claim. Once a claim or draw offer has been made, it cannot be withdrawn. If the claim is verified or the draw offer accepted, the game is over. Otherwise, the offer or claim is nullified and the game continues; the draw offer is no longer in effect.
The correct procedure for an offer of a draw is to first make a move, verbally offer the draw, then press the clock. The other player may decline the draw offer by making a move, in which case the draw offer is no longer in effect, or else indicate acceptance. The offer of a draw should be recorded by each player in their score sheet using the symbol (=) as per Appendix C.12 of FIDE Laws of Chess.
In early tournaments, draws were often replayed until one of the players won; however, this was found to be impractical and caused organizational difficulties. The 1867 Paris tournament even ignored draws altogether, effectively treating them as double losses. The 1867 Dundee tournament initiated the awarding of a half point for draws,[2] which is now standard practice. A minority of tournaments use a different scoring scheme, such as "football scoring" where 3 points are awarded to the winner and 1 point to each in the event of a draw. For the purpose of calculating Elo rating, these tournaments are treated as if they were using standard scoring.
Article 5 of the 2018 FIDE Laws of Chess gives the basic ways a game may end in a draw; more complicated ways are detailed in Article 9:[3]
Although these are the laws as laid down by FIDE and, as such, are used at almost all top-level tournaments, at lower levels different rules may operate, particularly with regard to rapid play finish provisions.
In games played with a time control, there are other ways a draw can occur.[12][13]
In chess games played at the top level, a draw is the most common outcome of a game: of around 22,000 games published in The Week in Chess played between 1999 and 2002 by players with a FIDE Elo rating of 2500 or above, 55 percent were draws. According to chess analyst Jeff Sonas, although an upward draw rate trend can be observed in general master-level play since the beginning of the 20th century, it is currently "holding pretty steady around 50%, and is only increasing at a very slow rate".[14] Draw rate of elite grandmasters, rated more than 2750 Elo, is, however, significantly higher, surpassing 70% in 2017 and 2018.[15]
In top-level correspondence chess under ICCF, where computer assistance is allowed, the draw rate is much higher than in the over-the-board chess: of 1512 games played in the World Championship finals and the Candidates' sections between 2010 and 2013, 82.3% ended in a draw.[16] Since that time, draw rate in top-level correspondence play has been rising steadily, reaching 97% in 2019.[17]
Yuri Averbakh gives these combinations for the weaker side to draw:
FIDE's most visible activity is organizing the World Chess Championship since 1948. FIDE also organizes world championships for women, juniors, seniors, and the disabled.[7] Another flagship event is the Chess Olympiad, a biennial chess tournament organized since 1924, in which national teams compete. In alternate years, FIDE also organizes the World Team Championship, in which the best teams from the previous Olympiad compete.
As part of the World Chess Championship cycle, FIDE also organizes the Candidates Tournament, which determines who will challenge the reigning World Champion, and the qualifying tournaments for the Candidates, such as the Chess World Cup, the FIDE Grand Prix, and the FIDE Grand Swiss Tournament 2019.
FIDE is recognized by the International Olympic Committee (IOC) as the supreme body responsible for the organization of chess and its championships at global and continental levels.[8] Other tournaments are not overseen directly by FIDE, but they generally observe FIDE rules and regulations. Some national chess organizations such as the US Chess Federation use minor differences to FIDE rules.
FIDE defines the rules of chess, both for individual games (i.e. the board and moves) and for the conduct of international competitions. The international competition rules are the basis for local competitions, although local bodies are allowed to modify these rules to a certain extent. FIDE awards a number of organizational titles, including International Arbiter, which signifies that the recipient is competent and trusted to oversee top-class competitions.[9]
FIDE calculates the Elo ratings of players[10] and awards titles for achievement in competitive play, such as the Grandmaster title. It also awards titles to composers and solvers of chess problems and studies.
FIDE funds and manages outreach programs, such as the Chess for Freedom program[11] and awards such as, since 2020, the Svetozar Gligoric Award for fair play.[12]
Correspondence chess (chess played by post, email or on online servers) is regulated by the International Correspondence Chess Federation, an independent body that cooperates with FIDE where appropriate.
In April 1914, an initiative was taken in St. Petersburg, Russia, to form an international chess federation. Another attempt was made in July 1914 during the Mannheim International Chess Tournament, but further efforts temporarily came to an end as a result of the outbreak of World War I. In 1920, another attempt to organize an international federation was made at the Gothenburg Tournament.[15]
In 1922, the Russian master Eugene Znosko-Borovsky, while participating in an international tournament in London, announced that a tournament would be held during the 8th Sports Olympic Games in Paris in 1924 and would be hosted by the French Chess Federation. On July 20, 1924 the participants at the Paris tournament founded FIDE as a kind of players' union.[15][18][19] In its early years, FIDE had little power, and was poorly financed.
FIDE's congresses in 1925 and 1926 expressed a desire to become involved in managing the world championship. FIDE was largely happy with the "London Rules", but claimed that the requirement for a purse of $10,000 was impracticable and called upon Capablanca to come to an agreement with the leading masters to revise the Rules.[20]
FIDE's third congress, in Budapest in 1926, also decided to organize a Chess Olympiad. The invitations were, however, late in being sent, with the result that only four countries participated, and the competition was called the Little Olympiad. The winner was Hungary, followed by Yugoslavia, Romania, and Germany. In 1927, FIDE began organizing the First Chess Olympiad during its 4th Congress in London. The official title of the tournament was the "Tournament of Nations", or "World Team Championship", but "Chess Olympiad" became a more popular title. The event was won by Hungary, with 16 teams competing.[15]
In 1928 FIDE recognized Bogoljubow as "Champion of FIDE" after he won a match against Max Euwe.[20] Alekhine, the reigning world champion, attended part of the 1928 Congress and agreed to place future matches for the world title under the auspices of FIDE, although any match with Capablanca should be under the same conditions as in Buenos Aires, 1927, i.e. including the requirement for a purse of at least $10,000. FIDE accepted this and decided to form a commission to modify the London Rules for future matches, though this commission never met; by the time of the 1929 Congress, a world championship match between Alekhine and Bogoljubow was under way, held neither under the auspices of FIDE nor in accordance with the London Rules.[20]
From the time of Emanuel Lasker's defeat of Wilhelm Steinitz in 1894, until 1946, a new World Champion had won the title by defeating the former champion in a match. Alexander Alekhine's death created an interregnum that made the normal procedure impossible. The situation was confused, with many respected players and commentators offering different solutions. FIDE found it difficult to organize the early discussions on how to resolve the interregnum, because problems with money and travel in the aftermath of World War II prevented many countries from sending representatives, most notably the Soviet Union. The shortage of clear information resulted in otherwise responsible magazines publishing rumors and speculation, which only made the situation more confused.[23] See Interregnum of World Chess Champions for more details.
This situation was exacerbated by the Soviet Union having long refused to join FIDE, and by this time it was clear that about half the credible contenders were Soviet citizens. The Soviet Union realized, however, it could not afford to be left out of the discussions regarding the vacant world championship, and in 1947 sent a telegram apologizing for the absence of Soviet representatives and requesting that the USSR be represented in future FIDE Committees.[23]
FIDE had a number of conflicts with the Soviet Chess Federation. These conflicts included:[26]
The events leading to Garry Kasparov's winning the world championship involved FIDE in two controversies. While arranging the Candidates Tournament semi-final matches to be played in 1983, FIDE accepted bids to host Kasparov versus Victor Korchnoi in Pasadena, California. The Soviet Union refused to accept this, either because it feared Kasparov would defect or because it thought Kasparov was the greater threat to reigning champion Anatoly Karpov. Their refusal would have meant that Kasparov forfeited his chance of challenging for the title. FIDE president Florencio Campomanes negotiated with the Soviet Union, and the match was played in London.[28][30]
In 1992 Nigel Short surprised the world by winning the Candidates Tournament and thus becoming the official challenger for Kasparov's world title. FIDE very quickly accepted a bid from Manchester (England) to host the title match in 1993. But at that time Short was travelling to Greece and could not be consulted as FIDE's rules required. On learning of the situation Short contacted Kasparov, who had distrusted FIDE and its president, Florencio Campomanes ever since Campomanes had stopped his title match against Karpov in 1984.  Kasparov and Short concluded that FIDE had failed to get them the best financial deal available and announced that they would "play under the auspices of a new body, the "Professional Chess Association" (PCA). FIDE stripped Kasparov of his FIDE title and dropped Kasparov and Short from the official rating list. It also announced a title match between Karpov and Jan Timman, whom Short had defeated in the semi-final and final stages of the Candidates Tournament.  Kasparov and Karpov won their matches and there were now two players claiming to be world champion.[35]
In 1994 Kasparov concluded that breaking away from FIDE had been a mistake, because both commercial sponsors and the majority of grandmasters disliked the split in the world championship.[36] Kasparov started trying to improve relations with FIDE and supported Campomanes' bid for re-election as president of FIDE. But many FIDE delegates regarded Campomanes as corrupt and in 1995 he agreed to resign provided his successor was Kirsan Ilyumzhinov, president of the Republic of Kalmykia.[37]
Finally in 2006 a re-unification match was played between Kramnik and Veselin Topalov, which Kramnik won after an unpleasant controversy which led to one game being awarded to Topalov.[37][39]
In 1999, FIDE was recognised by the International Olympic Committee (IOC). Two years later, it introduced the IOC's anti-drugs rules to chess, as part of its campaign for chess to become part of the Olympic Games.[40]
In 2012 FIDE entered into a commercial agreement, initially planned to last until 2021, with the company Agon Limited. This company was given rights to organize and commercially exploit the World Chess Championship and the associated events in the World Championship cycle.[41] The first tournament it organized was the London FIDE Grand Prix event in September 2012,[42] followed by the London Candidates Tournament in March 2013,[43] and the Chennai World Chess Championship in November 2013.[44]
Agon had been founded in 2012 in Jersey by Andrew Paulson as the sole shareholder.[48] On February 20, 2012, an agreement between Agon and FIDE was made, subject to approval by the 2012 FIDE General Assembly.[41] This approval was forthcoming in September 2012.[49] In October 2014, Agon was sold to its current CEO Ilya Merenzon for the sum of one pound.[42] At the September 2016 FIDE General Assembly, it was resolved that Agon should institute a corporate presence in a locale with more transparency. Merenzon said that they would register in the United Kingdom within a few months.[50] As a result, a new company, World Chess Limited, was registered shortly after, replacing Agon as the rights holder in the agreement with FIDE.
The condition that Agon would be the sole organizer of Championship events was disputed originally by principally the Bulgarian Chess Federation, with respect to the Candidates matches for 2012.[53] In early 2014, a purported agreement between Paulson and FIDE President Kirsan Ilyumzhinov was leaked, and then published by Chess.com (and others), which allegedly indicated that Paulson was simply a front man with Ilyumzhinov the ultimate benefactor of Agon.[54] In that Chess.com article Malcolm Pein is quoted as having twice been told by Paulson that Ilyuzmhinov owned Agon, and in a New In Chess article Nigel Short asserted he had also been told this personally by Paulson.[55] In response, FIDE's deputy vice president Georgios Makropoulos pointed out that the purported contract was a draft document.[56] The FIDE Ethics Commission ruled in September 2015 that Ilyumzhinov did not violate the FIDE Code of Ethics.[57][58]
In July 2018, Kirsan Ilyumzhinov was ousted as FIDE President, after having been in office for 23 years, since 1995. Being subjected to US sanctions for his business dealings with the Syrian government, Ilyumzhinov was forced out and did not run for re-election in the 2018 FIDE elections. The Greek Georgios Makropoulos, who had been General Secretary since 1990 and number two in the organization under Kirsan's Presidency, was the first to announce his ticket. He was followed by the Englishman Nigel Short, a world title contender in the World Chess Championship 1993 against Garry Kasparov. The last to announce his candidacy was Arkady Dvorkovich, an economist who had served as Russian deputy prime minister and was also a member of the Supervisory Board of the Russian Chess Federation. Dvorkovich was also one of the chief organizers of the 2018 FIFA World Cup. Dvorkovich was placed in the US Treasury pre-sanctions list in 2018 as a top Russian government employee.[59]
In the elections, held in Batumi (Georgia) in October 2018, Dvorkovich won by 103 votes to 78[60] against Makropoulos, after Nigel Short withdrew his candidacy at the last minute and expressed his support to the Russian candidate.
After the 2018 FIDE elections and the appointment of a new FIDE President, the new management took regaining control over the World Championship cycle as one of their top priorities. In January 2019, FIDE Director-General Emil Sutovsky announced that a new contract has been signed that continues a scaled-back relationship with World Chess (formerly known as AGON) through 2021. In virtue of this new agreement, FIDE reasserted control over the 2020 Candidates and the World Championship match, which from now on will undergo an open bidding procedure. Agon/World Chess only retained organizational and commercial rights over the FIDE Grand Prix Series, limited until 2021.
At FIDE's general assembly in Chennai, India, in August 2022 Dvorkovich got re-elected by 157 votes to 16 against Ukraine's Andrii Baryshpolets.[61]
On February 27, 2022, FIDE issued an official statement condemning the Russian invasion of Ukraine. As a consequence, Russia and Belarus were forbidden from hosting official FIDE events. The decision to hold the 2022 Chess Olympiad and the 2022 FIDE congress in Moscow was also revoked.[62] The Russian and Belarusian national teams were banned from participating in FIDE tournaments, although individual players could compete if they complied with strict regulations, in which case their federation and flag was replaced with FIDE and its banner.[63]
On March 22, 2022, FIDE decided to issue a six month ban from competing in rated tournaments against Russian grandmaster Sergey Karjakin. Karjakin had posted controversial statements on Twitter in which he declared his support for the invasion of Ukraine and for President Vladimir Putin's characterization of the war as a fight against Nazism. FIDE argued that Karjakin's statements had shed a negative light on chess and on the federation and found that he had violated the FIDE code of ethics.[64] Sergei Shipov, who also publicly commented in favor of Russia, was not sanctioned, because FIDE decided that his statements were less provocative.[62]
The World Chess Championship is played to determine the world champion in chess. The current world champion is Magnus Carlsen of Norway, who has held the title since 2013.[1]
The first event recognized as a world championship was the 1886 match between the two leading players in the world, Wilhelm Steinitz and Johannes Zukertort. Steinitz won, becoming the first world champion. From 1886 to 1946, the champion set the terms, requiring any challenger to raise a sizable stake and defeat the champion in a match in order to become the new world champion. Following the death of reigning world champion Alexander Alekhine in 1946, FIDE (the International Chess Federation) took over administration of the World Championship, beginning with the 1948 World Championship tournament. From 1948 to 1993, FIDE organized a set of tournaments to choose a new challenger every three years. In 1993, reigning champion Garry Kasparov broke away from FIDE, which led to a rival claimant to the title of World Champion for the next thirteen years. The titles were unified at the World Chess Championship 2006, and all subsequent matches have once again been administered by FIDE.
Since 2014, the championship has settled on a two-year cycle, although the 2020 match was postponed to 2021 because of the COVID-19 pandemic, and the next match will be held in 2023.[2] Magnus Carlsen has been world champion since he defeated Viswanathan Anand in 2013. He successfully defended the title in 2014, 2016, 2018, and 2021. In 2022, he announced that he would not defend his title a fifth time, and so the 2023 championship will be played between the top two finishers of the qualifying 2022 Candidates Tournament instead: Ian Nepomniachtchi of Russia and Ding Liren of China.
Though the world championship is open to all players, there are separate championships for women, under-20s and lower age groups, and seniors; as well as one for computers. There are also chess world championships in rapid, blitz, correspondence, problem solving, and Fischer Random Chess.
The idea of a chess world champion goes back at least to 1840, when a columnist in Fraser's Magazine wrote, "Will Gaul continue the dynasty by placing a fourth Frenchman on the throne of the world? the three last chess chiefs having been successively Philidor, Deschapelles, and De La Bourdonnais."[4][5]
After Morphy's retirement from chess, Anderssen was again regarded as the world's strongest active player,[16] a reputation he reinforced by winning the strong London 1862 chess tournament.[16]
However, apart from the Blackburne match, Steinitz played no competitive chess between the Vienna tournaments of 1873 and 1882. During that time, Zukertort emerged as the world's leading active player, winning the Paris 1878 chess tournament. Zukertort then won the London 1883 chess tournament by a convincing 3-point margin, ahead of nearly every leading player in the world, with Steinitz finishing second.[18][19] This tournament established Steinitz and Zukertort as the best two players in the world, and led to a match between these two, the World Chess Championship 1886,[19][20] won by Steinitz.
There is some debate over whether to date Steinitz's reign as world champion from his win over Anderssen in 1866, or from his win over Zukertort in 1886. The 1886 match was clearly agreed to be for the world championship,[21][14] but there is no indication that Steinitz was regarded as the defending champion.[22] There is also no known evidence of Steinitz being called the world champion after defeating Anderssen in 1866.[14] It has been suggested that Steinitz could not make such a claim while Morphy was alive[23] (Morphy died in 1884). There are a number of references to Steinitz as world champion in the 1870s, the earliest being after the first Zukertort match in 1872.[14] Later, in 1879, it was argued that Zukertort was world champion, since Morphy and Steinitz were not active.[14] However, later in his career, at least from 1887, Steinitz dated his reign from this 1866 match,[14] and early sources such as the New York Times in 1894,[24] and Emanuel Lasker in 1908,[14] and Reuben Fine in 1952[25] all do the same.
Many modern commentators divide Steinitz's reign into an "unofficial" one from 1866 to 1886, and an "official" one after 1886.[26][27][28] By this reckoning, the first World Championship match was in 1886, and Steinitz was the first official World Chess Champion.[29]
Steinitz successfully defended his world title against Mikhail Chigorin in 1889, Isidor Gunsberg in 1891, and Chigorin again in 1892.
Two young strong players emerged in late 1880s and early 1890s: Siegbert Tarrasch and Emanuel Lasker.[33] Tarrasch had the better tournament results at the time, but it was Lasker who was able to raise the money to challenge Steinitz.[33] Lasker won the 1894 match and succeeded Steinitz as world champion.
Lasker held the title from 1894 to 1921, the longest reign (27 years) of any champion. He won a return match against Steinitz in 1897, and then did not defend his title for ten years, before playing four title defences in four years. He comfortably defeated Frank Marshall in 1907 and Siegbert Tarrasch in 1908. In 1910, he almost lost his title in a short tied match against Carl Schlechter, although the exact conditions of this match are a mystery. He then defeated Dawid Janowski in the most one-sided title match in history later in 1910.
Further controversy arose when, in 1912, Lasker's terms for a proposed match with Akiba Rubinstein included a clause that, if Lasker should resign the title after a date had been set for the match, Rubinstein should become world champion.[35] When he resumed negotiations with Capablanca after World War I, Lasker insisted on a similar clause that if Lasker should resign the title after a date had been set for the match, Capablanca should become world champion.[34] On 27 June 1920 Lasker abdicated in favor of Capablanca because of public criticism of the terms of the match, naming Capablanca as his successor.[35] Some commentators questioned Lasker's right to name his successor;[35] Amos Burn raised the same objection but welcomed Lasker's resignation of the title.[35] Capablanca argued that, if the champion abdicated, the title must go to the challenger, as any other arrangement would be unfair to the challenger.[35] Lasker later agreed to play a match against Capablanca in 1921, announcing that, if he won, he would resign the title so that younger masters could compete for it.[35] Capablanca won their 1921 match by four wins, ten draws and no losses.[25]
The only match played under those rules was Capablanca vs Alekhine in 1927, although there has been speculation that the actual contract might have included a "two-game lead" clause.[37] Alekhine, Rubinstein and Nimzowitsch had all challenged Capablanca in the early 1920s but only Alekhine could raise the US$10,000 Capablanca demanded and only in 1927.[38] Capablanca was shockingly upset by the new challenger. Before the match, almost nobody gave Alekhine a chance against the dominant Cuban, but Alekhine overcame Capablanca's natural skill with his unmatched drive and extensive preparation (especially deep opening analysis, which became a hallmark of most future grandmasters). The aggressive Alekhine was helped by his tactical skill, which complicated the game.
Immediately after winning, Alekhine announced that he was willing to grant Capablanca a return match provided Capablanca met the requirements of the "London Rules".[37] Negotiations dragged on for several years, often breaking down when agreement seemed in sight.[25] Alekhine easily won two title matches against Efim Bogoljubov in 1929 and 1934.
In 1935, Alekhine was unexpectedly defeated by the Dutch Max Euwe, an amateur player who worked as a mathematics teacher. Alekhine convincingly won a rematch in 1937. World War II temporarily prevented any further world title matches, and Alekhine remained world champion until his death in 1946.
Before 1948 world championship matches were financed by arrangements similar to those Emanuel Lasker described for his 1894 match with Wilhelm Steinitz: either the challenger or both players, with the assistance of financial backers, would contribute to a purse; about half would be distributed to the winner's backers, and the winner would receive the larger share of the remainder (the loser's backers got nothing). The players had to meet their own travel, accommodation, food and other expenses out of their shares of the purse.[39] This system evolved out of the wagering of small stakes on club games in the early 19th century.[40]
Attempts to form an international chess federation were made at the time of the 1914 St. Petersburg, 1914 Mannheim and 1920 Gothenburg Tournaments.[45] On 20 July 1924 the participants at the Paris tournament founded FIDE as a kind of players' union.[45][46][47]
Before 1946 a new World Champion had won the title by defeating the former champion in a match. Alexander Alekhine's death in 1946 created an interregnum that made the normal procedure impossible. The situation was very confused, with many respected players and commentators offering different solutions. FIDE found it very difficult to organize the early discussions on how to resolve the interregnum because problems with money and travel so soon after the end of World War II prevented many countries from sending representatives. The shortage of clear information resulted in otherwise responsible magazines publishing rumors and speculation, which only made the situation more confusing.[50]
It did not help that the Soviet Union had long refused to join FIDE, and by this time it was clear that about half the credible contenders were Soviet citizens. But, realizing that it could not afford to be excluded from discussions about the vacant world championship, the Soviet Union sent a telegram in 1947 apologizing for the absence of Soviet representatives and requesting that the USSR be represented on future FIDE Committees.[50]
In 1956 FIDE introduced two apparently minor changes which Soviet grandmaster and chess official Yuri Averbakh alleged were instigated by the two Soviet representatives in FIDE, who were personal friends of reigning champion Mikhail Botvinnik. A defeated champion would have the right to a return match. FIDE also limited the number of players from the same country that could compete in the Candidates Tournament, on the grounds that it would reduce Soviet dominance of the tournament. Averbakh claimed that this was to Botvinnik's advantage as it reduced the number of Soviet players he might have to meet in the title match.[54] Botvinnik lost to Vasily Smyslov in 1957 but won the return match in 1958, and lost to Mikhail Tal in 1960 but won the return match in 1961. Thus Smyslov and Tal each held the world title for a year, but Botvinnik was world champion for rest of the time from 1948 to 1963.
The return match clause was not in place for the 1963 cycle. Tigran Petrosian won the 1962 Candidates and then defeated Botvinnik in 1963 to become world champion.
In 1993, Nigel Short broke the domination of Kasparov and Karpov by defeating Karpov in the candidates semi-finals followed by Jan Timman in the finals, thereby earning the right to challenge Kasparov for the title. However, before the match took place, both Kasparov and Short complained of corruption and a lack of professionalism within FIDE in organizing the match, and split from FIDE to set up the Professional Chess Association (PCA), under whose auspices they held their match. In response, FIDE stripped Kasparov of his title and held a championship match between Karpov and Timman. For the first time in history, there were two World Chess Champions: Kasparov defeated Short and Karpov beat Timman.
Meanwhile, FIDE had decided to scrap the Interzonal and Candidates system, instead having a large knockout event in which a large number of players contested short matches against each other over just a few weeks (see FIDE World Chess Championship 1998). Rapid and blitz games were used to resolve ties at the end of each round, a format which some felt did not necessarily recognize the highest quality play: Kasparov refused to participate in these events, as did Kramnik after he won the Classical title in 2000. In the first of these events, in 1998, champion Karpov was seeded directly into the final, but he later had to qualify alongside the other players. Karpov defended his title in the first of these championships in 1998, but resigned his title in protest at the new rules in 1999. Alexander Khalifman won the FIDE World Championship in 1999, Anand in 2000, Ruslan Ponomariov in 2002, and Rustam Kasimdzhanov in 2004.
The World Chess Championship 2006 reunification match between Topalov and Kramnik was held in late 2006. After much controversy, it was won by Kramnik. Kramnik thus became the first unified and undisputed World Chess Champion since Kasparov split from FIDE to form the PCA in 1993. This match, and all subsequent championships, have been administered by FIDE.
Kramnik played to defend his title at the World Chess Championship 2007 in Mexico. This was an 8-player double round robin tournament, the same format as was used for the FIDE World Chess Championship 2005. This tournament was won by Viswanathan Anand, thus making him the World Chess Champion. Because Anand's World Chess Champion title was won in a tournament rather than a match, a minority of commentators questioned the validity of his title.[66] Kramnik also made ambiguous comments about the value of Anand's title, but did not claim the title himself.[67] Subsequent world championship matches returned to the format of a match between the champion and a challenger.
The following two championships had special clauses arising from the 2006 unification. Kramnik was given the right to challenge for the title he lost in a tournament in the World Chess Championship 2008, which Anand won. Then Topalov, who as the loser of the 2006 match was excluded from the 2007 championship, was seeded directly into the Candidates final of the World Chess Championship 2010. He won the Candidates (against Gata Kamsky). Anand again won the championship match.[68][69]
The next championship, the World Chess Championship 2012, had short knock-out matches for the Candidates Tournament. This format was not popular with everyone, and world No. 1 Magnus Carlsen withdrew in protest. Boris Gelfand won the Candidates. Anand won the championship match again, in tie breaking rapid games, for his fourth consecutive world championship win.[70]
Since 2013, the Candidates Tournament has been an 8-player double round robin tournament, with the winner playing a match against the champion for the title. The Norwegian Magnus Carlsen won the 2013 Candidates and then convincingly defeated Anand in the World Chess Championship 2013.[71][72]
Beginning with the 2014 Championship cycle, the World Championship has followed a 2-year cycle: qualification for the Candidates in the odd year, the Candidates tournament early in the even year, and the World Championship match later in the even year. This and the next two cycles resulted in Carlsen successfully defending his title: against Anand in 2014;[73] against Sergey Karjakin in 2016;[74] and against Fabiano Caruana in 2018. Both the 2016 and 2018 defences were decided by tie-break in rapid games.[75]
The COVID-19 pandemic disrupted the 2020 Candidates Tournament, and caused the next match to be postponed from 2020 to 2021.[76] Carlsen again successfully defended his title, defeating Ian Nepomniachtchi in the World Chess Championship 2021.
Soon after the 2021 match, Carlsen indicated that he would not defend the title again.[77] This was confirmed in an announcement by FIDE on 20 July 2022.[78] As a consequence, the top two finishers of the Candidates Tournament, Ian Nepomniachtchi and Ding Liren, will play in the next championship in Astana, Kazakhstan, from 7 April to 1 May 2023.[79]
After the death of world champion Alexander Alekhine in 1946, the World Chess Championship 1948 was a one-off tournament to decide a new world champion.
Since 1948, the world championship has mainly operated on a two or three-year cycle, with four stages:
The table below organises the world champions in order of championship wins. A successful defense counts as a win for the purposes of this table, even if the match is drawn. The table is made more complicated by the split between the "Classical" (PCA) and FIDE world titles between 1993 and 2006.
Statistical rating systems give Steinitz a rather low ranking among world champions, mainly because he took several long breaks from competitive play. However, an analysis based on one of these rating systems shows that he was one of the most dominant players in the history of the game. Steinitz was unbeaten in match play for 32 years, from 1862 to 1894.
Although Steinitz became "world number one" by winning in the all-out attacking style that was common in the 1860s, he unveiled in 1873 a new positional style of play, and demonstrated that it was superior to the previous style. His new style was controversial and some even branded it as "cowardly", but many of Steinitz's games showed that it could also set up attacks as ferocious as those of the old school.
Steinitz was also a prolific writer on chess, and defended his new ideas vigorously. The debate was so bitter and sometimes abusive that it became known as the "Ink War". By the early 1890s, Steinitz's approach was widely accepted, and the next generation of top players acknowledged their debt to him, most notably his successor as world champion, Emanuel Lasker.
Traditional accounts of Steinitz's character depict him as ill-tempered and aggressive, but more recent research shows that he had long and friendly relationships with some players and chess organizations. Most notably from 1888 to 1889 he co-operated with the American Chess Congress in a project to define rules governing the conduct of future world championships. Steinitz was unskilled at managing money, and lived in poverty all his life.
Steinitz was born on May 14, 1836, in the Jewish ghetto of Prague (now capital of the Czech Republic; then in Bohemia, a part of the Austrian Empire). The youngest of a tailor's thirteen sons to survive, he learned to play chess at age 12.[1] He began playing serious chess in his twenties, after leaving Prague in 1857 to study mathematics in Vienna,[1] at the Vienna Polytechnic.[2] Steinitz spent two years at the university.[3]
Steinitz improved rapidly in chess during the late 1850s, progressing from third place in the 1859 Vienna City championship to first in 1861, with a score of 30/31.[4] During this period he was nicknamed "the Austrian Morphy".[5] This achievement meant that he had become the strongest player in Austria.[6]
In the years following his victory over Anderssen, Steinitz beat Henry Bird in 1866 (seven wins, five losses, five draws). He also comfortably beat Johannes Zukertort in 1872 (seven wins, four draws, one loss; Zukertort had proved himself one of the elite by beating Anderssen by a large margin in 1871).[9]
It took longer for Steinitz to reach the top in tournament play. In the next few years he took: third place at Paris 1867 behind Ignatz Kolisch and Simon Winawer; and second place at Dundee (1867; Gustav Neumann won),  and Baden-Baden 1870 chess tournament; behind Anderssen but ahead of Blackburne, Louis Paulsen and other strong players.[14] His first victory in a strong tournament was London 1872, ahead of Blackburne and Zukertort;[15] and the first tournament in which Steinitz finished ahead of Anderssen was the Vienna 1873 chess tournament, when Anderssen was 55 years old.[citation needed]
Steinitz's long lay-off caused some commentators to suggest that Zukertort, who had scored some notable tournament victories, should be regarded as the world chess champion.[12] As an example, The Chess Player's Chronicle in July 1883 opined that 'Steinitz was, at one time, fairly entitled to the position of champion...He has just taken an inferior place to Zukertort, in a tournament, and for the time being Zukertort, in the opinion of some, becomes champion'.[12]
Steinitz returned to serious competitive chess in the Vienna 1882 chess tournament, which has been described as the strongest chess tournament of all time at that point. Despite a shaky start he took equal first place with Szymon Winawer, ahead of James Mason, Zukertort, George Henry Mackenzie, Blackburne, Berthold Englisch, Paulsen and Mikhail Chigorin, and drew the play-off match.[23][24]
In 1883, shortly after the London tournament, Steinitz decided to leave England and moved to New York City, where he lived for the rest of his life.[24] This did not end the "Ink War": his enemies persuaded some of the American press to publish anti-Steinitz articles,[10][27] and in 1885 Steinitz founded the International Chess Magazine, which he edited until 1895. In his magazine he chronicled the lengthy negotiations for a match with Zukertort. He also managed to find supporters in other sections of the American press including Turf, Field and Farm and the St. Louis Globe-Democrat, both of which reported Steinitz's offer to forgo all fees, expenses or share in the stake and make the match "a benefit performance, solely for Mr Zukertort's pecuniary profit".[12]
Though not yet officially an American citizen, Steinitz wanted the United States flag to be placed next to him during the match. He became a US citizen on November 23, 1888, having resided for five years in New York, and changed his first name from Wilhelm to William.[7][unreliable source]
In 1888 the Havana Chess Club offered to sponsor a match between Steinitz and whomever he would select as a worthy opponent. Steinitz nominated the Russian Mikhail Chigorin,[7][unreliable source] on the condition that the invitation should not be presented as a challenge from him. There is some doubt about whether this was intended to be a match for the world championship: both Steinitz's letters and the publicity material just before the match conspicuously avoided the phrase. The proposed match was to have a maximum of 20 games,[7][unreliable source] and Steinitz had said that fixed-length matches were unsuitable for world championship contests because the first player to take the lead could then play for draws; and Steinitz was at the same time supporting the American Chess Congress's world championship project.[31] Whatever the status of the match, it was played in Havana in January to February 1889, and won by Steinitz (ten wins, one draw, six losses).[citation needed]
The American Chess Congress's final proposal was that the winner of a tournament to be held in New York in 1889 should be regarded as world champion for the time being, but must be prepared to face a challenge from the second or third placed competitor within a month.[31] Steinitz wrote that he would not play in the tournament and would not challenge the winner unless the second and third placed competitors failed to do so.[32] The tournament was duly played, but the outcome was not quite as planned: Mikhail Chigorin and Max Weiss tied for first place; their play-off resulted in four draws, and Weiss then wanted to get back to his work for the Rothschild Bank, conceding the title to Chigorin[citation needed].
German Dr. Siegbert Tarrasch turned down an opportunity in 1892 to challenge Steinitz in a world championship match, because of the demands of his medical practice.[citation needed]
Around this time Steinitz publicly spoke of retiring, but changed his mind when Emanuel Lasker, 32 years younger and comparatively untested at the top level, challenged him. Lasker had been earlier that year refused a non-title challenge by fellow German, Dr. Siegbert Tarrasch, who was at the time the world's most dominant tournament player.[33]
Initially, Lasker wanted to play for $5,000 a side, and a match was agreed at stakes of $3,000 a side, but Steinitz agreed to a series of reductions when Lasker found it difficult to raise the money, and the final figure was $2,000 each, which was less than for some of Steinitz's earlier matches (the final combined stake of $4,000 would be worth about $114,000 at 2016 values[34]). Although this was publicly praised as an act of sportsmanship on Steinitz's part,[15] Steinitz may have desperately needed the money.[35]
The match was played in 1894, at venues in New York, Philadelphia and Montreal, Canada. The 32-year age difference between the combatants was the largest in the history of world championship play, and remains so today.[36] Steinitz had previously declared he would win without doubt, so it came as a shock when Lasker won the first game. Steinitz responded by winning the second, and was able to maintain the balance until the sixth. However, Lasker won all the games from the seventh to the 11th, and Steinitz asked for a one-week rest. When the match resumed, Steinitz looked in better shape and won the 13th and 14th games. Lasker struck back in the 15th and 16th, and Steinitz was unable to compensate for his losses in the middle of the match. Hence Lasker won with ten wins, five losses and four draws.[37][38] Some commentators thought Steinitz's habit of playing "experimental" moves in serious competition was a major factor in his downfall.[39]
After losing the title, Steinitz played in tournaments more frequently than he had previously. He won at New York City 1894, and was fifth at Hastings 1895 (winning the first brilliancy prize for his game with Curt von Bardeleben). At Saint Petersburg 1895, a super-strong four player, multi-round-robin event, with Lasker, Chigorin and Pillsbury, he took second place behind Lasker. Later his results began to decline: 6th in Nuremberg 1896, 5th in Cologne 1898, 10th in London 1899.[7][unreliable source][40]
In early 1896, Steinitz defeated the Russian Emanuel Schiffers in a match (winning 6 games, drawing 1, losing 4).[35]
In November, 1896 to January, 1897 Steinitz played a return match with Lasker in Moscow, but won only 2 games, drawing 5, and losing 10.[41] This was the last world chess championship match for eleven years. Shortly after the match, Steinitz had a mental breakdown and was confined for 40 days in a Moscow sanatorium, where he played chess with the inmates.[7][unreliable source]
Steinitz lived with Caroline Golder (born 1846) in the 1860s, and their only daughter Flora was born in 1866.[10][50] Flora died in 1888 at the age of 21,[35] and Caroline died in 1892.[10] He married his second wife a few years later, and had two children by her. In 1897 he dedicated a pamphlet to the memory of his first wife and their daughter.[39]
In February 1897, the New York Times prematurely reported his death in a New York mental asylum.[51] Some authors claim that he contracted syphilis,[52] which may have been a cause of the mental breakdowns he suffered in his last years. In the months prior to his death, he spent some time in institutions as a result of his failing mental health.[53] His chess activities had not yielded any great financial rewards, and he died a pauper in the Manhattan State Hospital (Wards Island) on August 12, 1900, of a heart attack. Steinitz is buried in the Cemetery of the Evergreens in Brooklyn, New York.[54] His second wife and their two young children were still alive at the time of his death.[39]
The book of the Hastings 1895 chess tournament, written collectively by the players, described Steinitz as follows:[55]
Mr. Steinitz stands high as a theoretician and as a writer; he has a powerful pen, and when he chooses can use expressive English. He evidently strives to be fair to friends and foes alike, but appears sometimes to fail to see that after all he is much like many others in this respect. Possessed of a fine intellect, and extremely fond of the game, he is apt to lose sight of all other considerations, people and business alike. Chess is his very life and soul, the one thing for which he lives.
Steinitz's play up to and including 1872 was similar to that of his contemporaries: sharp, aggressive, and full of sacrificial play. This was the style in which he became "world number one" by beating Adolf Anderssen in 1866 and confirmed his position by beating Zukertort in 1872 and winning the 1872 London International tournament (Zukertort had claimed the rank of number two by beating Anderssen in 1871).[9]
Although Steinitz's play changed abruptly, he said he had been thinking along such lines for some years: 
Some of the games which I saw Paulsen play during the London Congress of 1862 gave a still stronger start to the modification of my own opinions, which has since developed, and I began to recognize that Chess genius is not confined to the more or less deep and brilliant finishing strokes after the original balance of power and position has been overthrown, but that it also requires the exercise of still more extraordinary powers, though perhaps of a different kind to maintain that balance or respectively to disturb it at the proper time in one's own favor.[28]
But when he contested the first World Championship match in 1886 against Johannes Zukertort, it became evident that Steinitz was playing on another level. Although Zukertort was at least Steinitz's equal in spectacular attacking play, Steinitz often outmaneuvered him fairly simply by the use of positional principles.[57][63]
By the end of his career, Steinitz was more highly esteemed as a theoretician than as a player. The comments about him in the book of the Hastings 1895 chess tournament focus on his theories and writings,[55] and Emanuel Lasker was more explicit: "He was a thinker worthy of a seat in the halls of a University. A player, as the world believed he was, he was not; his studious temperament made that impossible; and thus he was conquered by a player ..."[66]
As a result of his play and writings Steinitz, along with Paul Morphy, is considered by many chess commentators to be the founder of modern chess.[67]
Lasker, who took the championship from Steinitz, wrote, "I who vanquished him must see to it that his great achievement, his theories should find justice, and I must avenge the wrongs he suffered."[66]
Vladimir Kramnik emphasizes Steinitz's importance as a pioneer in the field of chess theory: "Steinitz was the first to realise that chess, despite being a complicated game, obeys some common principles. ... But as often happens the first time is just a try. ... I can't say he was the founder of a chess theory. He was an experimenter and pointed out that chess obeys laws that should be considered."[68]
Steinitz was the main chess correspondent of The Field (in London) from 1873 to 1882, and used this to present his ideas about chess strategy.[19] In 1885 he founded the International Chess Magazine in New York City and edited it until 1891. In addition to game commentaries and blow-by-blow accounts of the negotiations leading to his 1886 match with Johann Zukertort and of the American Chess Congress's world championship project, he wrote a long series of articles about Paul Morphy, who had died in 1884.[31][69] He wrote the book of the 1889 New York tournament, in which he annotated all 432 of the games,[62][70] and in 1889 he published a textbook, The Modern Chess Instructor.[70]
Steinitz also allegedly wrote a pamphlet entitled Capital, Labor, and Charity while confined at River Crest Sanitarium in New York during the final months of his life.[53]
Initially Steinitz played in the all-out attacking style of contemporaries like Anderssen, and then changed to the positional style with which he dominated competitive chess in the 1870s and 1880s.[9] Max Euwe wrote, "Steinitz aimed at positions with clear-cut features, to which his theory was best applicable."[77] However, he retained his capacity for brilliant attacks right to the end of his career; for example, in the 1895 Hastings tournament (when he was 59), he beat von Bardeleben in a spectacular game in which in the closing stages Steinitz deliberately exposed all his pieces to attack simultaneously (except his king, of course).[57] His most significant weaknesses were his habits of playing "experimental" moves and getting into unnecessarily difficult defensive positions in top-class competitive games.[9][39]
"Traditional" accounts of Steinitz describe him as having a sharp tongue and violent temper, perhaps partly because of his short stature (barely five feet) and congenital lameness.[1][27][57] He admitted that "Like the Duke of Parma, I always hold the sword in one hand and the olive branch in the other",[78] and under severe provocation he could become abusive in published articles.[79] He was aware of his own tendencies and said early in his career, "Nothing would induce me to take charge of a chess column ...Because I should be so fair in dispensing blame as well as praise that I should be sure to give offence and make enemies."[80] When he embarked on chess journalism, his brutally frank review of Wormald's The Chess Openings in 1875 proved him right on both counts.[81]
Carlsen became World Chess Champion in 2013 by defeating Viswanathan Anand. He retained his title against Anand the following year and won both the 2014 World Rapid Championship and World Blitz Championship, becoming the first player to hold all three titles simultaneously, a feat which he repeated in 2019 and 2022.[4][5] He defended his classical world title against Sergey Karjakin in 2016, against Fabiano Caruana in 2018, and against Ian Nepomniachtchi in 2021. Carlsen has declined to defend his title against Nepomniachtchi in 2023.[6]
Known for his attacking style as a teenager, Carlsen has since developed into a universal player. He uses a variety of openings to make it harder for opponents to prepare against him and reduce the utility of pre-game computer analysis. He has stated the middlegame is his favourite part of the game as it "comes down to pure chess".[7]
His father, a keen amateur chess player,[11] taught him to play at age 5, although he initially showed little interest in it.[12] He has three sisters, and in 2010 stated that one of the things that first motivated him to take up chess seriously was the desire to beat his elder sister at the game.[13]
After finishing primary school, Carlsen took a year off to participate in international chess tournaments in Europe during the autumn of 2003, then returned to complete secondary education at a sports school.[27][28] During the year away from school, he placed joint-third in the European Under-14 Championship[29] and ninth in the 2003 World Under-14 Championship.[30]
In October, Carlsen took first place at the Arnold Eikrem Memorial in Gausdal with a score 8/9 and a PR of 2792.[46]
At the 2006 international 'Bosna' tournament in Sarajevo, Carlsen shared first place with Liviu-Dieter Nisipeanu (who won on tiebreak evaluation) and Vladimir Malakhov; this could be regarded as Carlsen's first "A" elite tournament win, although it was not a clear first.[50]
Carlsen played for the first time in the Melody Amber blind and rapid chess tournament in Monte Carlo in March. In the 11 rounds, he achieved eight draws and three losses in the blindfold games, as well as three wins, seven draws and one loss in the rapid games. This resulted in a shared ninth place in the blindfold, shared second place in the rapid (behind Anand), and a shared eighth place overall.[64]
In March, Carlsen played for the second time in the Melody Amber blind and rapid chess tournament, held in Nice for the first time. In the 11 rounds he achieved four wins, four draws and two losses in the blindfold, and three wins, two losses, and six draws in the rapid. This resulted in a shared fifth place in the blindfold, shared third place in the rapid and a shared second place in the overall tournament.[73]
Playing in Group A of the 71st Corus chess tournament, Carlsen tied for fifth with a 2739 PR.[82] In the Linares chess tournament, Carlsen finished third with a 2777 PR.[83] Carlsen tied for second place with Veselin Topalov at the M-Tel Masters (category 21) tournament in Sofia, Bulgaria. He lost to eventual winner Alexei Shirov in their final game, dropping him from first.[84]
In the Tal Memorial, played from 5 to 14 November, Carlsen started with seven straight draws, but finished with wins over former FIDE World Champion Ruslan Ponomariov and Peter Leko. This result put Carlsen in shared second place behind former World Champion Vladimir Kramnik and equal with Ivanchuk.[87][88] After the Tal Memorial, Carlsen won the World Blitz Championship, played from 16 to 18 November in Moscow, Russia. His score of 28 wins, 6 draws and 8 losses left him three points ahead of Anand, who finished in second place.[89]
In early 2009 Carlsen engaged former World Champion Garry Kasparov as a personal trainer.[93] In September their partnership was revealed to the public by Norwegian newspapers.[94][95]
Responding to a question in an interview with Time magazine in December 2009 as to whether he used computers when studying chess, Carlsen explained that he does not use a chess set when studying on his own.[96]
On 5 November, Carlsen withdrew from the 2011 Candidates Tournament, having qualified as the highest rated challenger, citing dissatisfaction with the World Championship cycle format.
Carlsen won the FIDE World Rapid Championship, which was held in Dubai from 16 to 19 June.[152] He went on to claim the World Blitz Championship two days later, becoming the first player to simultaneously hold the title in all three FIDE rated time controls.[153]
Carlsen played ten games for Norway in the 42nd Chess Olympiad, scoring five wins and five draws, as the Norwegians placed 5th among the 180 teams in the open event.[176]
Throughout the year, Carlsen also participated in many online chess tournaments as part of the Meltwater Champions Chess Tour.
At the FIDE World Cup 2021 held in Sochi, Carlsen won the third place match against Fedoseev after being defeated in the semifinals by Duda, who eventually won the event.[243]
At the 2021 World Chess Championship, Carlsen defeated challenger Ian Nepomniachtchi in Game 6 after drawing the first five games. It was the first decisive result in a classical time limit World Chess Championship game in more than 5 years and at 136 moves was also the longest game in World Chess Championship history.[245] Subsequently, Carlsen also defeated Nepomniachtchi in Games 8, 9, and 11, thus retaining his championship title.[246] After the match, Carlsen announced that "Unless Firouzja wins the Candidates Tournament, it is unlikely that I will play the next world championship match."[247]
On 20 July, the deadline FIDE had given him, Carlsen announced that he would not defend his World Championship title against Nepomniachtchi in the 2023 World Chess Championship match.[250] Carlsen said he enjoyed playing chess tournaments more than championships,[251] and would still continue playing professional chess.[252]
Throughout the year, Carlsen competed in the Champions Chess Tour 2022.[253] He won the Airthings Masters, Charity Cup, and FTX Crypto Cup.
Carlsen joined the Norway team for the 44th Chess Olympiad.[253] He was the runner-up in the Chess.com Speed Championship.[254]
Carlsen won the World Rapid Chess Championship 2022 with a score of 10/13 (+8-1=4). He also won the World Blitz Chess Championship 2022 with a score of 16/21 (+13-2=6). This marks the third time in Carlsen's career that he has simultaneously held the Classical, Rapid, and Blitz World Titles, with 2014 and 2019 being the other two occurrences.
Two weeks later, Carlsen faced Niemann in the Julius Baer Generation Cup, a section of the Champions Chess Tour 2022, an online chess tour. Carlsen, playing as black, resigned after one move, continuing the scandal.[260]  On September 26, Carlsen finally broke his silence and accused Niemann of cheating.[261] Subsequently, on October 20 Niemann filed a lawsuit against Carlsen and four other defendants, alleging five counts of action, including libel and slander.[262][263]
In January, Carlsen participated in the Tata Steel Chess Tournament. He lost two games in a row, which had not happened since 2015, first to Anish Giri and then to the young Uzbek prodigy Nodirbek Abdusattorov. He recovered and finished 3rd, with the same score as Abdusattorov, but behind Giri, who won the tournament.[264] In February, Carlsen participated in the Airthings Masters 2023 which he won by defeating the American Grandmaster Hikaru Nakamura in the finals.
TPR (Tournament Performance ratings) of FIDE-rated events calculated according to FIDE.[265]
Carlsen won the Chess Oscars from 2009 to 2013. The Chess Oscar, organised by the Russian chess magazine 64, was awarded to the year's best player according to a worldwide poll of leading chess critics, writers, and journalists, but it was no longer awarded after 2013, as 64 ceased publication.[378][379]
In 2011, he was awarded the Peer Gynt Prize, a Norwegian prize awarded annually to "a person or institution that has achieved distinction in society".[385]
In 2013, Time magazine named Carlsen one of the 100 most influential people in the world.[386]
In the January 2006 FIDE rankings, at the age of 15 years and 32 days, Carlsen attained a 2625 Elo rating, which made him the youngest person to surpass 2600 (the record has since been broken by Wesley So, Wei Yi, and John M. Burke).[387] In the July 2007 FIDE rankings, at the age of 16 years and 213 days, Carlsen attained a 2710 Elo rating, which made him the youngest person to surpass 2700 (the record has since been broken by Wei Yi).[388]
The March 2010 FIDE rankings showed Carlsen with a new peak rating of 2813, a figure that only Kasparov had bettered at that time.[99] On the January 2013 FIDE rankings, Carlsen reached 2861, thus surpassing Garry Kasparov's 2851 record from July 1999.[136][137] In the May 2014 rankings, Carlsen achieved an all-time high record of 2882,[398] with a peak of 2889 on the live ratings list achieved on 21 April 2014.[399] In August 2019 he equalled his peak FIDE rating of 2882.[400]
Carlsen had an aggressive style of play as a youth,[403][404] and, according to Simen Agdestein, his play was characterised by "a fearless readiness to offer material for activity".[405] As he matured, Carlsen found that this risky playing style was not as well suited against the elite of the chess world. When he started playing in top tournaments, he had trouble getting much out of the opening. To progress, Carlsen's style became more universal, capable of handling all sorts of positions well. He opens with both 1.d4 and 1.e4, as well as 1.c4, and, on occasion, 1.Nf3, thus making it harder for opponents to prepare against him and reducing the effect of computer analysis.[7][406][407] He said in 2015 that the middlegame is his favourite part of the game as it comes down to "pure chess".[7]
In a 2016 interview, Anish Giri said: "Magnus and I are very close in terms of style, but in our approach to the game we're total opposites. Magnus tries to put the accent only on play, getting away from preparation, but for me preparation plays an enormous role."[408]
Anand said of Carlsen in 2012: "Magnus has an incredible innate sense. ... The majority of ideas occur to him absolutely naturally. He's also very flexible, he knows all the structures and he can play almost any position." He also compared Carlsen to Boris Spassky in his prime, and stated that "Magnus can literally do almost everything."[414] Kasparov expressed similar sentiments: "[Carlsen] has the ability to correctly evaluate any position, which only Karpov could boast of before him."[415]
When asked in a 2016 interview whether Carlsen's style resembles his own, Karpov answered: "It is quite possible. He grew up when I was in power, and perhaps he studied my games. He can convert a minimal advantage into a real one."[416]
In a 2012 interview, Vladimir Kramnik stated that Carlsen's "excellent physical shape" was a contributing factor to his success against other top players as it prevents "psychological lapses", which enables him to maintain a high standard of play over long games and at the end of tournaments, when the energy levels of others have dropped.[417] Levon Aronian said in 2015: "Magnus' main secret is his composure and the absence of any soul-searching after mistakes during a game."[418]
Tyler Cowen gave a point of view on Carlsen's playing style: "Carlsen is demonstrating one of his most feared qualities, namely his 'nettlesomeness,' to use a term coined for this purpose by Ken Regan, of the University at Buffalo.[419] Using computer analysis, you can measure which players do the most to cause their opponents to make mistakes. Carlsen has the highest nettlesomeness score by this metric, because his creative moves pressure the other player and open up a lot of room for mistakes. In contrast, a player such as Kramnik plays a high percentage of very accurate moves, and of course he is very strong, but those moves are in some way calmer and they are less likely to induce mistakes in response."[420]
Carlsen's endgame prowess has been described as among the greatest in history.[421][422][423] Jon Speelman, analysing several of Carlsen's endgames from the 2012 London Classic (in particular, his wins against McShane, Aronian, and Adams), described what he calls the "Carlsen effect": 
Carlsen also plays speed chess online under many screen names. He is known for meme openings such as the Bongcloud Attack and exceptionally strong play, despite banter and gags.[426]
Film director J. J. Abrams offered Carlsen a role in the movie Star Trek Into Darkness as "a chess player from the future", but Carlsen was unable to get a work permit in time for shooting.[435] In 2012, Carlsen was featured in a 60 Minutes segment[436] and appeared as a guest on The Colbert Report.[437] He was also interviewed by Rainn Wilson for SoulPancake in 2013.[438]
In August 2013, Carlsen became an ambassador for Nordic Semiconductor,[442] and in November was selected as one of the "sexiest men of 2013" by Cosmopolitan.[443] In 2017, Carlsen made a special guest appearance on The Simpsons in an episode where Homer's chess history is revealed.[444] In 2020, Carlsen announced that he had signed a two-year sponsorship deal with gambling company Unibet to act as a "global ambassador".[445]  Unibet parent company Kindred Group is also a sponsor of Offerspill Chess Club.  Offerspill was founded by Carlsen in 2019 after the Norwegian Chess Federation turned down Kindred's sponsorship offer.  It is now Norway's largest chess club; Carlsen is its current chairman.[446] In April 2022, the Kindred Group (through Unibet) extended its partnership with Carlsen for another two years, and also extended its sponsorship of Offerspill.[447]
In October 2013, Carlsen co-founded a company, Play Magnus AS, with Espen Agdestein and Anders Brandt.[448] Based in Oslo, Norway, Play Magnus' first product was an iOS app, called Play Magnus, that allows the user to play a chess engine created using a database of thousands of Carlsen's recorded games from the age of five and up. Carlsen stated he wished for the app to encourage more people to play chess.[449][450]
In March 2019, Play Magnus AS merged with Chess24.com, consolidating into the Play Magnus Group.[451] On 8 October 2020, Play Magnus Group was listed on the Oslo Stock Exchange. Magnus Chess, an entity controlled by Carlsen and his family, then owned 9.5% of Play Magnus Group.[452]
In August 2022, Chess.com finalised an offer of acquisition for Play Magnus Group, which officially closed on December 16, 2022. As part of the acquisition, Carlsen signed as a brand ambassador for Chess.com.[453][454][455]
As of 2016, Carlsen identifies as a social democrat and mostly follows a vegetarian diet; two of his sisters are vegetarians.[456]
In April 2022, Carlsen played poker at the Norwegian Championships Main Event and finished 25th out of 1050 players.[460][461]
The game of chess is commonly divided into three phases: the opening, middlegame, and endgame.[1] There is a large body of theory regarding how the game should be played in each of these phases, especially the opening and endgame. Those who write about chess theory, who are often also eminent players, are referred to as "theorists" or "theoreticians".
"Opening theory" commonly refers to consensus, broadly represented by current literature on the openings.[2] "Endgame theory" consists of statements regarding specific positions, or positions of a similar type, though there are few universally applicable principles.[3] "Middlegame theory" often refers to maxims or principles applicable to the middlegame.[4] The modern trend, however, is to assign paramount importance to analysis of the specific position at hand rather than to general principles.[5]
The earliest printed work on chess theory whose date can be established with some exactitude is Repeticion de Amores y Arte de Ajedrez by the Spaniard Luis Ramirez de Lucena, published c. 1497, which included among other things analysis of eleven chess openings. Some of them are known today as the Giuoco Piano, Ruy Lopez, Petrov's Defense, Bishop's Opening, Damiano's Defense, and Scandinavian Defense, though Lucena did not use those terms.[13]
Fifteen years after Lucena's book, Portuguese apothecary Pedro Damiano published the book Questo libro e da imparare giocare a scachi et de la partiti (1512) in Rome. It includes analysis of the Queen's Gambit Accepted, showing what happens when Black tries to keep the gambit pawn with ...b5.[19] Damiano's book "was, in contemporary terms, the first bestseller of the modern game."[20] Harry Golombek writes that it "ran through eight editions in the sixteenth century and continued on into the next century with unflagging popularity."[21] Modern players know Damiano primarily because his name is attached to the weak opening Damiano's Defense (1.e4 e5 2.Nf3 f6?), although he condemned rather than endorsed it.[22]
These books and later ones discuss games played with various openings, opening traps, and the best way for both sides to play. Certain sequences of opening moves began to be given names, some of the earliest being Damiano's Defense, the King's Gambit (1.e4 e5 2.f4), the Queen's Gambit (1.d4 d5 2.c4), and the Sicilian Defense (1.e4 c5).[23]
In the late 1930s to early 1950s Reuben Fine, one of the world's strongest players,[44] also became one of its leading theoreticians, publishing important works on the opening, middlegame, and endgame. These began with his revision of Modern Chess Openings, which was published in 1939.[45] In 1943, he published Ideas Behind the Chess Openings, which sought to explain the principles underlying the openings.[46] In 1948, he published his own opening treatise, Practical Chess Openings, a competitor to MCO.[47] In 1964, International Master I.A. Horowitz published the 789-page tome Chess Openings: Theory and Practice, which in addition to opening analysis includes a large number of illustrative games.[24]
In 1966, the first volume of Chess Informant was published in Belgrade, Yugoslavia, containing 466 annotated games from the leading chess tournaments and matches of the day.[48] The hugely influential Chess Informant series has revolutionized opening theory. Its great innovation is that it expresses games in languageless figurine algebraic notation and annotated them using no words, but rather seventeen symbols, whose meanings were explained at the beginning of the book in six different languages. This enabled readers around the world to read the same games and annotations, thus greatly accelerating the dissemination of chess ideas and the development of opening theory. The editors of Chess Informant later introduced other publications using the same principle, such as the five-volume Encyclopedia of Chess Openings and Encyclopedia of Chess Endings treatises.  Chess Informant was originally published twice a year, and since 1991 has been published thrice annually. Volume 100 was published in 2007.[49] It now uses 57 symbols, explained in 10 languages, to annotate games (see Punctuation (chess)), and is available in both print and electronic formats. In 2005, former World Champion Garry Kasparov wrote, "We are all Children of the Informant."[50]
In the 1990s and thereafter, the development of opening theory has been further accelerated by such innovations as extremely strong chess engines such as Fritz and Rybka, software such as ChessBase, and the sale of multi-million-game databases such as ChessBase's Mega 2013 database, with over 5.4 million games.[51] Today, the most important openings have been analyzed over 20 moves deep,[52] sometimes well into the endgame,[53][54] and it is not unusual for leading players to introduce theoretical novelties on move 25 or even later.[55][56][57]
Middlegame theory is considerably less developed than either opening theory or endgame theory.[63] Watson writes, "Players wishing to study this area of the game have a limited and rather unsatisfactory range of resources from which to choose."[64]
One of the earliest theories to gain attention was that of William Steinitz, who posited that a premature attack against one's opponent in an equal position could be repelled by skillful defence, and so a player's best bet was to slowly maneuver with the goal of accumulating small advantages.  Emanuel Lasker in Lasker's Manual of Chess and Max Euwe in The Development of Chess Style outlined theories that they attributed to Steinitz.
Leading player and theorist Aron Nimzowitsch's[65] influential books, My System (1925),[66] Die Blockade (1925) (in German),[67] and Chess Praxis (1936),[68][69] are among the most important works on the middlegame.[64]  Nimzowitsch called attention to the possibility of letting one's opponent occupy the centre with pawns while you exert control with your pieces as in the Nimzo-Indian or Queen's Indian defences. He pointed out how in positions with interlocking pawn chains, one could attack the chain at its base by advancing one's own pawns and carrying out a freeing move (pawn break).  He also drew attention to the strategy of occupying open files with one's rooks in order to later penetrate to the seventh rank where they could attack the enemy pawns and hem in the opponent's king.  Another of his key concepts was prophylaxis, moves aimed at limiting the opponent's mobility to the point where he would no longer have any useful moves.
Another key turning point in middlegame theory came with the release of Alexander Kotov's book Think like a Grandmaster in 1971.  Kotov outlined how a player calculates by developing a tree of variations in his head, and recommended that players only examine each branch of the tree once.  He also noted how some players seem to fall victim to what is now known as Kotov's Syndrome: they calculate out a large range of different lines, become dissatisfied with the result, and realizing that they are short on time, play a completely new candidate move without even checking whether it is sound.  More recently, Jonathan Tisdall, John Nunn and Andrew Soltis have elaborated on Kotov's tree theory further.
In 1999, Watson's Secrets of Modern Chess Strategy: Advances Since Nimzowitsch was published, in which Watson discusses the revolution in middlegame theory that has occurred since Nimzowitsch's time.[79]
Many significant chess treatises, beginning with the earliest works, have included some analysis of the endgame. Lucena's book (c. 1497) concluded with 150 examples of endgames and chess problems.[85]
Staunton's conclusions on these endgames were, anticipated by the British master George Walker, who wrote in 1846 (and perhaps earlier):
Although the two Bishops and Kt win, as a general proposition, against Rook, yet the two Knights with a Bishop cannot expect the same success; and the legitimate result of such conflict would be a draw. The Bishops, united, are stronger than the Knights, as they strike from a greater distance. When the two Knights are left with a Bishop, the Rook has also the chance of exchanging for the latter, which can hardly be avoided by his adversary, and the two Knights, alone, have not the mating power.[90]
In 1941 Reuben Fine published his monumental 573-page treatise Basic Chess Endings, the first attempt at a comprehensive treatise on the endgame.[91] A new edition, revised by Pal Benko, was published in 2003.[92]
Soviet writers published an important series of books on specific endings:  Rook Endings by Grigory Levenfish and Vasily Smyslov,[93] Pawn Endings by Yuri Averbakh and I. Maizelis,[94] Queen and Pawn Endings by Averbakh,[95] Bishop Endings by Averbakh,[96] Knight Endings by Averbakh and Vitaly Chekhover,[97] Bishop v. Knight Endings by Yuri Averbakh,[98] Rook v. Minor Piece Endings by Averbakh,[99] and Queen v. Rook/Minor Piece Endings by Averbakh, Chekhover, and V. Henkin.[100]  These books by Averbakh and others were collected into the five-volume Comprehensive Chess Endings in English.
Art is a diverse range of human activity, and resulting product, that involves creative or imaginative talent expressive of technical proficiency, beauty, emotional power, or conceptual ideas.[1][2][3]
There is no generally agreed definition of what constitutes art,[4][5][6] and its interpretation has varied greatly throughout history and across cultures. In the Western tradition, the three classical branches of visual art are painting, sculpture, and architecture.[7] Theatre, dance, and other performing arts, as well as literature, music, film and other media such as interactive media, are included in a broader definition of the arts.[1][8] Until the 17th century, art referred to any skill or mastery and was not differentiated from crafts or sciences. In modern usage after the 17th century, where aesthetic considerations are paramount, the fine arts are separated and distinguished from acquired skills in general, such as the decorative or applied arts.
The nature of art and related concepts, such as creativity and interpretation, are explored in a branch of philosophy known as aesthetics.[9] The resulting artworks are studied in the professional fields of art criticism and the history of art.
In the perspective of the history of art,[10] artistic works have existed for almost as long as humankind: from early prehistoric art to contemporary art; however, some theorists think that the typical concept of "artistic works" does not fit well outside modern Western societies.[11] One early sense of the definition of art is closely related to the older Latin meaning, which roughly translates to "skill" or "craft", as associated with words such as "artisan". English words derived from this meaning include artifact, artificial, artifice, medical arts, and military arts. However, there are many other colloquial uses of the word, all with some relation to its etymology.
The more recent and specific sense of the word art as an abbreviation for creative art or fine art emerged in the early 17th century.[17] Fine art refers to a skill used to express the artist's creativity, or to engage the audience's aesthetic sensibilities, or to draw the audience towards consideration of more refined or finer works of art.
Within this latter sense, the word art may refer to several things: (i) a study of a creative skill, (ii) a process of using the creative skill, (iii) a product of the creative skill, or (iv) the audience's experience with the creative skill. The creative arts (art as discipline) are a collection of disciplines which produce artworks (art as objects) that are compelled by a personal drive (art as activity) and convey a message, mood, or symbolism for the perceiver to interpret (art as experience). Art is something that stimulates an individual's thoughts, emotions, beliefs, or ideas through the senses. Works of art can be explicitly made for this purpose or interpreted on the basis of images or objects. For some scholars, such as Kant, the sciences and the arts could be distinguished by taking science as representing the domain of knowledge and the arts as representing the domain of the freedom of artistic expression.[18]
Often, if the skill is being used in a common or practical way, people will consider it a craft instead of art. Likewise, if the skill is being used in a commercial or industrial way, it may be considered commercial art instead of fine art. On the other hand, crafts and design are sometimes considered applied art. Some art followers have argued that the difference between fine art and applied art has more to do with value judgments made about the art than any clear definitional difference.[19] However, even fine art often has goals beyond pure creativity and self-expression. The purpose of works of art may be to communicate ideas, such as in politically, spiritually, or philosophically motivated art; to create a sense of beauty (see aesthetics); to explore the nature of perception; for pleasure; or to generate strong emotions. The purpose may also be seemingly nonexistent.
The nature of art has been described by philosopher Richard Wollheim as "one of the most elusive of the traditional problems of human culture".[20] Art has been defined as a vehicle for the expression or communication of emotions and ideas, a means for exploring and appreciating formal elements for their own sake, and as mimesis or representation. Art as mimesis has deep roots in the philosophy of Aristotle.[21] Leo Tolstoy identified art as a use of indirect means to communicate from one person to another.[21] Benedetto Croce and R. G. Collingwood advanced the idealist view that art expresses emotions, and that the work of art therefore essentially exists in the mind of the creator.[22][23] The theory of art as form has its roots in the philosophy of Kant, and was developed in the early 20th century by Roger Fry and Clive Bell. More recently, thinkers influenced by Martin Heidegger have interpreted art as the means by which a community develops for itself a medium for self-expression and interpretation.[24] George Dickie has offered an institutional theory of art that defines a work of art as any artifact upon which a qualified person or persons acting on behalf of the social institution commonly referred to as "the art world" has conferred "the status of candidate for appreciation".[25] Larry Shiner has described fine art as "not an essence or a fate but something we have made. Art as we have generally understood it is a European invention barely two hundred years old."[26]
Art may be characterized in terms of mimesis (its representation of reality), narrative (storytelling), expression, communication of emotion, or other qualities. During the Romantic period, art came to be seen as "a special faculty of the human mind to be classified with religion and science".[27]
Sculptures, cave paintings, rock paintings and petroglyphs from the Upper Paleolithic dating to roughly 40,000 years ago have been found,[33] but the precise meaning of such art is often disputed because so little is known about the cultures that produced them.
The first undisputed sculptures and similar art pieces, like the Venus of Hohle Fels, are the numerous objects found at the Caves and Ice Age Art in the Swabian Jura UNESCO World Heritage Site, where the oldest non-stationary works of human art yet discovered were found, in the form of carved animal and humanoid figurines, in addition to the oldest musical instruments unearthed so far, with the artifacts dating between 43.000 and 35.000 BC, so being the first centre of human art.[34][35][36][37]
Many great traditions in art have a foundation in the art of one of the great ancient civilizations: Ancient Egypt, Mesopotamia, Persia, India, China, Ancient Greece, Rome, as well as Inca, Maya, and Olmec. Each of these centers of early civilization developed a unique and characteristic style in its art. Because of the size and duration of these civilizations, more of their art works have survived and more of their influence has been transmitted to other cultures and later times. Some also have provided the first records of how artists worked. For example, this period of Greek art saw a veneration of the human physical form and the development of equivalent skills to show musculature, poise, beauty, and anatomically correct proportions.[38]
In Byzantine and Medieval art of the Western Middle Ages, much art focused on the expression of subjects about biblical and religious culture, and used styles that showed the higher glory of a heavenly world, such as the use of gold in the background of paintings, or glass in mosaics or windows, which also presented figures in idealized, patterned (flat) forms. Nevertheless, a classical realist tradition persisted in small Byzantine works, and realism steadily grew in the art of Catholic Europe.[39]
Renaissance art had a greatly increased emphasis on the realistic depiction of the material world, and the place of humans in it, reflected in the corporeality of the human body, and development of a systematic method of graphical perspective to depict recession in a three-dimensional picture space.[40]
In the east, Islamic art's rejection of iconography led to emphasis on geometric patterns, calligraphy, and architecture.[42] Further east, religion dominated artistic styles and forms too. India and Tibet saw emphasis on painted sculptures and dance, while religious painting borrowed many conventions from sculpture and tended to bright contrasting colors with emphasis on outlines. China saw the flourishing of many art forms: jade carving, bronzework, pottery (including the stunning terracotta army of Emperor Qin[43]), poetry, calligraphy, music, painting, drama, fiction, etc. Chinese styles vary greatly from era to era and each one is traditionally named after the ruling dynasty. So, for example, Tang dynasty paintings are monochromatic and sparse, emphasizing idealized landscapes, but Ming dynasty paintings are busy and colorful, and focus on telling stories via setting and composition.[44] Japan names its styles after imperial dynasties too, and also saw much interplay between the styles of calligraphy and painting. Woodblock printing became important in Japan after the 17th century.[45]
The western Age of Enlightenment in the 18th century saw artistic depictions of physical and rational certainties of the clockwork universe, as well as politically revolutionary visions of a post-monarchist world, such as Blake's portrayal of Newton as a divine geometer,[46] or David's propagandistic paintings. This led to Romantic rejections of this in favor of pictures of the emotional side and individuality of humans, exemplified in the novels of Goethe. The late 19th century then saw a host of artistic movements, such as academic art, Symbolism, impressionism and fauvism among others.[47][48]
The history of 20th-century art is a narrative of endless possibilities and the search for new standards, each being torn down in succession by the next. Thus the parameters of Impressionism, Expressionism, Fauvism, Cubism, Dadaism, Surrealism, etc. cannot be maintained very much beyond the time of their invention. Increasing global interaction during this time saw an equivalent influence of other cultures into Western art. Thus, Japanese woodblock prints (themselves influenced by Western Renaissance draftsmanship) had an immense influence on impressionism and subsequent development. Later, African sculptures were taken up by Picasso and to some extent by Matisse. Similarly, in the 19th and 20th centuries the West has had huge impacts on Eastern art with originally western ideas like Communism and Post-Modernism exerting a powerful influence.[49]
Modernism, the idealistic search for truth, gave way in the latter half of the 20th century to a realization of its unattainability. Theodor W. Adorno said in 1970, "It is now taken for granted that nothing which concerns art can be taken for granted any more: neither art itself, nor art in relationship to the whole, nor even the right of art to exist."[50] Relativism was accepted as an unavoidable truth, which led to the period of contemporary art and postmodern criticism, where cultures of the world and of history are seen as changing forms, which can be appreciated and drawn from only with skepticism and irony. Furthermore, the separation of cultures is increasingly blurred and some argue it is now more appropriate to think in terms of a global culture, rather than of regional ones.[51]
In The Origin of the Work of Art, Martin Heidegger, a German philosopher and a seminal thinker, describes the essence of art in terms of the concepts of being and truth. He argues that art is not only a way of expressing the element of truth in a culture, but the means of creating it and providing a springboard from which "that which is" can be revealed. Works of art are not merely representations of the way things are, but actually produce a community's shared understanding. Each time a new artwork is added to any culture, the meaning of what it is to exist is inherently changed.
Historically, art and artistic skills and ideas have often been spread through trade. An example of this is the Silk Road, where Hellenistic, Iranian, Indian and Chinese influences could mix. Greco Buddhist art is one of the most vivid examples of this interaction. The meeting of different cultures and worldviews also influenced artistic creation. An example of this is the multicultural port metropolis of Trieste at the beginning of the 20th century, where James Joyce met writers from Central Europe and the artistic development of New York City as a cultural melting pot.[52][53][54]
The creative arts are often divided into more specific categories, typically along perceptually distinguishable categories such as media, genre, styles, and form.[55] Art form refers to the elements of art that are independent of its interpretation or significance. It covers the methods adopted by the artist and the physical composition of the artwork, primarily non-semantic aspects of the work (i.e., figurae),[56] such as color, contour, dimension, medium, melody, space, texture, and value. Form may also include visual design principles, such as arrangement, balance, contrast, emphasis, harmony, proportion, proximity, and rhythm.[57]
Extreme Intentionalism holds that authorial intent plays a decisive role in the meaning of a work of art, conveying the content or essential main idea, while all other interpretations can be discarded.[59] It defines the subject as the persons or idea represented,[60] and the content as the artist's experience of that subject.[61] For example, the composition of Napoleon I on his Imperial Throne is partly borrowed from the Statue of Zeus at Olympia. As evidenced by the title, the subject is Napoleon, and the content is Ingres's representation of Napoleon as "Emperor-God beyond time and space".[57] Similarly to extreme formalism, philosophers typically reject extreme intentionalism, because art may have multiple ambiguous meanings and authorial intent may be unknowable and thus irrelevant. Its restrictive interpretation is "socially unhealthy, philosophically unreal, and politically unwise".[57]
Finally, the developing theory of post-structuralism studies art's significance in a cultural context, such as the ideas, emotions, and reactions prompted by a work.[62] The cultural context often reduces to the artist's techniques and intentions, in which case analysis proceeds along lines similar to formalism and intentionalism. However, in other cases historical and material conditions may predominate, such as religious and philosophical convictions, sociopolitical and economic structures, or even climate and geography. Art criticism continues to grow and develop alongside art.[57]
Art can connote a sense of trained ability or mastery of a medium. Art can also refer to the developed and efficient use of a language to convey meaning with immediacy or depth. Art can be defined as an act of expressing feelings, thoughts, and observations.[63]
There is an understanding that is reached with the material as a result of handling it, which facilitates one's thought processes.
A common view is that the epithet art, particular in its elevated sense, requires a certain level of creative expertise by the artist, whether this be a demonstration of technical ability, an originality in stylistic approach, or a combination of these two. Traditionally skill of execution was viewed as a quality inseparable from art and thus necessary for its success; for Leonardo da Vinci, art, neither more nor less than his other endeavors, was a manifestation of skill.[64] Rembrandt's work, now praised for its ephemeral virtues, was most admired by his contemporaries for its virtuosity.[65] At the turn of the 20th century, the adroit performances of John Singer Sargent were alternately admired and viewed with skepticism for their manual fluency,[66] yet at nearly the same time the artist who would become the era's most recognized and peripatetic iconoclast, Pablo Picasso, was completing a traditional academic training at which he excelled.[67][68]
A common contemporary criticism of some modern art occurs along the lines of objecting to the apparent lack of skill or ability required in the production of the artistic object. In conceptual art, Marcel Duchamp's Fountain is among the first examples of pieces wherein the artist used found objects ("ready-made") and exercised no traditionally recognised set of skills.[69] Tracey Emin's My Bed, or Damien Hirst's The Physical Impossibility of Death in the Mind of Someone Living follow this example and also manipulate the mass media. Emin slept (and engaged in other activities) in her bed before placing the result in a gallery as work of art. Hirst came up with the conceptual design for the artwork but has left most of the eventual creation of many works to employed artisans. Hirst's celebrity is founded entirely on his ability to produce shocking concepts.[70] The actual production in many conceptual and contemporary works of art is a matter of assembly of found objects. However, there are many modernist and contemporary artists who continue to excel in the skills of drawing and painting and in creating hands-on works of art.[71]
Motivated purposes of art refer to intentional, conscious actions on the part of the artists or creator. These may be to bring about political change, to comment on an aspect of society, to convey a specific emotion or mood, to address personal psychology, to illustrate another discipline, to (with commercial arts) sell a product, or used as a form of communication.[72][77]
The functions of art described above are not mutually exclusive, as many of them may overlap. For example, art for the purpose of entertainment may also seek to sell a product, i.e. the movie or video game.
Art can be divided into any number of steps one can make an argument for. This section divides the creative process into broad three steps, but there is no consensus on an exact number.[98]
In the first step, the artist envisions the art in their mind. By imagining what their art would look like, the artist begins the process of bringing the art into existence. Some postulate that merely thinking of the art creates it. Others argue thinking about it will directly increase the chances it will occur. Preparation of art may involve approaching and researching the subject matter. Artistic inspiration is one of the main drivers of art, and may be considered to stem from instinct, impressions, and feelings. The "illumination" of an artistic idea is to "come across the idea".[99]
In the second step, the artist executes the creation of their work. The art that the artist creates may depend on their mood, their surroundings, and their mental state. For example, The Black Paintings by Francisco de Goya, created in the elder years of his life, are thought to be so bleak because he was in isolation and because of his experience with war. He painted them directly on the walls of his apartment in Spain, and most likely never discussed them with anyone.[100] The Beatles stated drugs such as LSD and cannabis influenced some of their greatest hits, such as Revolver.[101][102] Trial and error are considered an integral part of the creation process.[103]
The last step is art appreciation, which has the sub-topic of critique. In one study, over half of visual arts student agreed that reflection is an essential step of the art process.[104] According to education journals, the reflection of art is considered an essential part of the experience.[105][106] However an important aspect of art is that others may view and appreciate it as well. While many focus on whether those viewing/listening/etc. believe the art to be good/successful or not, art has profound value beyond its commercial success as a provider of information and health in society.[107] Art enjoyment can bring about a wide spectrum of emotion due to beauty. Some art is meant to be practical, with its analysis studious and meant to stimulate discourse.[108]
Since ancient times, much of the finest art has represented a deliberate display of wealth or power, often achieved by using massive scale and expensive materials. Much art has been commissioned by political rulers or religious establishments, with more modest versions only available to the most wealthy in society.[109]
Nevertheless, there have been many periods where art of very high quality was available, in terms of ownership, across large parts of society, above all in cheap media such as pottery, which persists in the ground, and perishable media such as textiles and wood. In many different cultures, the ceramics of indigenous peoples of the Americas are found in such a wide range of graves that they were clearly not restricted to a social elite,[110] though other forms of art may have been. Reproductive methods such as moulds made mass-production easier, and were used to bring high-quality Ancient Roman pottery and Greek Tanagra figurines to a very wide market. Cylinder seals were both artistic and practical, and very widely used by what can be loosely called the middle class in the Ancient Near East.[111] Once coins were widely used, these also became an art form that reached the widest range of society.[112]
Another important innovation came in the 15th century in Europe, when printmaking began with small woodcuts, mostly religious, that were often very small and hand-colored, and affordable even by peasants who glued them to the walls of their homes. Printed books were initially very expensive, but fell steadily in price until by the 19th century even the poorest could afford some with printed illustrations.[113] Popular prints of many different sorts have decorated homes and other places for centuries.[114]
In 1661, the city of Basel, in Switzerland, opened the first public museum of art in the world, the Kunstmuseum Basel. Today, its collection is distinguished by an impressively wide historic span, from the early 15th century up to the immediate present. Its various areas of emphasis give it international standing as one of the most significant museums of its kind. These encompass: paintings and drawings by artists active in the Upper Rhine region between 1400 and 1600, and on the art of the 19th to 21st centuries.[115]
Public buildings and monuments, secular and religious, by their nature normally address the whole of society, and visitors as viewers, and display to the general public has long been an important factor in their design. Egyptian temples are typical in that the most largest and most lavish decoration was placed on the parts that could be seen by the general public, rather than the areas seen only by the priests.[116] Many areas of royal palaces, castles and the houses of the social elite were often generally accessible, and large parts of the art collections of such people could often be seen, either by anybody, or by those able to pay a small price, or those wearing the correct clothes, regardless of who they were, as at the Palace of Versailles, where the appropriate extra accessories (silver shoe buckles and a sword) could be hired from shops outside.[117]
Most modern public museums and art education programs for children in schools can be traced back to this impulse to have art available to everyone. However, museums do not only provide availability to art, but do also influence the way art is being perceived by the audience, as studies found.[121] Thus, the museum itself is not only a blunt stage for the presentation of art, but plays an active and vital role in the overall perception of art in modern society.
Museums in the United States tend to be gifts from the very rich to the masses. (The Metropolitan Museum of Art in New York City, for example, was created by John Taylor Johnston, a railroad executive whose personal art collection seeded the museum.) But despite all this, at least one of the important functions of art in the 21st century remains as a marker of wealth and social status.[122]
There have been attempts by artists to create art that can not be bought by the wealthy as a status object. One of the prime original motivators of much of the art of the late 1960s and 1970s was to create art that could not be bought and sold. It is "necessary to present something more than mere objects"[123] said the major post war German artist Joseph Beuys. This time period saw the rise of such things as performance art, video art, and conceptual art. The idea was that if the artwork was a performance that would leave nothing behind, or was an idea, it could not be bought and sold. "Democratic precepts revolving around the idea that a work of art is a commodity impelled the aesthetic innovation which germinated in the mid-1960s and was reaped throughout the 1970s. Artists broadly identified under the heading of Conceptual art ... substituting performance and publishing activities for engagement with both the material and materialistic concerns of painted or sculptural form ... [have] endeavored to undermine the art object qua object."[124]
In the decades since, these ideas have been somewhat lost as the art market has learned to sell limited edition DVDs of video works,[125] invitations to exclusive performance art pieces, and the objects left over from conceptual pieces. Many of these performances create works that are only understood by the elite who have been educated as to why an idea or video or piece of apparent garbage may be considered art. The marker of status becomes understanding the work instead of necessarily owning it, and the artwork remains an upper-class activity. "With the widespread use of DVD recording technology in the early 2000s, artists, and the gallery system that derives its profits from the sale of artworks, gained an important means of controlling the sale of video and computer artworks in limited editions to collectors."[126]
Art has long been controversial, that is to say disliked by some viewers, for a wide variety of reasons, though most pre-modern controversies are dimly recorded, or completely lost to a modern view. Iconoclasm is the destruction of art that is disliked for a variety of reasons, including religious ones. Aniconism is a general dislike of either all figurative images, or often just religious ones, and has been a thread in many major religions. It has been a crucial factor in the history of Islamic art, where depictions of Muhammad remain especially controversial. Much art has been disliked purely because it depicted or otherwise stood for unpopular rulers, parties or other groups. Artistic conventions have often been conservative and taken very seriously by art critics, though often much less so by a wider public. The iconographic content of art could cause controversy, as with late medieval depictions of the new motif of the Swoon of the Virgin in scenes of the Crucifixion of Jesus. The Last Judgment by Michelangelo was controversial for various reasons, including breaches of decorum through nudity and the Apollo-like pose of Christ.[127][128]
In the 20th century, Pablo Picasso's Guernica (1937) used arresting cubist techniques and stark monochromatic oils, to depict the harrowing consequences of a contemporary bombing of a small, ancient Basque town. Leon Golub's Interrogation III (1981), depicts a female nude, hooded detainee strapped to a chair, her legs open to reveal her sexual organs, surrounded by two tormentors dressed in everyday clothing. Andres Serrano's Piss Christ (1989) is a photograph of a crucifix, sacred to the Christian religion and representing Christ's sacrifice and final suffering, submerged in a glass of the artist's own urine. The resulting uproar led to comments in the United States Senate about public funding of the arts.[133][134]
The definition and evaluation of art has become especially problematic since the 20th century. Richard Wollheim distinguishes three approaches to assessing the aesthetic value of art: the Realist, whereby aesthetic quality is an absolute value independent of any human view; the Objectivist, whereby it is also an absolute value, but is dependent on general human experience; and the Relativist position, whereby it is not an absolute value, but depends on, and varies with, the human experience of different humans.[136]
The arrival of Modernism in the late 19th century lead to a radical break in the conception of the function of art,[137] and then again in the late 20th century with the advent of postmodernism. Clement Greenberg's 1960 article "Modernist Painting" defines modern art as "the use of characteristic methods of a discipline to criticize the discipline itself".[138] Greenberg originally applied this idea to the Abstract Expressionist movement and used it as a way to understand and justify flat (non-illusionistic) abstract painting:
Pop artists like Andy Warhol became both noteworthy and influential through work including and possibly critiquing popular culture, as well as the art world. Artists of the 1980s, 1990s, and 2000s expanded this technique of self-criticism beyond high art to all cultural image-making, including fashion images, comics, billboards and pornography.[141][142]
Duchamp once proposed that art is any activity of any kind-everything. However, the way that only certain activities are classified today as art is a social construction.[143] There is evidence that there may be an element of truth to this. In The Invention of Art: A Cultural History, Larry Shiner examines the construction of the modern system of the arts, i.e. fine art. He finds evidence that the older system of the arts before our modern system (fine art) held art to be any skilled human activity; for example, Ancient Greek society did not possess the term art, but techne. Techne can be understood neither as art or craft, the reason being that the distinctions of art and craft are historical products that came later on in human history. Techne included painting, sculpting and music, but also cooking, medicine, horsemanship, geometry, carpentry, prophecy, and farming, etc.[144]
Following Duchamp during the first half of the 20th century, a significant shift to general aesthetic theory took place which attempted to apply aesthetic theory between various forms of art, including the literary arts and the visual arts, to each other. This resulted in the rise of the New Criticism school and debate concerning the intentional fallacy. At issue was the question of whether the aesthetic intentions of the artist in creating the work of art, whatever its specific form, should be associated with the criticism and evaluation of the final product of the work of art, or, if the work of art should be evaluated on its own merits independent of the intentions of the artist.[145][146]
In 1946, William K. Wimsatt and Monroe Beardsley published a classic and controversial New Critical essay entitled "The Intentional Fallacy", in which they argued strongly against the relevance of an author's intention, or "intended meaning" in the analysis of a literary work. For Wimsatt and Beardsley, the words on the page were all that mattered; importation of meanings from outside the text was considered irrelevant, and potentially distracting.[147][148]
In another essay, "The Affective Fallacy", which served as a kind of sister essay to "The Intentional Fallacy" Wimsatt and Beardsley also discounted the reader's personal/emotional reaction to a literary work as a valid means of analyzing a text. This fallacy would later be repudiated by theorists from the reader-response school of literary theory. Ironically, one of the leading theorists from this school, Stanley Fish, was himself trained by New Critics. Fish criticizes Wimsatt and Beardsley in his 1970 essay "Literature in the Reader".[149][150]
As summarized by Berys Gaut and Paisley Livingston in their essay "The Creation of Art": "Structuralist and post-structuralists theorists and critics were sharply critical of many aspects of New Criticism, beginning with the emphasis on aesthetic appreciation and the so-called autonomy of art, but they reiterated the attack on biographical criticisms' assumption that the artist's activities and experience were a privileged critical topic."[151] These authors contend that: "Anti-intentionalists, such as formalists, hold that the intentions involved in the making of art are irrelevant or peripheral to correctly interpreting art. So details of the act of creating a work, though possibly of interest in themselves, have no bearing on the correct interpretation of the work."[152]
Gaut and Livingston define the intentionalists as distinct from formalists stating that: "Intentionalists, unlike formalists, hold that reference to intentions is essential in fixing the correct interpretation of works." They quote Richard Wollheim as stating that, "The task of criticism is the reconstruction of the creative process, where the creative process must in turn be thought of as something not stopping short of, but terminating on, the work of art itself."[152]
The end of the 20th century fostered an extensive debate known as the linguistic turn controversy, or the "innocent eye debate" in the philosophy of art. This debate discussed the encounter of the work of art as being determined by the relative extent to which the conceptual encounter with the work of art dominates over the perceptual encounter with the work of art.[153]
Decisive for the linguistic turn debate in art history and the humanities were the works of yet another tradition, namely the structuralism of Ferdinand de Saussure and the ensuing movement of poststructuralism. In 1981, the artist Mark Tansey created a work of art titled The Innocent Eye as a criticism of the prevailing climate of disagreement in the philosophy of art during the closing decades of the 20th century. Influential theorists include Judith Butler, Luce Irigaray, Julia Kristeva, Michel Foucault and Jacques Derrida. The power of language, more specifically of certain rhetorical tropes, in art history and historical discourse was explored by Hayden White. The fact that language is not a transparent medium of thought had been stressed by a very different form of philosophy of language which originated in the works of Johann Georg Hamann and Wilhelm von Humboldt.[154] Ernst Gombrich and Nelson Goodman in his book Languages of Art: An Approach to a Theory of Symbols came to hold that the conceptual encounter with the work of art predominated exclusively over the perceptual and visual encounter with the work of art during the 1960s and 1970s.[155] He was challenged on the basis of research done by the Nobel prize winning psychologist Roger Sperry who maintained that the human visual encounter was not limited to concepts represented in language alone (the linguistic turn) and that other forms of psychological representations of the work of art were equally defensible and demonstrable. Sperry's view eventually prevailed by the end of the 20th century with aesthetic philosophers such as Nick Zangwill strongly defending a return to moderate aesthetic formalism among other alternatives.[156]
 Disputes as to whether or not to classify something as a work of art are referred to as classificatory disputes about art. Classificatory disputes in the 20th century have included cubist and impressionist paintings, Duchamp's Fountain, the movies, J. S. G. Boggs' superlative imitations of banknotes, conceptual art, and video games.[158] Philosopher David Novitz has argued that disagreement about the definition of art are rarely the heart of the problem. Rather, "the passionate concerns and interests that humans vest in their social life" are "so much a part of all classificatory disputes about art."[159] According to Novitz, classificatory disputes are more often disputes about societal values and where society is trying to go than they are about theory proper. For example, when the Daily Mail criticized Hirst's and Emin's work by arguing "For 1,000 years art has been one of our great civilising forces. Today, pickled sheep and soiled beds threaten to make barbarians of us all" they are not advancing a definition or theory about art, but questioning the value of Hirst's and Emin's work.[160] In 1998, Arthur Danto, suggested a thought experiment showing that "the status of an artifact as work of art results from the ideas a culture applies to it, rather than its inherent physical or perceptible qualities. Cultural interpretation (an art theory of some kind) is therefore constitutive of an object's arthood."[161][162]
Anti-art is a label for art that intentionally challenges the established parameters and values of art;[163] it is a term associated with Dadaism and attributed to Marcel Duchamp just before World War I,[163] when he was making art from found objects.[163] One of these, Fountain (1917), an ordinary urinal, has achieved considerable prominence and influence on art.[163] Anti-art is a feature of work by Situationist International,[164] the lo-fi Mail art movement, and the Young British Artists,[163] though it is a form still rejected by the Stuckists,[163] who describe themselves as anti-anti-art.[165][166]
Architecture is often included as one of the visual arts; however, like the decorative arts, or advertising, it involves the creation of objects where the practical considerations of use are essential in a way that they usually are not in a painting, for example.[167]
Somewhat in relation to the above, the word art is also used to apply judgments of value, as in such expressions as "that meal was a work of art" (the cook is an artist), or "the art of deception" (the highly attained level of skill of the deceiver is praised). It is this use of the word as a measure of high quality and high value that gives the term its flavor of subjectivity. Making judgments of value requires a basis for criticism. At the simplest level, a way to determine whether the impact of the object on the senses meets the criteria to be considered art is whether it is perceived to be attractive or repulsive. Though perception is always colored by experience, and is necessarily subjective, it is commonly understood that what is not somehow aesthetically satisfying cannot be art. However, "good" art is not always or even regularly aesthetically appealing to a majority of viewers. In other words, an artist's prime motivation need not be the pursuit of the aesthetic. Also, art often depicts terrible images made for social, moral, or thought-provoking reasons. For example, Francisco Goya's painting depicting the Spanish shootings of 3 May 1808 is a graphic depiction of a firing squad executing several pleading civilians. Yet at the same time, the horrific imagery demonstrates Goya's keen artistic ability in composition and execution and produces fitting social and political outrage. Thus, the debate continues as to what mode of aesthetic satisfaction, if any, is required to define 'art'.[168][169]
The assumption of new values or the rebellion against accepted notions of what is aesthetically superior need not occur concurrently with a complete abandonment of the pursuit of what is aesthetically appealing. Indeed, the reverse is often true, that the revision of what is popularly conceived of as being aesthetically appealing allows for a re-invigoration of aesthetic sensibility, and a new appreciation for the standards of art itself. Countless schools have proposed their own ways to define quality, yet they all seem to agree in at least one point: once their aesthetic choices are accepted, the value of the work of art is determined by its capacity to transcend the limits of its chosen medium to strike some universal chord by the rarity of the skill of the artist or in its accurate reflection in what is termed the zeitgeist. Art is often intended to appeal to and connect with human emotion. It can arouse aesthetic or moral feelings, and can be understood as a way of communicating these feelings. Artists express something so that their audience is aroused to some extent, but they do not have to do so consciously. Art may be considered an exploration of the human condition; that is, what it is to be human.[170] By extension, it has been argued by Emily L. Spratt that the development of artificial intelligence, especially in regard to its uses with images, necessitates a re-evaluation of aesthetic theory in art history today and a reconsideration of the limits of human creativity.[171][172]
An essential legal issue are art forgeries, plagiarism, replicas and works that are strongly based on other works of art.
A chess problem, also called a chess composition, is a puzzle set by the composer using chess pieces on a chess board, which presents the solver with a particular task. For instance, a position may be given with the instruction that White is to move first, and checkmate Black in two moves against any possible defence. A chess problem fundamentally differs from over-the-board play in that the latter involves a struggle between Black and White, whereas the former involves a competition between the composer and the solver. Most positions which occur in a chess problem are 'unrealistic' in the sense that they are very unlikely to occur in over-the-board play.[1] There is a good deal of specialized jargon used in connection with chess problems.
The term "chess problem" is not sharply defined: there is no clear demarcation between chess compositions on the one hand and puzzles or tactical exercises on the other. In practice, however, the distinction is very clear.  There are common characteristics shared by compositions in the problem section of chess magazines, in specialist chess problem magazines, and in collections of chess problems in book form.[1]
Not every chess problem has every one of these features, but most have several:
Problems can be contrasted with tactical puzzles often found in chess columns or magazines in which the task is to find the best move or sequence of moves (usually leading to mate or gain of material) from a given position. Such puzzles are often taken from actual games, or at least have positions which look as if they could have arisen during a game, and are used for instructional purposes.  Most such puzzles fail to exhibit the above features.
In all the above types of problem, castling is assumed to be allowed unless it can be proved by retrograde analysis (see below) that the rook in question or king must have previously moved. En passant captures, on the other hand, are assumed not to be legal, unless it can be proved that the pawn to be captured must have moved two squares on the previous move.
There are several other types of chess problem which do not fall into any of the above categories. Some of these are really coded mathematical problems, expressed using the geometry and pieces of the chessboard. A famous such problem is the knight's tour, in which one is to determine the path of a knight that visits each square of the board exactly once.  Another is the eight queens problem, in which eight queens are to be placed on the board so that none is attacking any of the others.
Of far greater relation to standard chess problems, however, are the following, which have a rich history and have been revisited many times, with magazines, books and prizes dedicated to them:
Across most of the above genres, there is great interest in exploring fairy chess, in which non-standard boards, pieces or rules apply.
The role of aesthetic evaluation in the appreciation of chess problems is very significant, and indeed most composers and solvers consider such compositions to be an art form. Vladimir Nabokov wrote about the "originality, invention, conciseness, harmony, complexity, and splendid insincerity" of creating chess problems and spent considerable time doing so. There are no official standards by which to distinguish a beautiful problem from a poor one and such judgments can vary from individual to individual as well as from generation to generation.  Such variation is to be expected when it comes to aesthetic appraisal.  Nevertheless, modern taste generally recognises the following elements to be important in the aesthetic evaluation of a problem:
To the right is a directmate problem composed by Thomas Taverner in 1881.
The thematic approach to solving is to notice then that in the original position, Black is already almost in zugzwang.  If Black were compelled to play first, only Re3 and Bg5 would not allow immediate mate. However, each of those two moves blocks a flight square for the black king, and once White has removed his rook from h2 White can put some other piece on that square to deliver mate: 1...Re3 2.Bh2# and 1...Bg5 2.Qh2#.
Although most problems call for straightforward (though possibly difficult) solution, occasionally a problem will involve a humorous trick or twist. The problem at right, shown in Norwegian broadcaster NRK's airings from the World Championships in Dubai 2021, calls for White to move and give immediate checkmate in just a single move. The trick is to recognize that despite the arrangement of the Black men, the board is actually viewed from the White side (as shown by the Black king standing on a square of its own color, rather than on the opposite color as in the standard opening position). Thus the solution is 1 Nd3#; the Black pawns are moving down the board and cannot capture the White knight.
For reasons of space and internationality, various abbreviations are often used in chess problem journals to indicate a problem's stipulation (whether it is a mate in two, helpmate in four, or whatever). The most common are:
These are combined with a number to indicate in how many moves the goal must be achieved. "#3", therefore, indicates a mate in three, while "ser-h=14" indicates a series help stalemate in 14 (i.e., Black makes 14 moves in a row such that White can subsequently make one move to deliver stalemate).
In studies, the symbols "+" and "=" are used to indicate "White to play and win" and "White to play and draw" respectively.
Various tournaments (or tourneys) exist for both the composition and solving of chess problems.
Composition tourneys may be formal or informal. In formal tourneys, the competing problems are not published before they are judged, while in informal tourneys they are. Informal tourneys are often run by problem magazines and other publications with a regular problem section; it is common for every problem to have been published in a particular magazine within a particular year to be eligible for an informal award. Formal tourneys are often held to commemorate a particular event or person. The World Chess Composing Tournament (WCCT) is a formal tourney for national teams organised by the Permanent Commission of the FIDE for Chess Compositions (PCCC).
In both formal and informal tourneys, entries will normally be limited to a particular genre of problem (for example, mate in twos, moremovers, helpmates) and may or may not have additional restrictions (for example, problems in patrol chess, problems showing the Lacny theme, problems using fewer than nine units). Honours are usually awarded in three grades: these are, in descending order of merit, prizes, honourable mentions, and commendations. As many problems as the judge sees fit may be placed in each grade, and the problems within each grade may or may not be ranked (so an award may include a 1st Honourable Mention, a 2nd Honourable Mention, and a 3rd Honourable Mention, or just three unranked Honourable Mentions).
After an award is published, there is a period (typically around three months) in which individuals may claim honoured problems are anticipated (that is, that an identical problem, or nearly so, had been published at an earlier date) or unsound (i.e., that a problem has cooks or no solution). If such claims are upheld, the award may be adjusted accordingly. At the end of this period, the award becomes final. It is normal to indicate any honour a problem has received when it is republished.
Solving tournaments also fall into two main types. In tourneys conducted by correspondence, the participants send their entries by post or e-mail. These are often run on similar terms to informal composition tourneys; indeed, the same problems which are entries in the informal composition tourney are often also set in the solving tourney. It is impossible to eliminate the use of computers in such tournaments, though some problems, such as those with particularly long solutions, will not be well-suited to solution by computer.
Other solving tourneys are held with all participants present at a particular time and place. They have only a limited amount of time to solve the problems, and the use of any solving aid other than a chess set is prohibited. The most notable tournament of this type is the World Chess Solving Championship, organised by the PCCC.
In both types of tourney, each problem is worth a specified number of points, often with bonus points for finding cooks or correctly claiming no solution. Incomplete solutions are awarded an appropriate proportion of the points available. The solver amassing the most points is the winner.
Just as in over-the-board play, the titles International Grandmaster, International Master and FIDE Master are awarded by FIDE via the Permanent Commission of the FIDE for Chess Compositions (PCCC) for especially distinguished problem and study composers and solvers (unlike over-the-board chess, however, there have not been any women-only equivalents to these titles in problem chess).
For solvers, the GM and IM titles were both first awarded in 1982; the FM title followed in 1997. GM and IM titles can only be gained by participating in the official World Chess Solving Championship (WCSC): to become a GM, a solver must score at least 90 percent of the winner's points and on each occasion finish in at least tenth place three times within ten successive WCSCs. For the IM title they must score at least 80 percent of the winner's points and each time finish in at least fifteenth place twice within five successive WCSCs; alternatively, winning a single WCSC or scoring as many points as the winner in a single WCSC will earn the IM title. For the FM title, the solver must score at least 75 percent of the winners points and each time finish within the top 40 percent of participants in any two PCCC-approved solving competitions.
The title International Judge of Chess Compositions is given to individuals considered capable of judging composing tourneys at the highest level.
Western culture, also known as Western civilization, Occidental culture, or Western society, is the heritage of social norms, ethical values, traditional customs, belief systems, political systems, artifacts and technologies of the Western world. The term applies beyond Europe to countries and cultures whose histories are strongly connected to Europe by immigration, colonization or influence. Western culture is most strongly influenced by Greco-Roman culture, Germanic culture, and Christian culture.[1]
The expansion of Greek culture into the Hellenistic world of the eastern Mediterranean led to a synthesis between Greek and Near-Eastern cultures,[2] and major advances in literature, engineering, and science, and provided the culture for the expansion of early Christianity and the Greek New Testament.[3][4][5] This period overlapped with and was followed by Rome, which made key contributions in law, government, engineering and political organization.[6]
Western culture is characterized by a host of artistic, philosophic, literary and legal themes and traditions. Christianity, primarily the Roman Catholic Church,[7][8][9] and later Protestantism[10][11][12][13] has played a prominent role in the shaping of Western civilization since at least the 4th century,[14][15][16][17][18] as did Judaism.[19][20][21][22] A cornerstone of Western thought, beginning in ancient Greece and continuing through the Middle Ages and Renaissance, is the idea of rationalism in various spheres of life developed by Hellenistic philosophy, scholasticism and humanism. Empiricism later gave rise to the scientific method, the scientific revolution, and the Age of Enlightenment.
The West as a geographical area is unclear and undefined. There is some disagreement about which nations should or should not be included in the category, when, and why. Certainly related conceptual terminology has changed over time in scope, meaning, and use. The term "western" draws on an affiliation with, and/or a perception of, a shared philosophy, worldview, political, and religious heritage grounded in the Greco-Roman world, the Legacy of the Roman Empire, and medieval concepts of Christendom. For example, whether the Eastern Roman Empire (anachronistically/controversially referred to as the Byzantine Empire), or those countries heavily influenced by its legacy, should be counted as "Western" is an example of the possible ambiguity of the term. These questions can be traced back to the affiliatory nature of Roman culture to that of Classical Greece, a persistent Greek East and Latin West language split within the Roman Empire, and an eventual permanent splitting of the Roman Empire into Western and Eastern halves. And perhaps, at its worst, culminating in Pope Leo III's transfer of the Roman Empire from the Eastern Roman Empire to the Frankish King Charlemagne in the form of the Holy Roman Empire, the Great Schism, and the devastating Fourth Crusade. Conversely, traditions of scholarship around Plato, Aristotle, and Euclid had been forgotten in the Catholic west and were rediscovered by Italians from scholars fleeing the fall of the Eastern Roman Empire.[26] The resulting Renaissance was a conscious effort by Europeans to revive and surpass the ideas and achievements of the Greco-Roman world, eventually leading to the Age of Discovery, the Scientific Revolution, and the subsequent Age of Enlightenment. Similarly, complicated relationships between virtually all the countries and regions within a broadly defined "West" can be discussed due to a persistently fragmented political landscape resulting in a lack of uniformity and significant diversity between the various cultures affiliating with this shared socio-cultural heritage. Thus, those cultures identifying with the West and what it means to be western changes over time as the geopolitical circumstances of a place changes and what is meant by the term changes.
It has been disputed by some philosophers whether Western culture can be considered a historically sound, unified body of thought.[43] For example, Kwame Anthony Appiah points out that many of the fundamental influences on Western culture, such as those of Greek philosophy are also shared by the Islamic world to a certain extent.[43] Appiah argues that the origin of the Western and European identity can be traced back to the Muslim invasion of Iberia where Christians would form a common Christian or European identity.[43] Contemporary Latin chronicles from Spain described the victors in the Frankish victory over the Umayyads at the Battle of Tours as Europeans according to Appiah, denoting a shared sense of identity.[43]
The Greeks contrasted themselves with both their Eastern neighbours (such as the Trojans in Iliad) as well as their Northern neighbours (who they considered barbarians).[citation needed] Concepts of what is the West arose out of legacies of the Western and the Eastern Roman Empire. Later, ideas of the West were formed by the concepts of Latin Christendom and the Holy Roman Empire. What is thought of as Western thought today originates primarily from Greco-Roman and Christian traditions, with varying degrees of influence from the Germanic, Celtic and Slavic peoples, and includes the ideals of the Middle Ages, the Renaissance, Reformation and the Enlightenment.[47]
While the concept of a "West" did not exist until the emergence of the Roman Republic, the roots of the concept can be traced back to Ancient Greece. Since Homeric literature (the Trojan Wars), through the accounts of the Persian Wars of Greeks against Persians by Herodotus, and right up until the time of Alexander the Great, there was a paradigm of a contrast between Greeks and other civilizations.[48] Greeks felt they were the most civilized and saw themselves (in the formulation of Aristotle) as something between the advanced civilizations of the Near East (who they viewed as soft and slavish) and the wild barbarians of most of Europe to the north. During this period writers like Herodotus and Xenophon would highlight the importance of freedom in the Ancient Greek world, as opposed to the perceived slavery of the so-called barbaric world.[48]
Alexander's conquests led to the emergence of a Hellenistic civilization, representing a synthesis of Greek and Near-Eastern cultures in the Eastern Mediterranean region.[2] The Near-Eastern civilizations of Ancient Egypt and the Levant, which came under Greek rule, became part of the Hellenistic world. The most important Hellenistic centre of learning was Ptolemaic Egypt, which attracted Greek, Egyptian, Jewish, Persian, Phoenician and even Indian scholars.[49] Hellenistic science, philosophy, architecture, literature and art later provided a foundation embraced and built upon by the Roman Empire as it swept up Europe and the Mediterranean world, including the Hellenistic world in its conquests in the 1st century BCE.
Following the Roman conquest of the Hellenistic world, the concept of a "West" arose, as there was a cultural divide between the Greek East and Latin West. The Latin-speaking Western Roman Empire consisted of Western Europe and Northwest Africa, while the Greek-speaking Eastern Roman Empire consisted of the Balkans, Asia Minor, Egypt and Levant. The "Greek" East was generally wealthier and more advanced than the "Latin" West.[citation needed] With the exception of Italia, the wealthiest provinces of the Roman Empire were in the East, particularly Roman Egypt which was the wealthiest Roman province outside of Italia.[50][51] Nevertheless, the Celts in the West created some significant literature in the ancient world whenever they were given the opportunity (an example being the poet Caecilius Statius), and they developed a large amount of scientific knowledge themselves (as seen in their Coligny Calendar).
From the time of Alexander the Great (the Hellenistic period), Greek civilization came in contact with Jewish civilization. Christianity would eventually emerge from the syncretism of Hellenic culture, Roman culture, and Second Temple Judaism, gradually spreading across the Roman Empire and eclipsing its antecedents and influences.[52] The rise of Christianity reshaped much of the Greco-Roman tradition and culture; the Christianised culture would be the basis for the development of Western civilization after the fall of Rome (which resulted from increasing pressure from barbarians outside Roman culture). Roman culture also mixed with Celtic, Germanic, and Slavic cultures, which slowly became integrated into Western culture: starting mainly with their acceptance of Christianity.
The Medieval West referred specifically to the Catholic "Latin" West, also called "Frankish" during Charlemagne's reign, in contrast to the Orthodox East, where Greek remained the language of the Byzantine Empire.
After the fall of Rome, much of Greco-Roman art, literature, science and even technology were all but lost in the western part of the old empire. However, this would become the center of a new West. Europe fell into political anarchy, with many warring kingdoms and principalities. Under the Frankish kings, it eventually, and partially, reunified, and the anarchy evolved into feudalism.
Much of the basis of the post-Roman cultural world had been set before the fall of the Western Roman Empire, mainly through the integration and reshaping of Roman ideas through Christian thought. The Greek and Roman paganism was gradually replaced by Christianity, first with its legalisation with the Edict of Milan and then the Edict of Thessalonica which made it the State church of the Roman Empire. Roman Catholic Christianity, served as a unifying force in Christian parts of Europe, and in some respects replaced or competed with the secular authorities. The Jewish Christian tradition out of which it had emerged was all but extinguished, and antisemitism became increasingly entrenched or even integral to Christendom.[55][56] Much of art and literature, law, education, and politics were preserved in the teachings of the Church. The Eastern Orthodox Church founded many cathedrals, monasteries and seminaries, some of which continue to exist today.
After the fall of the Roman Empire, many of the classical Greek texts were translated into Arabic and preserved in the medieval Islamic world. The Greek classics along with Arabic science, philosophy and technology were transmitted to Western Europe and translated into Latin, sparking the Renaissance of the 12th century and 13th century.[23][24][25]
Medieval Christianity is credited with creating the first modern universities.[28][29] The Catholic Church established a hospital system in Medieval Europe that vastly improved upon the Roman valetudinaria[57] and Greek healing temples.[58] These hospitals were established to cater to "particular social groups marginalized by poverty, sickness, and age," according to the historian of hospitals, Guenter Risse.[30] Christianity played a role in ending practices common among pagan societies, such as human sacrifice, slavery,[59] infanticide and polygamy.[34] Francisco de Vitoria, a disciple of Thomas Aquinas and a Catholic thinker who studied the issue regarding the human rights of colonized natives, is recognized by the United Nations as a father of international law, and now also by historians of economics and democracy as a leading light for the West's democracy and rapid economic development.[60] Joseph Schumpeter, an economist of the twentieth century, referring to the Scholastics, wrote, "it is they who come nearer than does any other group to having been the 'founders' of scientific economics."[31]
In a broader sense, the Middle Ages, with its fertile encounter between Greek philosophical reasoning and Levantine monotheism was not confined to the West but also stretched into the old East. The philosophy and science of Classical Greece were largely forgotten in Europe after the collapse of the Western Roman Empire, other than in isolated monastic enclaves (notably in Ireland, which had become Christian but was never conquered by Rome).[61] The learning of Classical Antiquity was better preserved in the Eastern Roman Empire. Justinian's Corpus Juris Civilis Roman civil law code was created in the East in his capital of Constantinople,[62] and that city maintained trade and intermittent political control over outposts such as Venice in the West for centuries. Classical Greek learning was also subsumed, preserved, and elaborated in the rising Eastern world, which gradually supplanted Roman-Byzantine control as a dominant cultural-political force. Thus, much of the learning of classical antiquity was slowly reintroduced to European civilization in the centuries following the collapse of the Western Roman Empire.
The rediscovery of the Justinian Code in Western Europe early in the 10th century rekindled a passion for the discipline of law, which crossed many of the re-forming boundaries between East and West. In the Catholic or Frankish west, Roman law became the foundation on which all legal concepts and systems were based. Its influence is found in all Western legal systems, although in different manners and to different extents. The study of canon law, the legal system of the Catholic Church, fused with that of Roman law to form the basis of the refounding of Western legal scholarship. During the Reformation and Enlightenment, the ideas of civil rights, equality before the law, procedural justice, and democracy as the ideal form of society began to be institutionalized as principles forming the basis of modern Western culture, particularly in Protestant regions.
In the 14th century, starting from Italy and then spreading throughout Europe,[63] there was a massive artistic, architectural, scientific and philosophical revival, as a result of the Christian revival of Greek philosophy, and the long Christian medieval tradition that established the use of reason as one of the most important of human activities.[64] This period is commonly referred to as the Renaissance. In the following century, this process was further enhanced by an exodus of Greek Christian priests and scholars to Italian cities such as Florence and Venice after the end of the Byzantine Empire with the fall of Constantinople.
From the late 15th century to the 17th century, Western culture began to spread to other parts of the world through explorers and missionaries during the Age of Discovery, and by imperialists from the 17th century to the early 20th century. During the Great Divergence, a term coined by Samuel Huntington[73] the Western world overcame pre-modern growth constraints and emerged during the 19th century as the most powerful and wealthy world civilization of the time, eclipsing Qing China, Mughal India, Tokugawa Japan, and the Ottoman Empire. The process was accompanied and reinforced by the Age of Discovery and continued into the modern period. Scholars have proposed a wide variety of theories to explain why the Great Divergence happened, including lack of government intervention, geography, colonialism, and customary traditions.
The Age of Discovery faded into the Age of Enlightenment of the 18th century, during which cultural and intellectual forces in European society emphasized reason, analysis, and individualism rather than traditional lines of authority. It challenged the authority of institutions that were deeply rooted in society, such as the Catholic Church; there was much talk of ways to reform society with toleration, science and skepticism.
Coinciding with the Age of Enlightenment was the scientific revolution, spearheaded by Newton. This included the emergence of modern science, during which developments in mathematics, physics, astronomy, biology (including human anatomy) and chemistry transformed views of society and nature.[75][76][77][78][79][80][excessive citations] While its dates are disputed, the publication in 1543 of Nicolaus Copernicus's De revolutionibus orbium coelestium (On the Revolutions of the Heavenly Spheres) is often cited as marking the beginning of the scientific revolution, and its completion is attributed to the "grand synthesis" of Newton's 1687 Principia.
The Industrial Revolution was the transition to new manufacturing processes in the period from about 1760 to sometime between 1820 and 1840. This included going from hand production methods to machines, new chemical manufacturing and iron production processes, improved efficiency of water power, the increasing use of steam power, and the development of machine tools.[81] These transitions began in Great Britain and spread to Western Europe and North America within a few decades.[82]
The Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists say that the major impact of the Industrial Revolution was that the standard of living for the general population began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.[84][85][86] The precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes.[87][88][89][90] GDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy,[91] while the Industrial Revolution began an era of per-capita economic growth in capitalist economies.[92] Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals, plants[93] and fire.
The First Industrial Revolution evolved into the Second Industrial Revolution in the transition years between 1840 and 1870, when technological and economic progress continued with the increasing adoption of steam transport (steam-powered railways, boats, and ships), the large-scale manufacture of machine tools and the increasing use of machinery in steam-powered factories.[94][95][96]
Tendencies that have come to define modern Western societies include the concept of political pluralism, individualism, prominent subcultures or countercultures (such as New Age movements) and increasing cultural syncretism resulting from globalization and human migration. Western culture has been heavily influenced by the Renaissance, the Ages of Discovery and Enlightenment and the Industrial and Scientific Revolutions.[97][98]
In the 20th century, Christianity declined in influence in many Western countries, mostly in the European Union where some member states have experienced falling church attendance and membership in recent years,[99] and also elsewhere. Secularism (separating religion from politics and science) increased. Christianity remains the dominant religion in the Western world, where 70% are Christians.[100]
The West went through a series of great cultural and social changes between 1945 and 1980. The emergent mass media (film, radio, television and recorded music) created a global culture that could ignore national frontiers. Literacy became almost universal, encouraging the growth of books, magazines and newspapers. The influence of cinema and radio remained, while televisions became near essentials in every home.
By the mid-20th century, Western culture was exported worldwide, and the development and growth of international transport and telecommunication (such as transatlantic cable and the radiotelephone) played a decisive role in modern globalization. The West has contributed a great many technological, political, philosophical, artistic and religious aspects to modern international culture: having been a crucible of Catholicism, Protestantism, democracy, industrialisation; the first major civilisation to seek to abolish slavery during the 19th century, the first to enfranchise women (beginning in Australasia at the end of the 19th century) and the first to put to use such technologies as steam, electric and nuclear power. The West invented cinema, television, the personal computer and the Internet; developed sports such as soccer, cricket, golf, tennis, rugby, basketball, and volleyball; and transported humans to an astronomical object for the first time with the 1969 Apollo 11 Moon Landing.
It was, in recent history, and remains, the dominant power and director of human civilization.
While dance, music, visual art, story-telling, and architecture are human universals, they are expressed in the West in certain characteristic ways.[101]
In Western dance, music, plays and other arts, the performers are only very infrequently masked. There are essentially no taboos against depicting a god, or other religious figures, in a representational fashion.
In music, Catholic monks developed the first forms of modern Western musical notation to standardize liturgy throughout the worldwide Church,[102] and an enormous body of religious music has been composed for it through the ages. This led directly to the emergence and development of European classical music and its many derivatives. The Baroque style, which encompassed music, art, and architecture, was particularly encouraged by the post-Reformation Catholic Church as such forms offered a means of religious expression that was stirring and emotional, intended to stimulate religious fervor.[103]
The symphony, concerto, sonata, opera, and oratorio have their origins in Italy. Many musical instruments developed in the West have come to see widespread use all over the world; among them are the guitar, violin, piano, pipe organ, saxophone, trombone, clarinet, accordion, and the theremin. In turn, it has been claimed that some European instruments have roots in earlier Eastern instruments that were adopted from the medieval Islamic world.[104] The solo piano, symphony orchestra, and the string quartet are also significant musical innovations of the West.
Jan van Eyck, among other renaissance painters, made great advances in oil painting, and perspective drawings and paintings had their earliest practitioners in Florence.[105] In art, the Celtic knot is a very distinctive Western repeated motif. Depictions of the nude human male and female in photography, painting, and sculpture are frequently considered to have special artistic merit. Realistic portraiture is especially valued.
Photography and the motion picture as both a technology and basis for entirely new art forms were also developed in the West.
The ballet is a distinctively Western form of performance dance.[106] The ballroom dance is an important Western variety of dance for the elite. The polka, the square dance, the flamenco, and the Irish step dance are very well known Western forms of folk dance.
Greek and Roman theatre are considered the antecedents of modern theatre, and forms such as medieval theatre, Passion Plays, morality plays, and commedia dell'arte are considered highly influential. Elizabethan theatre, with playwrights including William Shakespeare, Christopher Marlowe, and Ben Jonson, is considered one of the most formative and important eras for modern drama.
The soap opera, a popular culture dramatic form, originated in the United States first on radio in the 1930s, then a couple of decades later on television. The music video was also developed in the West in the middle of the 20th century. Musical theatre was developed in the West in the 19th and 20th Centuries, from music hall, comic opera, and Vaudeville; with significant contributions from the Jewish diaspora, African-Americans, and other marginalized peoples.[107][108][109]
Western literature encompasses literary traditions of Europe, as well as Northern America and Latin America.[110]
The novel, which made its appearance in the 18th century, is an essentially European creation. Chinese and Japanese literature contain some works that may be thought of as novels, but only the European novel is couched in terms of a personal analysis of personal dilemmas.[101]
As in its artistic tradition, European literature pays deep tribute to human suffering.[101] Tragedy, from its ritually and mythologically inspired Greek origins to modern forms where struggle and downfall are often rooted in psychological or social, rather than mythical, motives, is also widely considered a specifically European creation and can be seen as a forerunner of some aspects of both the novel and of classical opera.
Important Western architectural motifs include the Doric, Corinthian, and Ionic orders of Greek architecture,[112] and the Romanesque, Gothic, Baroque, and Victorian styles, which are still widely recognized and used in contemporary Western architecture. Much of Western architecture emphasizes repetition of simple motifs, straight lines and expansive, undecorated planes. A modern ubiquitous architectural form that emphasizes this characteristic is the skyscraper, their modern equivalent first developed in New York and Chicago. The predecessor of the skyscraper can be found in the medieval towers erected in Bologna.
Stained glass windows of the Sainte-Chapelle in Paris, completed in 1248, mostly constructed between 1194 and 1220 in the Gothic style
The Palazzo Farnese, in Rome, built from 1534 to 1545, was designed by Sangallo and Michelangelo and is an important example of renaissance architecture
The Palais Garnier in Paris, built between 1861 and 1875, a Beaux-Arts masterpiece
Western foodways were, until recently, considered to have their roots in the cuisines of Classical Rome and Greece, but the influence of Arab and Near Eastern cuisine on the West has become a topic of research in recent decades. The Crusaders, known mostly for fighting over holy land, settled in the Levant and acclimated to the local culture and cuisine. Fulcher of Chartres said "For we who were occidentals have now become orientals." These cultural experiences, carried back to France by notables like Eleanor of Aquitaine influenced Western European foodways. Many Oriental ingredients were relatively new to the Western lands. Sugar, almonds, pistachios, rosewater, and dried citrus fruits were all novelties to the Crusaders who encountered them in Saracen lands. Pepper, ginger and cinnamon were the most widely used spices of the European courts and noble households. By the end of the Middle Ages cloves, nutmeg, mastic, galingale and other imported spices had become part of the Western cuisine.[113]
Saracen influence can be seen in medieval cookbooks. Some recipes retain their Arabic names in Italian translations of the Liber de Coquina. Known as bruet Sarassinois in the cuisine of North France, the concept of sweet and sour sauce is attested to in Greek tradition when Anthimus finishes his stew with vinegar and honey. Saracens combined sweet ingredients like date-juice and honey with pomegranate, lemons and citrus juices, or other sour ingredients. The technique of browning pieces of meat and simmering in liquid with vegetables is used in many recipes from the Baghdad cookery book. The same technique appears in the late-13th century Viandier. Fried pieces of beef simmered in wine with sugar and cloves was called bruet of Sarcynesse in English.[113]
A notable feature of Western culture is its strong emphasis and focus on innovation and invention through science and technology, and its ability to generate new processes, materials and material artifacts with its roots dating back to the Ancient Greeks. The scientific method as "a method or procedure that has characterized natural science since the 17th century, consisting in systematic observation, measurement, and experiment, and the formulation, testing, and modification of hypotheses" was fashioned by the 17th-century Italian Galileo Galilei,[115][116] with roots in the work of medieval scholars such as the 11th-century Iraqi physicist Ibn al-Haytham[117][118] and the 13th-century English friar Roger Bacon.[119]
By the will of the Swedish inventor Alfred Nobel the Nobel Prizes were established in 1895. The prizes in Chemistry, Literature, Peace, Physics, and Physiology or Medicine were first awarded in 1901.[120] The percentage of ethnically European Nobel prize winners during the first and second halves of the 20th century were respectively 98 and 94 percent.[121]
The West is credited with the development of the steam engine and adapting its use into factories, and for the generation of electric power.[122] The electrical motor, dynamo, transformer, electric light, and most of the familiar electrical appliances, were inventions of the West.[123][124][125][126] The Otto and the Diesel internal combustion engines are products whose genesis and early development were in the West.[127][128] Nuclear power stations are derived from the first atomic pile constructed in Chicago in 1942.[129]
Communication devices and systems including the telegraph, the telephone, radio, television, communications and navigation satellites, mobile phone, and the Internet were all invented by Westerners.[130][131][132][133][134][135][136][137] The pencil, ballpoint pen, Cathode ray tube, liquid-crystal display, light-emitting diode, camera, photocopier, laser printer, ink jet printer, plasma display screen and World Wide Web were also invented in the West.[138][139][140][141][142]
Ubiquitous materials including aluminum, clear glass, synthetic rubber, synthetic diamond and the plastics polyethylene, polypropylene, polyvinyl chloride and polystyrene were discovered and developed or invented in the West. Iron and steel ships, bridges and skyscrapers first appeared in the West. Nitrogen fixation and petrochemicals were invented by Westerners. Most of the elements were discovered and named in the West, as well as the contemporary atomic theories to explain them.[143][144][145][146][147][148][149][150]
The transistor, integrated circuit, memory chip, first programming language and computer were all first seen in the West. The ship's chronometer, the screw propeller, the locomotive, bicycle, automobile, and airplane were all invented in the West. Eyeglasses, the telescope, the microscope and electron microscope, all the varieties of chromatography, protein and DNA sequencing, computerised tomography, nuclear magnetic resonance, x-rays, and light, ultraviolet and infrared spectroscopy, were all first developed and applied in Western laboratories, hospitals and factories.[citation needed]
In medicine, the pure antibiotics were created in the West. The method of preventing Rh disease, the treatment of diabetes, and the germ theory of disease were discovered by Westerners. The eradication of smallpox, was led by a Westerner, Donald Henderson. Radiography, computed tomography, positron emission tomography and medical ultrasonography are important diagnostic tools developed in the West. Other important diagnostic tools of clinical chemistry, including the methods of spectrophotometry, electrophoresis and immunoassay, were first devised by Westerners. So were the stethoscope, the electrocardiograph, and the endoscope. Vitamins, hormonal contraception, hormones, insulin, beta blockers and ACE inhibitors, along with a host of other medically proven drugs, were first used to treat disease in the West. The double-blind study and evidence-based medicine are critical scientific techniques widely used in the West for medical purposes.[citation needed]
The world's most widely adopted system of measurement, the International System of Units, derived from the metric system, was first developed in France and evolved through contributions from various Westerners.[158][159]
In business, economics, and finance, double entry bookkeeping, credit cards, and the charge card were all first used in the West.[160][161]
Westerners are also known for their explorations of the globe and outer space. The first expedition to circumnavigate the Earth (1522) was by Westerners, as well as the first journey to the South Pole (1911), and the first Moon landing (1969).[162][163] The landing of robots on Mars (2004 and 2012) and on an asteroid (2001), the Voyager 2 explorations of the outer planets (Uranus in 1986 and Neptune in 1989), Voyager 1's passage into interstellar space (2013), and New Horizons' flyby of Pluto (2015) were significant recent Western achievements.[164][165][166][167][168]
The roots of modern-day Western mass media can be traced back to the late 15th century, when printing presses began to operate throughout wealthy European cities. The emergence of news media in the 17th century has to be seen in close connection with the spread of the printing press, from which the publishing press derives its name.[169]
In the 16th century, a decrease in the preeminence of Latin in its literary use, along with the impact of economic change, the discoveries arising from trade and travel, navigation to the New World, science and arts and the development of increasingly rapid communications through print led to a rising corpus of vernacular media content in European society.[170]
After the launch of the satellite Sputnik 1 by the Soviet Union in 1957, satellite transmission technology was dramatically realised, with the United States launching Telstar in 1962 linking live media broadcasts from the UK to the US. The first digital broadcast satellite (DBS) system began transmitting in US in 1975.[171]
Beginning in the 1990s, the Internet has contributed to a tremendous increase in the accessibility of Western media content. Departing from media offered in bundled content packages (magazines, CDs, television and radio slots), the Internet has primarily offered unbundled content items (articles, audio and video files).[172]
Western culture at some level is influenced by the Judeo-Christian-Islamic and Greco-Roman traditions.[1] These cultures had a number of similarities, such as a common emphasis on the individual, but they also embody fundamentally conflicting worldviews. For example, in Judaism and Christianity, God is the ultimate authority, while Greco-Roman tradition considers the ultimate authority to be reason. Christian attempts to reconcile these frameworks were responsible for the preservation of Greek philosophy.[1] Historically, Europe has been the center and cradle of Christian civilization.[173][174][175][176]
2012 Eurobarometer polls about religiosity in the European Union in 2012 found that Christianity was the largest religion in the European Union, accounting for 72% of the EU population.[181] Catholics are the largest Christian group, accounting for 48% of the EU citizens, while Protestants make up 12%, Eastern Orthodox make up 8% and other Christians make up 4%.[182] Non-believers/Agnostics account for 16%,[181] atheists account for 7%,[181] and Muslims account for 2%.[181]
As in other areas, the Jewish diaspora and Judaism exist in the Western world.
There are also small but increasing numbers of people across the Western world who seek to revive the indigenous religions of their European ancestors; such groups include Germanic, Roman, Hellenic, Celtic, Slavic, and polytheistic reconstructionist movements. Likewise, Wicca, New Age spirituality and other neo-pagan belief systems enjoy notable minority support in Western states.
Since classical antiquity, sport has been an important facet of Western cultural expression.[188][189]
A wide range of sports was already established by the time of Ancient Greece and the military culture and the development of sports in Greece influenced one another considerably. Sports became such a prominent part of their culture that the Greeks created the Olympic Games, which in ancient times were held every four years in a small village in the Peloponnesus called Olympia. Baron Pierre de Coubertin, a Frenchman, instigated the modern revival of the Olympic movement. The first modern Olympic games were held at Athens in 1896.
Jousting and hunting were popular sports in the European Middle Ages, and the aristocratic classes developed passions for leisure activities. A great number of popular global sports were first developed or codified in Europe. The modern game of golf originated in Scotland, where the first written record of golf is James II's banning of the game in 1457, as an unwelcome distraction to learning archery.[191]
The Industrial Revolution that began in Great Britain in the 18th century brought increased leisure time, leading to more opportunities for citizens to participate in athletic activities and also follow spectator sports. These trends continued with the advent of mass media and global communication. The bat and ball sport of cricket was first played in England during the 16th century and was exported around the globe via the British Empire. A number of popular modern sports were devised or codified in the United Kingdom during the 19th century and obtained global prominence; these include ping pong, modern tennis, association football, netball and rugby.[192]
Football (or soccer) remains hugely popular in Europe, but has grown from its origins to be known as the world game. Similarly, sports such as cricket, rugby, and netball were exported around the world, particularly among countries in the Commonwealth of Nations, thus India and Australia are among the strongest cricketing states, while victory in the Rugby World Cup has been shared among New Zealand, Australia, England, and South Africa.
Australian Rules Football, an Australian variation of football with similarities to Gaelic football and rugby, evolved in the British colony of Victoria in the mid-19th century. The United States also developed unique variations of English sports. English migrants took antecedents of baseball to America during the colonial period. The history of American football can be traced to early versions of rugby football and association football. Many games are known as "football" were being played at colleges and universities in the United States in the first half of the 19th century. American football resulted from several major divergences from rugby, most notably the rule changes instituted by Walter Camp, the "Father of American football". Basketball was invented in 1891 by James Naismith, a Canadian physical education instructor working in Springfield, Massachusetts, in the United States. Volleyball was created in Holyoke, Massachusetts, a city directly north of Springfield, in 1895.
Western culture has developed many themes and traditions, the most significant of which are:[citation needed]
At the same time, then as the printing press in the physical technological sense was invented, 'the press' in the extended sense of the word also entered the historical stage. The phenomenon of publishing was now born.
Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory,[1] algebra,[2] geometry,[1] and analysis,[3][4] respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.
Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications.[6][7] The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.
Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements.[8] Since its beginning, mathematics was essentially divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra[a] and infinitesimal calculus were introduced as new areas. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid lockstep increase in the development of both.[9] At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method,[10] which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics.
In Latin, and in English until around 1700, the term mathematics more commonly meant "astrology" (or sometimes "astronomy") rather than "mathematics"; the meaning gradually changed to its present one from about 1500 to 1800. This change has resulted in several mistranslations: For example, Saint Augustine's warning that Christians should beware of mathematici, meaning "astrologers", is sometimes mistranslated as a condemnation of mathematicians.[15]
At the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics.[23][10] The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas.[24] Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have "geometry" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.[25]
Number theory began with the manipulation of numbers, that is, natural numbers 



(

N

)
,


{\displaystyle (\mathbb {N} ),}

 and later expanded to integers 



(

Z

)


{\displaystyle (\mathbb {Z} )}

 and rational numbers 



(

Q

)
.


{\displaystyle (\mathbb {Q} ).}

 Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations.[26] Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria.[27] The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss.[28]
Many easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra.[29] Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort.[30]
Number theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), diophantine equations, and transcendence theory (problem oriented).[25]
Geometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields.[31]
A fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements.[32][33]
The resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space.[b][31]
Analytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions.[31]
In the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem.[35][10] In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space.[36]
Algebra is the art of manipulating equations and formulas. Diophantus (3rd century) and al-Khwarizmi (9th century) were the two main precursors of algebra.[38][39] Diophantus solved some equations involving unknown natural numbers by deducing new relations until he obtained the solution. Al-Khwarizmi introduced systematic methods for transforming equations, such as moving a term from one side of an equation into the other side. The term algebra is derived from the Arabic word al-jabr meaning 'the reunion of broken parts'[40] that he used for naming one of these methods in the title of his main treatise.
Until the 19th century, algebra consisted mainly of the study of linear equations (presently linear algebra), and polynomial equations in a single unknown, which were called algebraic equations (a term still in use, although it may be ambiguous). During the 19th century, mathematicians began to use variables to represent things other than numbers (such as matrices, modular integers, and geometric transformations), on which generalizations of arithmetic operations are often valid.[42] The concept of algebraic structure addresses this, consisting of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. The scope of algebra thus grew to include the study of algebraic structures. This object of algebra was called modern algebra or abstract algebra, as established by the influence and works of Emmy Noether.[43] (The latter term appears mainly in an educational context, in opposition to elementary algebra, which is concerned with the older way of manipulating formulas.)
Some types of algebraic structures have useful and often fundamental properties, in many areas of mathematics. Their study became autonomous parts of algebra, and include:[25]
The study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category theory.[44] The latter applies to every mathematical structure (not only algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology.[45]
Calculus, formerly called infinitesimal calculus, was introduced independently and simultaneously by 17th-century mathematicians Newton and Leibniz.[46] It is fundamentally the study of the relationship of variables that depend on each other. Calculus was expanded in the 18th century by Euler with the introduction of the concept of a function and many other results.[47] Presently, "calculus" refers mainly to the elementary part of this theory, and "analysis" is commonly used for advanced parts.
Analysis is further subdivided into real analysis, where variables represent real numbers, and complex analysis, where variables represent complex numbers. Analysis includes many subareas shared by other areas of mathematics which include:[25]
The four color theorem and optimal sphere packing were two major problems of discrete mathematics solved in the second half of the 20th century.[50] The P versus NP problem, which remains open to this day, is also important for discrete mathematics, since its solution would potentially impact a large number of computationally difficult problems.[51]
The two subjects of mathematical logic and set theory have belonged to mathematics since the end of the 19th century.[52][53] Before this period, sets were not considered to be mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians.[54]
Before Cantor's study of infinite sets, mathematicians were reluctant to consider actually infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets[55] but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led to the controversy over Cantor's set theory.[56]
In the same period, various areas of mathematics concluded the former intuitive definitions of the basic mathematical objects were insufficient for ensuring mathematical rigour. Examples of such intuitive definitions are "a set is a collection of objects", "natural number is what is used for counting", "a point is a shape with a zero length in every direction", "a curve is a trace left by a moving point", etc.
This became the foundational crisis of mathematics.[57] It was eventually solved in mainstream mathematics by systematizing the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have.[23] For example, in Peano arithmetic, the natural numbers are defined by "zero is a number", "each number has a unique successor", "each number but zero has a unique predecessor", and some rules of reasoning.[58] This mathematical abstraction from reality is embodied in the modern philosophy of formalism, as founded by David Hilbert around 1910.[59]
These problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theories), proof theory, type theory, computability theory and computational complexity theory.[25] Although these aspects of mathematical logic were introduced before the rise of computers, their use in compiler design, program certification, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories.[63]
The field of statistics is a mathematical application that is employed for the collection and processing of data samples, using procedures based on mathematical methods especially probability theory. Statisticians generate data with random sampling or randomized experiments.[65] The design of a statistical sample or experiment determines the analytical methods that will be used. Analysis of data from observational studies is done using statistical models and the theory of inference, using model selection and estimation. The models and consequential predictions should then be tested against new data.[d]
Statistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints. For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[66] Because of its use of optimization, the mathematical theory of statistics overlaps with other decision sciences, such as operations research, control theory, and mathematical economics.[67]
Computational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity.[68][69] Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis broadly includes the study of approximation and discretization with special focus on rounding errors.[70] Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.
Mathematics has developed a rich terminology covering a broad range of fields that study the properties of various abstract, idealized objects and how they interact. It is based on rigorous definitions that provide a standard foundation for communication. An axiom or postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning, a statement that is proven to be true becomes a theorem. A specialized theorem that is mainly used to prove another theorem is called a lemma. A proven instance that forms part of a more general finding is termed a corollary.[96]
Numerous technical terms used in mathematics are neologisms, such as polynomial and homeomorphism.[97] Other technical terms are words of the common language that are used in an accurate meaning that may differs slightly from their common meaning. For example, in mathematics, "or" means "one, the other or both", while, in common language, it is either ambiguous or means "one or the other but not both" (in mathematics, the latter is called "exclusive or"). Finally, many mathematical terms are common words that are used with a completely different meaning.[98] This may lead to sentences that are correct and true mathematical assertions, but appear to be nonsense to people who do not have the required background. For example, "every free module is flat" and "a field is always a ring".
Mathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws.[99] The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model.[100] Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used.[101] For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model.[102]
Until the 19th century, the development of mathematics in the West was mainly motivated by the needs of technology and science, and there was no clear distinction between pure and applied mathematics.[109] For example, the natural numbers and arithmetic were introduced for the need of counting, and geometry was motivated by surveying, architecture and astronomy. Later, Isaac Newton introduced infinitesimal calculus for explaining the movement of the planets with his law of gravitation. Moreover, most mathematicians were also scientists, and many scientists were also mathematicians.[110] However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece.[111]
In the 19th century, mathematicians such as Karl Weierstrass and Richard Dedekind increasingly focused their research on internal problems, that is, pure mathematics.[109][112] This led to split mathematics into pure mathematics and applied mathematics, the latter being often considered as having a lower value among mathematical purists. However, the lines between the two are frequently blurred.[113]
The aftermath of World War II led to a surge in the development of applied mathematics in the US and elsewhere.[114][115] Many of the theories developed for applications were found interesting from the point of view of pure mathematics, and many results of pure mathematics were shown to have applications outside mathematics; in turn, the study of these applications may give new insights on the "pure theory".[116][117]
An example of the first case is the theory of distributions, introduced by Laurent Schwartz for validating computations done in quantum mechanics, which became immediately an important tool of (pure) mathematical analysis.[118] An example of the second case is the decidability of the first-order theory of the real numbers, a problem of pure mathematics that was proved true by Alfred Tarski, with an algorithm that is impossible to implement because of a computational complexity that is much too high.[119] For getting an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry.[120]
In the present day, the distinction between pure and applied mathematics is more a question of personal research aim of mathematicians than a division of mathematics into broad areas.[121][122] The Mathematics Subject Classification has a section for "general applied mathematics" but does not mention "pure mathematics".[25] However, these terms are still used in names of some university departments, such as at the Faculty of Mathematics at the University of Cambridge.
The unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner.[7] It is the fact that many mathematical theories, even the "purest" have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced.[123] Examples of unexpected applications of mathematical theories can be found in many areas of mathematics.
A notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem.[124] A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It is almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses.[125]
In the 19th century, the internal development of geometry (pure mathematics) lead to define and study non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of the special relativity is a non-Euclidean space of dimension four, and spacetime of the general relativity is a (curved) manifold of dimension four.[126][127]
Mathematics and physics have influenced each other over their modern history. Modern physics uses mathematics abundantly,[131] and is also the motivation of major mathematical developments.[132] See above for examples of this strong interaction.
The rise of technology in the 20th century opened the way to a new science: computing.[e] This field is closely related to mathematics in several ways. Theoretical computer science is essentially mathematical in nature. Communication technologies apply branches of mathematics that may be very old (e.g., arithmetic), especially with respect to transmission security, in cryptography and coding theory. Discrete mathematics is useful in many areas of computer science, such as complexity theory, information theory, graph theory, and so on.[citation needed]
In return, computing has also become essential for obtaining new results. This is a group of techniques known as experimental mathematics, which is the use of experimentation to discover mathematical insights.[133] The most well-known example is the four-color theorem, which was proven in 1976 with the help of a computer. This revolutionized traditional mathematics, where the rule was that the mathematician should verify each part of the proof. In 1998, the Kepler conjecture on sphere packing seemed to also be partially proven by computer. An international team had since worked on writing a formal proof; it was finished (and verified) in 2015.[134]
Once written formally, a proof can be verified using a program called a proof assistant.[135] These programs are useful in situations where one is uncertain about a proof's correctness.[135]
A major open problem in theoretical computer science is P versus NP. It is one of the seven Millennium Prize Problems.[136]
Biology uses probability extensively - for example, in ecology or neurobiology.[137] Most of the discussion of probability in biology, however, centers on the concept of evolutionary fitness.[137]
Genotype evolution can be modeled with the Hardy-Weinberg principle.[citation needed]
Medicine uses statistical hypothesis testing, run on data from clinical trials, to determine whether a new treatment works.[citation needed]
Since the start of the 20th century, chemistry has used computing to model molecules in three dimensions. It turns out that the form of macromolecules in biology is variable and determines the action. Such modeling uses Euclidean geometry; neighboring atoms form a polyhedron whose distances and angles are fixed by the laws of interaction.[citation needed]
Structural geology and climatology use probabilistic models to predict the risk of natural catastrophes.[citation needed] Similarly, meteorology, oceanography, and planetology also use mathematics due to their heavy use of models.[citation needed]
Areas of mathematics used in the social sciences include probability/statistics and differential equations (stochastic or deterministic).[citation needed] These areas used in fields such as sociology, psychology, economics, finance, and linguistics.[citation needed]
Even so, mathematization of the social sciences is not without danger. In the controversial book Fashionable Nonsense (1997), Sokal and Bricmont denounced the unfounded or abusive use of scientific terminology, particularly from mathematics or physics, in the social sciences. The study of complex systems (evolution of unemployment, business capital, demographic evolution of a population, etc.) uses elementary mathematical knowledge. However, the choice of counting criteria, particularly for unemployment, or of models can be subject to controversy.[citation needed]
As of the 21st century, these disciplines are no longer considered sciences.[152]
The connection between mathematics and material reality has led to philosophical debates since at least the time of Pythagoras. The ancient philosopher Plato argued that abstractions  that reflect material reality have themselves a reality that exists outside space and time. As a result, the philosophical view that mathematical objects somehow exist on their own in abstraction is often referred to as Platonism. Independently of their possible philosophical opinions, modern mathematicians may be generally considered as Platonists, since they think of and talk of their objects of study as real objects.[153]
 Something becomes objective (as opposed to "subjective") as soon as we are convinced that it exists in the minds of others in the same form as it does in ours and that we can think about it and discuss it together.[154] Because the language of mathematics is so precise, it is ideally suited to defining concepts for which such a consensus exists. In my opinion, that is sufficient to provide us with a feeling of an objective existence, of a reality of mathematics ...
Nevertheless, Platonism and the concurrent views on abstraction do not explain the unreasonable effectiveness of mathematics.[155]
The concept of rigor in mathematics dates back to ancient Greece, where their society encouraged logical, deductive reasoning. However, this rigorous approach would tend to discourage exploration of new approaches, such as irrational numbers and concepts of infinity. The method of demonstrating rigorous proof was enhanced in the sixteenth century through the use of symbolic notation. In the 18th century, social transition led to mathematicians earning their keep through teaching, which led to more careful thinking about the underlying concepts of mathematics. This produced more rigorous approaches, while transitioning from geometric methods to algebraic and then arithmetic proofs.[10]
At the end of the 19th century, it appeared that the definitions of the basic concepts of mathematics were not accurate enough for avoiding paradoxes (non-Euclidean geometries and Weierstrass function) and contradictions (Russell's paradox). This was solved by the inclusion of axioms with the apodictic inference rules of mathematical theories; the re-introduction of axiomatic method pioneered by the ancient Greeks.[10] It results that "rigor" is no more a relevant concept in mathematics, as a proof is either correct or erroneous, and a "rigorous proof" is simply a pleonasm. Where a special concept of rigor comes into play is in the socialized aspects of a proof, wherein it may be demonstrably refuted by other mathematicians. After a proof has been accepted for many years or even decades, it can then be considered as reliable.[165]
Nevertheless, the concept of "rigor" may remain useful for teaching to beginners what is a mathematical proof.[166]
Mathematics has a remarkable ability to cross cultural boundaries and time periods. As a human activity, the practice of mathematics has a social side, which includes education, careers, recognition, popularization, and so on. In education, mathematics is a core part of the curriculum and forms an important element of the STEM academic disciplines. Prominent careers for professional mathematicians include math teacher or professor, statistician, actuary, financial analyst, economist, accountant, commodity trader, or computer consultant.[167]
During school, mathematical capabilities and positive expectations have a strong association with career interest in the field. Extrinsic factors such as feedback motivation by teachers, parents, and peer groups can influence the level of interest in mathematics.[176] Some students studying math may develop an apprehension or fear about their performance in the subject. This is known as math anxiety or math phobia, and is considered the most prominent of the disorders impacting academic performance. Math anxiety can develop due to various  factors such as parental and teacher attitudes, social stereotypes, and personal traits. Help to counteract the anxiety can come from changes in instructional approaches, by interactions with parents and teachers, and by tailored treatments for the individual.[177]
The validity of a mathematical theorem relies only on the rigor of its proof, which could theoretically be done automatically by a computer program. This does not mean that there is no place for creativity in a mathematical work. On the contrary, many important mathematical results (theorems) are solutions of problems that other mathematicians failed to solve, and the invention of a way for solving them may be a fundamental way of the solving process.[178][179] An extreme example is Apery's theorem: Roger Apery provided only the ideas for a proof, and the formal proof was given only several months later by three other mathematicians.[180]
Creativity and rigor are not the only psychological aspects of the activity of mathematicians. Some mathematicians can see their activity as a game, more specifically as solving puzzles.[181] This aspect of mathematical activity is emphasized in recreational mathematics.
Some feel that to consider mathematics a science is to downplay its artistry and history in the seven traditional liberal arts.[184] One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematical results are created (as in art) or discovered (as in science).[128] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.
In the 20th century, the mathematician L. E. J. Brouwer even initiated a philosophical perspective known as intuitionism, which primarily identifies mathematics with certain creative processes in the mind.[59] Intuitionism is in turn one flavor of a stance known as constructivism, which only considers a mathematical object valid if it can be directly constructed, not merely guaranteed by logic indirectly. This leads committed constructivists to reject certain results, particularly arguments like existential proofs based on the law of excluded middle.[185]
In the end, neither constructivism nor intuitionism displaced classical mathematics or achieved mainstream acceptance. However, these programs have motivated specific developments, such as intuitionistic logic and other foundational insights, which are appreciated in their own right.[185]
The curve in red has a logarithmic shape, which reflects the following two phenomena:
Humans, as well as some other animals, find symmetric patterns to be more beautiful.[189] Mathematically, the symmetries of an object form a group known as the symmetry group.[190]
Popular mathematics is the act of presenting mathematics without technical terms.[193] Presenting mathematics may be hard since the general public suffers from mathematical anxiety and mathematical objects are highly abstract.[194] However, popular mathematics writing can overcome this by using applications or cultural links.[195] Despite this, mathematics is rarely the topic of popularization in printed or televised media.
There are many biographies about mathematicians, but mathematics is a poorly explored theme in literature and film, though it is present.
The most prestigious award in mathematics is the Fields Medal,[201][202] established in 1936 and awarded every four years (except around World War II) to up to four individuals.[203][204] It is considered the mathematical equivalent of the Nobel Prize.[204]
A famous list of 23 open problems, called "Hilbert's problems", was compiled in 1900 by German mathematician David Hilbert.[213] This list has achieved great celebrity among mathematicians,[214] and, as of 2022[update], at least thirteen of the problems (depending how some are interpreted) have been solved.[215]
Computer science is the study of computation, automation, and information.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).[4][5][6] Computer science is generally considered an academic discipline and distinct from computer programming.[7]
The fundamental concern of computer science is determining what can and cannot be automated.[2][9][3][10][11] The Turing Award is generally recognized as the highest distinction in computer science.[12][13]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[17]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[21] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[24] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[25]
Although first proposed in 1956,[32] the term "computer science" appears in a 1959 article in Communications of the ACM,[33]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[34] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[33]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[35] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[36] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[37] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[43] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[44]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science,[45] mathematics,[46] or engineering.[47] Allen Newell and Herbert A. Simon argued in 1975, 
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[47]
 It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[47] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[47] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[47]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[47] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[47]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[48] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[49] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[50] identifiable in some branches of artificial intelligence).[51]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[52]
Computer science is no more about computers than astronomy is about telescopes.
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[29] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[56] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[57]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[58]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[59] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[60] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[61]
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[62] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[63] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[64] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[65]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[66] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[67]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[73]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[74][75] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[76]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[79] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[80] According to a 2021 report, only 51% of high schools in the US offer computer science.[81]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[82][83] and several others are following.[84]
A professional practitioner or researcher involved in the discipline is called a psychologist. Some psychologists can also be classified as behavioral or cognitive scientists. Some psychologists attempt to understand the role of mental functions in individual and social behavior. Others explore the physiological and neurobiological processes that underlie cognitive functions and behaviors.
Psychologists are involved in research on perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. Psychologists' interests extend to interpersonal relationships, psychological resilience, family resilience, and other areas within social psychology. They also consider the unconscious mind.[3] Research psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. Some, but not all, clinical and counseling psychologists rely on symbolic interpretation.
While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts, psychology ultimately aims to benefit society.[4][5][6] Many psychologists are involved in some kind of therapeutic role, practicing psychotherapy in clinical, counseling, or school settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, or hospitals). Another group of psychologists is employed in industrial and organizational settings.[7] Yet others are involved in work on human development, aging, sports, health, forensic science, education, and the media.
In 1890, William James defined psychology as "the science of mental life, both of its phenomena and their conditions."[11] This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by radical behaviorists such as John B. Watson, who in 1913 asserted that the discipline is a "natural science", the theoretical goal of which "is the prediction and control of behavior."[12] Since James defined "psychology", the term more strongly implicates scientific experimentation.[13][12] Folk psychology refers to ordinary people's, as contrasted with psychology professionals', understanding of the mental states and behaviors of people.[14]
The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders.[15] Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise),[16] addressed the workings of the mind.[17] As early as the 4th century BC, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes.[18] In 387 BCE, Plato suggested that the brain is where mental processes take place, and in 335 BCE Aristotle suggested that it was the heart.[19]
Influenced by Hinduism, Indian philosophy explored distinctions in types of awareness. A central idea of the Upanishads and other Vedic texts that formed the foundations of Hinduism was the distinction between a person's transient mundane self and their eternal, unchanging soul. Divergent Hindu doctrines and Buddhism have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga encompasses a range of techniques used in pursuit of this goal. Theosophy, a religion established by Russian-American philosopher Helena Blavatsky, drew inspiration from these doctrines during her time in British India.[21][22]
The German psychologist Hermann Ebbinghaus, a researcher at the University of Berlin, was another 19th-century contributor to the field. He pioneered the experimental study of memory and developed quantitative models of learning and forgetting.[30] In the early twentieth century, Wolfgang Kohler, Max Wertheimer, and Kurt Koffka co-founded the school of Gestalt psychology (not to be confused with the Gestalt therapy of Fritz Perls). The approach of Gestalt psychology is based upon the idea that individuals experience things as unified wholes. Rather than reducing thoughts and behavior into smaller component elements, as in structuralism, the Gestaltists maintained that whole of experience is important, and differs from the sum of its parts.
American psychology gained status upon the U.S.'s entry into World War I. A standing committee headed by Robert Yerkes administered mental tests ("Army Alpha" and "Army Beta") to almost 1.8 million soldiers.[37] Subsequently, the Rockefeller family, via the Social Science Research Council, began to provide funding for behavioral research.[38][39] Rockefeller charities funded the National Committee on Mental Hygiene, which disseminated the concept of mental illness and lobbied for applying ideas from psychology to child rearing.[37][40] Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations helped establish research on sexuality in the U.S.[41] Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also influenced American psychology. In the 1910s and 1920s, eugenics became a standard topic in psychology classes.[42] In contrast to the US, in the UK psychology was met with antagonism by the scientific and medical establishments, and up until 1939, there were only six psychology chairs in universities in England.[43]
After the war, new institutions were created although some psychologists, because of their Nazi affiliation, were discredited. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called Psyche. With funding from the Rockefeller Foundation, Mitscherlich established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students.[53]
The International Union of Psychological Science (IUPsyS) is the world federation of national psychological societies. The IUPsyS was founded in 1951 under the auspices of the United Nations Educational, Cultural and Scientific Organization (UNESCO).[31][58] Psychology departments have since proliferated around the world, based primarily on the Euro-American model.[21][58] Since 1966, the Union has published the International Journal of Psychology.[31] IAAP and IUPsyS agreed in 1976 each to hold a congress every four years, on a staggered basis.[57]
The Interamerican Psychological Society, founded in 1951, aspires to promote psychology across the Western Hemisphere. It holds the Interamerican Congress of Psychology and ha had 1,000 members in year 2000. The European Federation of Professional Psychology Associations, founded in 1981, represents 30 national associations with a total of 100,000 individual members. At least 30 other international organizations represent psychologists in different regions.[57]
In some places, governments legally regulate who can provide psychological services or represent themselves as a "psychologist."[59] The APA defines a psychologist as someone with a doctoral degree in psychology.[60]
As a discipline, psychology has long sought to fend off accusations that it is a "soft" science. Philosopher of science Thomas Kuhn's 1962 critique implied psychology overall was in a pre-paradigm state, lacking agreement on the type of overarching theory found in mature sciences such as chemistry and physics.[61] Because some areas of psychology rely on research methods such as surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion cannot be directly measured and are often inferred from subjective self-reports, which may be problematic. Experimental psychologists have devised a variety of ways to indirectly measure these elusive phenomenological entities.[62][63][64]
The contemporary field of behavioral neuroscience focuses on the physical basis of behavior. Behaviorial neuroscientists use animal models, often relying on rats, to study the neural, genetic, and cellular mechanisms that underlie behaviors involved in learning, memory, and fear responses.[69] Cognitive neuroscientists, by using neural imaging tools, investigate the neural correlates of psychological processes in humans. Neuropsychologists conduct psychological assessments to determine how an individual's behavior and cognition are related to the brain. The biopsychosocial model  is a cross-disciplinary, holistic model that concerns the ways in which interrelationships of biological, psychological, and socio-environmental factors affect health and behavior.[70]
Evolutionary psychology approaches thought and behavior from a modern evolutionary perspective. This perspective suggests that psychological adaptations evolved to solve recurrent problems in human ancestral environments. Evolutionary psychologists attempt to find out how human psychological traits are evolved adaptations, the results of natural selection or sexual selection over the course of human evolution.[71]
A tenet of behavioral research is that a large part of both human and lower-animal behavior is learned. A principle associated with behavioral research is that the mechanisms involved in learning apply to humans and non-human animals. Behavioral researchers have developed a treatment known as behavior modification, which is used to help individuals replace undesirable behaviors with desirable ones.
Clark L. Hull, Edwin Guthrie, and others did much to help behaviorism become a widely used paradigm.[33] A new method of "instrumental" or "operant" conditioning added the concepts of reinforcement and punishment to the model of behavior change. Radical behaviorists avoided discussing the inner workings of the mind, especially the unconscious mind, which they considered impossible to assess scientifically.[77] Operant conditioning was first described by Miller and Kanorski and popularized in the U.S. by B.F. Skinner, who emerged as a leading intellectual of the behaviorist movement.[78][79]
The Association for Behavior Analysis International was founded in 1974 and by 2003 had members from 42 countries. The field has gained a foothold in Latin America and Japan.[85] Applied behavior analysis is the term used for the application of the principles of operant conditioning to change socially significant behavior (it supersedes the term, "behavior modification").[86]
The Stroop effect is the fact that naming the color of the first set of words is easier and quicker than the second.
Cognitive psychology involves the study of mental processes, including perception, attention, language comprehension and production, memory, and problem solving.[87] Researchers in the field of cognitive psychology are sometimes called cognitivists. They rely on an information processing model of mental functioning. Cognitivist research is informed by functionalism and experimental psychology.
Starting in the 1950s, the experimental techniques developed by Wundt, James, Ebbinghaus, and others re-emerged as experimental psychology became increasingly cognitivist and, eventually, constituted a part of the wider, interdisciplinary cognitive science.[88][89] Some called this development the cognitive revolution because it rejected the anti-mentalist dogma of behaviorism as well as the strictures of psychoanalysis.[89]
Albert Bandura helped along the transition in psychology from behaviorism to cognitive psychology. Bandura and other social learning theorists advanced the idea of vicarious learning. In other words, they advanced the view that a child can learn by observing the immediate social environment and not necessarily from having been reinforced for enacting a behavior, although they did not rule out the influence of reinforcement on learning a behavior.[90]
Technological advances also renewed interest in mental states and mental representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena to the structure and function of the brain. The rise of computer science, cybernetics, and artificial intelligence underlined the value of comparing information processing in humans and machines.
A popular and representative topic in this area is cognitive bias, or irrational thought. Psychologists (and economists) have classified and described a sizeable catalogue of biases which recur frequently in human thought. The availability heuristic, for example, is the tendency to overestimate the importance of something which happens to come readily to mind.[91]
Elements of behaviorism and cognitive psychology were synthesized to form cognitive behavioral therapy, a form of psychotherapy modified from techniques developed by American psychologist Albert Ellis and American psychiatrist Aaron T. Beck.
Social psychology is concerned with how behaviors, thoughts, feelings, and the social environment influence human interactions.[93] Social psychologists study such topics as the influence of others on an individual's behavior (e.g. conformity, persuasion) and the formation of beliefs, attitudes, and stereotypes about other people. Social cognition fuses elements of social and cognitive psychology for the purpose of understanding how people process, remember, or distort social information. The study of group dynamics involves research on the nature of leadership, organizational communication, and related phenomena. In recent years, social psychologists have become interested in implicit measures, mediational models, and the interaction of person and social factors in accounting for behavior. Some concepts that sociologists have applied to the study of psychiatric disorders, concepts such as the social role, sick role, social class, life events, culture, migration, and total institution, have influenced social psychologists.[94]
Psychoanalytic theory is not monolithic. Other well-known psychoanalytic thinkers who diverged from Freud include Alfred Adler, Carl Jung, Erik Erikson, Melanie Klein, D.W. Winnicott, Karen Horney, Erich Fromm, John Bowlby, Freud's daughter Anna Freud, and Harry Stack Sullivan. These individuals ensured that psychoanalysis would evolve into diverse schools of thought. Among these schools are ego psychology, object relations, and interpersonal, Lacanian, and relational psychoanalysis.
Humanistic psychology, which has been influenced by existentialism and phenomenology,[104] stresses free will and self-actualization.[105] It emerged in the 1950s as a movement within academic psychology, in reaction to both behaviorism and psychoanalysis.[106] The humanistic approach seeks to view the whole person, not just fragmented parts of the personality or isolated cognitions.[107] Humanistic psychology also focuses on personal growth, self-identity, death, aloneness, and freedom. It emphasizes subjective meaning, the rejection of determinism, and concern for positive growth rather than pathology. Some founders of the humanistic school of thought were American psychologists Abraham Maslow, who formulated a hierarchy of human needs, and Carl Rogers, who created and developed client-centered therapy.
Later, positive psychology opened up humanistic themes to scientific study. Positive psychology is the study of factors which contribute to human happiness and well-being, focusing more on people who are currently healthy. In 2010, Clinical Psychological Review published a special issue devoted to positive psychological interventions, such as gratitude journaling and the physical expression of gratitude. It is, however, far from clear that positive psychology is effective in making people happier.[108][109] Positive psychological interventions have been limited in scope, but their effects are thought to be somewhat better than placebo effects.
The American Association for Humanistic Psychology, formed in 1963, declared:
Humanistic psychology is primarily an orientation toward the whole of psychology rather than a distinct area or school. It stands for respect for the worth of persons, respect for differences of approach, open-mindedness as to acceptable methods, and interest in exploration of new aspects of human behavior. As a "third force" in contemporary psychology, it is concerned with topics having little place in existing theories and systems: e.g., love, creativity, self, growth, organism, basic need-gratification, self-actualization, higher values, being, becoming, spontaneity, play, humor, affection, naturalness, warmth, ego-transcendence, objectivity, autonomy, responsibility, meaning, fair-play, transcendental experience, peak experience, courage, and related concepts.[110]
Austrian existential psychiatrist and Holocaust survivor Viktor Frankl drew evidence of meaning's therapeutic power from reflections upon his own internment.[114] He created a variation of existential psychotherapy called logotherapy, a type of existentialist analysis that focuses on a will to meaning (in one's life), as opposed to Adler's Nietzschean doctrine of will to power or Freud's will to pleasure.[115]
Study of the unconscious mind, a part of the psyche outside the individual's awareness but that is believed to influence conscious thought and behavior, was a hallmark of early psychology. In one of the first psychology experiments conducted in the United States, C.S. Peirce and Joseph Jastrow found in 1884 that research subjects could choose the minutely heavier of two weights even if consciously uncertain of the difference.[128] Freud popularized the concept of the unconscious mind, particularly when he referred to an uncensored intrusion of unconscious thought into one's speech (a Freudian slip) or to his efforts to interpret dreams.[129] His 1901 book The Psychopathology of Everyday Life catalogues hundreds of everyday events that Freud explains in terms of unconscious influence. Pierre Janet advanced the idea of a subconscious mind, which could contain autonomous mental elements unavailable to the direct scrutiny of the subject.[130]
The concept of unconscious processes has remained important in psychology. Cognitive psychologists have used a "filter" model of attention. According to the model, much information processing takes place below the threshold of consciousness, and only certain stimuli, limited by their nature and number, make their way through the filter. Much research has shown that subconscious priming of certain ideas can covertly influence thoughts and behavior.[130] Because of the unreliability of self-reporting, a major hurdle in this type of research involves demonstrating that a subject's conscious mind has not perceived a target stimulus. For this reason, some psychologists prefer to distinguish between implicit and explicit memory. In another approach, one can also describe a subliminal stimulus as meeting an objective but not a subjective threshold.[131]
The automaticity model of John Bargh and others involves the ideas of automaticity and unconscious processing in our understanding of social behavior,[132][133] although there has been dispute with regard to replication.[134][135]
Some experimental data suggest that the brain begins to consider taking actions before the mind becomes aware of them.[136] The influence of unconscious forces on people's choices bears on the philosophical question of free will. John Bargh, Daniel Wegner, and Ellen Langer describe free will as an illusion.[132][133][137]
Some psychologists study motivation or the subject of why people or lower animals initiate a behavior at a particular time. It also involves the study of why humans and lower animals continue or terminate a behavior. Psychologists such as William James initially used the term motivation to refer to intention, in a sense similar to the concept of will in European philosophy. With the steady rise of Darwinian and Freudian thinking, instinct also came to be seen as a primary source of motivation.[138] According to drive theory, the forces of instinct combine into a single source of energy which exerts a constant influence. Psychoanalysis, like biology, regarded these forces as demands originating in the nervous system. Psychoanalysts believed that these forces, especially the sexual instincts, could become entangled and transmuted within the psyche. Classical psychoanalysis conceives of a struggle between the pleasure principle and the reality principle, roughly corresponding to id and ego. Later, in Beyond the Pleasure Principle, Freud introduced the concept of the death drive, a compulsion towards aggression, destruction, and psychic repetition of traumatic events.[139] Meanwhile, behaviorist researchers used simple dichotomous models (pleasure/pain, reward/punishment) and well-established principles such as the idea that a thirsty creature will take pleasure in drinking.[138][140] Clark Hull formalized the latter idea with his drive reduction model.[141]
Developmental psychology refers to the scientific study of how and why the thought processes, emotions, and behaviors of humans change over the course of their lives.[146] Some credit Charles Darwin with conducting the first systematic study within the rubric of developmental psychology, having published in 1877 a short paper detailing the development of innate forms of communication based on his observations of his infant son.[147] The main origins of the discipline, however, are found in the work of Jean Piaget. Like Piaget, developmental psychologists originally focused primarily on the development of cognition from infancy to adolescence. Later, developmental psychology extended itself to the study cognition over the life span. In addition to studying cognition, developmental psychologists have also come to focus on affective, behavioral, moral, social, and neural development.
Developmental psychologists who study children use a number of research methods. For example, they make observations of children in natural settings such as preschools[148] and engage them in experimental tasks.[149] Such tasks often resemble specially designed games and activities that are both enjoyable for the child and scientifically useful. Developmental researchers have even devised clever methods to study the mental processes of infants.[150] In addition to studying children, developmental psychologists also study aging and processes throughout the life span, including old age.[151] These psychologists draw on the full range of psychological theories to inform their research.[146]
All researched psychological traits are influenced by both genes and environment, to varying degrees.[152][153] These two sources of influence are often confounded in observational research of individuals and families. An example of this confounding can be shown in the transmission of depression from a depressed mother to her offspring. A theory based on environmental transmission would hold that an offspring, by virtue of their having a problematic rearing environment managed by a depressed mother, is at risk for developing depression. On the other hand, a hereditarian theory would hold that depression risk in an offspring is influenced to some extent by genes passed to the child from the mother. Genes and environment in these simple transmission models are completely confounded. A depressed mother may both carry genes that contribute to depression in her offspring and also create a rearing environment that increases the risk of depression in her child.
Behavioral genetics researchers have employed methodologies that help to disentangle this confound and understand the nature and origins of individual differences in behavior.[71] Traditionally the research has involved twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, gene-focused research has contributed to understanding genetic contributions to the development of psychological traits.
The availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic,[154][155][156][157][158] where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to work toward understanding the genetic and environmental bases of behavior and their interaction.
Psychology encompasses many subfields and includes different approaches to the study of mental processes and behavior.
The provision of psychological health services is generally called clinical psychology in the U.S. Sometimes, however, members of the school psychology and counseling psychology professions engage in practices that resemble that of clinical psychologists. Clinical psychologists typically include people who have graduated from doctoral programs in clinical psychology. In Canada, some of the members of the abovementioned groups usually fall within the larger category of professional psychology. In Canada and the U.S., practitioners get bachelor's degrees and doctorates; doctoral students in clinical psychology usually spend one year in a predoctoral internship and one year in postdoctoral internship. In Mexico and most other Latin American and European countries, psychologists do not get bachelor's and doctoral degrees; instead, they take a three-year professional course following high school.[60] Clinical psychology is at present the largest specialization within psychology.[168] It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychological distress, dysfunction, and/or mental illness. Clinical psychologists also try to promote subjective well-being and personal growth. Central to the practice of clinical psychology are psychological assessment and psychotherapy although clinical psychologists may also engage in research, teaching, consultation, forensic testimony, and program development and administration.[169]
Credit for the first psychology clinic in the United States typically goes to Lightner Witmer, who established his practice in Philadelphia in 1896. Another modern psychotherapist was Morton Prince, an early advocate for the establishment of psychology as a clinical and academic discipline.[168] In the first part of the twentieth century, most mental health care in the United States was performed by psychiatrists, who are medical doctors. Psychology entered the field with its refinements of mental testing, which promised to improve the diagnosis of mental problems. For their part, some psychiatrists became interested in using psychoanalysis and other forms of psychodynamic psychotherapy to understand and treat the mentally ill.[37][170]
Psychotherapy as conducted by psychiatrists blurred the distinction between psychiatry and psychology, and this trend continued with the rise of community mental health facilities. Some in the clinical psychology community adopted behavioral therapy, a thoroughly non-psychodynamic model that used behaviorist learning theory to change the actions of patients. A key aspect of behavior therapy is empirical evaluation of the treatment's effectiveness. In the 1970s, cognitive-behavior therapy emerged with the work of Albert Ellis and Aaron Beck. Although there are similarities between behavior therapy and cognitive-behavior therapy, cognitive-behavior therapy required the application of cognitive constructs. Since the 1970s, the popularity of cognitive-behavior therapy among clinical psychologists increased. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear, panic, anxiety) can be deconditioned.[171]
Mental health care today involves psychologists and social workers in increasing numbers. In 1977, National Institute of Mental Health director Bertram Brown described this shift as a source of "intense competition and role confusion."[37] Graduate programs issuing doctorates in clinical psychology emerged in the 1950s and underwent rapid increase through the 1980s. The PhD degree is intended to train practitioners who could also conduct scientific research. The PsyD degree is more exclusively designed to train practitioners.[60]
Some clinical psychologists focus on the clinical management of patients with brain injury. This subspecialty is known as clinical neuropsychology. In many countries, clinical psychology is a regulated mental health profession. The emerging field of disaster psychology (see crisis intervention) involves professionals who respond to large-scale traumatic events.[172]
Diagnosis in clinical psychology usually follows the Diagnostic and Statistical Manual of Mental Disorders (DSM).[180] The study of mental illnesses is called abnormal psychology.
Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Educational psychologists can be found in preschools, schools of all levels including post secondary institutions, community organizations and learning centers, Government or private research firms, and independent or private consultant[181] [[Madisonodell5/sandbox]]. The work of developmental psychologists such as Lev Vygotsky, Jean Piaget, and Jerome Bruner has been influential in creating teaching methods and educational practices. Educational psychology is often included in teacher education programs in places such as North America, Australia, and New Zealand.
School psychology combines principles from educational psychology and clinical psychology to understand and treat students with learning disabilities; to foster the intellectual growth of gifted students; to facilitate prosocial behaviors in adolescents; and otherwise to promote safe, supportive, and effective learning environments. School psychologists are trained in educational and behavioral assessment, intervention, prevention, and consultation, and many have extensive training in research.[182]
Industrial and organizational (I/O) psychology involves research and practices that apply psychological theories and principles to organizations and individuals' work-lives.[183] In the field's beginnings, industrialists brought the nascent field of psychology to bear on the study of scientific management techniques for improving workplace efficiency. The field was at first called economic psychology or business psychology; later, industrial psychology, employment psychology, or psychotechnology.[184] An influential early study examined workers at Western Electric's Hawthorne plant in Cicero, Illinois from 1924 to 1932. Western Electric experimented on factory workers to assess their responses to changes in illumination, breaks, food, and wages. The researchers came to focus on workers' responses to observation itself, and the term Hawthorne effect is now used to describe the fact that people's behavior can change when they think they're being observed.[185] Although the Hawthorne research can be found in psychology textbooks, the research and its findings were weak at best.[186][187]
The name industrial and organizational psychology emerged in the 1960s. In 1973, it became enshrined in the name of the Society for Industrial and Organizational Psychology, Division 14 of the American Psychological Association.[184] One goal of the discipline is to optimize human potential in the workplace. Personnel psychology is a subfield of I/O psychology. Personnel psychologists apply the methods and principles of psychology in selecting and evaluating workers. Another subfield, organizational psychology, examines the effects of work environments and management styles on worker motivation, job satisfaction, and productivity.[188] Most I/O psychologists work outside of academia, for private and public organizations and as consultants.[184] A psychology consultant working in business today might expect to provide executives with information and ideas about their industry, their target markets, and the organization of their company.[189][190]
Organizational behavior (OB) is an allied field involved in the study of human behavior within organizations.[191] One way to differentiate I/O psychology from OB is to note that I/O psychologists train in university psychology departments and OB specialists, in business schools.
One role for psychologists in the military has been to evaluate and counsel soldiers and other personnel. In the U.S., this function began during World War I, when Robert Yerkes established the School of Military Psychology at Fort Oglethorpe in Georgia. The school provided psychological training for military staff.[37][192] Today, U.S. Army psychologists perform psychological screening, clinical psychotherapy, suicide prevention, and treatment for post-traumatic stress, as well as provide prevention-related services, for example, smoking cessation.[193] The United States Army's Mental Health Advisory Teams implement psychological interventions to help combat troops experiencing mental problems.[194][195]
Psychologists may also work on a diverse set of campaigns known broadly as psychological warfare. Psychological warfare chiefly involves the use of propaganda to influence enemy soldiers and civilians. This so-called black propaganda is designed to seem as if it originates from a source other than the Army.[196] The CIA's MKULTRA program involved more individualized efforts at mind control, involving techniques such as hypnosis, torture, and covert involuntary administration of LSD.[197] The U.S. military used the name Psychological Operations (PSYOP) until 2010, when these activities were reclassified as Military Information Support Operations (MISO), part of Information Operations (IO).[198] Psychologists have sometimes been involved in assisting the interrogation and torture of suspects, staining the records of the psychologists involved.[199]
An example of the contribution of psychologists to social change involves the research of Kenneth and Mamie Phipps Clark. These two African American psychologists studied segregation's adverse psychological impact on Black children. Their research findings played a role in the desegregation case Brown v. Board of Education (1954).[200]
The impact of psychology on social change includes the discipline's broad influence on teaching and learning. Research has shown that compared to the "whole word" or "whole language" approach, the phonics approach to reading instruction is more efficacious.[201]
Medical facilities increasingly employ psychologists to perform various roles. One aspect of health psychology is the psychoeducation of patients: instructing them in how to follow a medical regimen. Health psychologists can also educate doctors and conduct research on patient compliance.[202][203] Psychologists in the field of public health use a wide variety of interventions to influence human behavior. These range from public relations campaigns and outreach to governmental laws and policies. Psychologists study the composite influence of all these different tools in an effort to influence whole populations of people.[204]
Psychologists work with organizations to apply findings from psychological research to improve the health and well-being of employees. Some work as external consultants hired by organizations to solve specific problems, whereas others are full-time employees of the organization. Applications include conducting surveys to identify issues and designing interventions to make work healthier. Some of the specific health areas include:
Interventions that improve climates are a way to address accidents and violence. Interventions that reduce stress at work or provide employees with tools to better manage it can help in areas where stress is an important component.
Industrial psychology became interested in worker fatigue during World War I, when government ministers in Britain were concerned about the impact of fatigue on workers in munitions factories but not other types of factories.[212][213] In the U. K. some interest in worker well-being emerged with the efforts of Charles Samuel Myers and his National Institute of Industrial Psychology (NIIP) during the inter-War years.[214] In the U. S. during the mid-twentieth century industrial psychologist Arthur Kornhauser pioneered the study of occupational mental health, linking industrial working conditions to mental health as well as the spillover of an unsatisfying job into a worker's personal life.[215][216] Zickar accumulated evidence to show that "no other industrial psychologist of his era was as devoted to advocating management and labor practices that would improve the lives of working people."[215]
As interest in the worker health expanded toward the end of the twentieth century, the field of occupational health psychology (OHP) emerged. OHP is a branch of psychology that is interdisciplinary.[217][218][45][219] OHP is concerned with the health and safety of workers.[45][219] OHP addresses topic areas such as the impact of occupational stressors on physical and mental health, mistreatment of workers (e.g., bullying and violence), work-family balance, the impact of involuntary unemployment on physical and mental health, the influence of psychosocial factors on safety and accidents, and interventions designed to improve/protect worker health.[45][220] OHP grew out of  health psychology, industrial and organizational psychology, and occupational medicine.[221] OHP has also been informed by disciplines outside psychology, including industrial engineering, sociology, and economics.[222][223]
Although this type of psychological research is much less abundant than quantitative research, some psychologists conduct qualitative research. This type of research can involve interviews, questionnaires, and first-hand observation.[225] While hypothesis testing is rare, virtually impossible, in qualitative research, qualitative studies can be helpful in theory and hypothesis generation, interpreting seemingly contradictory quantitative findings, and understanding why some interventions fail and others succeed.[226]
A true experiment with random assignment of research participants (sometimes called subjects) to rival conditions allows researchers to make strong inferences about causal relationships. When there are large numbers of research participants, the random assignment (also called random allocation) of those participants to rival conditions ensures that the individuals in those conditions will, on average, be similar on most characteristics, including characteristics that went unmeasured. In an experiment, the researcher alters one or more variables of influence, called independent variables, and measures resulting changes in the factors of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment.
A quasi-experiment refers to a situation in which there are rival conditions under study but random assignment to the different conditions is not possible. Investigators must work with preexisting groups of people. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity.[229] For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes and, perhaps, statistically adjust for any initial differences in reading level.
Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data collected are consistent with the predictions. These predictions are likely to originate from one or more abstract scientific hypotheses about how the phenomenon under study actually works.[230]
Surveys are used in psychology for the purpose of measuring attitudes and traits, monitoring changes in mood, and checking the validity of experimental manipulations (checking research participants' perception of the condition they were assigned to). Psychologists have commonly used paper-and-pencil surveys. However, surveys are also conducted over the phone or through e-mail. Web-based surveys are increasingly used to conveniently reach many subjects.
Observational studies are commonly conducted in psychology. In cross-sectional observational studies, psychologists collect data at a single point in time. The goal of many cross-sectional studies is the assess the extent factors are correlated with each other. By contrast, in longitudinal studies psychologists collect data on the same sample at two or more points in time. Sometimes the purpose of longitudinal research is to study trends across time such as the stability of traits or age-related changes in behavior. Because some studies involve endpoints that psychologists cannot ethically study from an experimental standpoint, such as identifying the causes of depression, they conduct longitudinal studies a large group of depression-free people, periodically assessing what is happening in the individuals' lives. In this way psychologists have an opportunity to test causal hypotheses regarding conditions that commonly arise in people's lives that put them at risk for depression. Problems that affect longitudinal studies include selective attrition, the type of problem in which bias is introduced when a certain type of research participant disproportionately leaves a study.
Exploratory data analysis refers to a variety of practices that researchers use to reduce a great many variables to a small number overarching factors. In Peirce's three modes of inference, exploratory data analysis corresponds to abduction.[231] Meta-analysis is the technique research psychologists use to integrate results from many studies of the same variables and arriving at a grand average of the findings.[232]
A classic and popular tool used to relate mental and neural activity is the electroencephalogram (EEG), a technique using amplified electrodes on a person's scalp to measure voltage changes in different parts of the brain. Hans Berger, the first researcher to use EEG on an unopened skull, quickly found that brains exhibit signature "brain waves": electric oscillations which correspond to different states of consciousness. Researchers subsequently refined statistical methods for synthesizing the electrode data, and identified unique brain wave patterns such as the delta wave observed during non-REM sleep.[233]
Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior.[237] This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that couldn't be directly observed in a human. Computational neuroscience uses mathematical models to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.
Animal experiments aid in investigating many aspects of human psychology, including perception, emotion, learning, memory, and thought, to name a few. In the 1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, and rats and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in order to ascertain its unique effects upon dependent variables. These conditions are approximated best in laboratory settings. In contrast, human environments and genetic backgrounds vary so widely, and depend upon so many factors, that it is difficult to control important variables for human subjects. There are pitfalls, however, in generalizing findings from animal studies to humans through animal models.[238]
Comparative psychology refers to the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area explores the behavior of many species, from insects to primates. It is closely related to other disciplines that study animal behavior such as ethology.[239] Research in comparative psychology sometimes appears to shed light on human behavior, but some attempts to connect the two have been quite controversial, for example the Sociobiology of E.O. Wilson.[240] Animal models are often used to study neural processes related to human behavior, e.g. in cognitive neuroscience.
Qualitative research is often designed to answer questions about the thoughts, feelings, and behaviors of individuals. Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations are made.
Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identified five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers[241] sometimes aim to enrich our understanding of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's application of psychological and sociological theories, in his book Escape from Freedom, to understanding why many ordinary Germans supported Hitler.[242]
Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out.
Program evaluation involves the systematic collection, analysis, and application of information to answer questions about projects, policies and programs, particularly about their effectiveness.[244][245] In both the public and private sectors, stakeholders often want to know the extent which the programs they are funding, implementing, voting for, receiving, or objecting to are producing the intended effects. While program evaluation first focuses on effectiveness, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.[246]
Metascience involves the application of scientific methodology to study science itself. The field of metascience has revealed problems in psychological research. Some psychological research has suffered from bias,[247] problematic reproducibility,[248] and misuse of statistics.[249] These findings have led to calls for reform from within and from outside the scientific community.[250]
In 1959, statistician Theodore Sterling examined the results of psychological studies and discovered that 97% of them supported their initial hypotheses, implying possible publication bias.[251][252][253] Similarly, Fanelli (2010)[254] found that 91.5% of psychiatry/psychology studies confirmed the effects they were looking for, and concluded that the odds of this happening (a positive result) was around five times higher than in fields such as space science or geosciences. Fanelli argued that this is because researchers in "softer" sciences have fewer constraints to their conscious and unconscious biases.
A replication crisis in psychology has emerged. Many notable findings in the field have not been replicated. Some researchers were even accused of publishing fraudulent results.[255][256][257] Systematic efforts, including efforts by the Reproducibility Project of the Center for Open Science, to assess the extent of the problem found that as many as two-thirds of highly publicized findings in psychology failed to be replicated.[258] Reproducibility has generally been stronger in cognitive psychology (in studies and journals) than social psychology[258] and subfields of differential psychology.[259][260] Other subfields of psychology have also been implicated in the replication crisis, including clinical psychology,[261][262][263] developmental psychology,[264][265][266] and a field closely related to psychology,  educational research.[267][268][269][270][271]
Focus on the replication crisis has led to other renewed efforts in the discipline to re-test important findings.[272][273] In response to concerns about publication bias and data dredging (conducting a large number of statistical tests on a great many variables but restricting reporting to the results that were statistically significant), 295 psychology and medical journals have adopted result-blind peer review where studies are accepted not on the basis of their findings and after the studies are completed, but before the studies are conducted and upon the basis of the methodological rigor of their experimental designs and the theoretical justifications for their proposed statistical analysis before data collection or analysis is conducted.[274][275] In addition, large-scale collaborations among researchers working in multiple labs in different countries have taken place. The collaborators regularly make their data openly available for different researchers to assess.[276] Allen and Mehler[277] estimated that 61 percent of result-blind studies have yielded null results, in contrast to an estimated 5 to 20 percent in traditional research.
Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting great certainty in unimportant facts.[278] Some psychologists have responded with an increased use of effect size statistics, rather than sole reliance on p-values.[279]
Ethical standards in the discipline have changed over time. Some famous past studies are today considered unethical and in violation of established codes (the Canadian Code of Conduct for Research Involving Humans, and the Belmont Report). The American Psychological Association has advanced a set of ethical principles and a code of conduct for the profession.[289]
The most important contemporary standards include informed and voluntary consent. After World War II, the Nuremberg Code was established because of Nazi abuses of experimental subjects. Later, most countries (and scientific journals) adopted the Declaration of Helsinki. In the U.S., the National Institutes of Health established the Institutional Review Board in 1966, and in 1974 adopted the National Research Act (HR 7724). All of these measures encouraged researchers to obtain informed consent from human participants in experimental studies. A number of influential but ethically dubious studies led to the establishment of this rule; such studies included the MIT-Harvard Fernald School radioisotope studies, the Thalidomide tragedy, the Willowbrook hepatitis study, and Stanley Milgram's studies of obedience to authority.
Universities have ethics committees dedicated to protecting the rights (e.g., voluntary nature of participation in the research, privacy) and well-being (e.g., minimizing distress) of research participants. University ethics committees evaluate proposed research to ensure that researchers protect the rights and well-being of participants; an investigator's research project cannot be conducted unless approved by such an ethics committee.[290]
The ethics code of the American Psychological Association originated in 1951 as "Ethical Standards of Psychologists". This code has guided the formation of licensing laws in most American states. It has changed multiple times over the decades since its adoption. In 1989, the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission. The 1992 incarnation was the first to distinguish between "aspirational" ethical standards and "enforceable" ones. Members of the public have a five-year window to file ethics complaints about APA members with the APA ethics committee; members of the APA have a three-year window.[291]
Some of the ethical issues considered most important are the requirement to practice only within the area of competence, to maintain confidentiality with the patients, and to avoid sexual relations with them. Another important principle is informed consent, the idea that a patient or research subject must understand and freely choose a procedure they are undergoing.[291] Some of the most common complaints against clinical psychologists include sexual misconduct.[291]
Research on other animals is also governed by university ethics committees. Research on nonhuman animals cannot proceed without permission of the ethics committee of the researcher's home institution. Current ethical guidelines state that using non-human animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research.[292] Keeping this in mind, psychologists can use certain research techniques on animals that could not be used on humans.
Computer science is the study of computation, automation, and information.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory, and automation) to practical disciplines (including the design and implementation of hardware and software).[4][5][6] Computer science is generally considered an academic discipline and distinct from computer programming.[7]
The fundamental concern of computer science is determining what can and cannot be automated.[2][9][3][10][11] The Turing Award is generally recognized as the highest distinction in computer science.[12][13]
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[17]
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[18] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[19] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[20] He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer".[21] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[21] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[22] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[23] the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[24] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[25]
Although first proposed in 1956,[32] the term "computer science" appears in a 1959 article in Communications of the ACM,[33]
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[34] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[33]
His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[35] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[36] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[37] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term "Software Engineering" means, and how computer science is defined.[43] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[44]
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
Despite the word "science" in its name, there is debate over whether or not computer science is a discipline of science,[45] mathematics,[46] or engineering.[47] Allen Newell and Herbert A. Simon argued in 1975, 
Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[47]
 It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[47] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[47] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[47]
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods.[47] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[47]
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[48] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[49] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[50] identifiable in some branches of artificial intelligence).[51]
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[52]
Computer science is no more about computers than astronomy is about telescopes.
Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?"[29] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems,[56] is an open problem in the theory of computation.
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[57]
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
[58]
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[59] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[60] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[61]
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[62] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[63] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model.[64] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[65]
This branch of computer science aims to manage networks between computers worldwide.
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[66] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[67]
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[73]
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[74][75] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[76]
In the US, with 14,000 school districts deciding the curriculum, provision was fractured.[79] According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.[80] According to a 2021 report, only 51% of high schools in the US offer computer science.[81]
Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula,[82][83] and several others are following.[84]
Computer chess includes both hardware (dedicated computers) and software capable of playing chess. Computer chess provides opportunities for players to practice even in the absence of human opponents, and also provides opportunities for analysis, entertainment and training. Computer chess applications that play at the level of a chess master or higher are available on hardware from supercomputers to smart phones. Standalone chess-playing machines are also available. Stockfish, GNU Chess, Fruit, and other free open source applications are available for various platforms.
Computer chess applications, whether implemented in hardware or software, utilize different strategies than humans to choose their moves: they use heuristic methods to build, search and evaluate trees representing sequences of moves from the current position and attempt to execute the best such sequence during play. Such trees are typically quite large, thousands to millions of nodes. The computational speed of modern computers, capable of processing tens of thousands to hundreds of thousands of nodes or more per second, along with extension and reduction heuristics that narrow the tree to mostly relevant nodes, make such an approach effective.
The first chess machines capable of playing chess or reduced chess-like games were software programs running on digital computers early in the vacuum-tube computer age (1950s). The early programs played so poorly that even a beginner could defeat them.  Within 40 years, in 1997, chess engines running on super-computers or specialized hardware were capable of defeating even the best human players. By 2006, programs running on desktop PCs had attained the same capability. In 2006, Monty Newborn, Professor of Computer Science at McGill University, declared: "the science has been done". Nevertheless, solving chess is not currently possible for modern computers due to the game's extremely large number of possible variations.[1]
Computer chess was once considered the "Drosophila of AI", the edge of knowledge engineering. The field is now considered a scientifically completed paradigm, and playing chess is a mundane computing activity.[2]
Chess machines/programs are available in several different forms: stand-alone chess machines (usually a microprocessor running a software chess program, but sometimes as a specialized hardware machine), software programs running on standard PCs, web sites, and apps for mobile devices. Programs run on everything from super-computers to smartphones.  Hardware requirements for programs are minimal; the apps are no larger than a few megabytes on disk, use a few megabytes of memory (but can use much more, if it is available), and any processor 300Mhz or faster is sufficient.  Performance will vary modestly with processor speed, but sufficient memory to hold a large transposition table (up to several gigabytes or more) is more important to playing strength than processor speed.
Most available commercial chess programs and machines can play at super-grandmaster strength (Elo 2700 or more), and take advantage of multi-core and hyperthreaded computer CPU architectures.  Top programs such as Stockfish have surpassed even world champion caliber players.  Most chess programs comprise a chess engine connected to a GUI, such as Winboard or Chessbase.  Playing strength, time controls, and other performance-related settings are adjustable from the GUI.  Most GUIs also allow the player to set up and to edit positions, to reverse moves, to offer and to accept draws (and resign), to request and to receive move recommendations, and to show the engine's analysis as the game progresses.
There are a few chess engines such as Sargon, IPPOLIT, Stockfish, Crafty, Fruit, Leela Chess Zero and GNU Chess which can be downloaded (or source code otherwise obtained) from the Internet free of charge.
Perhaps the most common type of chess software are programs that simply play chess.  A human player makes a move on the board, the AI calculates and plays a subsequent move, and the human and AI alternate turns until the game ends.  The chess engine, which calculates the moves, and the graphical user interface (GUI) are sometimes separate programs.  Different engines can be connected to the GUI, permitting play against different styles of opponent.  Engines often have  a simple text command-line interface, while GUIs may offer a variety of piece sets, board styles, or even 3D or animated pieces.  Because recent engines are so capable, engines or GUIs may offer some way of handicapping the engine's ability, to improve the odds for a win by the human player. Universal Chess Interface (UCI) engines such Fritz or Rybka may have a built in mechanism for reducing the Elo rating of the engine (via UCI's uci_limitstrength and uci_elo parameters).  Some versions of Fritz have a Handicap and Fun mode for limiting the current engine or changing the percentage of mistakes it makes or changing its style. Fritz also has a Friend Mode where during the game it tries to match the level of the player.
Chess databases allow users to search through a large library of historical games, analyze them, check statistics, and formulate an opening repertoire. Chessbase (for PC) is a common program for these purposes amongst professional players, but there are alternatives such as Shane's Chess Information Database (Scid) [3] for Windows, Mac or Linux, Chess Assistant[4] for PC,[5] Gerhard Kalab's Chess PGN Master for Android[6] or Giordano Vicoli's Chess-Studio for iOS.[7]
Programs such as Playchess allow you to play games against other players over the internet.
Chess training programs teach chess. Chessmaster had playthrough tutorials by IM Josh Waitzkin and GM Larry Christiansen. Stefan Meyer-Kahlen offers Shredder Chess Tutor based on the Step coursebooks of Rob Brunia and Cor Van Wijgerden.  World champions Magnus Carlsen's Play Magnus company released a Magnus Trainer app for Android and iOS. Chessbase has Fritz and Chesster for children.  Convekta provides a large number of training apps such as CT-ART and its Chess King line based on tutorials by GM Alexander Kalinin and Maxim Blokh.
In the late 1970s chess programs suddenly began defeating highly skilled human players.[9] The year of Hearst's statement, Northwestern University's Chess 4.5 at the Paul Masson American Chess Championship's Class B level became the first to win a human tournament. Levy won his bet in 1978 by beating Chess 4.7, but it achieved the first computer victory against a Master-class player at the tournament level by winning one of the six games.[10] In 1980 Belle began often defeating Masters. By 1982 two programs played at Master level and three were slightly weaker.[9]
At the 1982 North American Computer Chess Championship, Monroe Newborn predicted that a chess program could become world champion within five years; tournament director and International Master Michael Valvo predicted ten years; the Spracklens predicted 15; Ken Thompson predicted more than 20; and others predicted that it would never happen. The most widely held opinion, however, stated that it would occur around the year 2000.[12] In 1989, Levy was defeated by Deep Thought in an exhibition match. Deep Thought, however, was still considerably below World Championship level, as the reigning world champion, Garry Kasparov, demonstrated in two strong wins in 1989. It was not until a 1996 match with IBM's Deep Blue that Kasparov lost his first game to a computer at tournament time controls in Deep Blue versus Kasparov, 1996, game 1. This game was, in fact, the first time a reigning world champion had lost to a computer using regular time controls. However, Kasparov regrouped to win three and draw two of the remaining five games of the match, for a convincing victory.
In the early 2000s, commercially available programs such as Junior and Fritz were able to draw matches against former world champion Garry Kasparov and classical world champion Vladimir Kramnik.
Advanced Chess is a form of chess developed in 1998 by Kasparov where a human plays against another human, and both have access to computers to enhance their strength. The resulting "advanced" player was argued by Kasparov to be stronger than a human or computer alone. This has been proven in numerous occasions, such as at Freestyle Chess events.
Players today are inclined to treat chess engines as analysis tools rather than opponents.[21] Chess grandmaster Andrew Soltis stated in 2016 "The computers are just much too good" and that world champion Magnus Carlsen won't play computer chess because "he just loses all the time and there's nothing more depressing than losing without even being in the game."[22]
Since the era of mechanical machines that played rook and king endings and electrical machines that played other games like hex in the early years of the 20th century, scientists and theoreticians have sought to develop a procedural representation of how humans learn, remember, think and apply knowledge, and the game of chess, because of its daunting complexity, became the "Drosophila of artificial intelligence (AI)".[23]  The procedural resolution of complexity became synonymous with thinking, and early computers, even before the chess automaton era, were popularly referred to as "electronic brains".  Several different schema were devised starting in the latter half of the 20th century to represent knowledge and thinking, as applied to playing the game of chess (and other games like checkers):
Using "ends-and-means" heuristics a human chess player can intuitively determine optimal outcomes and how to achieve them regardless of the number of moves necessary, but a computer must be systematic in its analysis. Most players agree that looking at least five moves ahead (ten plies) when necessary is required to play well. Normal tournament rules give each player an average of three minutes per move. On average there are more than 30 legal moves per chess position, so a computer must examine a quadrillion possibilities to look ahead ten plies (five full moves); one that could examine a million positions a second would require more than 30 years.[9]
So a limited lookahead (search) to some depth, followed by using domain-specific knowledge to evaluate the resulting terminal positions was proposed. A kind of middle-ground position, given good moves by both sides, would result, and its evaluation would inform the player about the goodness or badness of the moves chosen. Searching and comparing operations on the tree were well suited to computer calculation; the representation of subtle chess knowledge in the evaluation function was not.  The early chess programs suffered in both areas: searching the vast tree required computational resources far beyond those available, and what chess knowledge was useful and how it was to be encoded would take decades to discover.
The developers of a chess-playing computer system must decide on a number of fundamental implementation issues. These include:
Adriaan de Groot interviewed a number of chess players of varying strengths, and concluded that both masters and beginners look at around forty to fifty positions before deciding which move to play. What makes the former much better players is that they use pattern recognition skills built from experience. This enables them to examine some lines in much greater depth than others by simply not considering moves they can assume to be poor. More evidence for this being the case is the way that good human players find it much easier to recall positions from genuine chess games, breaking them down into a small number of recognizable sub-positions, rather than completely random arrangements of the same pieces. In contrast, poor players have the same level of recall for both.
The equivalent of this in computer chess are evaluation functions for leaf evaluation, which correspond to the human players' pattern recognition skills, and the use of machine learning techniques in training them, such as Texel tuning, stochastic gradient descent, and reinforcement learning, which corresponds to building experience in human players. This allows modern programs to examine some lines in much greater depth than others by using forwards pruning and other selective heuristics to simply not consider moves the program assume to be poor through their evaluation function, in the same way that human players do. The only fundamental difference between a computer program and a human in this sense is that a computer program can search much deeper than a human player could, allowing it to search more nodes and bypass the horizon effect to a much greater extent than is possible with human players.
Starting in the late 1990s, programmers began to develop separately engines (with a command-line interface which calculates which moves are strongest in a position) or a graphical user interface (GUI) which provides the player with a chessboard they can see, and pieces that can be moved.  Engines communicate their moves to the GUI using a protocol such as the Chess Engine Communication Protocol (CECP) or Universal Chess Interface (UCI). By dividing chess programs into these two pieces, developers can write only the user interface, or only the engine, without needing to write both parts of the program. (See also chess engine.)
Developers have to decide whether to connect the engine to an opening book and/or endgame tablebases or leave this to the GUI.
The data structure used to represent each chess position is key to the performance of move generation and position evaluation. Methods include pieces stored in an array ("mailbox" and "0x88"), piece positions stored in a list ("piece list"), collections of bit-sets for piece locations ("bitboards"), and huffman coded positions for compact long-term storage.
Computer chess programs consider chess moves as a game tree. In theory, they examine all moves, then all counter-moves to those moves, then all moves countering them, and so on, where each individual move by one player is called a "ply". This evaluation continues until a certain maximum search depth or the program determines that a final "leaf" position has been reached (e.g. checkmate).
One particular type of search algorithm used in computer chess are minimax search algorithms, where at each ply the "best" move by the player is selected; one player is trying to maximize the score, the other to minimize it. By this alternating process, one particular terminal node whose evaluation represents the searched value of the position will be arrived at.  Its value is backed up to the root, and that evaluation becomes the valuation of the position on the board.  This search process is called minimax.
In addition, various selective search heuristics, such as quiescence search, forward pruning, search extensions and search reductions, are also used as well. These heuristics are triggered based on certain conditions in an attempt to weed out obviously bad moves (history moves) or to investigate interesting nodes (e.g. check extensions, passed pawns on seventh rank, etc.). These selective search heuristics have to be used very carefully however. Over extend and the program wastes too much time looking at uninteresting positions. If too much is pruned or reduced, there is a risk cutting out interesting nodes.
Monte Carlo tree search (MCTS) is a heuristic search algorithm which expands the search tree based on random sampling of the search space. A version of Monte Carlo tree search commonly used in computer chess is PUCT, Predictor and Upper Confidence bounds applied to Trees.
DeepMind's AlphaZero and Leela Chess Zero uses MCTS instead of minimax. Such engines use batching on graphics processing units in order to calculate their evaluation functions and policy (move selection), and therefore require a parallel search algorithm as calculations on the GPU are inherently parallel. The minimax and alpha-beta pruning algorithms used in computer chess are inherently serial algorithms, so would not work well with batching on the GPU. On the other hand, MCTS is a good alternative, because the random sampling used in Monte Carlo tree search lends itself well to parallel computing, and is why nearly all engines which support calculations on the GPU use MCTS instead of alpha-beta.
Of course, faster hardware and additional memory can improve chess program playing strength.  Hyperthreaded architectures can improve performance modestly if the program is running on a single core or a small number of cores.  Most modern programs are designed to take advantage of multiple cores to do parallel search.  Other programs are designed to run on a general purpose computer and allocate move generation, parallel search, or evaluation to dedicated processors or specialized co-processors.
The first paper on search was by Claude Shannon in 1950.[24] He predicted the two main possible search strategies which would be used, which he labeled "Type A" and "Type B",[25] before anyone had programmed a computer to play chess.
Type A programs would use a "brute force" approach, examining every possible position for a fixed number of moves using a pure naive minimax algorithm. Shannon believed this would be impractical for two reasons.
Second, it ignored the problem of quiescence, trying to only evaluate a position that is at the end of an exchange of pieces or other important sequence of moves ('lines'). He expected that adapting minimax to cope with this would greatly increase the number of positions needing to be looked at and slow the program down still further. He expected that adapting type A to cope with this would greatly increase the number of positions needing to be looked at and slow the program down still further.
This led naturally to what is referred to as "selective search" or "type B search", using chess knowledge (heuristics) to select a few presumably good moves from each position to search, and prune away the others without searching. Instead of wasting processing power examining bad or trivial moves, Shannon suggested that type B programs would use two improvements:
This would enable them to look further ahead ('deeper') at the most significant lines in a reasonable time. However, early attempts at selective search often resulted in the best move or moves being pruned away. As a result, little or no progress was made for the next 25 years dominated by this first iteration of the selective search paradigm. The best program produced in this early period was Mac Hack VI in 1967; it played at the about the same level as the average amateur (C class on the United States Chess Federation rating scale).
Meanwhile, hardware continued to improve, and in 1974, brute force searching was implemented for the first time in the Northwestern University Chess 4.0 program. In this approach, all alternative moves at a node are searched, and none are pruned away. They discovered that the time required to simply search all the moves was much less than the time required to apply knowledge-intensive heuristics to select just a few of them, and the benefit of not prematurely or inadvertently pruning away good moves resulted in substantially stronger performance.
In the 1980s and 1990s, progress was finally made in the selective search paradigm, with the development of quiescence search, null move pruning, and other modern selective search heuristics. These heuristics had far fewer mistakes than earlier heuristics did, and was found to be worth the extra time it saved because it could search deeper and widely adopted by many engines. While many modern programs do use alpha-beta search as a substrate for their search algorithm, these additional selective search heuristics used in modern programs means that the program no longer does a "brute force" search. Instead they heavily rely on these selective search heuristics to extend lines the program considers good and prune and reduce lines the program considers bad, to the point where most of the nodes on the search tree are pruned away, enabling modern programs to search very deep.
In the 1970s, most chess programs ran on super computers like Control Data Cyber 176s or Cray-1s, indicative that during that developmental period for computer chess, processing power was the limiting factor in performance.  Most chess programs struggled to search to a depth greater than 3 ply.  It was not until the hardware chess machines of the 1980s, that a relationship between processor speed and knowledge encoded in the evaluation function became apparent.
It has been estimated that doubling the computer speed gains approximately fifty to seventy Elo points in playing strength (Levy & Newborn 1991:192).
For most chess positions, computers cannot look ahead to all possible final positions. Instead, they must look ahead a few plies and compare the possible positions, known as leaves. The algorithm that evaluates leaves is termed the "evaluation function", and these algorithms are often vastly different between different chess programs. Evaluation functions typically evaluate positions in hundredths of a pawn (called a centipawn), where by convention, a positive evaluation favors White, and a negative evaluation favors Black. However, some evaluation function output win/draw/loss percentages instead of centipawns.
Historically, handcrafted evaluation functions consider material value along with other factors affecting the strength of each side. When counting up the material for each side, typical values for pieces are 1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen. (See Chess piece relative value.) The king is sometimes given an arbitrarily high value such as 200 points (Shannon's paper) to ensure that a checkmate outweighs all other factors (Levy & Newborn 1991:45). In addition to points for pieces, most handcrafted evaluation functions take many factors into account, such as pawn structure, the fact that a pair of bishops are usually worth more, centralized pieces are worth more, and so on. The protection of kings is usually considered, as well as the phase of the game (opening, middle or endgame). Machine learning techniques such as Texel turning, stochastic gradient descent, or reinforcement learning are usually used to optimise handcrafted evaluation functions.
Most modern evaluation functions make use of neural networks. The most common evaluation function in use today is the efficiently updatable neural network, which is a shallow neural network whose inputs are piece-square tables. Piece-square tables are a set of 64 values corresponding to the squares of the chessboard, and there typically exists a piece-square table for every piece and colour, resulting in 12 piece-square tables and thus 768 inputs into the neural network. In addition, some engines use deep neural networks in their evaluation function. Neural networks are usually trained using some reinforcement learning algorithm, in conjunction with supervised learning or unsupervised learning.
The output of the evaluation function is a single scalar, quantized in centipawns or other units, which is, in the case of handcrafted evaluation functions, a weighted summation of the various factors described, or in the case of neural network based evaluation functions, the output of the head of the neural network. The evaluation putatively represents or approximates the value of the subtree below the evaluated node as if it had been searched to termination, i.e. the end of the game.  During the search, an evaluation is compared against evaluations of other leaves, eliminating nodes that represent bad or poor moves for either side, to yield a node which by convergence, represents the value of the position with best play by both sides.
Endgame play had long been one of the great weaknesses of chess programs because of the depth of search needed. Some otherwise master-level programs were unable to win in positions where even intermediate human players could force a win.
To solve this problem, computers have been used to analyze some chess endgame positions completely, starting with king and pawn against king. Such endgame tablebases are generated in advance using a form of retrograde analysis, starting with positions where the final result is known (e.g., where one side has been mated) and seeing which other positions are one move away from them, then which are one move from those, etc. Ken Thompson was a pioneer in this area.
The results of the computer analysis sometimes surprised people. In 1977 Thompson's Belle chess machine used the endgame tablebase for a king and rook against king and queen and was able to draw that theoretically lost ending against several masters (see Philidor position#Queen versus rook).  This was despite not following the usual strategy to delay defeat by keeping the defending king and rook close together for as long as possible. Asked to explain the reasons behind some of the program's moves, Thompson was unable to do so beyond saying the program's database simply returned the best moves.
Over the years, other endgame database formats have been released including the Edward Tablebase, the De Koning Database and the Nalimov Tablebase which is used by many chess programs such as Rybka, Shredder and Fritz. Tablebases for all positions with six pieces are available.[26] Some seven-piece endgames have been analyzed by Marc Bourzutschky and Yakov Konoval.[27] Programmers using the Lomonosov supercomputers in Moscow have completed a chess tablebase for all endgames with seven pieces or fewer (trivial endgame positions are excluded, such as six white pieces versus a lone black king).[28][29] In all of these endgame databases it is assumed that castling is no longer possible.
Many tablebases do not consider the fifty-move rule, under which a game where fifty moves pass without a capture or pawn move can be claimed to be a draw by either player. This results in the tablebase returning results such as "Forced mate in sixty-six moves" in some positions which would actually be drawn because of the fifty-move rule. One reason for this is that if the rules of chess were to be changed once more, giving more time to win such positions, it will not be necessary to regenerate all the tablebases. It is also very easy for the program using the tablebases to notice and take account of this 'feature' and in any case if using an endgame tablebase will choose the move that leads to the quickest win (even if it would fall foul of the fifty-move rule with perfect play). If playing an opponent not using a tablebase, such a choice will give good chances of winning within fifty moves.
The Nalimov tablebases, which use state-of-the-art compression techniques, require 7.05 GB of hard disk space for all five-piece endings. To cover all the six-piece endings requires approximately 1.2 TB. It is estimated that a seven-piece tablebase requires between 50 and 200 TB of storage space.[30]
Endgame databases featured prominently in 1999, when Kasparov played an exhibition match on the Internet against the rest of the world. A seven piece Queen and pawn endgame was reached with the World Team fighting to salvage a draw. Eugene Nalimov helped by generating the six piece ending tablebase where both sides had two Queens which was used heavily to aid analysis by both sides.
Chess engines, like human beings, may save processing time as well as select strong variations as expounded by the masters, by referencing an opening book stored in a disk database.  Opening books cover the opening moves of a game to variable depth, depending on opening and variation, but usually to the first 10-12 moves (20-24 ply).  Since the openings have been studied in depth by the masters for centuries, and some are known to well into the middle game, the valuations of specific variations by the masters will usually be superior to the general heuristics of the program.
While at one time, playing an out-of-book move in order to put the chess program onto its own resources might have been an effective strategy because chess opening books were selective to the program's playing style, and programs had notable weaknesses relative to humans, that is no longer true today.[when?]  The opening books stored in computer databases are most likely far more extensive than even the best prepared humans, and playing an early out-of-book move may result in the computer finding the unusual move in its book and saddling the opponent with a sharp disadvantage. Even if it does not, playing out-of-book may be much better for tactically sharp chess programs than for humans who have to discover strong moves in an unfamiliar variation over the board.
CEGT,[31] CSS,[32] SSDF,[33] WBEC,[34] REBEL,[35] FGRL,[36] and IPON[37] maintain rating lists allowing fans to compare the strength of engines. Various versions of Stockfish, Komodo, Leela Chess Zero, and Fat Fritz dominate the rating lists in the early 2020s.
CCRL  (Computer Chess Rating Lists) is an organisation that tests computer chess engines' strength by playing the programs against each other. CCRL was founded in 2006 to promote computer-computer competition and tabulate results on a rating list.[38]
The idea of creating a chess-playing machine dates back to the eighteenth century.  Around 1769, the chess playing automaton called The Turk, created by Hungarian inventor Farkas Kempelen, became famous before being exposed as a hoax. Before the development of digital computing, serious trials based on automata such as El Ajedrecista of 1912 which played a king and rook versus king ending, were too complex and limited to be useful for playing full games of chess.  The field of mechanical chess research languished until the advent of the digital computer in the 1950s.
Since then, chess enthusiasts and computer engineers have built, with increasing degrees of seriousness and success, chess-playing machines and computer programs. One of the few chess grandmasters to devote himself seriously to computer chess was former World Chess Champion Mikhail Botvinnik, who wrote several works on the subject.  He also held a doctorate in electrical engineering.  Working with relatively primitive hardware available in the Soviet Union in the early 1960s, Botvinnik had no choice but to investigate software move selection techniques; at the time only the most powerful computers could achieve much beyond a three-ply full-width search, and Botvinnik had no such machines.  In 1965 Botvinnik was a consultant to the ITEP team in a US-Soviet computer chess match (see Kotok-McCarthy).
In 1978, an early rendition of Ken Thompson's hardware chess machine Belle, entered and won the North American Computer Chess Championship over the dominant Northwestern University Chess 4.7.
In 2016, NPR asked experts to characterize the playing style of computer chess engines. Murray Campbell of IBM stated that "Computers don't have any sense of aesthetics... They play what they think is the objectively best move in any position, even if it looks absurd, and they can play any move no matter how ugly it is." Grandmasters Andrew Soltis and Susan Polgar stated that computers are more likely to retreat than humans are.[22]
While neural networks have been used in the evaluation functions of chess engines since the late 1980s, with programs such as NeuroChess, Morph, Blondie25, Giraffe, AlphaZero, and MuZero,[41][42][43][44][45] neural networks did not become widely adopted by chess engines until the arrival of efficiently updatable neural networks in the summer of 2020. Efficiently updatable neural networks were originally developed in computer shogi in 2018 by Yu Nasu,[46][47] and had to be first ported to a derivative of Stockfish called Stockfish NNUE on 31 May 2020,[48] and integrated into the official Stockfish engine on 6 August 2020,[49][50] before other chess programmers began to adopt neural networks into their engines.
Some people, such as the Royal Society's Venki Ramakrishnan, believe that AlphaZero lead to the widespread adoption of neural networks in chess engines.[51] However, AlphaZero influenced very few engines to begin using neural networks, and those tended to be new experimental engines such as Leela Chess Zero, which began specifically to replicate the AlphaZero paper. The deep neural networks used in AlphaZero's evaluation function required expensive graphics processing units, which were not compatible with existing chess engines. The vast majority of chess engines only use central processing units, and computing and processing information on the GPUs require special libraries in the backend such as Nvidia's CUDA, which none of the engines had access to. Thus the vast majority of chess engines such as Komodo and Stockfish continued to use handcrafted evaluation functions until efficiently updatable neural networks were ported to computer chess in 2020, which did not require either the use of GPUs or libraries like CUDA at all. Even then, the neural networks used in computer chess are fairly shallow, and the deep reinforcement learning methods pioneered by AlphaZero are still extremely rare in computer chess.
These chess playing systems include custom hardware with approx. dates of introduction (excluding dedicated microcomputers):
In the late 1970s to early 1990s, there was a competitive market for dedicated chess computers. This market changed in the mid-1990s when computers with dedicated processors could no longer compete with the fast processors in personal computers.
These programs can be run on MS-DOS, and can be run on 64-bit Windows 10 via emulators such as DOSBox or Qemu:[74]
Progress has also been made from the other side: as of 2012, all 7 and fewer pieces (2 kings and up to 5 other pieces) endgames have been solved.
A "chess engine" is software that calculates and orders which moves are the strongest to play in a given position.  Engine authors focus on improving the play of their engines, often just importing the engine into a graphical user interface (GUI) developed by someone else.  Engines communicate with the GUI by standardized protocols such as the nowadays ubiquitous Universal Chess Interface developed by Stefan Meyer-Kahlen and Franz Huber. There are others, like the Chess Engine Communication Protocol developed by Tim Mann for GNU Chess and Winboard. Chessbase has its own proprietary protocol, and at one time Millennium 2000 had another protocol used for ChessGenius.  Engines designed for one operating system and protocol may be ported to other OS's or protocols.
Chess engines are regularly matched against each other at dedicated chess engine tournaments.
In 1997, the Internet Chess Club released its first Java client for playing chess online against other people inside one's webbrowser.[78]  This was probably one of the first chess web apps. Free Internet Chess Server followed soon after with a similar client.[79]  In 2004, International Correspondence Chess Federation opened up a web server to replace their email-based system.[80] Chess.com started offering Live Chess in 2007.[81] Chessbase/Playchess has long had a downloadable client, and added a web-based client in 2013.[82]
Another popular web app is tactics training.  The now defunct Chess Tactics Server opened its site in 2006,[83] followed by Chesstempo the next year,[84] and Chess.com added its Tactics Trainer in 2008.[85] Chessbase added a tactics trainer web app in 2015.[86]
Chessbase took their chess game database online in 1998.[87]  Another early chess game databases was Chess Lab, which started in 1999.[88] New In Chess had initially tried to compete with Chessbase by releasing a NICBase program for Windows 3.x, but eventually, decided to give up on software, and instead focus on their online database starting in 2002.[89]
One could play against the engine Shredder online from 2006.[90]  In 2015, Chessbase added a play Fritz web app,[91] as well as My Games for storing one's games.[92]
Starting in 2007, Chess.com offered the content of the training program, Chess Mentor, to their customers online.[93]  Top GMs such as Sam Shankland and Walter Browne have contributed lessons.
Deep Blue was a chess-playing expert system run on a unique purpose-built IBM supercomputer. It was the first computer to win a game, and the first to win a match, against a reigning world champion under regular time controls. Development began in 1985 at Carnegie Mellon University under the name ChipTest. It then moved to IBM, where it was first renamed Deep Thought, then again in 1989 to Deep Blue. It first played world champion Garry Kasparov in a six-game match in 1996, where it lost four games to two. It was upgraded in 1997 and in a six-game re-match, it defeated Kasparov by winning three games and drawing one. Deep Blue's victory is considered a milestone in the history of artificial intelligence and has been the subject of several books and films.
While a doctoral student at Carnegie Mellon University, Feng-hsiung Hsu began development of a chess-playing supercomputer under the name ChipTest. The machine won the North American Computer Chess Championship in 1987 and Hsu and his team followed up with a successor, Deep Thought, in 1988.[2][3] After receiving his doctorate in 1989, Hsu and Murray Campbell joined IBM Research to continue their project to build a machine that could defeat a world chess champion.[4] Their colleague Thomas Anantharaman briefly joined them at IBM before leaving for the finance industry and being replaced by programmer Arthur Joseph Hoane.[5][6] Jerry Brody, a long-time employee of IBM Research, subsequently joined the team in 1990.[7]
After Deep Thought's two-game 1989 loss to Kasparov, IBM held a contest to rename the chess machine: the winning name was "Deep Blue," submitted by Peter Fitzhugh Brown,[8] was a play on IBM's nickname, "Big Blue."[a] After a scaled-down version of Deep Blue played Grandmaster Joel Benjamin,[10] Hsu and Campbell decided that Benjamin was the expert they were looking for to help develop Deep Blue's opening book, so hired him to assist with the preparations for Deep Blue's matches against Garry Kasparov.[11] In 1995, a Deep Blue prototype played in the eighth World Computer Chess Championship, playing Wchess to a draw before ultimately losing to Fritz in round five, despite playing as White.[12]
In 1997, the Chicago Tribune mistakenly reported that Deep Blue had been sold to United Airlines, a confusion based upon its physical resemblance to IBM's mainstream RS6000/SP2 systems.[13]
Today, one of the two racks that made up Deep Blue is held by the National Museum of American History, having previously been displayed in an exhibit about the Information Age,[14] while the other rack was acquired by the Computer History Museum in 1997, and is displayed in the Revolution exhibit's "Artificial Intelligence and Robotics" gallery.[15] Several books were written about Deep Blue, among them Behind Deep Blue: Building the Computer that Defeated the World Chess Champion by Deep Blue developer Feng-hsiung Hsu.[16]
In the 44th move of the first game of their second match, unknown to Kasparov, a bug in Deep Blue's code led it to enter an unintentional loop, which it exited by taking a randomly selected valid move.[24] Kasparov did not take this possibility into account, and misattributed the seemingly pointless move to "superior intelligence".[21] Subsequently, Kasparov experienced a decline in performance in the following game,[24] though he denies this was due to anxiety in the wake of Deep Blue's inscrutable move.[25]
After his loss, Kasparov said that he sometimes saw unusual creativity in the machine's moves, suggesting that during the second game, human chess players had intervened on behalf of the machine. IBM denied this, saying the only human intervention occurred between games.[26][27] Kasparov demanded a rematch, but IBM had dismantled Deep Blue after its victory and refused the rematch.[28] The rules allowed the developers to modify the program between games, an opportunity they said they used to shore up weaknesses in the computer's play that were revealed during the course of the match. Kasparov requested printouts of the machine's log files, but IBM refused, although the company later published the logs on the Internet.[29]
Kasparov initially called Deep Blue an "alien opponent" but later belittled it, stating that it was "as intelligent as your alarm clock".[30] According to Martin Amis, two grandmasters who played Deep Blue agreed that it was "like a wall coming at you".[31][32] Hsu had the rights to use the Deep Blue design independently of IBM, but also independently declined Kasparov's rematch offer.[33] In 2003 the documentary film Game Over: Kasparov and the Machine investigated Kasparov's claims that IBM had cheated. In the film, some interviewees describe IBM's investment in Deep Blue as an effort to boost its stock value.[34]
Following Deep Blue's victory, AI specialist Omar Syed designed a new game, Arimaa, which was intended to be very simple for humans but very difficult for computers to master;[35][36] however, in 2015, computers proved capable of defeating strong Arimaa players.[37] Since Deep Blue's victory, computer scientists have developed software for other complex board games with competitive communities. AlphaGo defeated top Go players in the 2010s.[38][39]
Computer scientists such as Deep Blue developer Campbell believed that playing chess was a good measurement for the effectiveness of artificial intelligence, and by beating a world champion chess player, IBM showed that they had made significant progress.[3] Deep Blue is also responsible for the popularity of using games as a display medium for artificial intelligence, as in the cases of IBM Watson or AlphaGo.[40]
Before the second match, the program's rules were fine-tuned by grandmaster Joel Benjamin. The opening library was provided by grandmasters Miguel Illescas, John Fedorowicz, and Nick de Firmian.[45] When Kasparov requested that he be allowed to study other games that Deep Blue had played so as to better understand his opponent, IBM refused, leading Kasparov to study many popular PC chess games to familiarize himself with computer gameplay.[46]
The 1997 match was widely covered by the media, and Deep Blue became a celebrity.[1]  After the match, it was reported that IBM had dismantled Deep Blue, but in fact it remained in operation for several years.[2]
Deep Blue's win was seen as symbolically significant, a sign that artificial intelligence was catching up to human intelligence, and could defeat one of humanity's great intellectual champions.[3] Later analysis tended to play down Kasparov's loss as a result of uncharacteristically bad play on Kasparov's part, and play down the intellectual value of chess as a game that can be defeated by brute force.[4][5]
Deep Blue's victory switched the canonical example of a game where humans outmatched machines to the ancient Chinese game of Go, a game of simple rules and far more possible moves than chess, which requires more intuition and is far less susceptible to brute force.[7] Go is widely played in China, South Korea, and Japan, and was considered one of the four arts of the Chinese scholar in antiquity. Go programs were able to defeat only amateur players until Google DeepMind's AlphaGo program defeated the European Go champion Fan Hui in 2015 and then surprisingly defeated top-ranked Lee Sedol in the match AlphaGo versus Lee Sedol in 2016.[8] While Deep Blue mainly relied on brute computational force to evaluate millions of positions, AlphaGo also relied on neural networks and reinforcement learning.
February 10. The first game began with the Sicilian Defence, Alapin Variation. The first game of the 1996 match was the first game to be won by a chess-playing computer against a reigning world champion under normal chess tournament conditions, and in particular, classical time controls.
February 11. The second game transposed to an open line of the Catalan Opening. Kasparov played in what could be called a preemptive style, blocking all Deep Blue's development attempts. The game lasted for 73 moves but eventually Deep Blue's operator had to resign the game for the computer in a position where both players had a bishop but Kasparov had three pawns to Deep Blue's one.
February 13. As in the first game, Kasparov played the Sicilian Defence to which Deep Blue again responded with the Alapin Variation. The game lasted for 39 moves and was drawn.
February 14. The fourth game was the second to end in a draw, although at one point Deep Blue's team declined Kasparov's draw offer. The opening transposed to a line of the Queen's Gambit Declined.
February 16. The fifth game was the turning point of the match. Its opening transposed to the Scotch Four Knights Game, an opening combining the characteristics of the Scotch Game and the Four Knights Game.  Game 5 was considered an embarrassment for the Deep Blue team because they had declined Kasparov's draw offer after the 23rd move. This was the only game in the match that Black won.
February 17. The sixth game, like the fourth, transposed to the same line of the Queen's Gambit Declined. The final game was an illustration of just how badly chess engines could play in some positions at the time. Employing anti-computer tactics and keeping the focus of the game on long-term planning, Kasparov slowly improved his position throughout the mid-game while Deep Blue wasted time doing very little to improve its position. By the end of the game, Deep Blue's pieces were crammed into its queenside corner, with no moves to make aside from shuffling its king. Kasparov had all the time in the world to finish the rout.[22] Kasparov's next move would probably have been 44.Qe7 to exchange the queens. That would have allowed his pawn, which was about to promote, to advance.[23]
Deep Blue's 44th move in this game was allegedly the result of a bug in which Deep Blue, unable to determine a desirable move, resorted to a fail-safe.[28][29][30]
May 4. The second game began with the Ruy Lopez opening, Smyslov Variation. Kasparov eventually resigned, although post-game analysis indicates that he could have held a draw in the final position. After this game Kasparov accused IBM of cheating, by alleging that a grandmaster (presumably a top rival) had been behind a certain move. The claim was repeated in the documentary Game Over: Kasparov and the Machine.[32][33]
At the time it was reported that Kasparov missed the fact that after 45...Qe3 46.Qxd6 Re8, Black (Kasparov) can force a draw by perpetual check. His friends told him so the next morning.[36] They suggested 47.h4 h5!, a position after which the black queen can perpetually check White. This is possible as Deep Blue moved 44.Kf1 instead of an alternate move of its king. Regarding the end of game 2 and 44.Kf1 in particular, chess journalist Mig Greengard in the Game Over film states, "It turns out, that the position in, here at the end is actually a draw, and that, one of Deep Blue's final moves was a terrible error, because Deep Blue has two choices here. It can move its king here or move its king over here. It picked the wrong place to step." Another person in that film, four-time US champion Yasser Seirawan, then concludes that "The computer had left its king a little un-defended. And Garry could have threatened a perpetual check, not a win but a perpetual check."
The moves that surprised Kasparov enough to allege cheating were 36.axb5! axb5 37.Be4! after which Black is lost. A more materialistic machine could have won two pawns with 36.Qb6 Rd8 37.axb5 Rab8 38.Qxa6, but after 38...e4! Black would have acquired strong counterplay.[37] Deep Blue could have also won material with the move 37.Qb6.  Kasparov and many others thought the move 37.Be4! ignored material gain by force and was too sophisticated for a computer, suggesting there had been some sort of human intervention during the game.
May 6. In the third game, Kasparov chose to employ the irregular 1.d3, the Mieses Opening.  The game then transposed to a line of the English Opening. Kasparov believed that by playing an esoteric opening, the computer would get out of its opening book and play the opening worse than it would have done using the book. Although this is nowadays a common tactic, it was a relatively new idea at the time.[38] Despite this anti-computer tactic, the game was drawn.
If White plays 50.g8=Q, then Black can force a draw by threefold repetition, starting with 50...Rd1+ and then 51...Rd2+.
Garry Kimovich Kasparov[a] (born 13 April 1963) is a Russian chess grandmaster, former World Chess Champion, political activist and writer. His peak FIDE rating of 2851,[3] achieved in 1999, was the highest recorded until being surpassed by Magnus Carlsen in 2013. From 1984 until his retirement in 2005, Kasparov was ranked world no. 1 for a record 255 months overall. Kasparov also holds records for the most consecutive professional tournament victories (15) and Chess Oscars (11).
Kasparov became the youngest-ever undisputed World Chess Champion in 1985 at age 22 by defeating then-champion Anatoly Karpov.[4] He held the official FIDE world title until 1993, when a dispute with FIDE led him to set up a rival organisation, the Professional Chess Association.[5] In 1997 he became the first world champion to lose a match to a computer under standard time controls when he was defeated by the IBM supercomputer Deep Blue in a highly publicised match. He continued to hold the "Classical" World Chess Championship until his defeat by Vladimir Kramnik in 2000. Despite losing the PCA title, he continued winning tournaments and was the world's highest-rated player when he retired from regular competitive chess in 2005.
Since retiring, he has devoted his time to politics and writing. He formed the United Civil Front movement and was a member of The Other Russia, a coalition opposing the administration and policies of Vladimir Putin. In 2008, he announced an intention to run as a candidate in that year's Russian presidential race, but after encountering logistical problems in his campaign, for which he blamed "official obstruction", he withdrew.[6][7][8] In the wake of the Russian mass protests that began in 2011, he announced in 2013 that he had left Russia for the immediate future out of fear of persecution.[9] Following his flight from Russia, he lived in New York City with his family.[10][11] In 2014, he obtained Croatian citizenship and has maintained a residence in Podstrana near Split.[12][13][14]
Kasparov is currently chairman of the Human Rights Foundation and chairs its International Council. In 2017, he founded the Renew Democracy Initiative (RDI), an American political organisation promoting and defending liberal democracy in the U.S. and abroad. He serves as chairman of the group. Kasparov is also a security ambassador for the software company Avast.[15]
According to Kasparov himself, he was named after United States President Harry Truman,[25] "whom my father admired for taking a strong stand against communism. It was a rare name in Russia, until Harry Potter came along."[26]
Kasparov began the serious study of chess after he came across a problem set up by his parents and proposed a solution.[27] When he was seven years old, his father died of leukaemia.[28] At the age of twelve, Kasparov, upon the request of his mother Klara and with the consent of the family, adopted Klara's surname Kasparov, which was done to avoid possible anti-Semitic tensions common in the USSR at the time.[29][30]
From age seven, Kasparov attended the Young Pioneer Palace in Baku and, at ten, began training at Mikhail Botvinnik's chess school under coach Vladimir Makogonov. Makogonov helped develop Kasparov's positional skills and taught him to play the Caro-Kann Defence and the Tartakower System of the Queen's Gambit Declined.[31] Kasparov won the Soviet Junior Championship in Tbilisi in 1976, scoring 7/9 points, at age thirteen. He repeated the feat the following year, winning with a score of 8.5/9. He was being coached by Alexander Shakarov during this time.[32]
In 1978, Kasparov participated in the Sokolsky Memorial tournament in Minsk. He had received a special invitation to enter the tournament but took first place and became a chess master. Kasparov has stressed that this event was a turning point in his life and that it convinced him to choose chess as his career: "I will remember the Sokolsky Memorial as long as I live", he wrote. He has also said that after the victory, he thought he had a very good shot at the World Championship.[33]
He first qualified for the Soviet Chess Championship at age 15 in 1978, the youngest-ever player at that level. He won the 64-player Swiss system tournament at Daugavpils on tiebreak over Igor V. Ivanov to capture the sole qualifying place.[34]
Kasparov rose quickly through the FIDE world rankings. Due to an oversight by the Russian Chess Federation, which believed the tournament was for juniors, he participated in a grandmaster tournament in Banja Luka, SR Bosnia and Herzegovina (part of Yugoslavia at the time), in 1979 while still unrated. He was a replacement for the Soviet defector Viktor Korchnoi, who was originally invited but withdrew due to the threat of a boycott from the Soviets.[35] Kasparov won this high-class tournament, emerging with a provisional rating of 2595, enough to catapult him to the top group of chess players (at the time, number 15 in the world).[36] The next year, 1980, he won the World Junior Chess Championship in Dortmund, West Germany. Later that year, he made his debut as the second reserve for the Soviet Union at the Chess Olympiad at Valletta, Malta, and became a Grandmaster.[37]
As a teenager, Kasparov shared the USSR Chess Championship in 1981 with Lev Psakhis (12.5/17), although Psakhis won their game.[38] His first win in a superclass-level international tournament was scored at Bugojno, SR Bosnia and Herzegovina, Yugoslavia in 1982. He earned a place in the 1982 Moscow Interzonal tournament, which he won, to qualify for the Candidates Tournament.[39] At age 19, he was the youngest Candidate since Bobby Fischer, who was 15 when he qualified in 1958. At this stage, he was already the No. 2-rated player in the world, trailing only World Champion Karpov on the January 1983 list.[40]
The match became the first, and so far only, world championship match to be abandoned without result. Kasparov's relations with Campomanes and FIDE became strained,[51][52] and matters came to a head in 1993 with Kasparov's complete break-away from FIDE.[53]
In November 1986, Kasparov had created the Grandmasters Association (GMA) to represent professional players and give them more say in FIDE's activities. Kasparov assumed a leadership role. GMA's major achievement was in organising a series of six World Cup tournaments for the world's top players.[64] This caused a somewhat uneasy relationship to develop between Kasparov and FIDE.[65]
This stand-off lasted until 1993, by which time a new challenger had qualified through the Candidates cycle: Nigel Short, a British grandmaster who had defeated Karpov in a qualifying match and then Jan Timman in the finals held in early 1993. After a confusing and compressed bidding process produced lower financial estimates than expected,[66] the world champion and his challenger decided to play outside FIDE's jurisdiction. Their match took place under the auspices of the Professional Chess Association (PCA), an organisation established by Kasparov. At this point, a fracture occurred in the lineage of the FIDE World Championship. In an interview in 2007, Kasparov called the break with FIDE the worst mistake of his career, as it hurt the game in the long run.[67]
FIDE removed Kasparov and Short from its rating list. Subsequently, the PCA created a rating list of its own, which featured all the world top players regardless of their relation to FIDE. There were now two World Champions: PCA champion Kasparov and FIDE champion Karpov.[72] The title remained split for 13 years.
Kasparov defended his PCA title in a 1995 match against Viswanathan Anand at the World Trade Center in New York City. Kasparov won the match by four wins to one, with thirteen draws.[73]
Kasparov tried to organise another World Championship match under a different organisation, the World Chess Association (WCA), with Linares organiser Luis Rentero. Alexei Shirov and Kramnik played a candidates match to decide the challenger, which Shirov won in an upset. But when Rentero admitted that the funds required and promised had never materialised, the WCA collapsed. Yet another body stepped in, BrainGames.com, headed by Raymond Keene. After a match with Shirov could not be agreed by BrainGames.com and talks with Anand collapsed, a match was instead arranged against Kramnik.[74]
During this period, Kasparov was approached by Oakham School in the United Kingdom, at the time the only school in the country with a full-time chess coach,[75] and developed an interest in the use of chess in education. In 1997, Kasparov supported a scholarship programme at the school.[76] Kasparov also won the Marca Leyenda trophy that year.[77]
In 1999, he played a well-known game against Topalov wherein he won after a rook sacrifice and king hunt.[78][79]
The Kasparov-Kramnik match took place in London during the latter half of 2000. Kramnik had been a student of Kasparov's at the famous Botvinnik/Kasparov chess school in Russia and had served on Kasparov's team for the 1995 match with Anand.[80]
Kasparov won a series of major tournaments and remained the PCA top-rated player in the world, ahead of both Kramnik and the FIDE World Champion. In 2001 he refused an invitation to the 2002 Dortmund Candidates Tournament for the Classical title, claiming his results had earned him a rematch with Kramnik.[82]
Because of Kasparov's continuing strong results and status as FIDE world No. 1, he was included in the so-called "Prague Agreement", masterminded by Yasser Seirawan and intended to reunite the two World Championships. Kasparov was to play a match against the FIDE World Champion Ponomariov in September 2003.[84] But this match was called off after Ponomariov refused to sign his contract for it without reservation. In its place, there were plans for a match against Rustam Kasimdzhanov, winner of the FIDE World Chess Championship 2004, to be held in January 2005 in the United Arab Emirates. These also fell through owing to a lack of funding. Plans to hold the match in Turkey instead came too late. Kasparov announced in January 2005 that he was tired of waiting for FIDE to arrange a match and had decided to stop all efforts to become undisputed World Champion once more.[85]
After winning the prestigious Linares tournament for the ninth time, Kasparov announced on 10 March 2005 that he would retire from regular competitive chess. He cited as the reason a lack of personal goals in the chess world. When winning the Russian championship in 2004, he commented that it had been the last major title he had never won outright. He also expressed frustration at the failure to reunify the world championship.[86][85]
Kasparov said he might play in some rapid chess events for fun, but he intended to spend more time on his books, including the My Great Predecessors series, and work on the links between decision-making in chess and other areas of life. He also stated that he would continue to involve himself in Russian politics, which he viewed as "headed down the wrong path."[87][88]
Kasparov coached Carlsen for approximately one year, beginning in February 2009. The collaboration remained secret until September 2009.[91] Under Kasparov's tutelage, Carlsen in October 2009 became the youngest ever to achieve a FIDE rating higher than 2800, and he rose from world number four to world number one. While the pair initially planned to work together throughout 2010,[92] in March of that year it was announced that Carlsen had split from Kasparov and would no longer be using him as a trainer.[93] According to an interview with the German magazine Der Spiegel, Carlsen indicated that he would remain in contact and that he would continue to attend training sessions with Kasparov;[94] however, no further training sessions were held, and the cooperation fizzled out over the course of the spring.[95]
Kasparov began training the U.S. grandmaster Hikaru Nakamura in January 2011. The first of several training sessions was held in New York just before Nakamura participated in the Tata Steel Chess tournament in Wijk aan Zee, the Netherlands.[98] In December 2011, it was announced that their cooperation had come to an end.[99]
Kasparov played and won all 19 games of a simultaneous exhibition in Pula, Croatia on 19 August 2015.[106] At the Chess Club and Scholastic Center of Saint Louis on 28 and 29 April 2016, Kasparov played a 6-round exhibition blitz round-robin tournament with Fabiano Caruana, Wesley So and Nakamura in an event called the Ultimate Blitz Challenge.[107] He finished the tournament third with 9.5/18, behind Nakamura (11/18) and So (10/18). At the post-tournament interview, Kasparov announced that he would donate his winnings from playing the next top-level blitz exhibition match to assist funding of the American Olympic Team.[108]
A few days before the election took place, the New York Times Magazine had published a report on the viciously fought campaign. Included was information about a leaked contract between Kasparov and former FIDE Secretary General Ignatius Leong from Singapore, in which the Kasparov campaign reportedly "offered to pay Leong US$500,000 and to pay $250,000 a year for four years to the ASEAN Chess Academy, an organisation Leong helped create to teach the game, specifying that Leong would be responsible for delivering 11 votes from his region [...]".[113] In September 2015, the FIDE Ethics Commission found Kasparov and Leong guilty of violating its Code of Ethics[114] and later suspended them for two years from all FIDE functions and meetings.[115]
Kasparov came out of retirement to participate in the inaugural St. Louis Rapid and Blitz tournament from 14 to 19 August 2017, scoring 3.5/9 in the rapid and 9/18 in the blitz. He finished eighth in a strong field of 10, including Nakamura, Caruana, former world champion Anand and the eventual winner, Levon Aronian.[116][117] Kasparov promised that any tournament money he earned would go towards charities to promote chess in Africa.[118]
In 2020, he participated in 9LX, a Chess 960 tournament, and finished eighth of a field of 10 players.[119] His game against Carlsen, who tied for first place, was drawn.[120]
He launched Kasparovchess, a subscription-based online chess community featuring documentaries, lessons, puzzles, podcasts, articles, interviews and playing zones, in 2021.[121]
Kasparov played in the blitz section of the Grand Chess Tour 2021 event in Zagreb, Croatia. He performed poorly, however, scoring 0.5/9 on the first day and 2.0/9 on the second day, getting his only win against Jorden Van Foreest.[122][123] He also participated in 9LX 2, finishing fifth in a field of 10 players, with a score of 5/9.[124]
For the 1994 Moscow Olympiad, he had a significant organisational role, in helping to put together the event on short notice, after Thessaloniki cancelled its offer to host only a few weeks before the scheduled dates. Kasparov's detailed Olympiad record follows:[126]
Kasparov made his international debut for the USSR at age 16 in the 1980 European Team Championship and played for Russia in the 1992 edition of that championship. He won a total of five medals. His detailed record in this event follows:[127]
Kasparov received a Chess Oscar eleven times as the best chess player of the year in 1982-1983, 1985-1988, 1995-1996, 1999, and 2001-2002.[129] Between 1981 and 1991, he won or tied for first place in every tournament he entered.[130] In 1999, Kasparov reached an Elo rating at 2851 points, a record that stood for 13.5 years. On 10 December 2012, Carlsen achieved an unofficial rating of 2861 points, with which he topped the next release of the rating in January 2013.[131][132] With the exception of the PCA period and sharing first place with Kramnik in 1997, Kasparov led the rating list from 1985 to 2006. On 1 January 2006, Kasparov ranked first with a coefficient of 2812. However, he was excluded from the FIDE rating list of 1 April 2006 because he had not participated in tournaments for the previous 12 months.[133] In 2007, the international consulting company Synectics published a rating of 100 living geniuses in science, politics, art and entrepreneurship, in which Kasparov ranked 25th.[134]
The rivalry between Kasparov and Karpov (often referred to as the "two Ks")[135] is one of the greatest in the history of chess. In six years they played five matches and played 144 games.[136][137] For a long time there was personal enmity between Karpov and Kasparov.[138] The conflict between the two chess players also had a political connotation. Karpov was considered a representative of the Soviet nomenklatura, while Kasparov was young and popular, positioned himself as a "child of change", willingly gave candid interviews, and (especially in the West) had an aura of a rebel, although he never was a dissident.[137] Kasparov's 1985 victory coincided with the start of perestroika the Soviet Union.[139]
Carlsen, whom Kasparov coached from 2009 to 2010, said of Kasparov, "I've never seen someone with such a feel for dynamics in complex positions."[140] Kramnik has opined that "[Kasparov's] capacity for study is second to none", and said "There is nothing in chess he has been unable to deal with."[141]
Vladimir Kramnik called Kasparov a chess player with virtually no weaknesses.[142] His games are characterised by a dynamic style of play with a focus on tactics, depth of strategy, subtle calculation and original opening ideas.[143] Kasparov was known for his extensive opening preparation and aggressive play in it.[144][145] Sergey Shipov attributed Kasparov's moral and volitional qualities (impulsiveness and psychological instability) and excessive reliance on options, which can lead to overwork and mistakes, to Kasparov's few shortcomings.[146]
Kasparov's attacking style of play has been compared by many to Alekhine,[147][148] his chess idol since childhood. Kasparov has described his style as being influenced chiefly by Alekhine, Tal and Fischer.[149] Other influences on Kasparov were his early coaches. At a young age, he met with experienced teachers Alexander Nikitin and Alexander Shakarov. Shakarov collected and systematised materials, and then became the keeper of Kasparov's information bank. Revolutionary for chess was the active involvement in the analysis of computer programs, and it was Kasparov and his team who took the first steps in this direction.[150][151] In 1973, Kasparov entered the Botvinnik school and immediately attracted attention. Botvinnik commented on the young schoolboy: "Garry's speed and memory capacity are amazing. He counts deep variations and finds unexpected moves. The power of combinational vision makes him similar to Alekhine himself".[152]
Another well-known case of winning an important game thanks to a novelty in the opening is Kasparov's 10th game of the 1995 match against Anand. On the 14th move, in a well-known position of the open variation of the Spanish Game (Rui Lopez), Kasparov discovered a new idea with a rook sacrifice, which brought a decisive attack.[156]
In January 1990, Kasparov achieved the (then) highest FIDE rating ever, passing 2800 and breaking Fischer's old record of 2785. By the July 1999 and January 2000 FIDE rating lists, Kasparov had reached a 2851 Elo rating, at that time the highest rating ever achieved.[160] He held that record for the highest rating ever achieved until Carlsen attained a new record high rating of 2861 in January 2013.[161]
Kasparov holds the record for most consecutive professional tournament victories, placing first or equal first in 15 individual tournaments from 1981 to 1990.[162] The streak was broken by Vasyl Ivanchuk at Linares 1991, where Kasparov placed second, half a point behind him after losing their individual game. The details of this record winning streak follow:[39]
At the end of the 1990s, Kasparov went on another long streak of 10 consecutive super-tournament wins.[163]
In these tournament victories, Kasparov had a score of 53 wins, 61 draws and 1 loss in 115 games with his only loss coming against Ivan Sokolov in Wijk aan Zee 1999.
Acorn Computers acted as one of the sponsors for Kasparov's Candidates semi-final match against Korchnoi in 1983. This was Kasparov's first introduction to computers. Kasparov was awarded a BBC Micro, which he took back with him to Baku, making it perhaps the first Western-made microcomputer to reach Baku at that time.[164]
Computer chess magazine editor Frederic Friedel consulted with Kasparov in 1985 on how a chess database program would be useful preparation for competition. Friedel founded Chessbase two years later, and he gave a copy of the program to Kasparov, who started using it in his preparation.[165] That same year, Kasparov played against 32 chess computers in Hamburg, winning all games.[166] Several commercially available Kasparov computers were made in the 1980s, the Saitek Kasparov Turbo King models.[167][168][169] On 22 October 1989, Kasparov defeated the chess computer Deep Thought in both games of a two-game match.[170] In December 1992, Kasparov visited Friedel in his hotel room in Cologne, and played 37 blitz games against Fritz 2 winning 24, drawing 4 and losing 9.[171]
Kasparov cooperated in producing video material for the computer game Kasparov's Gambit released by Electronic Arts in November 1993. In April 1994, Intel acted as a sponsor for the first Professional Chess Association Grand Prix event in Moscow played a time control of 25 minutes per game. In May, Chessbase's Fritz 3 running on an Intel Pentium PC defeated Kasparov in their first in the Intel Express blitz tournament in Munich, but Kasparov managed to tie it for first, and then win the playoff with 3 wins and 2 draws. The next day, Kasparov lost to Fritz 3 again in a game on ZDF TV.[172] In August, Kasparov was knocked out of the London Intel Grand Prix by Richard Lang's ChessGenius 2 program in the first round.Cite error: A <ref> tag is missing the closing </ref> (see the help page).
After the loss, Kasparov said that he sometimes saw deep intelligence and creativity in the machine's moves, suggesting that during the second game, human chess players, in contravention of the rules, intervened. IBM denied that it cheated, saying the only human intervention occurred between games. The rules provided for the developers to modify the program between games, an opportunity they said they used to shore up weaknesses in the computer's play revealed during the course of the match.[176] Kasparov requested printouts of the machine's log files but IBM refused, although the company later published the logs on the Internet.[177] Much later, it was suggested that the behaviour Kasparov noted had resulted from a glitch in the computer program.[178] Plans for further engagement between Kasparov and IBM, including a rematch, did not come to fruition, due to the accusations of cheating.[179]
In 2021, Kasparov promoted a series of 32 NFTs that detailed important moments in his career. The top four sold for more than $11,000.[186][187][188]
Kasparov's grandfather was a staunch communist, but the young Kasparov gradually began to have doubts about the Soviet Union's political system at age 13 when he travelled abroad for the first time to Paris for a chess tournament. In 1981, at age 18, he read Solzhenitsyn's The Gulag Archipelago, a copy of which he bought while abroad.[189] Nevertheless, Kasparov joined the Communist Party of the Soviet Union (CPSU) in 1984, and was elected to the Central Committee of Komsomol in 1987. In 1990, he left the party.[190]
In May 1990, Kasparov took part in the creation of the Democratic Party of Russia, which at first was a liberal anti-communist party, later shifting to centrism.[18] Kasparov left the party on 28 April 1991, after its conference.[191] He was also involved with the creation of the "Choice of Russia" bloc of parties in June 1993. He took part in the election campaign of Boris Yeltsin in 1996. In 2001 he voiced his support for the Russian television channel NTV.[18]
After his retirement from chess in 2005, Kasparov turned to politics and created the United Civil Front, a social movement whose main goal is to "work to preserve electoral democracy in Russia".[192] He has vowed to "restore democracy" to Russia by restoring the rule of law.[193][194][195] A year later the United Civil Front became part of The Other Russia, a coalition which opposes Putin's government that Kasparov was instrumental in setting up. The Other Russia has been boycotted by the leaders of Russia's mainstream opposition parties, Yabloko and Union of Right Forces due to its inclusion of both nationalist and radical groups. Kasparov has criticised these groups as being secretly under the auspices of the Kremlin.[196]
In April 2005, Kasparov was in Moscow at a promotional event when he was struck over the head with a chessboard he had just signed. The assailant was reported to have said "I admired you as a chess player, but you gave that up for politics" immediately before the attack.[197] Kasparov has been the subject of a number of other episodes since, including police brutality and alleged harassment from the Russian secret service.[198][199]
Kasparov helped organise the Saint Petersburg Dissenters' March on 3 March 2007 and The March of the Dissenters on 24 March 2007, both involving several thousand people rallying against Putin and Saint Petersburg Governor Valentina Matviyenko's policies.[200][201]
Kasparov led a pro-democracy demonstration in Moscow in April 2007. Soon after it started, however, over 9,000 police descended on the group and seized almost everyone. Kasparov, who was briefly arrested, was warned by the prosecution office on the eve of the march that anyone participating risked being detained. He was held for some 10 hours and then fined and released.[202] He was later summoned by the FSB for violations of Russian anti-extremism laws.[203]
On 30 September 2007, Kasparov entered the Russian presidential race, receiving 379 of 498 votes at a congress held in Moscow by The Other Russia.[205] In October 2007, Kasparov announced his intention of standing for the Russian presidency as the candidate of the "Other Russia" coalition and vowed to fight for a "democratic and just Russia". Later that month he travelled to the United States, where he appeared on several popular television programs, which were hosted by Stephen Colbert, Wolf Blitzer, Bill Maher, and Chris Matthews.[206]
In November 2007, Kasparov and other protesters were detained by police at an Other Russia rally in Moscow, which drew 3,000 demonstrators to protest election rigging. Following an attempt by about 100 protesters to march through police lines to the electoral commission, which had barred Other Russia candidates from parliamentary elections, arrests were made. The Russian authorities stated a rally had been approved but not any marches, resulting in several detained demonstrators.[207] Kasparov was subsequently charged with resisting arrest and organising an unauthorised protest, and was given a jail sentence of five days. Kasparov appealed the charges, citing that he had been following orders given by the police, although it was denied. He was released from jail on 29 November.[208] Putin castigated Kasparov at the rally for his use of English when speaking rather than Russian.[209]
In December 2007, Kasparov announced that he had to withdraw his presidential candidacy due to inability to rent a meeting hall where at least 500 of his supporters could assemble. With the deadline expiring on that date, he explained it was impossible for him to run. Russian election laws required sufficient meeting hall space for assembling supporters. Kasparov's spokeswoman accused the government of using pressure to deter anyone from renting a hall for the gathering and said that the electoral commission had rejected a proposal that would have allowed for smaller gathering sizes rather than one large gathering at a meeting hall.[210]
Kasparov was among the 34 first signatories and a key organiser of the online anti-Putin campaign "Putin must go", started on 10 March 2010. The campaign was begun by a coalition of opposition to Putin who regard his rule as lacking any rule of law. Within the text is a call to Russian law enforcement to ignore Putin's orders. By June 2011, there were 90,000 signatures. While the identity of the petition author remained anonymous, there was wide speculation that it was indeed Kasparov.[211][212][213][214] On 31 January 2012, Kasparov hosted a meeting of opposition leaders planning a mass march on 4 February 2012, the third major opposition rally held since the disputed State Duma elections of December 2011. Among other opposition leaders attending were Alexey Navalny and Yevgenia Chirikova.[215]
Kasparov was arrested and beaten outside a Moscow court on 17 August 2012 while attending the sentencing in the case involving the all-female punk band Pussy Riot.[216] On 24 August, he was cleared of charges that he had taken part in an unauthorised protest against the conviction of three members of the band. Judge Yekaterina Veklich said there were "no grounds to believe the testimony of the police".[217] He later thanked all the bloggers and reporters who provided video evidence that contradicted the testimony of the police.[218] Kasparov wrote in February 2013 that "fascism has come to Russia. ...Project Putin, just like the old Project Hitler, is but the fruit of a conspiracy by the ruling elite. Fascist rule was never the result of the free will of the people. It was always the fruit of a conspiracy by the ruling elites!"[219]
Kasparov denied rumours in April 2013 that he planned to leave Russia for good. "I found these rumors to be deeply saddening and, moreover, surprising," he wrote. "I was unable to respond immediately because I was in such a state of shock that such an incredibly inaccurate statement, the likes of which is constantly distributed by the Kremlin's propagandists, came this time from Ilya Yashin, a fellow member of the Opposition Coordination Council (KSO) and my former colleague from the Solidarity movement."[220] He also accused prominent Russian journalist Vladimir Posner of failing to stand up to Putin and to earlier Russian and Soviet leaders.[221] Further, at the 2013 Women in the World conference, Kasparov told The Daily Beast's Michael Moynihan that democracy no longer existed in what he called Russia's "dictatorship".[222]
Kasparov said at a press conference in June 2013 that if he returned to Russia he doubted he would be allowed to leave again, given Putin's ongoing crackdown on dissenters. "So for the time being," he said, "I refrain from returning to Russia." He explained shortly thereafter in an article for The Daily Beast that this had not been intended as "a declaration of leaving my home country, permanently or otherwise", but merely an expression of "the dark reality of the situation in Russia today, where nearly half the members of the opposition's Coordinating Council are under criminal investigation on concocted charges". He noted that the Moscow prosecutor's office was "opening an investigation that would limit my ability to travel", making it impossible for him to fulfil "professional speaking engagements" and hindering his "work for the nonprofit Kasparov Chess Foundation, which has centers in New York City, Brussels, and Johannesburg to promote chess in education".[222]
Kasparov further wrote in his June 2013 Daily Beast article that the mass protests in Moscow 18 months earlier against fraudulent Russian elections had been "a proud moment for me". He recalled that after joining the opposition movement in March 2005, he had been criticised for seeking to unite "every anti-Putin element in the country to march together regardless of ideology". Therefore, the sight of "hundreds of flags representing every group from liberals to nationalists all marching together for 'Russia Without Putin' was the fulfillment of a dream." Yet most Russians, he lamented, had continued to "slumber" even as Putin had "taken off the flimsy mask of democracy to reveal himself in full as the would-be KGB dictator he has always been".[223]
Kasparov responded with several sardonic Twitter postings to a September 2013 The New York Times op-ed by Putin. "I hope Putin has taken adequate protections," he tweeted. "Now that he is a Russian journalist his life may be in grave danger!" Also: "Now we can expect NY Times op-eds by Mugabe on fair elections, Castro on free speech, & Kim Jong-un on prison reform. The Axis of Hypocrisy."[224]
Kasparov wrote in July 2013 about the trial in Kirov of fellow opposition leader Alexei Navalny, who had been convicted "on concocted embezzlement charges", only to see the prosecutor, surprisingly, ask for his release the next day pending appeal. "The judicial process and the democratic process in Russia," wrote Kasparov, "are both elaborate mockeries created to distract the citizenry at home and to help Western leaders avoid confronting the awkward fact that Russia has returned to a police state". Still, Kasparov felt that whatever had caused the Kirov prosecutor's about-face, "my optimism tells me it was a positive sign. After more than 13 years of predictable repression under Putin, anything different is good."[225]
Kasparov spoke out against the 2014 Russian annexation of Crimea and has stated that control of Crimea should be returned to Ukraine after the overthrow of Putin without additional conditions.[228] Kasparov's website was blocked by the Russian government censorship agency, Roskomnadzor, at the behest of the public prosecutor, allegedly due to Kasparov's opinions on the Crimean crisis. Kasparov's block was made in unison with several other notable Russian sites that were accused of inciting public outrage. Reportedly, several of the blocked sites received an affidavit noting their violations. However, Kasparov stated that his site had received no such notice of violations after its block.[229] In 2015, a whole note on Kasparov was removed from a Russian language encyclopaedia of greatest Soviet players after an intervention from "senior leadership".[230]
In October 2015, Kasparov published a book titled Winter Is Coming: Why Vladimir Putin and the Enemies of the Free World Must Be Stopped. In the book, Kasparov likens Putin to Adolf Hitler and explains the need for the West to oppose Putin sooner, rather than appeasing him and postponing the eventual confrontation. According to his publisher, "Kasparov wants this book out fast, in a way that has potential to influence the discussion during the primary season."[231][232] In 2018, he said that "anything is better than Putin because that eliminates the probability of a nuclear war. Putin is insane."[10]
Following reports of Russian ransomware attacks against American agencies and companies in 2021, Kasparov stated that "the only language that Putin understands is power, and his power is his money," arguing that the United States should target the bank accounts of Russian oligarchs to force Russia to rein in its criminals' cyberattacks.[233]
Kasparov spoke out against the 2022 invasion of Ukraine by Russian forces on Twitter: "The only way this really ends is the fall of Putin's regime by collapse of Russian economy and defeat in Ukraine."[234] He also believed that "pressure must be kept up" in terms of sanctions and condemnations against Russia's actions[235] and joined with other prominent Russian figures-in-exile to form the Anti-War Committee of Russia.[236] He said that Russia should be "thrown back into the Stone Age to make sure that the oil and gas industry and any other sensitive industries that are vital for survival of the regime cannot function without Western technological support."[237]
On 20 May 2022, Kasparov was designated as "foreign agent" by the Ministry of Justice of the Russian Federation.[238]
Kasparov received the Keeper of the Flame award in 1991 from the Center for Security Policy, a Washington, D.C. based far-right, anti-Muslim think tank. In his acceptance speech Kasparov lauded the defeat of communism while also urging the United States to give no financial assistance to central Soviet leaders.[239][240][241][242] Kasparov gave speeches at other think tanks such as the Hoover Institution.[239]
In the 2016 United States presidential election, Kasparov described Republican Donald Trump as "a celebrity showman with racist leanings and authoritarian tendencies"[244] and criticised him for calling for closer ties with Putin.[245] After Trump's running mate, Mike Pence, called Putin a strong leader, Kasparov said that Putin is a strong leader "in the same way arsenic is a strong drink".[246][247] He also disparaged the economic policies of Democratic primary candidate Bernie Sanders, but showed respect for Sanders as "a charismatic speaker and a passionate believer in his cause".[248] Kasparov opined that Henry Kissinger "was selling the Trump Administration on the idea of a mirror of 1972 [Richard Nixon's visit to China], except, instead of a Sino-U.S. alliance against the U.S.S.R., this would be a Russian-American alliance against China."[10]
In an interview discussing the Nagorno-Karabakh conflict, Kasparov stated that the Republic of Artsakh has a right to independence, and that Azerbaijan has no sovereign right over it. He considers this stance to be objective and without bias, as Soviet law allowed for autonomous republics (such as the Nagorno-Karabakh Autonomous Oblast) to vote for independence separately and were given an equal right for self-determination, a factor he felt often went ignored.[249] Kasparov recalled that he was criticised by Armenians for not taking a strong stance when the Karabakh movement began in 1988, explaining that he was living in Baku with 200,000 other Armenians at the time and did not want to increase tensions.[250] Kasparov and his family later fled Baku in January 1990 to escape pogroms against Armenians.[251][252]
He welcomed the Velvet Revolution in Armenia in 2018.[253] Kasparov supports Armenian genocide recognition.[254][255][256][257]
Kasparov was critical of the violence unleashed by the Spanish police against the 2017 independence referendum in Catalonia and accused the Spanish PM Mariano Rajoy of "betraying" the European promise of peace.[266] After the Catalan regional election held later the same year, Kasparov wrote: "Despite unprecedented pressure from Madrid, Catalonian separatists won a majority. Europe must speak and help find a peaceful path toward resolution and avoid more violence".[267][268] Kasparov recommended that Spain look to how Britain handled the 2014 Scottish independence referendum, adding: "look only at how Turkey and Iraq have treated the separatist Kurds. That cannot be the road for Spain and Catalonia."[269]
Kasparov has written books on chess. He published a controversial[275] autobiography when still in his early 20s. Originally entitled Child of Change, it was later published as Unlimited Challenge. This book was updated several times after he became World Champion. Its content is mainly literary, with a small chess component of key unannotated games. He published an annotated games collection in 1983, Fighting Chess: My Games and Career,[276] which has been updated in further editions. He also wrote a book annotating the games from his World Chess Championship 1985 victory, World Chess Championship Match: Moscow, 1985.[277]
In 2000, Kasparov co-authored Kasparov Against the World: The Story of the Greatest Online Challenge[283] with grandmaster Daniel King. The 202-page book analyses the 1999 Kasparov versus the World game, and holds the record for the longest analysis devoted to a single chess game.[284]
In 2003, the first volume of his five-volume work Garry Kasparov on My Great Predecessors was published. This volume, which deals with the world chess champions Wilhelm Steinitz, Emanuel Lasker, Capablanca, Alekhine and some of their strong contemporaries, received praise from reviewers (including Short), while attracting criticism from others for historical inaccuracies and analysis of games directly copied from unattributed sources.[citation needed] Through suggestions on the book's website, most of these shortcomings were corrected in following editions and translations. Despite this, the first volume won the British Chess Federation's Book of the Year award in 2003. Volume two, covering Max Euwe, Botvinnik, Smyslov and Tal appeared later in 2003. Volume three, covering Tigran Petrosian and Boris Spassky appeared in early 2004. In December 2004, Kasparov released volume four, which covers Samuel Reshevsky, Miguel Najdorf, and Bent Larsen (none of these three were World Champions), but focuses primarily on Fischer. The fifth volume, devoted to the chess careers of World Champion Karpov and challenger Korchnoi, was published in March 2006.[285]
Kasparov published three volumes of his games, spanning his entire career.
In October 2015, Kasparov published a book titled Winter Is Coming: Why Vladimir Putin and the Enemies of the Free World Must Be Stopped. The title is a reference to the HBO television series Game of Thrones. In the book, Kasparov writes about the need for an organisation composed solely of democratic countries to replace the United Nations. In an interview, he called the United Nations a "catwalk for dictators".[189]
Kasparov believes that the conventional history of civilisation is incorrect. Specifically, he contends that the history of ancient civilisations is based on misdating of events and achievements that occurred in the medieval period.[286][287] He has cited several aspects of ancient history that, he argues, are likely to be anachronisms.[288]
Kasparov wrote How Life Imitates Chess, an examination of the parallels between decision-making in chess and in the business world, in 2007.[293] In 2008, Kasparov published a sympathetic obituary for Fischer: "I am often asked if I ever met or played Bobby Fischer. The answer is no, I never had that opportunity. But even though he saw me as a member of the evil chess establishment that he felt had robbed and cheated him, I am sorry I never had a chance to thank him personally for what he did for our sport."[294]
Kasparov is the chief advisor for the book publisher Everyman Chess.[295] He works closely with Mig Greengard and his comments can often be found on Greengard's blog.[296][297] Kasparov collaborated with Max Levchin and Peter Thiel on The Blueprint, a book calling for a revival of world innovation, planned for release in March 2013 but cancelled after the authors disagreed on its contents.[298] In an editorial comment on Google's AlphaZero chess-playing system, Kasparov argued that chess has become the model for reasoning in the same way that the fruit fly Drosophila melanogaster became a model organism for geneticists: "I was pleased to see that AlphaZero had a dynamic, open style like my own," he wrote in late 2018.[299]
He has been married three times: to Masha, with whom he had a daughter before divorcing;[310] to Yulia, with whom he had a son before their 2005 divorce;[310] and to Daria (Dasha), with whom he has two children, a daughter born in 2006 and a son born in 2015.[239][10] Kasparov's wife manages his business activities worldwide through Kasparov International Management Inc.[311]
In computer chess, a chess engine is a computer program that analyzes chess or chess variant positions, and generates a move or list of moves that it regards as strongest.[1]
A chess engine is usually a back end with a command-line interface with no graphics or windowing.  Engines are usually used with a front end, a windowed graphical user interface such as Chessbase or WinBoard that the user can interact with via a keyboard, mouse or touchscreen.  This allows the user to play against multiple engines without learning a new user interface for each, and allows different engines to play against each other.
Many chess engines are now available for mobile phones and tablets, making them even more accessible.
The meaning of the term "chess engine" has evolved over time.  In 1986, Linda and Tony Scherzer entered their program Bebe into the 4th World Computer Chess Championship, running it on "Chess Engine," their brand name for the chess computer hardware[2] made, and marketed by their company Sys-10, Inc.[3]  By 1990 the developers of Deep Blue, Feng-hsiung Hsu and Murray Campbell, were writing of giving their program a 'searching engine,' apparently referring to the software rather than the hardware.[4]  In December 1991, Computer-schach & Spiele referred to Chessbase's recently released Fritz as a 'Schach-motor,' the German translation for 'chess engine.[5]  By early 1993, Marty Hirsch was drawing a distinction between commercial chess programs such as Chessmaster 3000 or Battle Chess on the one hand, and 'chess engines' such as ChessGenius or his own MChess Pro on the other.  In his characterization, commercial chess programs were low in price, had fancy graphics, but did not place high on the SSDF (Swedish Chess Computer Association) rating lists while engines were more expensive, and did have high ratings.[6]
In 1994, Shay Bushinsky was working on an early version of his Junior program.  He wanted to focus on the chess playing part rather than the graphics, and so asked Tim Mann how he could get Junior to communicate with Winboard.  Tim's answer formed the basis for what became known as the Chess Engine Communication Protocol or Winboard engines, originally a subset of the GNU Chess command line interface.[7]
Also in 1994, Stephen J. Edwards released the Portable Game Notation (PGN) specification.  It mentions PGN reading programs not needing to have a "full chess engine."  It also mentions three "graphical user interfaces" (GUI): XBoard, pgnRead and Slappy the database.[8]
Common Winboard engines would include Crafty, ProDeo (based on Rebel), Chenard, Zarkov and Phalanx.
In 2000, Stefan Meyer-Kahlen and Franz Huber released the Universal Chess Interface, a more detailed protocol that introduced a wider set of features. Chessbase soon after dropped support for Winboard engines, and added support for UCI to their engine GUI's and Chessbase programs.  Most of the top engines are UCI these days: Stockfish, Komodo, Leela Chess Zero, Houdini, Fritz 15-16, Rybka, Shredder, Fruit, Critter, Ivanhoe and Ruffian.
Chess engines increase in playing strength continually. This is partly due to the increase in processing power that enables calculations to be made to ever greater depths in a given time. In addition, programming techniques have improved, enabling the engines to be more selective in the lines that they analyze and to acquire a better positional understanding. A chess engine often uses a vast previously computed opening "book" to increase its playing strength for the first several moves, up to possibly 20 moves or more in deeply analyzed lines.[citation needed]
Some chess engines maintain a database of chess positions, along with previously computed evaluations and best moves, in effect, a kind of "dictionary" of recurring chess positions.  Since these positions are pre-computed, the engine merely plays one of the indicated moves in the database, thereby saving computing time, resulting in stronger, faster play.
Some chess engines use endgame tablebases to increase their playing strength during the endgame. An endgame tablebase includes all possible endgame positions with a small amount of material. Each position is conclusively determined as a win, loss, or draw for the player whose turn it is to move, and the number of moves to the end with best play by both sides. The tablebase identifies for every position the move which will win the fastest against an optimal defense, or the move that will lose the slowest against an optimal offense. Such tablebases are available for all chess endgames with seven pieces or fewer (trivial endgame positions are excluded, such as six white pieces versus a lone black king).[12][13]
When the maneuvering in an ending to achieve an irreversible improvement takes more moves than the horizon of calculation of a chess engine, an engine is not guaranteed to find the best move without the use of an endgame tablebase, and in many cases can fall foul of the fifty-move rule as a result. Many engines use permanent brain (continuing to calculate during the opponent's turn) as a method to increase their strength.
The results of computer tournaments give one view of the relative strengths of chess engines. However, tournaments do not play a statistically significant number of games for accurate strength determination. In fact, the number of games that need to be played between fairly evenly matched engines, in order to achieve significance, runs into the thousands and is, therefore, impractical within the framework of a tournament.[22] Most tournaments also allow any types of hardware, so only engine/hardware combinations are being compared.
Historically, commercial programs have been the strongest engines. If an amateur engine wins a tournament or otherwise performs well (for example, Zappa in 2005), then it is quickly commercialized. Titles gained in these tournaments garner much prestige for the winning programs, and are thus used for marketing purposes. However, after the rise of volunteer distributed computing projects such as Leela Chess Zero and Stockfish and testing frameworks such as FishTest and OpenBench in the late 2010s, free and open source programs have largely displaced commercial programs as the strongest engines in tournaments.
Chess engine rating lists aim to provide statistically significant measures of relative engine strength. These lists play multiple games between engines. Some also standardize the opening books, the time controls, and the computer hardware the engines use, in an attempt to measure the strength differences of the engines only. These lists provide not only a ranking, but also margins of error on the given ratings.
The ratings on the rating lists, although calculated by using the Elo system (or similar rating methods), have no direct relation to FIDE Elo ratings or to other chess federation ratings of human players. Except for some man versus machine games which the SSDF had organized many years ago (when engines were far from today's strength), there is no calibration between any of these rating lists and player pools. Hence, the results which matter are the ranks and the differences between the ratings, and not the absolute values.
Missing from many rating lists are IPPOLIT and its derivatives. Although very strong and open source, there are allegations from commercial software interests that they were derived from a disassembled binary of Rybka.[23] Due to the controversy, all these engines have been blacklisted from many tournaments and rating lists. Rybka in turn was accused of being based on Fruit,[24] and in June 2011, the ICGA formally claimed Rybka was derived from Fruit and Crafty and banned Rybka from the International Computer Games Association World Computer Chess Championship, and revoked its previous victories (2007, 2008, 2009, and 2010).[25] The ICGA received some criticism for this decision.[26] Despite all this, Rybka is still included on many rating lists, such as CCRL and CEGT, in addition to Houdini, a derivative of the IPPOLIT derivative Robbolito,[27] and Fire, a derivative of Houdini. In addition, Fat Fritz 2, a derivative of Stockfish,[28] is also included on most of the rating lists.
There are a number of factors that vary among the chess engine rating lists:
These differences affect the results, and make direct comparisons between rating lists difficult.
Engines can be tested by measuring their performance on specific positions. Typical is the use of test suites where for each given position there is one best move to find. These positions can be geared towards positional, tactical or endgame play. The Nolot test suite, for instance, focuses on deep sacrifices.[30] The BT2450 and BT2630 test suites measure the tactical capability of a chess engine and have been used by REBEL.[31][32] There is also a general test suite called Brilliancy which was compiled mostly from How to Reassess Your Chess Workbook.[33] The Strategic Test Suite (STS) tests an engine's strategical strength.[34] Another modern test suite is Nightmare II which contains 30 chess puzzles.[35][irrelevant citation]
In 1999, Garry Kasparov played a chess game called "Kasparov versus the World" over the Internet, hosted by the MSN Gaming Zone. Both sides used computer (chess engine) assistance. The "World Team" included the participation of over 50,000 people from more than 75 countries, deciding their moves by plurality vote. The game lasted four months, ending after Kasparov's 62nd move when he announced a forced checkmate in 28 moves found with the computer program Deep Junior. The World Team voters resigned on October 22. After the game, Kasparov said: "It is the greatest game in the history of chess. The sheer number of ideas, the complexity, and the contribution it has made to chess make it the most important game ever played."[36]
For larger boards, however, there are few chess engines that can play effectively, and indeed chess games played on an unbounded chessboard (infinite chess) are virtually untouched by chess-playing software, although theoretically a program using a MuZero-derived algorithm could handle an unbounded state space.
XBoard/Winboard was one of the earliest graphical user interfaces (GUI).  Tim Mann created it to provide a GUI for the GNU Chess engine, but after that, other engines such as Crafty appeared which used the Winboard protocol.  Eventually, the program Chessmaster included the option to import other Winboard engines in addition to the King engine which was included.
In 1995, Chessbase began offering the Fritz engine as a separate program within the Chessbase database program and within the Fritz GUI.  Soon after, they added the Junior and Shredder engines to their product line up, packaging them within the same GUI as was used for Fritz.  In the late 1990s, the Fritz GUI was able to run Winboard engines via an adapter, but after 2000, Chessbase simply added support for UCI engines, and no longer invested much effort in Winboard.
In 2000, Stefan Meyer-Kahlen started selling Shredder in a separate UCI GUI of his own design, allowing UCI or Winboard engines to be imported into it.
Convekta's Chess Assistant and Lokasoft's ChessPartner also added the ability to import Winboard and UCI engines into their products.  Shane Hudson developed Shane's Chess Information Database, a free GUI for Linux, Mac and Windows.  Martin Blume developed Arena,[21] another free GUI for Linux and Windows.  Lucas Monge entered the field with the free Lucas Chess GUI.[37]  All three can handle both UCI and Winboard engines.
On Android, Aart Bik came out with Chess for Android,[38] another free GUI, and Gerhard Kalab's Chess PGN Master[39] and Peter Osterlund's Droidfish[40] can also serve as GUIs for engines.
Algebraic notation (or AN) is the standard method for recording and describing the moves in a game of chess. Also called standard notation, it is based on coordinate notation, a system of coordinates to uniquely identify each square on the chessboard.[1] It is used by most books, magazines, and newspapers. In English-speaking countries, the parallel method of descriptive notation was generally used in chess publications until about 1980. A few players still use descriptive notation, but it is no longer recognized by FIDE, the international chess governing body.
Algebraic notation exists in various forms and languages and is based on a system developed by Philipp Stamma. Stamma used the modern names of the squares, but he used p for pawn moves and the original file of a piece (a through h) instead of the initial letter of the piece name.[2]
The term "algebraic notation" may be considered a misnomer, as the system is unrelated to algebra.
Each piece type (other than pawns) is identified by an uppercase letter. English-speaking players use the letters K for king, Q for queen, R for rook, B for bishop, and N for knight (since K is already used and is a silent letter in knight). S (from the German Springer) was also used for the knight in the early days of algebraic notation and is still used in some chess problems (where N stands for nightrider, a popular fairy chess piece).
In both standard algebraic notation and FAN, pawns are not identified by a letter or symbol, but rather by the absence of one.
In short algebraic notation (SAN), each move of a piece is indicated by the piece's uppercase letter, plus the coordinate of the destination square. For example, Be5 (bishop moves to e5), Nf3 (knight moves to f3). For pawn moves, a letter indicating pawn is not used, only the destination square is given. For example, c5 (pawn moves to c5).
When a piece makes a capture, an "x" is inserted immediately before the destination square. For example, Bxe5 (bishop captures the piece on e5). When a pawn makes a capture, the file from which the pawn departed is used to identify the pawn. For example, exd5 (pawn on the e-file captures the piece on d5). In older German, Russian, or Italian publications, a colon (:) is sometimes used instead of "x", either in the same place the "x" would go (B:e5) or at the end (Be5:).
En passant captures are indicated by specifying the capturing pawn's file of departure, the "x", the destination square (not the square of the captured pawn), and (optionally) the suffix "e.p." indicating the capture was en passant.[5] For example, exd6 e.p.
When two (or more) identical pieces can move to the same square, the moving piece is uniquely identified by specifying the piece's letter, followed by (in descending order of preference):
Theoretically, it may be necessary to specify both the file and rank of departure if neither alone is sufficient to identify the piece, but this almost never happens in practice as it would require at least two promoted pieces (for example three queens) to be necessary.
In the diagram, both black rooks could legally move to f8, so the move of the d8-rook to f8 is disambiguated as Rdf8. For the white rooks on the a-file which could both move to a3, it is necessary to provide the rank of the moving piece, i.e., R1a3.
In the case of the white queen on h4 moving to e1, neither the rank nor file alone are sufficient to disambiguate from the other white queens. As such, this move is written Qh4e1.
As above, an "x" can be inserted to indicate a capture; for example, if the final case were a capture, it would be written as Qh4xe1.
When a pawn promotes, the piece promoted to is indicated at the end of the move notation, for example: e8Q (promoting to queen). In standard FIDE notation, no punctuation is used; in Portable Game Notation (PGN) and many publications, pawn promotion is indicated by the equals sign (e8=Q). Other formulations used in chess literature include parentheses (e.g. e8(Q)) and a forward slash (e.g. e8/Q).
FIDE specifies draw offers to be recorded by an equals sign with parentheses "(=)" after the move on the score sheet.[6] This is not usually included in published game scores.
Besides the standard (or short) algebraic notation already described, several similar systems have been used.
In long algebraic notation (LAN), also known as full/fully expanded algebraic notation, both the starting and ending squares are specified, for example: e2e4. Sometimes these are separated by a hyphen, e.g. Nb1-c3, while captures are indicated by an "x", e.g. Rd3xd7. Long algebraic notation takes more space and is no longer commonly used in print; however, it has the advantage of clarity. Some books using primarily short algebraic notation use the long notation instead of the disambiguation forms described earlier. Both short and long algebraic notation are acceptable for use in FIDE rated games.
A form of long algebraic notation (without piece names) is also used by the Universal Chess Interface (UCI) standard, which is a common way for graphical chess programs to communicate with chess engines (e.g., for AI).
In international correspondence chess the use of algebraic notation may cause confusion, since different languages employ different names (and therefore different initial letters) for the pieces, and some players may be unfamiliar with the Latin alphabet. Hence, the standard for transmitting moves by post or email is ICCF numeric notation, which identifies squares using numerical co-ordinates, and identifies both the departure and destination squares. For example, the move 1.e4 is rendered as 1.5254. In recent years, the majority of correspondence games have been played on on-line servers rather than by email or post, leading to a decline in the use of ICCF numeric notation.
Portable Game Notation (PGN) is a text-based file format for storing chess games, which uses standard English algebraic notation and a small amount of markup.[9] PGN can be processed by almost all chess software, as well as being easily readable by humans. For example, the Game of the Century could be represented as follows in PGN:
A game or series of moves is generally written in one of two ways; in two columns, as White/Black pairs, preceded by the move number and a period:
Moves may be interspersed with commentary (annotations). When the game score resumes with a Black move, an ellipsis (...) fills the position of the White move, for example:
Algebraic notation is described in 1847 by Howard Staunton in his book The Chess-Player's Handbook. Staunton credits the idea to German authors, and in particular to "Alexandre, Jaenisch, and the Handbuch [des Schachspiels]."[11] While algebraic notation has been used in German and Russian chess literature since the 19th century, the Anglosphere was slow to adopt it, using descriptive notation for much of the 20th century. Beginning in the 1970s, algebraic notation gradually became more common in English language publications, and by 1980 it had become the prevalent notation. In 1981, FIDE stopped recognizing descriptive notation, and algebraic notation became the accepted international standard.
Chess diagram showing algebraic notation in Howard Staunton's The Chess-Player's Handbook (1866)
The table contains names for all the pieces as well as the words for chess, check, and checkmate in several languages.[12] Several languages use the Arabic loanword alfil for the piece called bishop in English; in this context it is a chess-specific term which no longer has its original meaning of "elephant".
Though not technically a part of algebraic notation, the following are some symbols commonly used by annotators, for example in publications Chess Informant and Encyclopaedia of Chess Openings, to give editorial comment on a move or position.
The rules of chess (also known as the laws of chess) govern the play of the game of chess. Chess is a two-player abstract strategy board game. Each player controls sixteen pieces of six types on a chessboard. Each type of piece moves in a distinct way. The object of the game is to checkmate (threaten with inescapable capture) the opponent's king. A game can end in various ways besides checkmate: a player can resign, and there are several ways a game can end in a draw.
Besides the basic moves of the pieces, rules also govern the equipment used, time control, conduct and ethics of players, accommodations for physically challenged players, and recording of moves using chess notation. Procedures for resolving irregularities that can occur during a game are provided as well.
Chess is played on a chessboard, a square board divided into a grid of 64 squares (eight-by-eight) of alternating color (similar to the board used in draughts).[1] Regardless of the actual colors of the board, the lighter-colored squares are called "light" or "white", and the darker-colored squares are called "dark" or "black". Sixteen "white" and sixteen "black" pieces are placed on the board at the beginning of the game. The board is placed so that a white square is in each player's near-right corner. Horizontal rows are called ranks, and vertical columns are called files.
At the beginning of the game, the pieces are arranged as shown in the diagram: for each side one king, one queen, two rooks, two bishops, two knights, and eight pawns. The pieces are placed, one per square, as follows:
Popular mnemonics used to remember the setup are "queen on her own color" and "white on right". The latter refers to setting up the board so that the square closest to each player's right is white.[2]
The player controlling the white pieces is named "White"; the player controlling the black pieces is named "Black". White moves first, then players alternate moves. Making a move is required; it is not legal to skip a move, even when having to move is detrimental. Play continues until a king is checkmated, a player resigns, or a draw is declared, as explained below. In addition, if the game is being played under a time control, a player who exceeds the time limit loses the game unless they cannot be checkmated.
The official chess rules do not include a procedure for determining who plays White. Instead, this decision is left open to tournament-specific rules (e.g. a Swiss system tournament or round-robin tournament) or, in the case of non-competitive play, mutual agreement, in which case some kind of random choice is often employed. A common method is for one player to conceal a piece (usually a pawn) of each color in either hand; the other player chooses a hand to open and receives the color of the piece that is revealed.
Each type of chess piece has its own method of movement. A piece moves to a vacant square except when capturing an opponent's piece.
Except for any move of the knight and castling, pieces cannot jump over other pieces. A piece is captured (or taken) when an attacking enemy piece replaces it on its square. The captured piece is thereby permanently removed from the game.[3] The king can be put in check but cannot be captured (see below).
Castling consists of moving the king two squares towards a rook, then placing the rook on the other side of the king, adjacent to it. It is not allowed to move both king and rook in the same time, because "Each move must be played with one hand only."[6] Castling is only permissible if all of the following conditions hold:[7]
An unmoved king and an unmoved rook of the same color on the same rank are said to have castling rights.
When a pawn advances two squares from its original square and ends the turn adjacent to a pawn of the opponent's on the same rank, it may be captured en passant by that pawn of the opponent's, as if it had moved only one square forward. This capture is only legal on the opponent's next move immediately following the first pawn's advance. The diagrams on the right demonstrate an instance of this: if the white pawn moves from a2 to a4, the black pawn on b4 can capture it en passant, moving from b4 to a3 and the white pawn on a4 is removed from the board.
If a player advances a pawn to its eighth rank, the pawn is then promoted (converted) to a queen, rook, bishop, or knight of the same color at the choice of the player (a queen is usually chosen). The choice is not limited to previously captured pieces. Hence it is theoretically possible for a player to have up to nine queens or up to ten rooks, bishops, or knights if all of their pawns are promoted. If the desired piece is not available, the player must call the arbiter to provide the piece.[8][9]
A king is in check when it is under attack by at least one enemy piece. A piece unable to move because it would place its own king in check (it is pinned against its own king) may still deliver check to the opposing player.
It is illegal to make a move that places or leaves one's king in check. The possible ways to get out of check are:
If it is not possible to get out of check, the king is checkmated and the game is over (see the next section).
In informal games, it is customary to announce "check" when making a move that puts the opponent's king in check. However, in formal competitions, check is rarely announced.[14]
If a player's king is placed in check and there is no legal move that player can make to escape check, then the king is said to be checkmated, the game ends, and that player loses.[16] Unlike the other pieces, the king is never captured.[17]
The diagram shows an example checkmate position. The white king is threatened by the black queen; the empty square to which the king could move is also threatened; and the king cannot capture the queen, because it would then be in check by the rook.
Under FIDE Laws, a resignation by one player results in a draw if their opponent has no way to checkmate them via any series of legal moves, or a loss by that player otherwise.[20]
The game ends in a draw if any of these conditions occur:
There is no longer a rule specifically defining perpetual check as a draw. In such a situation, either the threefold repetition rule or the fifty-move rule will eventually come into effect. More often, the players will simply agree to a draw.[23][24]
A dead position is defined as a position where neither player can checkmate their opponent's king by any sequence of legal moves.[26]
A game played under time control will end as a loss for a player who uses up all of the time allotted on the player's clock, which is called flag-fall, unless the opponent has no possibility of effecting checkmate (see Timing). There are different types of time control. A player may have a fixed amount of time for the entire game, or may have to make a certain number of moves within a specified time. Also, a small increment of time may be added for each move made.
The following rules are the rules used for over-the-board (OTB) games. They are defined by the FIDE Laws of Chess.[5] The FIDE Laws of Chess define the rules for standard chess, rapid chess, blitz chess, and guidelines for Chess960. For standard chess, the players must record the moves, which is optional in rapid chess and blitz Chess.[29] 
Some rules are specifically adapted for disabled players. As the rules cover OTB play, they cannot be directly applied to computer chess or online chess, played on a computer device. The rules for correspondence chess are defined by the ICCF.
The movement of pieces is to be done with one hand. Once the hand is taken off a piece after moving it, the move cannot be retracted unless the move is illegal.[30] As for the touch-move rule, an arbiter who observes a violation of this rule must intervene immediately.[31] A player must claim a violation of the rule immediately before making a move, or lose the right to claim.[32]
When castling, a player should first move the king with one hand and then move the rook with the same hand.[33] In the case of a promotion, if a player releases the pawn on the eighth rank, the player must promote the pawn. After the pawn has moved, the player may touch any piece not on the board and the promotion is not finalized until the new piece is released on the promotion square.[34]
In serious play, if a player having the move touches a piece as if having the intention of moving it, then the player must move it if it can be legally moved. So long as the hand has not left the piece on a new square, any legal move can be made with the piece. If a player touches one of the opponent's pieces then that piece must be captured if there is a legal move that does so. If none of the touched pieces can be moved or captured, there is no penalty.[35] An arbiter who observes a violation of this rule must intervene immediately.[31] A player must claim a violation of the rule immediately before making a move, or lose the right to claim.[32]
When castling, the king must be the first piece touched.[36] If the player touches the king and a rook, the player must castle with that rook if it is legal to do so.[37] If the player completes a two-square king move without touching a rook, the player must move the correct rook accordingly if castling in that direction is legal. If a player starts to castle illegally, another legal king move must be made if possible, including castling with the other rook.[38]
If a player moves a pawn to its eighth rank, it cannot be substituted for a different move of the pawn when the player has stopped touching it. The move is not complete, however, until the promoted piece is released on that square.
If a player touches a piece to adjust its physical position within a square, he must first alert his opponent by saying J'adoube or "I adjust".[39] Once the game has started, only the player with the move may touch the pieces on the board.[40]
Tournament games are played under time constraints, called time controls, using a chess clock. Each player is timed separately and must make moves within the time control or forfeit the game. There are different types of time controls applied. For standard chess, different periods can be defined with different fixed times (e.g. first 40 moves in 100 minutes, next 20 moves in 50 minutes, remaining moves in 15 minutes). For rapid and blitz chess, only one period can be defined where all moves must be performed. Additionally, an increment or delay per move may be defined.[41]
In the last period of a standard chess game or rapid games, if played without increment, a special set of rules applies regarding the clock, referenced as "Quickplay Finishes".[51] These rules allow a player with under two minutes time to request an increment introduced, or request a draw based on claiming no progress or no effort, to be ruled by the arbiter. These rules have been relevant when playing with mechanical clocks, which do not allow setting an increment and are today with digital clocks of second importance only, as playing with increment is recommended.[52]
Each square of the chessboard is identified with a unique pair of a letter and a number. The vertical files are labeled a through h, from White's left (i.e. the queenside) to White's right. Similarly, the horizontal ranks are numbered from 1 to 8, starting from the one nearest White's side of the board. Each square of the board, then, is uniquely identified by its file letter and rank number. The white king, for example, starts the game on square e1. The black knight on b8 can move to a6 or c6.
In formal competition, each player is obliged to record each move as it is played in algebraic chess notation in order to settle disputes about illegal positions, overstepping time control, and making claims of draws by the fifty-move rule or repetition of position. Moves recorded in any other systems of notation cannot be used in evidence in such a dispute. Other chess notation systems include ICCF numeric notation for international correspondence chess and descriptive chess notation, formerly standard in English speaking countries. The current rule is that a move must be made on the board before it is written on paper or recorded with an electronic device.[53][54][55]
Both players should indicate offers of a draw by writing "=" at that move on their score sheets.[56] Notations about the time on the clocks can be made. A player with less than five minutes left to complete all the remaining moves is not required to record the moves (unless a delay of at least thirty seconds per move is being used). The score sheet must be made available to the arbiter at all times. A player may respond to an opponent's move before writing it down.[57]
Formerly common, adjournments are no longer standard practice in chess competition.
When an adjournment is made, the player whose move it is writes their next move on their scoresheet but does not make the move on the chessboard. This is referred to as a sealed move. Both opponents' scoresheets are then placed in the sealed-move envelope and the envelope is sealed. The names of the players, the colors, the position, the time on the clocks and other game data are recorded on the envelope; the envelope may also be signed by both players. The arbiter then keeps possession of the envelope until it is time to restart the game, at which time the arbiter opens the envelope, makes the sealed move on the board, and starts the opponent's clock.
An illegal move[58] is a move not made according to a piece's possible defined movements [59] or made according to its possible movements but such that its own king is left or placed in check.[60] Furthermore, pressing the clock without making a move or making a move with two hands is considered and penalized as an illegal move.[61]
A player who makes an illegal move must retract that move and make a legal move. That move must be made with the same piece if possible, because the touch-move rule applies. If the illegal move was an attempt to castle, the touch-move rule applies to the king but not to the rook. If the mistake is noticed, the game should be restarted from the position in which the error occurred. The arbiter should adjust the time on the clock according to the best evidence.[62] Some regional organizations have different rules.[63][64]
A player may correct an illegal move if the player has not pressed the clock. If a player has pressed the clock, the illegal move may be stated by the arbiter intervening or by the opponent claiming the illegal move. In standard chess, the illegal move must be claimed before the end of the game.[62] In the most used form of rapid chess and blitz chess, if the arbiter does not intervene and the opponent moves, the illegal move is accepted and without penalty.[65][66]
According to the FIDE Laws of Chess, the first stated completed illegal move results in a time penalty. The time penalty consists of giving the opponent two minutes extra time in standard and rapid chess, one minute extra time in blitz.[67] The second stated completed illegal move by the same player results in the loss of the game,[68] unless the position is such that it is impossible for the opponent to win by any series of legal moves (e.g. if the opponent has a bare king) in which case the game is drawn.[46] A move is completed when it has been made and the player has pressed the clock.[69]
Under USCF rules, if a player completes an illegal move in blitz chess, the player's opponent may claim a win before making a move (if the opponent has enough material to win). One way to claim this win is to take a King left in check by the opponent. Once the illegal move has been answered, the move stands.[70]
For standard chess and the most used form of rapid and blitz chess there are the following rules. If it is discovered during the game that the starting position was incorrect, the game is restarted. If it is discovered during the game that the board is oriented incorrectly, the game is continued with the pieces transferred to a correctly oriented board. If the game starts with colors reversed, the game is restarted if less than 10 moves have been made by both players, otherwise the game is continued.[71] If the clock setting is found to be incorrect during the game, it is corrected according to best judgement.[72] Some regional organizations have different rules.[73][74]
If a player knocks over pieces, it is the same player's responsibility to restore them to their correct positions, on that player's time. If it is discovered that an illegal move has been made, or that pieces have been displaced, the game is restored to the position before the irregularity. If that position cannot be determined, the game is restored to the last known correct position.[75]
An illegal position is a position which cannot be reached by any series of legal moves.[76]
Players may not use any notes, outside sources of information (including computers), or advice from other people. Analysis on another board is not permitted. Scoresheets are to record objective facts about the game only, such as time on the clock or draw offers. Players may not leave the competition area without permission of the arbiter.[77]
High standards of etiquette and ethics are expected. Players should shake hands before and after the game. Generally a player should not speak during the game, except to offer a draw, resign, or to call attention to an irregularity. An announcement of "check" is commonly made in informal games but is not recommended in officially sanctioned games. A player may not distract or annoy another player by any means, including repeatedly offering a draw.[78]
Due to increasing concerns about the use of chess engines and outside communication, mobile phone usage is banned. The first forfeit by a high-profile player, for phone ringing during play, occurred in 2003.[79] In 2014 FIDE extended this to ban all mobile phones from the playing area during chess competitions, under penalty of forfeiture of the game or even expulsion from the tournament. The rules allow for less rigid enforcement in minor events.[80]
In games subject to time control, a chess clock is used, consisting of two adjacent clocks and buttons to stop one clock while starting the other, such that the two component clocks never run simultaneously. The clock can be analog or digital though a digital clock is highly preferred under both USCF and FIDE rulesets. This is since most tournaments now include either an increment (extra time being added prior or after the move) or delay (a countdown to when a clock starts again) to their time controls. Before the start of the game, either the arbiter or whoever is playing Black decides where the chess clock is placed.
Between 1475 and 1500, the queen and the bishop also acquired their current moves, which made them much stronger pieces.[90][91] When all of these changes were accepted, the game was in essentially its modern form.[92]
Two new rules concerning draws were introduced, each of which have changed through the years:
Another group of new laws included (1) the touch-move rule and the accompanying "j'adoube/adjust" rule; (2) that White moves first (in 1889[94]); (3) the orientation of the board; (4) the procedure if an illegal move was made; (5) the procedure if the king had been left in check for some moves; and (6) issues regarding the behavior of players and spectators. The Staunton chess set was introduced in 1849 and it became the standard style of pieces. The size of pieces and squares of the board was standardized.[95]
The rules of national FIDE affiliates (such as the United States Chess Federation, or USCF) are based on the FIDE rules, with slight variations.[106][107][108] Some other differences are noted above. Kenneth Harkness published popular rulebooks in the United States starting in 1956, and the USCF continues to publish rulebooks for use in tournaments it sanctions.
In 2008, FIDE added the variant Chess960 to the appendix of the "Laws of Chess". Chess960 uses a random initial set-up of main pieces, with the conditions that the king is placed somewhere between the two rooks, and bishops on opposite-color squares. The castling rules are extended to cover all these positions.[109]
In the 21st century, rules about such things as mobile phones and unauthorised use of chess engines were introduced.[80]
From time to time, rules have been introduced at certain tournaments to discourage players from agreeing to short draws. One such case was the "no drawing or resigning during the first 30 moves" rule used at the 2009 London Chess Classic.[110]
FIDE's most visible activity is organizing the World Chess Championship since 1948. FIDE also organizes world championships for women, juniors, seniors, and the disabled.[7] Another flagship event is the Chess Olympiad, a biennial chess tournament organized since 1924, in which national teams compete. In alternate years, FIDE also organizes the World Team Championship, in which the best teams from the previous Olympiad compete.
As part of the World Chess Championship cycle, FIDE also organizes the Candidates Tournament, which determines who will challenge the reigning World Champion, and the qualifying tournaments for the Candidates, such as the Chess World Cup, the FIDE Grand Prix, and the FIDE Grand Swiss Tournament 2019.
FIDE is recognized by the International Olympic Committee (IOC) as the supreme body responsible for the organization of chess and its championships at global and continental levels.[8] Other tournaments are not overseen directly by FIDE, but they generally observe FIDE rules and regulations. Some national chess organizations such as the US Chess Federation use minor differences to FIDE rules.
FIDE defines the rules of chess, both for individual games (i.e. the board and moves) and for the conduct of international competitions. The international competition rules are the basis for local competitions, although local bodies are allowed to modify these rules to a certain extent. FIDE awards a number of organizational titles, including International Arbiter, which signifies that the recipient is competent and trusted to oversee top-class competitions.[9]
FIDE calculates the Elo ratings of players[10] and awards titles for achievement in competitive play, such as the Grandmaster title. It also awards titles to composers and solvers of chess problems and studies.
FIDE funds and manages outreach programs, such as the Chess for Freedom program[11] and awards such as, since 2020, the Svetozar Gligoric Award for fair play.[12]
Correspondence chess (chess played by post, email or on online servers) is regulated by the International Correspondence Chess Federation, an independent body that cooperates with FIDE where appropriate.
In April 1914, an initiative was taken in St. Petersburg, Russia, to form an international chess federation. Another attempt was made in July 1914 during the Mannheim International Chess Tournament, but further efforts temporarily came to an end as a result of the outbreak of World War I. In 1920, another attempt to organize an international federation was made at the Gothenburg Tournament.[15]
In 1922, the Russian master Eugene Znosko-Borovsky, while participating in an international tournament in London, announced that a tournament would be held during the 8th Sports Olympic Games in Paris in 1924 and would be hosted by the French Chess Federation. On July 20, 1924 the participants at the Paris tournament founded FIDE as a kind of players' union.[15][18][19] In its early years, FIDE had little power, and was poorly financed.
FIDE's congresses in 1925 and 1926 expressed a desire to become involved in managing the world championship. FIDE was largely happy with the "London Rules", but claimed that the requirement for a purse of $10,000 was impracticable and called upon Capablanca to come to an agreement with the leading masters to revise the Rules.[20]
FIDE's third congress, in Budapest in 1926, also decided to organize a Chess Olympiad. The invitations were, however, late in being sent, with the result that only four countries participated, and the competition was called the Little Olympiad. The winner was Hungary, followed by Yugoslavia, Romania, and Germany. In 1927, FIDE began organizing the First Chess Olympiad during its 4th Congress in London. The official title of the tournament was the "Tournament of Nations", or "World Team Championship", but "Chess Olympiad" became a more popular title. The event was won by Hungary, with 16 teams competing.[15]
In 1928 FIDE recognized Bogoljubow as "Champion of FIDE" after he won a match against Max Euwe.[20] Alekhine, the reigning world champion, attended part of the 1928 Congress and agreed to place future matches for the world title under the auspices of FIDE, although any match with Capablanca should be under the same conditions as in Buenos Aires, 1927, i.e. including the requirement for a purse of at least $10,000. FIDE accepted this and decided to form a commission to modify the London Rules for future matches, though this commission never met; by the time of the 1929 Congress, a world championship match between Alekhine and Bogoljubow was under way, held neither under the auspices of FIDE nor in accordance with the London Rules.[20]
From the time of Emanuel Lasker's defeat of Wilhelm Steinitz in 1894, until 1946, a new World Champion had won the title by defeating the former champion in a match. Alexander Alekhine's death created an interregnum that made the normal procedure impossible. The situation was confused, with many respected players and commentators offering different solutions. FIDE found it difficult to organize the early discussions on how to resolve the interregnum, because problems with money and travel in the aftermath of World War II prevented many countries from sending representatives, most notably the Soviet Union. The shortage of clear information resulted in otherwise responsible magazines publishing rumors and speculation, which only made the situation more confused.[23] See Interregnum of World Chess Champions for more details.
This situation was exacerbated by the Soviet Union having long refused to join FIDE, and by this time it was clear that about half the credible contenders were Soviet citizens. The Soviet Union realized, however, it could not afford to be left out of the discussions regarding the vacant world championship, and in 1947 sent a telegram apologizing for the absence of Soviet representatives and requesting that the USSR be represented in future FIDE Committees.[23]
FIDE had a number of conflicts with the Soviet Chess Federation. These conflicts included:[26]
The events leading to Garry Kasparov's winning the world championship involved FIDE in two controversies. While arranging the Candidates Tournament semi-final matches to be played in 1983, FIDE accepted bids to host Kasparov versus Victor Korchnoi in Pasadena, California. The Soviet Union refused to accept this, either because it feared Kasparov would defect or because it thought Kasparov was the greater threat to reigning champion Anatoly Karpov. Their refusal would have meant that Kasparov forfeited his chance of challenging for the title. FIDE president Florencio Campomanes negotiated with the Soviet Union, and the match was played in London.[28][30]
In 1992 Nigel Short surprised the world by winning the Candidates Tournament and thus becoming the official challenger for Kasparov's world title. FIDE very quickly accepted a bid from Manchester (England) to host the title match in 1993. But at that time Short was travelling to Greece and could not be consulted as FIDE's rules required. On learning of the situation Short contacted Kasparov, who had distrusted FIDE and its president, Florencio Campomanes ever since Campomanes had stopped his title match against Karpov in 1984.  Kasparov and Short concluded that FIDE had failed to get them the best financial deal available and announced that they would "play under the auspices of a new body, the "Professional Chess Association" (PCA). FIDE stripped Kasparov of his FIDE title and dropped Kasparov and Short from the official rating list. It also announced a title match between Karpov and Jan Timman, whom Short had defeated in the semi-final and final stages of the Candidates Tournament.  Kasparov and Karpov won their matches and there were now two players claiming to be world champion.[35]
In 1994 Kasparov concluded that breaking away from FIDE had been a mistake, because both commercial sponsors and the majority of grandmasters disliked the split in the world championship.[36] Kasparov started trying to improve relations with FIDE and supported Campomanes' bid for re-election as president of FIDE. But many FIDE delegates regarded Campomanes as corrupt and in 1995 he agreed to resign provided his successor was Kirsan Ilyumzhinov, president of the Republic of Kalmykia.[37]
Finally in 2006 a re-unification match was played between Kramnik and Veselin Topalov, which Kramnik won after an unpleasant controversy which led to one game being awarded to Topalov.[37][39]
In 1999, FIDE was recognised by the International Olympic Committee (IOC). Two years later, it introduced the IOC's anti-drugs rules to chess, as part of its campaign for chess to become part of the Olympic Games.[40]
In 2012 FIDE entered into a commercial agreement, initially planned to last until 2021, with the company Agon Limited. This company was given rights to organize and commercially exploit the World Chess Championship and the associated events in the World Championship cycle.[41] The first tournament it organized was the London FIDE Grand Prix event in September 2012,[42] followed by the London Candidates Tournament in March 2013,[43] and the Chennai World Chess Championship in November 2013.[44]
Agon had been founded in 2012 in Jersey by Andrew Paulson as the sole shareholder.[48] On February 20, 2012, an agreement between Agon and FIDE was made, subject to approval by the 2012 FIDE General Assembly.[41] This approval was forthcoming in September 2012.[49] In October 2014, Agon was sold to its current CEO Ilya Merenzon for the sum of one pound.[42] At the September 2016 FIDE General Assembly, it was resolved that Agon should institute a corporate presence in a locale with more transparency. Merenzon said that they would register in the United Kingdom within a few months.[50] As a result, a new company, World Chess Limited, was registered shortly after, replacing Agon as the rights holder in the agreement with FIDE.
The condition that Agon would be the sole organizer of Championship events was disputed originally by principally the Bulgarian Chess Federation, with respect to the Candidates matches for 2012.[53] In early 2014, a purported agreement between Paulson and FIDE President Kirsan Ilyumzhinov was leaked, and then published by Chess.com (and others), which allegedly indicated that Paulson was simply a front man with Ilyumzhinov the ultimate benefactor of Agon.[54] In that Chess.com article Malcolm Pein is quoted as having twice been told by Paulson that Ilyuzmhinov owned Agon, and in a New In Chess article Nigel Short asserted he had also been told this personally by Paulson.[55] In response, FIDE's deputy vice president Georgios Makropoulos pointed out that the purported contract was a draft document.[56] The FIDE Ethics Commission ruled in September 2015 that Ilyumzhinov did not violate the FIDE Code of Ethics.[57][58]
In July 2018, Kirsan Ilyumzhinov was ousted as FIDE President, after having been in office for 23 years, since 1995. Being subjected to US sanctions for his business dealings with the Syrian government, Ilyumzhinov was forced out and did not run for re-election in the 2018 FIDE elections. The Greek Georgios Makropoulos, who had been General Secretary since 1990 and number two in the organization under Kirsan's Presidency, was the first to announce his ticket. He was followed by the Englishman Nigel Short, a world title contender in the World Chess Championship 1993 against Garry Kasparov. The last to announce his candidacy was Arkady Dvorkovich, an economist who had served as Russian deputy prime minister and was also a member of the Supervisory Board of the Russian Chess Federation. Dvorkovich was also one of the chief organizers of the 2018 FIFA World Cup. Dvorkovich was placed in the US Treasury pre-sanctions list in 2018 as a top Russian government employee.[59]
In the elections, held in Batumi (Georgia) in October 2018, Dvorkovich won by 103 votes to 78[60] against Makropoulos, after Nigel Short withdrew his candidacy at the last minute and expressed his support to the Russian candidate.
After the 2018 FIDE elections and the appointment of a new FIDE President, the new management took regaining control over the World Championship cycle as one of their top priorities. In January 2019, FIDE Director-General Emil Sutovsky announced that a new contract has been signed that continues a scaled-back relationship with World Chess (formerly known as AGON) through 2021. In virtue of this new agreement, FIDE reasserted control over the 2020 Candidates and the World Championship match, which from now on will undergo an open bidding procedure. Agon/World Chess only retained organizational and commercial rights over the FIDE Grand Prix Series, limited until 2021.
At FIDE's general assembly in Chennai, India, in August 2022 Dvorkovich got re-elected by 157 votes to 16 against Ukraine's Andrii Baryshpolets.[61]
On February 27, 2022, FIDE issued an official statement condemning the Russian invasion of Ukraine. As a consequence, Russia and Belarus were forbidden from hosting official FIDE events. The decision to hold the 2022 Chess Olympiad and the 2022 FIDE congress in Moscow was also revoked.[62] The Russian and Belarusian national teams were banned from participating in FIDE tournaments, although individual players could compete if they complied with strict regulations, in which case their federation and flag was replaced with FIDE and its banner.[63]
On March 22, 2022, FIDE decided to issue a six month ban from competing in rated tournaments against Russian grandmaster Sergey Karjakin. Karjakin had posted controversial statements on Twitter in which he declared his support for the invasion of Ukraine and for President Vladimir Putin's characterization of the war as a fight against Nazism. FIDE argued that Karjakin's statements had shed a negative light on chess and on the federation and found that he had violated the FIDE code of ethics.[64] Sergei Shipov, who also publicly commented in favor of Russia, was not sanctioned, because FIDE decided that his statements were less provocative.[62]
Chess is played all over the world. The international governing body of chess is FIDE, established in 1924. Most national chess federations are now members of FIDE; several supranational chess organizations are also affiliated with FIDE.
The table below lists the member federations of FIDE[1] and their national championship. The number of grandmasters[2] and players registered for each federation in the FIDE database[3] as of September 2016 are also indicated.
The list below includes nations that no longer exist as well as national federations that are currently not members of FIDE.
Transferred from en.wikipedia to Commons by Laurens using CommonsHelper. 
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
Transferred from en.wikipedia to Commons by Laurens using CommonsHelper. 
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
To protect the wiki against automated account creation, we kindly ask you to enter the words that appear below in the box (more info):
To protect the wiki against automated account creation, we kindly ask you to enter the words that appear below in the box (more info):
Full help contents page
Training for students
A single-page guide to contributing
A training adventure game
Resources for new editors
People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using.
Many IP addresses change periodically, and are often shared by several people. You may create an account or log in to avoid future confusion with other logged out users. Creating an account also hides your IP address.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
Wikipedia is a compendium of the world's knowledge. If you know what you are looking for, type it into Wikipedia's search box. If, however, you need a bird's eye view of what Wikipedia has to offer, see its main contents pages below, which in turn list more specific pages.
Wikipedia's main contents systems are arranged into these subject classifications. Each subject is further divided into subtopics. 
Timelines list events chronologically, sometimes including links to articles with more detail.  There are several ways to find timelines:
You can help us keep Wikipedia up to date! The list below is for encyclopedia entries that describe and pertain to events happening on a current basis. 
Speaking of reference works, various third-party classification systems have been mapped to Wikipedia articles, which can be accessed from these pages:
Bibliographies list sources on a given topic, for verification or further reading outside Wikipedia:
Overview articles summarize in prose a broad topic like biology, and also have illustrations and links to subtopics like cell biology, biographies like Carl Linnaeus, and other related articles like Human Genome Project.
Outline pages have trees of topics in an outline format, which in turn are linked to further outlines and articles providing more detail.  Outlines show how important subtopics relate to each other based on how they are arranged in the tree, and they are useful as a more condensed, non-prose alternative to overview articles.
List pages enumerate items of a particular type, such as the List of sovereign states or List of South Africans.  Wikipedia has "lists of lists" when there are too many items to fit on a single page, when the items can be sorted in different ways, or as a way of navigating lists on a topic (for example Lists of countries and territories or Lists of people).  There are several ways to find lists:
Portals contain featured articles and images, news, categories, excerpts of key articles, links to related portals, and to-do lists for editors.  There are two ways to find portals:
Glossaries are lists of terms with definitions. Wikipedia includes hundreds of alphabetical glossaries; they can be found in two ways:
Wikipedia's collection of category pages is a classified index system.  It is automatically generated from category tags at the bottoms of articles and most other pages. Nearly all of the articles available so far on the website can be found through these subject indexes.
If you are simply looking to browse articles by topic, there are two top-level pages to choose from:
Category:Contents is technically at the top of the category hierarchy, but contains many categories useful to editors but not readers. Special:Categories lists every category alphabetically.
Vital articles are lists of subjects for which the English Wikipedia should have corresponding high-quality articles. They serve as centralized watchlists to track the quality status of Wikipedia's most important articles and to give editors guidance on which articles to prioritize for improvement.
Featured content represents the best of Wikipedia, and has undergone a thorough review process to ensure that it meets the highest encyclopedic standards. Presented by type:
Good articles are articles that meet a core set of editorial standards, the good article criteria, and successfully pass through the good article nomination process. They are well written, contain factually accurate and verifiable information, are broad in coverage, neutral in point of view, stable, and illustrated, where possible, by relevant images with suitable copyright licenses.
Growing collections of Wikipedia articles are starting to become available as spoken word recordings as well.
Said Al-Khatry (born 1947) is an Omani sport shooter. He competed in the 1984 Summer Olympics.[1]
This biographical article relating to sport shooting in Oman is a stub. You can help Wikipedia by expanding it.
Wikipedia is a dynamic free online encyclopedia that anyone can edit in good faith, and tens of millions already have!
Wikipedia's purpose is to benefit readers by containing information on all branches of knowledge. Hosted by the Wikimedia Foundation, Wikipedia consists of freely editable content, whose articles also have numerous links to guide readers to more information.
Written collaboratively by largely anonymous volunteers, anyone with Internet access and not blocked, can write and make changes to Wikipedia articles (except in limited cases where editing is restricted to prevent disruption or vandalism). Since its creation on January 15, 2001, Wikipedia has grown into the world's largest reference website, attracting over a billion visitors monthly. It currently has more than sixty million articles in more than 300 languages, including 6,623,677 articles in English with 129,357 active contributors in the past month.
The fundamental principles of Wikipedia are summarized in its five pillars. The Wikipedia community has developed many policies and guidelines, but you do not need to be familiar with every one of them before contributing.
Anyone can edit Wikipedia's text, references, and images. What is written is more important than who writes it. The content must conform with Wikipedia's policies, including being verifiable by published sources. Editors' opinions, beliefs, personal experiences, unreviewed research, libelous material, and copyright violations will not remain. Wikipedia's software allows easy reversal of errors, and experienced editors watch and patrol bad edits.
Wikipedia differs from printed references in important ways. It is continually created and updated, and encyclopedic articles on new events appear within minutes rather than months or years. Because anyone can improve Wikipedia, it has become more comprehensive, clear, and balanced than any other encyclopedia. Its contributors improve the quality and quantity of the articles as well as remove misinformation, errors, and vandalism. Any reader can fix a mistake or add more information to articles (see Researching with Wikipedia). 
Wikipedia has tested the wisdom of the crowd since 2001 and found that it succeeds.
We could not find the above page on our servers.
Alternatively, you can visit the Main Page or read more information about this type of error.
This page provides help with the most common questions about Wikipedia.
You can also search Wikipedia's help pages using the search box below, or browse the Help menu or the Help directory.
The Readers' FAQ and our about page contain the most commonly sought information about Wikipedia.
There are other ways to browse and explore Wikipedia articles; many can be found at Wikipedia:Contents. See our disclaimer for cautions about Wikipedia's limitations.
For mobile access, press the mobile view link at the very bottom of every desktop view page.
Contributing is easy: see how to edit a page. For a quick summary on participating, see contributing to Wikipedia, and for a friendly tutorial, see our introduction. For a listing of introductions and tutorials by topic, see getting started. The Simplified Manual of Style and Cheatsheet can remind you of basic wiki markup.
The simple guide to vandalism cleanup can help you undo malicious edits.
If you're looking for places you can help out, the Task Center is the place to go, or check out what else is happening at the community portal. You can practice editing and experiment in a sandboxyour sandbox.
If there is a problem with an article about yourself, a family member, a friend or a colleague, please read Biographies of living persons/Help.
If you spot a problem with an article, you can fix it directly, by clicking on the "Edit" link at the top of that page. See the "edit an article" section of this page for more information.
If you don't feel ready to fix the article yourself, post a message on the article's talk page. This will bring the matter to the attention of others who work on that article. There is a "Talk" link at the beginning of every article page.
Check Your first article to see if your topic is appropriate, then the Article wizard will walk you through creating the article.
Once you have created an article, see Writing better articles for guidance on how to improve it and what to include (like reference citations).
For contributing images, audio or video files, see the Introduction to uploading images. Then the Upload wizard will guide you through that process.
Answers to common problems can be found at frequently asked questions.
Or check out where to ask questions or make comments.
New users having problems editing Wikipedia should ask at the Teahouse. More complex questions can be posed at the Help desk. Volunteers will respond as soon as they're able.
Or ask for help on your talk page and a volunteer will visit you there!
You can get live help with editing in the help chatroom.
For help with technical issues, ask at the Village pump.
If searching Wikipedia has not answered your question (for example, questions like "Which country has the world's largest fishing fleet?"), try the Reference Desk. Volunteers there will attempt to answer your questions on any topic, or point you toward the information you need.
Screen readers are a form of assistive technology for people with disabilities. A list of screen readers is available including a section, Software aids for people with reading difficulties.
Reader software examples include Spoken Web, JAWS, and NonVisual Desktop Access (NVDA). In addition, Fangs screen reader emulator is an open-source extension for the Pale Moon browser that simulates how a web page would look in JAWS.
Full help contents page
Training for students
A single-page guide to contributing
A training adventure game
Resources for new editors
This page provides a listing of current collaborations, tasks, and news about English Wikipedia. New to Wikipedia? See the contributing to Wikipedia page or our tutorial for everything you need to know to get started. For a listing of internal project pages of interest, see the department directory.
For a listing of ongoing discussions and current requests, see the Dashboard.
You can help improve the articles listed below! This list updates frequently, so check back here for more tasks to try. (See Wikipedia:Maintenance or the  Task Center for further information.)
Help counter systemic bias by creating new articles on important women.
Welcome to the community bulletin board, which is a page used for announcements from WikiProjects and other groups. Included here are coordinated efforts, events, projects, and other general announcements.
Also consider posting WikiProject, Task Force, and Collaboration news at The Signpost's WikiProject Report page.
Latest tech news from the Wikimedia technical community. Please tell other users about these changes. Not all changes will affect you. Translations are available.
Discussions in the following areas have requested wider attention via Requests for comment:
The School and university projects page collects information about Wikipedia editing projects for school and university classes, including an archive of many past class projects.
A list of current classes using Wikipedia can be found at current projects.
Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.
Uploads locally to Wikipedia; must comply with the non-free content criteria
Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript.
Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again.
Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. 
In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed.
The filename you chose seems to be very short, or overly generic. Please don't use:
If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
This should not be done, except in very rare exceptional cases.
Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.
Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the Wikimedia Commons.
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
Please note that by "entirely self-made" we really mean just that. 
Do not use this section for any of the following:
Editors who falsely declare such items as their "own work" will be blocked from editing.
Use this only if there is an explicit licensing statement in the source. 
The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
If the source website doesn't say so explicitly, please do not upload the file.
Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it.
 Please remember that you will need to demonstrate that:
Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually.
Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
If this is an article you are only planning to write, please write it first and upload the file afterwards.
The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
Please upload this file only if it is going to be used in an actual article.
If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
The page Example is not a real article, but a disambiguation page pointing to a number of other pages.
Please check and enter the exact title of the actual target article you meant.
If neither of these two statements applies, then please do not upload this image.
This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
In view of this, please explain how the use of this file will be minimal.
Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you.
This is the data that will be submitted to upload:
This might take a minute or two, depending on the size of the file and the speed of your internet connection.
Once uploading is completed, you will find your new file at this link:
Your file has been uploaded successfully and can now be found here:
Please follow the link and check that the image description page has all the information you meant to include.
If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
To insert this file into an article, you may want to use code similar to the following:
If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages.
Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page.
Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet. See more at Help:Related changes.
Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding.
Uploads locally to Wikipedia; must comply with the non-free content criteria
Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript.
Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again.
Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account. Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it.     
You may already be able to upload files on the Wikimedia Commons, but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there.
Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload. 
In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed.
The filename you chose seems to be very short, or overly generic. Please don't use:
If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used.
This should not be done, except in very rare exceptional cases.
Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead.
If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to:
It is very important that you read through the following options and questions, and provide all required information truthfully and carefully.
Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, the Wikimedia Commons.
Files uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. 
However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here.
Please note that by "entirely self-made" we really mean just that. 
Do not use this section for any of the following:
Editors who falsely declare such items as their "own work" will be blocked from editing.
Use this only if there is an explicit licensing statement in the source. 
The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this.
If the source website doesn't say so explicitly, please do not upload the file.
Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. 
This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it.
 Please remember that you will need to demonstrate that:
Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the "http://en.wikipedia.org/..." URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually.
Please check the spelling, and make sure you enter the name of an existing article in which you will include this file.
If this is an article you are only planning to write, please write it first and upload the file afterwards.
The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc.
Please upload this file only if it is going to be used in an actual article.
If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that.
The page Example is not a real article, but a disambiguation page pointing to a number of other pages.
Please check and enter the exact title of the actual target article you meant.
If neither of these two statements applies, then please do not upload this image.
This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like.
In view of this, please explain how the use of this file will be minimal.
Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then:
Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader.
If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions. Thank you.
This is the data that will be submitted to upload:
This might take a minute or two, depending on the size of the file and the speed of your internet connection.
Once uploading is completed, you will find your new file at this link:
Your file has been uploaded successfully and can now be found here:
Please follow the link and check that the image description page has all the information you meant to include.
If you want to change the description, just go to the image page, click the "edit" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version.
To insert this file into an article, you may want to use code similar to the following:
If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the ":" after the initial brackets!):
See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages.
Thank you for using the File Upload Wizard.Please leave your feedback, comments, bug reports or suggestions on the talk page.
This page contains a list of special pages. Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{editprotected}} to draw the attention of administrators).
This is the current revision of this page, as edited by The Blade of the Northern Lights (talk | contribs) at 19:27, 5 October 2022 (Same issue, commented out text accidentally got split into two lines of markup). The present address (URL) is a permanent link to this version.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
Pages transcluded onto the current version of this page (help):
Please remember to check your manual of style, standards guide or instructor's guidelines for the exact syntax to suit your needs.  For more detailed advice, see Citing Wikipedia.
Wikipedia contributors. (2022, October 5). Main Page. In Wikipedia, The Free Encyclopedia. Retrieved 16:44, February 26, 2023, from https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180
Wikipedia contributors. "Main Page." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 5 Oct. 2022. Web. 26 Feb. 2023.
Wikipedia contributors, 'Main Page',  Wikipedia, The Free Encyclopedia, 5 October 2022, 19:27 UTC, <https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180> [accessed 26 February 2023]
Wikipedia contributors, "Main Page,"  Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180 (accessed February 26, 2023).
Wikipedia contributors. Main Page [Internet].  Wikipedia, The Free Encyclopedia;  2022 Oct 5, 19:27 UTC [cited 2023 Feb 26].  Available from: 
https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180.
Wikipedia contributors. Main Page. Wikipedia, The Free Encyclopedia. October 5, 2022, 19:27 UTC. Available at: https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1114291180. Accessed February 26, 2023.
When using the LaTeX package url (\usepackage{url} somewhere in the preamble), which tends to give much more nicely formatted web addresses, the following may be preferred:
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
If you have a question related to the Main Page, please search the talk page archives first to check if it has previously been addressed:
001 002 003 004 005 006 007 008 009
010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099
100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206
To report an error in current or upcoming Main Page content, please add it to the appropriate section below.
Our friend the old style calendar again. She was born on 14 February 1869 old style.  That calendar was in use in Russia until 1918 so as per the guidelines at Wikipedia:Selected_anniversaries; "the event should have occurred on the day in question in the calendar in use at the time (per MOS:JG)", we should use the old style date - Dumelow (talk) 07:58, 24 February 2023 (UTC)Reply[reply]
Has been orange-tagged for almost a year for an update.  How much of an update you can give an article from 1600+ years ago, I don't know, but it should be resolved either way before main page appearance - Dumelow (talk) 06:47, 25 February 2023 (UTC)Reply[reply]
The date does not appear in the source cited - Dumelow (talk) 09:50, 26 February 2023 (UTC)Reply[reply]
Date of birth uncited - Dumelow (talk) 10:00, 26 February 2023 (UTC)Reply[reply]
"At the time, the chart was titled Best-Selling Popular Record Albums and was "based on reports received from more than 200 dealers" throughout the United States."
"That Bing Crosby's Merry Christmas "reached the top in December 1945 and peaked for two more weeks in January 1946, for a total of six consecutive weeks at number one." is largely uncited.  The table cites only the 1946 chart position - Dumelow (talk) 08:28, 23 February 2023 (UTC)Reply[reply]
"It again reached the top in late November for an additional five weeks, making it the longest reigning album of the year".  The table shows six entries for this song in November and December, so that's six weeks right? - Dumelow (talk) 08:29, 23 February 2023 (UTC)Reply[reply]
"The second longest-reigning album of the previous year, Glenn Miller, recorded by Glenn Miller & His Orchestra"
Overall, I think I can have the article itself fixed up and a proposal for fixes to the blurb by twelve hours from now. Should leave some time for an admin to make the update before Monday. Firefangledfeathers (talk / contribs) 17:25, 24 February 2023 (UTC)Reply[reply]
The hurricane animation is very distracting; almost like an advertisement. 
I would strongly urge the community to not turn Wikipedia's main page into some kind of Yahoo portal. Please keep it static. Yes, some kids may consider this "boring". I consider it a prerequisite for "informative". CapnZapp (talk) 06:29, 17 February 2023 (UTC)Reply[reply]
I'm more concerned with its copyright status, which I've raised on the Commons talk page. --Paul_012 (talk) 04:05, 18 February 2023 (UTC)Reply[reply]
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
You do not have permission to edit this page, for the following reasons:
Pages transcluded onto the current version of this page (help):
Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
You do not have permission to edit this page, for the following reasons:
Pages transcluded onto the current version of this page (help):
Wikipedia[note 3] is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. Wikipedia is the largest and most-read reference work in history.[3] It is consistently one of the 10 most popular websites ranked by Similarweb and formerly Alexa; as of 2022,[update] Wikipedia was ranked the 5th most popular site in the world.[4] It is hosted by the Wikimedia Foundation, an American non-profit organization funded mainly through donations.[5]
Wikipedia has been praised for its enablement of the democratization of knowledge, extent of coverage, unique structure, culture, and reduced degree of commercial bias. It has been criticized for exhibiting systemic bias, particularly gender bias against women and alleged ideological bias.[12][13] The reliability of Wikipedia was frequently criticized in the 2000s, but has improved over time, as Wikipedia has been generally praised in the late 2010s and early 2020s.[3][12][14] The website's coverage of controversial topics such as American politics and major events like the COVID-19 pandemic and the Russian invasion of Ukraine has received substantial media attention.[15][16][17] It has been censored by world governments, ranging from specific pages to the entire site.[18][19] On 3 April 2018, Facebook and YouTube announced that they would help users detect fake news by suggesting fact-checking links to related Wikipedia articles.[20][21] Articles on breaking news are often accessed as a source of frequently updated information about those events.[22][23]
Various collaborative online encyclopedias were attempted before the start of Wikipedia, but with limited success.[24] Wikipedia began as a complementary project for Nupedia, a free online English-language encyclopedia project whose articles were written by experts and reviewed under a formal process.[25] It was founded on March 9, 2000, under the ownership of Bomis, a web portal company. Its main figures were Bomis CEO Jimmy Wales and Larry Sanger, editor-in-chief for Nupedia and later Wikipedia.[1][26] Nupedia was initially licensed under its own Nupedia Open Content License, but before Wikipedia was founded, Nupedia switched to the GNU Free Documentation License at the urging of Richard Stallman.[27] Wales is credited with defining the goal of making a publicly editable encyclopedia,[28][29] while Sanger is credited with the strategy of using a wiki to reach that goal.[30] On January 10, 2001, Sanger proposed on the Nupedia mailing list to create a wiki as a "feeder" project for Nupedia.[31]
The domains wikipedia.com (later redirecting to wikipedia.org) and wikipedia.org were registered on January 12, 2001,[32] and January 13, 2001,[33] respectively, and Wikipedia was launched on January 15, 2001[25] as a single English-language edition at www.wikipedia.com,[34] and announced by Sanger on the Nupedia mailing list.[28] Its integral policy of "neutral point-of-view"[35] was codified in its first few months. Otherwise, there were initially relatively few rules, and it operated independently of Nupedia.[28] Bomis originally intended it as a business for profit.[36]
Citing fears of commercial advertising and lack of control, users of the Spanish Wikipedia forked from Wikipedia to create Enciclopedia Librecode: spa promoted to code: es  in February 2002.[42] Wales then announced that Wikipedia would not display advertisements, and changed Wikipedia's domain from wikipedia.com to wikipedia.org.[43][44]
In November 2009, a researcher at the Rey Juan Carlos University in Madrid, Spain found that the English Wikipedia had lost 49,000 editors during the first three months of 2009; in comparison, it lost only 4,900 editors during the same period in 2008.[51][52] The Wall Street Journal cited the array of rules applied to editing and disputes related to such content among the reasons for this trend.[53] Wales disputed these claims in 2009, denying the decline and questioning the study's methodology.[54] Two years later, in 2011, he acknowledged a slight decline, noting a decrease from "a little more than 36,000 writers" in June 2010 to 35,800 in June 2011. In the same interview, he also claimed the number of editors was "stable and sustainable".[55] A 2013 MIT Technology Review article, "The Decline of Wikipedia", questioned this claim, revealing that since 2007, Wikipedia had lost a third of its volunteer editors, and that those remaining had focused increasingly on minutiae.[56] In July 2012, The Atlantic reported that the number of administrators was also in decline.[57] In the November 25, 2013, issue of New York magazine, Katherine Ward stated, "Wikipedia, the sixth-most-used website, is facing an internal crisis."[58]
The number of active English Wikipedia editors has since remained steady after a long period of decline.[59][60]
On January 20, 2014, Subodh Varma reporting for The Economic Times indicated that not only had Wikipedia's growth stalled, it "had lost nearly ten percent of its page views last year. There was a decline of about two billion between December 2012 and December 2013. Its most popular versions are leading the slide: page-views of the English Wikipedia declined by twelve percent, those of German version slid by 17 percent and the Japanese version lost nine percent."[70] Varma added, "While Wikipedia's managers think that this could be due to errors in counting, other experts feel that Google's Knowledge Graphs project launched last year may be gobbling up Wikipedia users."[70] When contacted on this matter, Clay Shirky, associate professor at New York University and fellow at Harvard's Berkman Klein Center for Internet & Society said that he suspected much of the page-view decline was due to Knowledge Graphs, stating, "If you can get your question answered from the search page, you don't need to click [any further]."[70] By the end of December 2016, Wikipedia was ranked the fifth most popular website globally.[71]
As of January 2023, 55,791 English Wikipedia articles have been cited 92,300 times in scholarly journals,[78] from which cloud computing was the most cited page.[79]
On January 18, 2023, Wikipedia debuted a new website redesign, called 'Vector 2022".[80][81] It featured a redesigned menu bar, moving the table of contents to the left as a sidebar, and numerous changes in the locations of buttons like the language selection tool.[81][82] The update initially received backlash, most notably when editors of the Swahili Wikipedia unanimously voted to revert the changes.[80][83]
Unlike traditional encyclopedias, Wikipedia follows the procrastination principle regarding the security of its content, meaning that it waits until a problem arises to fix it.[84]
Due to Wikipedia's increasing popularity, some editions, including the English version, have introduced editing restrictions for certain cases. For instance, on the English Wikipedia and some other language editions, only registered users may create a new article.[85] On the English Wikipedia, among others, particularly controversial, sensitive, or vandalism-prone pages have been protected to varying degrees.[86][87] A frequently vandalized article can be "semi-protected" or "extended confirmed protected", meaning that only "autoconfirmed" or "extended confirmed" editors can modify it.[88] A particularly contentious article may be locked so that only administrators can make changes.[89] A 2021 article in the Columbia Journalism Review identified Wikipedia's page-protection policies as "perhaps the most important" means at its disposal to "regulate its market of ideas".[90]
In certain cases, all editors are allowed to submit modifications, but review is required for some editors, depending on certain conditions. For example, the German Wikipedia maintains "stable versions" of articles which have passed certain reviews.[91] Following protracted trials and community discussion, the English Wikipedia introduced the "pending changes" system in December 2012.[92] Under this system, new and unregistered users' edits to certain controversial or vandalism-prone articles are reviewed by established users before they are published.[93]
Although changes are not systematically reviewed, the software that powers Wikipedia provides tools allowing anyone to review changes made by others. Each article's History page links to each revision.[note 4][94] On most articles, anyone can undo others' changes by clicking a link on the article's History page. Anyone can view the latest changes to articles, and anyone registered may maintain a "watchlist" of articles that interest them so they can be notified of changes.[95] "New pages patrol" is a process where newly created articles are checked for obvious problems.[96]
In 2003, economics PhD student Andrea Ciffolilli argued that the low transaction costs of participating in a wiki created a catalyst for collaborative development, and that features such as allowing easy access to past versions of a page favored "creative construction" over "creative destruction".[97]
Any change or edit that manipulates content in a way that deliberately compromises Wikipedia's integrity is considered vandalism. The most common and obvious types of vandalism include additions of obscenities and crude humor; it can also include advertising and other types of spam.[98] Sometimes editors commit vandalism by removing content or entirely blanking a given page. Less common types of vandalism, such as the deliberate addition of plausible but false information, can be more difficult to detect. Vandals can introduce irrelevant formatting, modify page semantics such as the page's title or categorization, manipulate the article's underlying code, or use images disruptively.[99]
Obvious vandalism is generally easy to remove from Wikipedia articles; the median time to detect and fix it is a few minutes.[100][101] However, some vandalism takes much longer to detect and repair.[102]
In the Seigenthaler biography incident, an anonymous editor introduced false information into the biography of American political figure John Seigenthaler in May 2005, falsely presenting him as a suspect in the assassination of John F. Kennedy.[102] It remained uncorrected for four months.[102] Seigenthaler, the founding editorial director of USA Today and founder of the Freedom Forum First Amendment Center at Vanderbilt University, called Wikipedia co-founder Jimmy Wales and asked whether he had any way of knowing who contributed the misinformation. Wales said he did not, although the perpetrator was eventually traced.[103][104] After the incident, Seigenthaler described Wikipedia as "a flawed and irresponsible research tool".[102] The incident led to policy changes at Wikipedia for tightening up the verifiability of biographical articles of living people.[105]
In 2010, Daniel Tosh encouraged viewers of his show, Tosh.0, to visit the show's Wikipedia article and edit it at will. On a later episode, he commented on the edits to the article, most of them offensive, which had been made by the audience and had prompted the article to be locked from editing.[106][107]
Wikipedians often have disputes regarding content, which may result in repeated competing changes to an article, known as "edit warring".[108][109] It is widely seen as a resource-consuming scenario where no useful knowledge is added,[110] and criticized as creating a competitive[111] and conflict-based editing culture associated with traditional masculine gender roles.[112][113]
Content in Wikipedia is subject to the laws (in particular, copyright laws) of the United States and of the US state of Virginia, where the majority of Wikipedia's servers are located.[114][115] By using the site, one agrees to the Wikimedia Foundation Terms of Use and Privacy Policy; some of the main rules are that contributors are legally responsible for their edits and contributions, that they should follow the policies that govern each of the independent project editions, and they may not engage in activities, whether legal or illegal, that may be harmful to other users.[116][117] In addition to the terms, the Foundation has developed policies, described as the "official policies of the Wikimedia Foundation".[118]
The editorial principles of the Wikipedia community are embodied in the "Five pillars" and in numerous policies and guidelines intended to appropriately shape content.[119] The rules developed by the community are stored in wiki form, and Wikipedia editors write and revise the website's policies and guidelines.[120] Editors can enforce the rules by deleting or modifying non-compliant material.[121] Originally, rules on the non-English editions of Wikipedia were based on a translation of the rules for the English Wikipedia. They have since diverged to some extent.[91]
According to the rules on the English Wikipedia community, each entry in Wikipedia must be about a topic that is encyclopedic and is not a dictionary entry or dictionary-style.[122] A topic should also meet Wikipedia's standards of "notability", which generally means that the topic must have been covered in mainstream media or major academic journal sources that are independent of the article's subject.[123] Further, Wikipedia intends to convey only knowledge that is already established and recognized.[124] It must not present original research.[125] A claim that is likely to be challenged requires a reference to a reliable source, as do all quotations.[122] Among Wikipedia editors, this is often phrased as "verifiability, not truth" to express the idea that the readers, not the encyclopedia, are ultimately responsible for checking the truthfulness of the articles and making their own interpretations.[126] This can at times lead to the removal of information that, though valid, is not properly sourced.[127] Finally, Wikipedia must not take sides.[128]
Wikipedia's initial anarchy integrated democratic and hierarchical elements over time.[129][130] An article is not considered to be owned by its creator or any other editor, nor by the subject of the article.[131]
Editors in good standing in the community can request extra user rights, granting them the technical ability to perform certain special actions. In particular, editors can choose to run for "adminship",[132] which includes the ability to delete pages or prevent them from being changed in cases of severe vandalism or editorial disputes.[133] Administrators are not supposed to enjoy any special privilege in decision-making; instead, their powers are mostly limited to making edits that have project-wide effects and thus are disallowed to ordinary editors, and to implement restrictions intended to prevent disruptive editors from making unproductive edits.[133]
By 2012, fewer editors were becoming administrators compared to Wikipedia's earlier years, in part because the process of vetting potential administrators had become more rigorous.[134] In 2022, there was a particularly contentious request for adminship over the candidate's anti-Trump views; ultimately, they were granted adminship.[135]
Over time, Wikipedia has developed a semiformal dispute resolution process. To determine community consensus, editors can raise issues at appropriate community forums, seek outside input through third opinion requests, or initiate a more general community discussion known as a "request for comment".[136]
The Arbitration Committee presides over the ultimate dispute resolution process. Although disputes usually arise from a disagreement between two opposing views on how an article should read, the Arbitration Committee explicitly refuses to directly rule on the specific view that should be adopted.[138] Statistical analyses suggest that the committee ignores the content of disputes and rather focuses on the way disputes are conducted,[139] functioning not so much to resolve disputes and make peace between conflicting editors, but to weed out problematic editors while allowing potentially productive editors back in to participate.[138] Therefore, the committee does not dictate the content of articles, although it sometimes condemns content changes when it deems the new content violates Wikipedia policies (for example, if the new content is considered biased).[note 5] Commonly used solutions include cautions and probations (used in 63% of cases) and banning editors from articles (43%), subject matters (23%), or Wikipedia (16%).[138] Complete bans from Wikipedia are generally limited to instances of impersonation and anti-social behavior.[140] When conduct is not impersonation or anti-social, but rather edit warring and other violations of editing policies, solutions tend to be limited to warnings.[138]
Each article and each user of Wikipedia has an associated and dedicated "talk" page. These form the primary communication channel for editors to discuss, coordinate and debate.[141]
Wikipedia's community has been described as cultlike,[142] although not always with entirely negative connotations.[143] Its preference for cohesiveness, even if it requires compromise that includes disregard of credentials, has been referred to as "anti-elitism".[144]
The English Wikipedia has 6,623,857 articles, 45,088,151 registered editors, and 129,357 active editors. An editor is considered active if they have made one or more edits in the past 30 days.[150]
Editors who fail to comply with Wikipedia cultural rituals, such as signing talk page comments, may implicitly signal that they are Wikipedia outsiders, increasing the odds that Wikipedia insiders may target or discount their contributions. Becoming a Wikipedia insider involves non-trivial costs: the contributor is expected to learn Wikipedia-specific technological codes, submit to a sometimes convoluted dispute resolution process, and learn a "baffling culture rich with in-jokes and insider references".[151] Editors who do not log in are in some sense second-class citizens on Wikipedia,[151] as "participants are accredited by members of the wiki community, who have a vested interest in preserving the quality of the work product, on the basis of their ongoing participation",[152] but the contribution histories of anonymous unregistered editors recognized only by their IP addresses cannot be attributed to a particular editor with certainty.[152]
A 2008 study found that Wikipedians were less agreeable, open, and conscientious than others,[154] although a later commentary pointed out serious flaws, including that the data showed higher openness and that the differences with the control group and the samples were small.[155] According to a 2009 study, there is "evidence of growing resistance from the Wikipedia community to new content".[156]
Several studies have shown that most Wikipedia contributors are male. Notably, the results of a Wikimedia Foundation survey in 2008 showed that only 13 percent of Wikipedia editors were female.[157] Because of this, universities throughout the United States tried to encourage women to become Wikipedia contributors.[158] Similarly, many of these universities, including Yale and Brown, gave college credit to students who create or edit an article relating to women in science or technology.[158] Andrew Lih, a professor and scientist, said that the reason he thought the number of male contributors outnumbered the number of females so greatly was because identifying as a woman may expose oneself to "ugly, intimidating behavior".[citation needed][159] Data has shown that Africans are underrepresented among Wikipedia editors.[160]
Distribution of the 60,598,804 articles in different language editions (as of February 26, 2023)[161]
There are currently 329 language editions of Wikipedia (also called language versions, or simply Wikipedias). As of February 2023, the six largest, in order of article count, are the English, Cebuano, German, Swedish, French, and Dutch Wikipedias.[162] The second and fourth-largest Wikipedias owe their position to the article-creating bot Lsjbot, which as of 2013[update] had created about half the articles on the Swedish Wikipedia, and most of the articles in the Cebuano and Waray Wikipedias. The latter are both languages of the Philippines.
Since Wikipedia is based on the Web and therefore worldwide, contributors to the same language edition may use different dialects or may come from different countries (as is the case for the English edition). These differences may lead to some conflicts over spelling differences (e.g. colour versus color)[166] or points of view.[167]
Though the various language editions are held to global policies such as "neutral point of view", they diverge on some points of policy and practice, most notably on whether images that are not licensed freely may be used under a claim of fair use.[168][169]
Jimmy Wales has described Wikipedia as "an effort to create and distribute a free encyclopedia of the highest possible quality to every single person on the planet in their own language".[170] Though each language edition functions more or less independently, some efforts are made to supervise them all. They are coordinated in part by Meta-Wiki, the Wikimedia Foundation's wiki devoted to maintaining all its projects (Wikipedia and others).[171] For instance, Meta-Wiki provides important statistics on all language editions of Wikipedia,[172] and it maintains a list of articles every Wikipedia should have.[173] The list concerns basic content by subject: biography, history, geography, society, culture, science, technology, and mathematics.[173] It is not rare for articles strongly related to a particular language not to have counterparts in another edition. For example, articles about small towns in the United States might be available only in English, even when they meet the notability criteria of other language Wikipedia projects.[123]
Translated articles represent only a small portion of articles in most editions, in part because those editions do not allow fully automated translation of articles. Articles available in more than one language may offer "interwiki links", which link to the counterpart articles in other editions.[175][176]
A study published by PLOS One in 2012 also estimated the share of contributions to different editions of Wikipedia from different regions of the world. It reported that the proportion of the edits made from North America was 51% for the English Wikipedia, and 25% for the simple English Wikipedia.[174]
On March 1, 2014, The Economist, in an article titled "The Future of Wikipedia", cited a trend analysis concerning data published by the Wikimedia Foundation stating that "[t]he number of editors for the English-language version has fallen by a third in seven years."[177] The attrition rate for active editors in English Wikipedia was cited by The Economist as substantially in contrast to statistics for Wikipedia in other languages (non-English Wikipedia). The Economist reported that the number of contributors with an average of five or more edits per month was relatively constant since 2008 for Wikipedia in other languages at approximately 42,000 editors within narrow seasonal variances of about 2,000 editors up or down. The number of active editors in English Wikipedia, by sharp comparison, was cited as peaking in 2007 at approximately 50,000 and dropping to 30,000 by the start of 2014.[177]
In contrast, the trend analysis for Wikipedia in other languages (non-English Wikipedia) shows success in retaining active editors on a renewable and sustained basis, with their numbers remaining relatively constant at approximately 42,000. No comment was made concerning which of the differentiated edit policy standards from Wikipedia in other languages (non-English Wikipedia) would provide a possible alternative to English Wikipedia for effectively improving substantial editor attrition rates on the English-language Wikipedia.[177]
Various Wikipedians have criticized Wikipedia's large and growing regulation, which includes more than fifty policies and nearly 150,000 words as of 2014.[update][178][137]
Critics have stated that Wikipedia exhibits systemic bias. In 2010, columnist and journalist Edwin Black described Wikipedia as being a mixture of "truth, half-truth, and some falsehoods".[179] Articles in The Chronicle of Higher Education and The Journal of Academic Librarianship have criticized Wikipedia's "Undue Weight" policy, concluding that Wikipedia explicitly is not designed to provide correct information about a subject, but rather focus on all the major viewpoints on the subject, give less attention to minor ones, and creates omissions that can lead to false beliefs based on incomplete information.[180][181][182]
Journalists Oliver Kamm and Edwin Black alleged (in 2010 and 2011 respectively) that articles are dominated by the loudest and most persistent voices, usually by a group with an "ax to grind" on the topic.[179][183] A 2008 article in Education Next Journal concluded that as a resource about controversial topics, Wikipedia is subject to manipulation and spin.[184]
In 2020, Omer Benjakob and Stephen Harrison noted that "Media coverage of Wikipedia has radically shifted over the past two decades: once cast as an intellectual frivolity, it is now lauded as the 'last bastion of shared reality' online."[185]
Multiple news networks and pundits have accused Wikipedia of being ideologically biased. In February 2021, Fox News accused Wikipedia of whitewashing communism and socialism and having too much "leftist bias".[186] In 2022, libertarian John Stossel opined that Wikipedia, a site he financially supported at one time, appeared to have gradually taken a significant turn in bias to the political left, specifically on political topics.[187]
As a consequence of the open structure, Wikipedia "makes no guarantee of validity" of its content, since no one is ultimately responsible for any claims appearing in it.[196] Concerns have been raised by PC World in 2009 regarding the lack of accountability that results from users' anonymity,[197] the insertion of false information,[198] vandalism, and similar problems.
Wikipedia's open structure inherently makes it an easy target for Internet trolls, spammers, and various forms of paid advocacy seen as counterproductive to the maintenance of a neutral and verifiable online encyclopedia.[94][207]
In response to paid advocacy editing and undisclosed editing issues, Wikipedia was reported in an article in The Wall Street Journal to have strengthened its rules and laws against undisclosed editing.[208] The article stated that: "Beginning Monday [from the date of the article, June 16, 2014], changes in Wikipedia's terms of use will require anyone paid to edit articles to disclose that arrangement. Katherine Maher, the nonprofit Wikimedia Foundation's chief communications officer, said the changes address a sentiment among volunteer editors that, 'we're not an advertising service; we're an encyclopedia.'"[208][209][210][211][212] These issues, among others, had been parodied since the first decade of Wikipedia, notably by Stephen Colbert on The Colbert Report.[213]
Legal Research in a Nutshell (2011), cites Wikipedia as a "general source" that "can be a real boon" in "coming up to speed in the law governing a situation" and, "while not authoritative, can provide basic facts as well as leads to more in-depth resources".[214]
Some university lecturers discourage students from citing any encyclopedia in academic work, preferring primary sources;[215] some specifically prohibit Wikipedia citations.[216][217] Wales stresses that encyclopedias of any type are not usually appropriate to use as citable sources, and should not be relied upon as authoritative.[218] Wales once (2006 or earlier) said he receives about ten emails weekly from students saying they got failing grades on papers because they cited Wikipedia; he told the students they got what they deserved. "For God's sake, you're in college; don't cite the encyclopedia", he said.[219][220]
In February 2007, an article in The Harvard Crimson newspaper reported that a few of the professors at Harvard University were including Wikipedia articles in their syllabi, although without realizing the articles might change.[221] In June 2007, former president of the American Library Association Michael Gorman condemned Wikipedia, along with Google, stating that academics who endorse the use of Wikipedia are "the intellectual equivalent of a dietitian who recommends a steady diet of Big Macs with everything".[222]
Contrarily, a 2016 article in the Universal Journal of Educational Research argued that "Wikipedia can be used for serious student projects..." and that Wikipedia is a good place to learn academic writing styles.[223] A 2020 research study published in Studies in Higher Education argued that Wikipedia could be applied in the higher education "flipped classroom", an educational model where students learn before coming to class and apply it in classroom activities. The experimental group was instructed to learn before class and get immediate feedback before going in (the flipped classroom model), while the control group was given direct instructions in class (the conventional classroom model). The groups were then instructed to collaboratively develop Wikipedia entries, which would be graded in quality after the study. The results showed that the experimental group yielded more Wikipedia entries and received higher grades in quality. The study concluded that learning with Wikipedia in flipped classrooms was more effective than in conventional classrooms, proving that Wikipedia could be used as an educational tool in higher education.[224]
On March 5, 2014, Julie Beck writing for The Atlantic magazine in an article titled "Doctors' #1 Source for Healthcare Information: Wikipedia", stated that "Fifty percent of physicians look up conditions on the (Wikipedia) site, and some are editing articles themselves to improve the quality of available information."[225] Beck continued to detail in this article new programs of Amin Azzam at the University of San Francisco to offer medical school courses to medical students for learning to edit and improve Wikipedia articles on health-related issues, as well as internal quality control programs within Wikipedia organized by James Heilman to improve a group of 200 health-related articles of central medical importance up to Wikipedia's highest standard of articles using its Featured Article and Good Article peer-review evaluation process.[225] In a May 7, 2014 follow-up article in The Atlantic titled "Can Wikipedia Ever Be a Definitive Medical Text?", Julie Beck quotes WikiProject Medicine's James Heilman as stating: "Just because a reference is peer-reviewed doesn't mean it's a high-quality reference."[226] Beck added that: "Wikipedia has its own peer review process before articles can be classified as 'good' or 'featured'. Heilman, who has participated in that process before, says 'less than one percent' of Wikipedia's medical articles have passed."[226]
Wikipedia seeks to create a summary of all human knowledge in the form of an online encyclopedia, with each topic covered encyclopedically in one article. Since it has terabytes of disk space, it can have far more topics than can be covered by any printed encyclopedia.[227] The exact degree and manner of coverage on Wikipedia is under constant review by its editors, and disagreements are not uncommon (see deletionism and inclusionism).[228][229] Wikipedia contains materials that some people may find objectionable, offensive, or pornographic.[230] The "Wikipedia is not censored" policy has sometimes proved controversial: in 2008, Wikipedia rejected an online petition against the inclusion of images of Muhammad in the English edition of its Muhammad article, citing this policy.[231] The presence of politically, religiously, and pornographically sensitive materials in Wikipedia has led to the censorship of Wikipedia by national authorities in China[232] and Pakistan,[233] amongst other countries.[234][235][236]
A 2008 study conducted by researchers at Carnegie Mellon University and Palo Alto Research Center gave a distribution of topics as well as growth (from July 2006 to January 2008) in each field:[237]
These numbers refer only to the number of articles: it is possible for one topic to contain a large number of short articles and another to contain a small number of large ones. Through its "Wikipedia Loves Libraries" program, Wikipedia has partnered with major public libraries such as the New York Public Library for the Performing Arts to expand its coverage of underrepresented subjects and articles.[238]
A 2011 study conducted by researchers at the University of Minnesota indicated that male and female editors focus on different coverage topics. There was a greater concentration of females in the "people and arts" category, while males focus more on "geography and science".[239]
Research conducted by Mark Graham of the Oxford Internet Institute in 2009 indicated that the geographic distribution of article topics is highly uneven, Africa being the most underrepresented.[240] Across 30 language editions of Wikipedia, historical articles and sections are generally Eurocentric and focused on recent events.[241]
An editorial in The Guardian in 2014 claimed that more effort went into providing references for a list of female porn actors than a list of women writers.[242] Data has also shown that Africa-related material often faces omission; a knowledge gap that a July 2018 Wikimedia conference in Cape Town sought to address.[160]
When multiple editors contribute to one topic or set of topics, systemic bias may arise, due to the demographic backgrounds of the editors. In 2011, Wales claimed that the unevenness of coverage is a reflection of the demography of the editors, citing for example "biographies of famous women through history and issues surrounding early childcare".[55] The October 22, 2013, essay by Tom Simonite in MIT's Technology Review titled "The Decline of Wikipedia" discussed the effect of systemic bias and policy creep on the downward trend in the number of editors.[56]
Taha Yasseri of the University of Oxford, in 2013, studied the statistical trends of systemic bias at Wikipedia introduced by editing conflicts and their resolution.[243][244] His research examined the counterproductive work behavior of edit warring. Yasseri contended that simple reverts or "undo" operations were not the most significant measure of counterproductive behavior at Wikipedia and relied instead on the statistical measurement of detecting "reverting/reverted pairs" or "mutually reverting edit pairs". Such a "mutually reverting edit pair" is defined where one editor reverts the edit of another editor who then, in sequence, returns to revert the first editor in the "mutually reverting edit pairs". The results were tabulated for several language versions of Wikipedia. The English Wikipedia's three largest conflict rates belonged to the articles George W. Bush, anarchism, and Muhammad.[244] By comparison, for the German Wikipedia, the three largest conflict rates at the time of the Oxford study were for the articles covering Croatia, Scientology, and 9/11 conspiracy theories.[244]
Researchers from Washington University in St. Louis developed a statistical model to measure systematic bias in the behavior of Wikipedia's users regarding controversial topics. The authors focused on behavioral changes of the encyclopedia's administrators after assuming the post, writing that systematic bias occurred after the fact.[245][246]
Wikipedia has been criticized for allowing information about graphic content.[247] Articles depicting what some critics have called objectionable content (such as feces, cadaver, human penis, vulva, and nudity) contain graphic pictures and detailed information easily available to anyone with access to the internet, including children.[248]
The site also includes sexual content such as images and videos of masturbation and ejaculation, illustrations of zoophilia, and photos from hardcore pornographic films in its articles. It also has non-sexual photographs of nude children.[249]
In April 2010, Sanger wrote a letter to the Federal Bureau of Investigation, outlining his concerns that two categories of images on Wikimedia Commons contained child pornography, and were in violation of US federal obscenity law.[251][252] Sanger later clarified that the images, which were related to pedophilia and one about lolicon, were not of real children, but said that they constituted "obscene visual representations of the sexual abuse of children", under the PROTECT Act of 2003.[253] That law bans photographic child pornography and cartoon images and drawings of children that are obscene under American law.[253] Sanger also expressed concerns about access to the images on Wikipedia in schools.[254] Wikimedia Foundation spokesman Jay Walsh strongly rejected Sanger's accusation,[255] saying that Wikipedia did not have "material we would deem to be illegal. If we did, we would remove it."[255] Following the complaint by Sanger, Wales deleted sexual images without consulting the community. After some editors who volunteered to maintain the site argued that the decision to delete had been made hastily, Wales voluntarily gave up some of the powers he had held up to that time as part of his co-founder status. He wrote in a message to the Wikimedia Foundation mailing-list that this action was "in the interest of encouraging this discussion to be about real philosophical/content issues, rather than be about me and how quickly I acted".[256] Critics, including Wikipediocracy, noticed that many of the pornographic images deleted from Wikipedia since 2010 have reappeared.[257]
In January 2006, a German court ordered the German Wikipedia shut down within Germany because it stated the full name of Boris Floricic, aka "Tron", a deceased hacker. On February 9, 2006, the injunction against Wikimedia Deutschland was overturned, with the court rejecting the notion that Tron's right to privacy or that of his parents was being violated.[260]
Wikipedia has a "Volunteer Response Team" that uses Znuny, a free and open-source software fork of OTRS[261] to handle queries without having to reveal the identities of the involved parties. This is used, for example, in confirming the permission for using individual images and other media in the project.[262]
Wikipedia was described in 2015 as harboring a battleground culture of sexism and harassment.[263][264] The perceived toxic attitudes and tolerance of violent and abusive language were reasons put forth in 2013 for the gender gap in Wikipedia editorship.[265] Edit-a-thons have been held to encourage female editors and increase the coverage of women's topics.[266]
A comprehensive 2008 survey, published in 2016, by Julia B. Bear of Stony Brook University's College of Business and Benjamin Collier of Carnegie Mellon University found significant gender differences in confidence in expertise, discomfort with editing, and response to critical feedback. "Women reported less confidence in their expertise, expressed greater discomfort with editing (which typically involves conflict), and reported more negative responses to critical feedback compared to men."[271]
Maher served as executive director until April 2021.[281] Maryana Iskander was named the incoming CEO in September 2021, and took over that role in January 2022. She stated that one of her focuses would be increasing diversity in the Wikimedia community.[282]
The operation of Wikipedia depends on MediaWiki, a custom-made, free and open source wiki software platform written in PHP and built upon the MySQL database system.[284] The software incorporates programming features such as a macro language, variables, a transclusion system for templates, and URL redirection.[285] MediaWiki is licensed under the GNU General Public License (GPL) and it is used by all Wikimedia projects, as well as many other wiki projects.[284][286] Originally, Wikipedia ran on UseModWiki written in Perl by Clifford Adams (Phase I), which initially required CamelCase for article hyperlinks; the present double bracket style was incorporated later.[287] Starting in January 2002 (Phase II), Wikipedia began running on a PHP wiki engine with a MySQL database; this software was custom-made for Wikipedia by Magnus Manske. The Phase II software was repeatedly modified to accommodate the exponentially increasing demand. In July 2002 (Phase III), Wikipedia shifted to the third-generation software, MediaWiki, originally written by Lee Daniel Crocker.
Several MediaWiki extensions are installed to extend the functionality of the MediaWiki software.[288]
In April 2005, a Lucene extension[289][290] was added to MediaWiki's built-in search and Wikipedia switched from MySQL to Lucene for searching. Lucene was later replaced by CirrusSearch which is based on Elasticsearch.[291]
In July 2013, after extensive beta testing, a WYSIWYG (What You See Is What You Get) extension, VisualEditor, was opened to public use.[292][293][294] It was met with much rejection and criticism, and was described as "slow and buggy".[295] The feature was changed from opt-out to opt-in afterward.[296]
Computer programs called bots have often been used to perform simple and repetitive tasks, such as correcting common misspellings and stylistic issues, or to start articles such as geography entries in a standard format from statistical data.[297][298][299] One controversial contributor, Sverker Johansson, created articles with his bot Lsjbot, which was reported to create up to 10,000 articles on the Swedish Wikipedia on certain days.[300] Additionally, there are bots designed to automatically notify editors when they make common editing errors (such as unmatched quotes or unmatched parentheses).[301] Edits falsely identified by bots as the work of a banned editor can be restored by other editors. An anti-vandal bot is programmed to detect and revert vandalism quickly.[298] Bots are able to indicate edits from particular accounts or IP address ranges, as occurred at the time of the shooting down of the MH17 jet incident in July 2014 when it was reported that edits were made via IPs controlled by the Russian government.[302] Bots on Wikipedia must be approved before activation.[303]
According to Andrew Lih, the current expansion of Wikipedia to millions of articles would be difficult to envision without the use of such bots.[304]
As of 2021,[update] page requests are first passed to a front-end layer of Varnish caching servers and back-end layer caching is done by Apache Traffic Server.[305] Requests that cannot be served from the Varnish cache are sent to load-balancing servers running the Linux Virtual Server software, which in turn pass them to one of the Apache web servers for page rendering from the database.[305] The web servers deliver pages as requested, performing page rendering for all the language editions of Wikipedia. To increase speed further, rendered pages are cached in a distributed memory cache until invalidated, allowing page rendering to be skipped entirely for most common page accesses.[306]
Multiple Wikimedia projects have internal news publications. Wikimedia's online newspaper The Signpost was founded in 2005 by Michael Snow, a Wikipedia administrator who would join the Wikimedia Foundation's board of trustees in 2008.[317][318] The publication covers news and events from the English Wikipedia, the Wikimedia Foundation, and Wikipedia's sister projects.[319] Other past and present community news publications on English Wikipedia include the Wikiworld webcomic,[320] the Wikipedia Weekly podcast,[321] and newsletters of specific WikiProjects like The Bugle from WikiProject Military History[322] and the monthly newsletter from The Guild of Copy Editors.[323] There are also several publications from the Wikimedia Foundation and multilingual publications such as Wikimedia Diff[324] and This Month in Education.[325]
When the project was started in 2001, all text in Wikipedia was covered by the GNU Free Documentation License (GFDL), a copyleft license permitting the redistribution, creation of derivative works, and commercial use of content while authors retain copyright of their work.[329] The GFDL was created for software manuals that come with free software programs licensed under the GPL. This made it a poor choice for a general reference work: for example, the GFDL requires the reprints of materials from Wikipedia to come with a full copy of the GFDL text.[330] In December 2002, the Creative Commons license was released; it was specifically designed for creative works in general, not just for software manuals. The Wikipedia project sought the switch to the Creative Commons.[331] Because the GFDL and Creative Commons were incompatible, in November 2008, following the request of the project, the Free Software Foundation (FSF) released a new version of the GFDL designed specifically to allow Wikipedia to relicense its content to CC BY-SA by August 1, 2009.[332] In April 2009, Wikipedia and its sister projects held a community-wide referendum which decided the switch in June 2009.[333][334][335][336]
The handling of media files (e.g. image files) varies across language editions. Some language editions, such as the English Wikipedia, include non-free image files under fair use doctrine,[337] while the others have opted not to, in part because of the lack of fair use doctrines in their home countries (e.g. in Japanese copyright law). Media files covered by free content licenses (e.g. Creative Commons' CC BY-SA) are shared across language editions via Wikimedia Commons repository, a project operated by the Wikimedia Foundation.[338] Wikipedia's accommodation of varying international copyright laws regarding images has led some to observe that its photographic coverage of topics lags behind the quality of the encyclopedic text.[339]
The Wikimedia Foundation is not a licensor of content on Wikipedia or its related projects but merely a hosting service for contributors to and licensors of Wikipedia, a position which was successfully defended in 2004 in a court in France.[340][341]
Because Wikipedia content is distributed under an open license, anyone can reuse or re-distribute it at no charge.[342] The content of Wikipedia has been published in many forms, both online and offline, outside the Wikipedia website.
Thousands of "mirror sites" exist that republish content from Wikipedia; two prominent ones that also include content from other reference sources are Reference.com and Answers.com.[343][344] Another example is Wapedia, which began to display Wikipedia content in a mobile-device-friendly format before Wikipedia itself did.[345] Some web search engines make special use of Wikipedia content when displaying search results: examples include Microsoft Bing (via technology gained from Powerset)[346] and DuckDuckGo.
Collections of Wikipedia articles have been published on optical discs. An English version released in 2006 contained about 2,000 articles.[347] The Polish-language version from 2006 contains nearly 240,000 articles,[348] the German-language version from 2007/2008 contains over 620,000 articles,[349] and the Spanish-language version from 2011 contains 886,000 articles.[350] Additionally, "Wikipedia for Schools", the Wikipedia series of CDs / DVDs produced by Wikipedia and SOS Children, is a free selection from Wikipedia designed for education towards children eight to seventeen.[351]
There have been efforts to put a select subset of Wikipedia's articles into printed book form.[352][353] Since 2009, tens of thousands of print-on-demand books that reproduced English, German, Russian, and French Wikipedia articles have been produced by the American company Books LLC and by three Mauritian subsidiaries of the German publisher VDM.[354]
Obtaining the full contents of Wikipedia for reuse presents challenges, since direct cloning via a web crawler is discouraged.[359] Wikipedia publishes "dumps" of its contents, but these are text-only; as of 2023,[update] there is no dump available of Wikipedia's images.[360] Wikimedia Enterprise is a for-profit solution to this.[361]
Several languages of Wikipedia also maintain a reference desk, where volunteers answer questions from the general public. According to a study by Pnina Shachaf in the Journal of Documentation, the quality of the Wikipedia reference desk is comparable to a standard library reference desk, with an accuracy of 55 percent.[362]
Bloomberg Businessweek reported in July 2014 that Google's Android mobile apps have dominated the largest share of global smartphone shipments for 2013, with 78.6% of market share over their next closest competitor in iOS with 15.2% of the market.[363] At the time of the appointment of new Wikimedia Foundation executive Lila Tretikov, Wikimedia representatives made a technical announcement concerning the number of mobile access systems in the market seeking access to Wikipedia. Soon after, the representatives stated that Wikimedia would be applying an all-inclusive approach to accommodate as many mobile access systems as possible in its efforts for expanding general mobile access, including BlackBerry and the Windows Phone system, making market share a secondary issue.[278] The Android app for Wikipedia was released on July 23, 2014, to over 500,000 installs and generally positive reviews, scoring over four of a possible five in a poll of approximately 200,000 users downloading from Google.[364][365] The version for iOS was released on April 3, 2013, to similar reviews.[366]
Access to Wikipedia from mobile phones was possible as early as 2004, through the Wireless Application Protocol (WAP), via the Wapedia service.[345] In June 2007, Wikipedia launched en.mobile.wikipedia.org, an official website for wireless devices. In 2009, a newer mobile service was officially released, located at en.m.wikipedia.org, which caters to more advanced mobile devices such as the iPhone, Android-based devices, or WebOS-based devices.[367] Several other methods of mobile access to Wikipedia have emerged since. Many devices and applications optimize or enhance the display of Wikipedia content for mobile devices, while some also incorporate additional features such as use of Wikipedia metadata like geoinformation.[368][369]
Wikipedia Zero was an initiative of the Wikimedia Foundation to expand the reach of the encyclopedia to the developing countries by partnering with mobile operators to allow free access.[370][371] It was discontinued in February 2018 due to lack of participation from mobile operators.[370]
Andrew Lih and Andrew Brown both maintain editing Wikipedia with smartphones is difficult and this discourages new potential contributors.[372][373] Lih states that the number of Wikipedia editors has been declining after several years,[372] and Tom Simonite of MIT Technology Review claims the bureaucratic structure and rules are a factor in this. Simonite alleges some Wikipedians use the labyrinthine rules and guidelines to dominate others and those editors have a vested interest in keeping the status quo.[56] Lih alleges there is a serious disagreement among existing contributors on how to resolve this. Lih fears for Wikipedia's long-term future while Brown fears problems with Wikipedia will remain and rival encyclopedias will not replace it.[372][373]
Access to the Chinese Wikipedia has been blocked in mainland China since May 2015.[19][374][375] This was done after Wikipedia started to use HTTPS encryption, which made selective censorship more difficult.[376]
In 2017, Quartz reported that the Chinese government had begun creating an unofficial version of Wikipedia. However, unlike Wikipedia, the website's contents would only be editable by scholars from state-owned Chinese institutions. The article stated it had been approved by the State Council of the People's Republic of China in 2011.[377]
According to "Wikipedia Readership Survey 2011", the average age of Wikipedia readers is 36, with a rough parity between genders. Almost half of Wikipedia readers visit the site more than five times a month, and a similar number of readers specifically look for Wikipedia in search engine results. About 47 percent of Wikipedia readers do not realize that Wikipedia is a non-profit organization.[385]
Wikipedia has also been used as a source in journalism,[401][402] often without attribution, and several reporters have been dismissed for plagiarizing from Wikipedia.[403][404][405][406]
In 2006, Time magazine recognized Wikipedia's participation (along with YouTube, Reddit, MySpace, and Facebook) in the rapid growth of online collaboration and interaction by millions of people worldwide.[407] On September 16, 2007, The Washington Post reported that Wikipedia had become a focal point in the 2008 US election campaign, saying: "Type a candidate's name into Google, and among the first results is a Wikipedia page, making those entries arguably as important as any ad in defining a candidate. Already, the presidential entries are being edited, dissected and debated countless times each day."[408] An October 2007 Reuters article, titled "Wikipedia page the latest status symbol", reported the recent phenomenon of how having a Wikipedia article vindicates one's notability.[409]
One of the first times Wikipedia was involved in a governmental affair was on September 28, 2007, when Italian politician Franco Grillini raised a parliamentary question with the minister of cultural resources and activities about the necessity of freedom of panorama. He said that the lack of such freedom forced Wikipedia, "the seventh most consulted website", to forbid all images of modern Italian buildings and art, and claimed this was hugely damaging to tourist revenues.[410]
In 2007, readers of brandchannel.com voted Wikipedia as the fourth-highest brand ranking, receiving 15 percent of the votes in answer to the question "Which brand had the most impact on our lives in 2006?"[415]
In 2015, Wikipedia was awarded both the annual Erasmus Prize, which recognizes exceptional contributions to culture, society or social sciences,[417] and the Spanish Princess of Asturias Award on International Cooperation.[418] Speaking at the Asturian Parliament in Oviedo, the city that hosts the awards ceremony, Jimmy Wales praised the work of the Asturian Wikipedia users.[419]
Many parodies target Wikipedia's openness and susceptibility to inserted inaccuracies, with characters vandalizing or modifying the online encyclopedia project's articles.
In an April 2007 episode of the American television comedy The Office, office manager (Michael Scott) is shown relying on a hypothetical Wikipedia article for information on negotiation tactics to assist him in negotiating lesser pay for an employee.[422] Viewers of the show tried to add the episode's mention of the page as a section of the actual Wikipedia article on negotiation, but this effort was prevented by other users on the article's talk page.[423]
"My Number One Doctor", a 2007 episode of the television show Scrubs, played on the perception that Wikipedia is an unreliable reference tool with a scene in which Perry Cox reacts to a patient who says that a Wikipedia article indicates that the raw food diet reverses the effects of bone cancer by retorting that the same editor who wrote that article also wrote the Battlestar Galactica episode guide.[424]
In 2008, the comedy website CollegeHumor produced a video sketch named "Professor Wikipedia", in which the fictitious Professor Wikipedia instructs a class with a medley of unverifiable and occasionally absurd statements.[425]
The Dilbert comic strip from May 8, 2009, features a character supporting an improbable claim by saying "Give me ten minutes and then check Wikipedia."[426]
In July 2009, BBC Radio 4 broadcast a comedy series called Bigipedia, which was set on a website which was a parody of Wikipedia.[427] Some of the sketches were directly inspired by Wikipedia and its articles.[428]
On August 23, 2013, the New Yorker website published a cartoon with this caption: "Dammit, Manning, have you considered the pronoun war that this is going to start on your Wikipedia page?"[429] The cartoon referred to Chelsea Elizabeth Manning (born Bradley Edward Manning), an American activist, politician, and former United States Army soldier who had recently come out as a trans woman.[430]
In December 2015, John Julius Norwich stated, in a letter published in The Times newspaper, that as a historian he resorted to Wikipedia "at least a dozen times a day", and had never yet caught it out. He described it as "a work of reference as useful as any in existence", with so wide a range that it is almost impossible to find a person, place, or thing that it has left uncovered and that he could never have written his last two books without it.[431]
Wikipedia has spawned several sister projects, which are also wikis run by the Wikimedia Foundation. These other Wikimedia projects include Wiktionary, a dictionary project launched in December 2002,[432] Wikiquote, a collection of quotations created a week after Wikimedia launched,[433] Wikibooks, a collection of collaboratively written free textbooks and annotated texts,[434] Wikimedia Commons, a site devoted to free-knowledge multimedia,[435] Wikinews, for collaborative journalism,[436] and Wikiversity, a project for the creation of free learning materials and the provision of online learning activities.[437] Another sister project of Wikipedia, Wikispecies, is a catalogue of all species, but is not open for public editing.[438] In 2012, Wikivoyage, an editable travel guide,[439] and Wikidata, an editable knowledge base, launched.[440]
Wikipedia's influence on the biography publishing business has been a concern for some. Book publishing data tracker Nielsen BookScan stated in 2013 that biography sales were dropping "far more sharply".[446] Kathryn Hughes, professor of life writing at the University of East Anglia and author of two biographies wrote, "The worry is that, if you can get all that information from Wikipedia, what's left for biography?"[446]
Wikipedia has been widely used as a corpus for linguistic research in computational linguistics, information retrieval and natural language processing.[447][448] In particular, it commonly serves as a target knowledge base for the entity linking problem, which is then called "wikification",[449] and to the related problem of word-sense disambiguation.[450] Methods similar to wikification can in turn be used to find "missing" links in Wikipedia.[451]
A 2017 MIT study suggests that words used on Wikipedia articles end up in scientific publications.[457][458]
Studies related to Wikipedia have been using machine learning and artificial intelligence to support various operations. One of the most important areas is the automatic detection of vandalism[459][460] and data quality assessment in Wikipedia.[461]
In February 2022, civil servants from the UK's Department for Levelling Up, Housing and Communities were found to have used Wikipedia for research in the drafting of the Levelling Up White Paper after journalists at The Independent noted that parts of the document had been lifted directly from Wikipedia articles on Constantinople and the list of largest cities throughout history.[462]
Several interactive multimedia encyclopedias incorporating entries written by the public existed long before Wikipedia was founded. The first of these was the 1986 BBC Domesday Project, which included text (entered on BBC Micro computers) and photographs from more than a million contributors in the UK, and covered the geography, art, and culture of the UK. This was the first interactive multimedia encyclopedia (and was also the first major multimedia document connected through internal links), with the majority of articles being accessible through an interactive map of the UK. The user interface and part of the content of the Domesday Project were emulated on a website until 2008.[463]
Several free-content, collaborative encyclopedias were created around the same period as Wikipedia (e.g. Everything2),[464] with many later being merged into the project (e.g. GNE).[465] One of the most successful early online encyclopedias incorporating entries by the public was h2g2, which was created by Douglas Adams in 1999. The h2g2 encyclopedia is relatively lighthearted, focusing on articles which are both witty and informative.[466]
Subsequent collaborative knowledge websites have drawn inspiration from Wikipedia. Others use more traditional peer review, such as Encyclopedia of Life and the online wiki encyclopedias Scholarpedia and Citizendium.[467][468] The latter was started by Sanger in an attempt to create a reliable alternative to Wikipedia.[469][470]
Free content, libre content, libre information, or free information, is any kind of functional work, work of art, or other creative content that meets the definition of a free cultural work.[1]
A free cultural work is, according to the definition of Free Cultural Works, one that has no significant legal restriction on people's freedom to:
Free content encompasses all works in the public domain and also those copyrighted works whose licenses honor and uphold the freedoms mentioned above. Because the Berne Convention in most countries by default grants copyright holders monopolistic control over their creations, copyright content must be explicitly declared free, usually by the referencing or inclusion of licensing statements from within the work.
Although there are a great many different definitions in regular everyday use, free content is legally very similar, if not like an identical twin, to open content. An analogy is a use of the rival terms free software and open-source, which describe ideological differences rather than legal ones.[3][4][5] For instance, the Open Knowledge Foundation's Open Definition describes "open" as synonymous to the definition of free in the "Definition of Free Cultural Works" (as also in the Open Source Definition and Free Software Definition).[6] For such free/open content both movements recommend the same three Creative Commons licenses, the CC BY, CC BY-SA, and CC0.[7][8][9][10]
Copyright is a legal concept, which gives the author or creator of a work legal control over the duplication and public performance of their work. In many jurisdictions, this is limited by a time period after which the works then enter the public domain. Copyright laws are a balance between the rights of creators of intellectual and artistic works and the rights of others to build upon those works. During the time period of copyright the author's work may only be copied, modified, or publicly performed with the consent of the author, unless the use is a fair use. Traditional copyright control limits the use of the work of the author to those who either pay royalties to the author for usage of the author's content or limit their use to fair use. Secondly, it limits the use of content whose author cannot be found.[11] Finally, it creates a perceived barrier between authors by limiting derivative works, such as mashups and collaborative content.[12]
The public domain is a range of creative works whose copyright has expired or was never established, as well as ideas and facts[note 1] which are ineligible for copyright. A public domain work is a work whose author has either relinquished to the public or no longer can claim control over, the distribution and usage of the work. As such, any person may manipulate, distribute, or otherwise use the work, without legal ramifications. A work in the public domain or released under a permissive license may be referred to as "copycenter".[13]
Copyleft is a play on the word copyright and describes the practice of using copyright law to remove restrictions on distributing copies and modified versions of a work.[14] The aim of copyleft is to use the legal framework of copyright to enable non-author parties to be able to reuse and, in many licensing schemes, modify content that is created by an author. Unlike works in the public domain, the author still maintains copyright over the material, however, the author has granted a non-exclusive license to any person to distribute, and often modify, the work. Copyleft licenses require that any derivative works be distributed under the same terms and that the original copyright notices be maintained. A symbol commonly associated with copyleft is a reversal of the copyright symbol, facing the other way; the opening of the C points left rather than right. Unlike the copyright symbol, the copyleft symbol does not have a codified meaning.[15]
Projects that provide free content exist in several areas of interest, such as software, academic literature, general literature, music, images, video, and engineering. Technology has reduced the cost of publication and reduced the entry barrier sufficiently to allow for the production of widely disseminated materials by individuals or small groups. Projects to provide free literature and multimedia content have become increasingly prominent owing to the ease of dissemination of materials that are associated with the development of computer technology. Such dissemination may have been too costly prior to these technological developments.
In media, which includes textual, audio, and visual content, free licensing schemes such as some of the licenses made by Creative Commons have allowed for the dissemination of works under a clear set of legal permissions. Not all Creative Commons licenses are entirely free; their permissions may range from very liberal general redistribution and modification of the work to a more restrictive redistribution-only licensing. Since February 2008, Creative Commons licenses which are entirely free carry a badge indicating that they are "approved for free cultural works".[16] Repositories exist which exclusively feature free material and provide content such as photographs, clip art, music,[17] and literature.[18] While extensive reuse of free content from one website in another website is legal, it is usually not sensible because of the duplicate content problem. Wikipedia is amongst the most well-known databases of user-uploaded free content on the web. While the vast majority of content on Wikipedia is free content, some copyrighted material is hosted under fair-use criteria.
Free and open-source software, which is also often referred to as open source software and free software, is a maturing technology with major companies using free software to provide both services and technology to both end-users and technical consumers. The ease of dissemination has allowed for increased modularity, which allows for smaller groups to contribute to projects as well as simplifying collaboration. Open source development models have been classified as having a similar peer-recognition and collaborative benefit incentives that are typified by more classical fields such as scientific research, with the social structures that result from this incentive model decreasing production cost.[19] Given sufficient interest in a software component, by using peer-to-peer distribution methods, distribution costs of software may be reduced, removing the burden of infrastructure maintenance from developers. As distribution resources are simultaneously provided by consumers, these software distribution models are scalable, that is the method is feasible regardless of the number of consumers. In some cases, free software vendors may use peer-to-peer technology as a method of dissemination.[20] In general, project hosting and code distribution is not a problem for the most of free projects as a number of providers offer them these services free.
Free content principles have been translated into fields such as engineering, where designs and engineering knowledge can be readily shared and duplicated, in order to reduce overheads associated with project development. Open design principles can be applied in engineering and technological applications, with projects in mobile telephony, small-scale manufacture,[21] the automotive industry,[22][23] and even agricultural areas. Technologies such as distributed manufacturing can allow computer-aided manufacturing and computer-aided design techniques to be able to develop small-scale production of components for the development of new, or repair of existing, devices. Rapid fabrication technologies underpin these developments, which allow end-users of technology to be able to construct devices from pre-existing blueprints, using software and manufacturing hardware to convert information into physical objects.
In academic work, the majority of works are not free, although the percentage of works that are open access is growing rapidly. Open access refers to online research outputs that are free of all restrictions on access (e.g. access tolls) and free of many restrictions on use (e.g. certain copyright and license restrictions).[24] Authors may see open access publishing as a method of expanding the audience that is able to access their work to allow for greater impact of the publication, or may support it for ideological reasons.[25][26][27] Open access publishers such as PLOS and BioMed Central provide capacity for review and publishing of free works; though such publications are currently more common in science than humanities. Various funding institutions and governing research bodies have mandated that academics must produce their works to be open-access, in order to qualify for funding, such as the US National Institutes of Health, Research Councils UK (effective 2016) and the European Union (effective 2020).[28][29][30][31] At an institutional level some universities, such as the Massachusetts Institute of Technology, have adopted open access publishing by default by introducing their own mandates.[32] Some mandates may permit delayed publication and may charge researchers for open access publishing.[33][34]
Open content publication has been seen as a method of reducing costs associated with information retrieval in research, as universities typically pay to subscribe for access to content that is published through traditional means[10][35][36] whilst improving journal quality by discouraging the submission of research articles of reduced quality.[10] Subscriptions for non-free content journals may be expensive for universities to purchase, though the article are written and peer-reviewed by academics themselves at no cost to the publisher. This has led to disputes between publishers and some universities over subscription costs, such as the one which occurred between the University of California and the Nature Publishing Group.[37][38] For teaching purposes, some universities, including MIT, provide freely available course content, such as lecture notes, video resources and tutorials. This content is distributed via Internet resources to the general public. Publication of such resources may be either by a formal institution-wide program,[39] or alternately via informal content provided by individual academics or departments.
Open content describes any work that others can copy or modify freely by attributing to the original creator, but without needing to ask for permission. This has been applied to a range of formats, including textbooks, academic journals, films and music. The term was an expansion of the related concept of open-source software.[40] Such content is said to be under an open license.
The concept of applying free software licenses to content was introduced by Michael Stutz, who in 1997 wrote the paper "Applying Copyleft to Non-Software Information" for the GNU Project. The term "open content" was coined by David A. Wiley in 1998 and evangelized via the Open Content Project, describing works licensed under the Open Content License (a non-free share-alike license, see 'Free content' below) and other works licensed under similar terms.[40]
It has since come to describe a broader class of content without conventional copyright restrictions. The openness of content can be assessed under the '5Rs Framework' based on the extent to which it can be reused, revised, remixed and redistributed by members of the public without violating copyright law.[41] Unlike free content and content under open-source licenses, there is no clear threshold that a work must reach to qualify as 'open content'.
Although open content has been described as a counterbalance to copyright,[42] open content licenses rely on a copyright holder's power to license their work, as copyleft which also utilizes copyright for such a purpose.
In 2003 Wiley announced that the Open Content Project has been succeeded by Creative Commons and their licenses, where he joined as "Director of Educational Licenses".[43][44]
In 2005, the Open Icecat project was launched, in which product information for e-commerce applications was created and published under the Open Content License. It was embraced by the tech sector, which was already quite open source minded.
Another successor project is the Open Knowledge Foundation,[49] founded by Rufus Pollock in Cambridge, in 2004[50] as a global non-profit network to promote and share open content and data.[51] In 2007 the OKF gave an Open Knowledge Definition for "content such as music, films, books; data be it scientific, historical, geographic or otherwise; government and other administrative information".[52] In October 2014 with version 2.0 Open Works and Open Licenses were defined and "open" is described as synonymous to the definitions of open/free in the Open Source Definition, the Free Software Definition and the Definition of Free Cultural Works.[53] A distinct difference is the focus given to the public domain and that it focuses also on the accessibility (open access) and the readability (open formats). Among several conformant licenses, six are recommended, three own (Open Data Commons Public Domain Dedication and Licence, Open Data Commons Attribution License, Open Data Commons Open Database License) and the CC BY, CC BY-SA, and CC0 Creative Commons licenses.[54][55][56]
The website of the Open Content Project once defined open content as 'freely available for modification, use and redistribution under a license similar to those used by the open-source / free software community'.[40] However, such a definition would exclude the Open Content License because that license forbids charging for content; a right required by free and open-source software licenses.[citation needed]
The term since shifted in meaning. Open content is "licensed in a manner that provides users with free and perpetual permission to engage in the 5R activities."[41]
The 5Rs are put forward on the Open Content Project website as a framework for assessing the extent to which content is open:
This broader definition distinguishes open content from open-source software, since the latter must be available for commercial use by the public. However, it is similar to several definitions for open educational resources, which include resources under noncommercial and verbatim licenses.[57][58]
The later Open Definition by the Open Knowledge Foundation define open knowledge with open content and open data as sub-elements and draws heavily on the Open Source Definition; it preserves the limited sense of open content as free content,[59] unifying both.
"Open access" refers to toll-free or gratis access to content, mainly published originally peer-reviewed scholarly journals. Some open access works are also licensed for reuse and redistribution (libre open access), which would qualify them as open content.
Over the past decade, open content has been used to develop alternative routes towards higher education. Traditional universities are expensive, and their tuition rates are increasing.[60] Open content allows a free way of obtaining higher education that is "focused on collective knowledge and the sharing and reuse of learning and scholarly content."[61] There are multiple projects and organizations that promote learning through open content, including OpenCourseWare, Khan Academy and the Saylor Academy. Some universities, like MIT, Yale, and Tufts are making their courses freely available on the internet.[62]
The textbook industry is one of the educational industries in which open content can make the biggest impact.[63] Traditional textbooks, aside from being expensive, can also be inconvenient and out of date, because of publishers' tendency to constantly print new editions.[64] Open textbooks help to eliminate this problem, because they are online and thus easily updatable. Being openly licensed and online can be helpful to teachers, because it allows the textbook to be modified according to the teacher's unique curriculum.[63] There are multiple organizations promoting the creation of openly licensed textbooks. Some of these organizations and projects include the University of Minnesota's Open Textbook Library, Connexions, OpenStax College, the Saylor Academy, Open Textbook Challenge and Wikibooks.
According to the current definition of open content on the OpenContent website, any general, royalty-free copyright license would qualify as an open license because it 'provides users with the right to make more kinds of uses than those normally permitted under the law. These permissions are granted to users free of charge.'[41]
However, the narrower definition used in the Open Definition effectively limits open content to libre content, any free content license, defined by the Definition of Free Cultural Works, would qualify as an open content license. According to this narrower criteria, the following still-maintained licenses qualify:
Encyclopedias have existed for around 2,000 years and have evolved considerably during that time as regards language (written in a major international or a vernacular language), size (few or many volumes), intent (presentation of a global or a limited range of knowledge), cultural perspective (authoritative, ideological, didactic, utilitarian), authorship (qualifications, style), readership (education level, background, interests, capabilities), and the technologies available for their production and distribution (hand-written manuscripts, small or large print runs, Internet). As a valued source of reliable information compiled by experts, printed versions found a prominent place in libraries, schools and other educational institutions.
The appearance of digital and open-source versions in the 21st century, such as Wikipedia, has vastly expanded the accessibility, authorship, readership, and variety of encyclopedia entries.[10]
Indeed, the purpose of an encyclopedia is to collect knowledge disseminated around the globe; to set forth its general system to the men with whom we live, and transmit it to those who will come after us, so that the work of preceding centuries will not become useless to the centuries to come; and so that our offspring, becoming better instructed, will at the same time become more virtuous and happy, and that we should not die without having rendered a service to the human race in the future years to come.
The modern encyclopedia was developed from the dictionary in the 18th century. Historically, both encyclopedias and dictionaries have been researched and written by well-educated, well-informed content experts, but they are significantly different in structure. A dictionary is a linguistic work which primarily focuses on alphabetical listing of words and their definitions. Synonymous words and those related by the subject matter are to be found scattered around the dictionary, giving no obvious place for in-depth treatment. Thus, a dictionary typically provides limited information, analysis or background for the word defined. While it may offer a definition, it may leave the reader lacking in understanding the meaning, significance or limitations of a term, and how the term relates to a broader field of knowledge.
To address those needs, an encyclopedia article is typically not limited to simple definitions, and is not limited to defining an individual word, but provides a more extensive meaning for a subject or discipline. In addition to defining and listing synonymous terms for the topic, the article is able to treat the topic's more extensive meaning in more depth and convey the most relevant accumulated knowledge on that subject. An encyclopedia article also often includes many maps and illustrations, as well as bibliography and statistics.[5] An encyclopedia is, theoretically, not written in order to convince, although one of its goals is indeed to convince its reader of its own veracity.
There are four major elements that define an encyclopedia: its subject matter, its scope, its method of organization, and its method of production:
Some works entitled "dictionaries" are actually similar to encyclopedias, especially those concerned with a particular field (such as the Dictionary of the Middle Ages, the Dictionary of American Naval Fighting Ships, and Black's Law Dictionary). The Macquarie Dictionary, Australia's national dictionary, became an encyclopedic dictionary after its first edition in recognition of the use of proper nouns in common communication, and the words derived from such proper nouns.
There are some broad differences between encyclopedias and dictionaries. Most noticeably, encyclopedia articles are longer, fuller and more thorough than entries in most general-purpose dictionaries.[3][18] There are differences in content as well. Generally speaking, dictionaries provide linguistic information about words themselves, while encyclopedias focus more on the things for which those words stand.[6][7][8][9] Thus, while dictionary entries are inextricably fixed to the word described, encyclopedia articles can be given a different entry name. As such, dictionary entries are not fully translatable into other languages, but encyclopedia articles can be.[6]
In practice, however, the distinction is not concrete, as there is no clear-cut difference between factual, "encyclopedic" information and linguistic information such as appears in dictionaries.[8][18][19] Thus encyclopedias may contain material that is also found in dictionaries, and vice versa.[19] In particular, dictionary entries often contain factual information about the thing named by the word.[18][19]
The earliest encyclopedic work to have survived to modern times is the Naturalis Historia of Pliny the Elder, a Roman statesman living in the 1st century AD.[5][20][21][22] He compiled a work of 37 chapters covering natural history, architecture, medicine, geography, geology, and all aspects of the world around him.[22] This work became very popular in Antiquity, was one of the first classical manuscripts to be printed in 1470, and has remained popular ever since as a source of information on the Roman world, and especially Roman art, Roman technology and Roman engineering.
Another Christian encyclopedia was the Institutiones divinarum et saecularium litterarum of Cassiodorus (543-560) dedicated to the Christian divinity and to the seven liberal arts.[21][5] The encyclopedia of Suda, a massive 10th-century Byzantine encyclopedia, had 30,000 entries, many drawing from ancient sources that have since been lost, and often derived from medieval Christian compilers. The text was arranged alphabetically with some slight deviations from common vowel order and place in the Greek alphabet.[21]
Before the advent of the printing press, encyclopedic works were all hand copied and thus rarely available, beyond wealthy patrons or monastic men of learning: they were expensive, and usually written for those extending knowledge rather than those using it.
During the Renaissance, the creation of printing allowed a wider diffusion of encyclopedias and every scholar could have his or her own copy. The De expetendis et fugiendis rebus by Giorgio Valla was posthumously printed in 1501 by Aldo Manuzio in Venice. This work followed the traditional scheme of liberal arts. However, Valla added the translation of ancient Greek works on mathematics (firstly by Archimedes), newly discovered and translated. The Margarita Philosophica by Gregor Reisch, printed in 1503, was a complete encyclopedia explaining the seven liberal arts.
In the United States, the 1950s and 1960s saw the introduction of several large popular encyclopedias, often sold on installment plans. The best known of these were World Book and Funk and Wagnalls. As many as 90% were sold door to door.[20] Jack Lynch says in his book You Could Look It Up that encyclopedia salespeople were so common that they became the butt of jokes. He describes their sales pitch saying, "They were selling not books but a lifestyle, a future, a promise of social mobility." A 1961 World Book ad said, "You are holding your family's future in your hands right now," while showing a feminine hand holding an order form.[36]
By the late 20th century, encyclopedias were being published on CD-ROMs for use with personal computers. This was the usual way computer users accessed encyclopedic knowledge from the 1980s and 1990s. Later DVD discs replaced CD-ROMs and from mid-2000s internet encyclopedias became dominant and replaced disc-based software encyclopedias.[5]
CD-ROM encyclopedias were usually a macOS or Microsoft Windows (3.0, 3.1 or 95/98) application on a CD-ROM disc. The user would execute the encyclopedia's software program to see a menu that allowed them to start browsing the encyclopedia's articles, and most encyclopedias also supported a way to search the contents of the encyclopedia. The article text was usually hyperlinked and also included photographs, audio clips (for example in articles about historical speeches or musical instruments), and video clips. In the CD-ROM age the video clips had usually a low resolution, often 160x120 or 320x240 pixels. Such encyclopedias which made use of photos, audio and video were also called multimedia encyclopedias. However, because of the online encyclopedia, CD-ROM encyclopedias have been declared obsolete.[by whom?]
Microsoft's Encarta, launched in 1993, was a landmark example as it had no printed equivalent. Articles were supplemented with video and audio files as well as numerous high-quality images. After sixteen years, Microsoft discontinued the Encarta line of products in 2009.[37] Other examples of CD-ROM encyclopedia are Grolier Multimedia Encyclopedia and Britannica.
Digital encyclopedias enable "Encyclopedia Services" (such as Wikimedia Enterprise) to facilitate programatic access to the content.[38]
The concept of a free encyclopedia began with the Interpedia proposal on Usenet in 1993, which outlined an Internet-based online encyclopedia to which anyone could submit content and that would be freely accessible. Early projects in this vein included Everything2 and Open Site. In 1999, Richard Stallman proposed the GNUPedia, an online encyclopedia which, similar to the GNU operating system, would be a "generic" resource. The concept was very similar to Interpedia, but more in line with Stallman's GNU philosophy.
It was not until Nupedia and later Wikipedia that a stable free encyclopedia project was able to be established on the Internet.
The English Wikipedia, which was started in 2001, became the world's largest encyclopedia in 2004 at the 300,000 article stage.[39] By late 2005, Wikipedia had produced over two million articles in more than 80 languages with content licensed under the copyleft GNU Free Documentation License. As of August 2009, Wikipedia had over 3 million articles in English and well over 10 million combined in over 250 languages. Wikipedia currently has 6,623,776 articles in English.
Since 2003, other free encyclopedias like the Chinese-language Baidu Baike and Hudong, as well as English language encyclopedias such as Citizendium and Knol have appeared, the latter of which has been discontinued.
You're welcome to edit anonymously, but there are many benefits of registering an account. It's quick and free.
You can test out how editing feels by editing one of the "sandbox" test pages below:
Edit page visually or Edit using wiki markup
Edit page visually or Edit using wiki markup
Just type some text and click Publish page when you're happy with the way it looks. Don't worry about breaking anything; these pages are open areas for experimentation.
Wikipedia has many community pages in addition to its articles.
See also Wikipedia:Statistics and Category:Wikipedia statistics for a fuller list of pages that provide, analyze or discuss Wikipedia statistics.
This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.
English is a West Germanic language in the Indo-European language family, with its earliest forms spoken by the inhabitants of early medieval England.[3][4][5] It is named after the Angles, one of the ancient Germanic peoples that migrated to the island of Great Britain. Existing on a dialect continuum with Scots and then most closely related to the Low German and Frisian languages, English is genealogically Germanic. However, its vocabulary also shows major influences from French (about 28% of Modern English words) and Latin (also about 28%),[6] plus some grammar and a small amount of core vocabulary influenced by Old Norse (a North Germanic language).[7][8][9] Speakers of English are called Anglophones.
The earliest forms of English, collectively known as "Old English", evolved from a group of North Sea Germanic dialects brought to Great Britain by Anglo-Saxon settlers in the 5th century and further mutated by Norse-speaking Viking settlers starting in the 8th and 9th centuries. Middle English began in the late 11th century after the Norman Conquest of England, when considerable Old French (especially Old Norman French) and Latin-derived vocabulary was incorporated into English over some three hundred years.[10][11] Early Modern English began in the late 15th century with the start of the Great Vowel Shift and the Renaissance trend of borrowing further Latin and Greek words and roots into English, concurrent with the introduction of the printing press to London. This era notably culminated in the King James Bible and the works of William Shakespeare.[12][13]
English is an Indo-European language and belongs to the West Germanic group of the Germanic languages.[19] Old English originated from a Germanic tribal and linguistic continuum along the Frisian North Sea coast, whose languages gradually evolved into the Anglic languages in the British Isles, and into the Frisian languages and Low German/Low Saxon on the continent. The Frisian languages, which together with the Anglic languages form the Anglo-Frisian languages, are the closest living relatives of English. Low German/Low Saxon is also closely related, and sometimes English, the Frisian languages, and Low German are grouped together as the Ingvaeonic (North Sea Germanic) languages, though this grouping remains debated.[8] Old English evolved into Middle English, which in turn evolved into Modern English.[20] Particular dialects of Old and Middle English also developed into a number of other Anglic languages, including Scots[21] and the extinct Fingallian and Forth and Bargy (Yola) dialects of Ireland.[22]
Like Icelandic and Faroese, the development of English in the British Isles isolated it from the continental Germanic languages and influences, and it has since diverged considerably. English is not mutually intelligible with any continental Germanic language, differing in vocabulary, syntax, and phonology, although some of these, such as Dutch or Frisian, do show strong affinities with English, especially with its earlier stages.[23]
Old English is essentially a distinct language from Modern English and is virtually impossible for 21st-century unstudied English-speakers to understand. Its grammar was similar to that of modern German: nouns, adjectives, pronouns, and verbs had many more inflectional endings and forms, and word order was much freer than in Modern English. Modern English has case forms in pronouns (he, him, his) and has a few verb inflections (speak, speaks, speaking, spoke, spoken), but Old English had case endings in nouns as well, and verbs had more person and number endings.[39][40][41] Its closest relative is Old Frisian, but even some centuries after the Anglo-Saxon migration, Old English retained considerable mutual intelligibility with other Germanic varieties. Even in the 9th and 10th centuries, amidst the Danelaw and other Viking invasions, there is historical evidence that Old Norse and Old English retained considerable mutual intelligibility.[42] Theoretically, as late as the 900s AD, a commoner from England could hold a conversation with a commoner from Scandinavia. Research continues into the details of the myriad tribes in peoples in England and Scandinavia and the mutual contacts between them.[42]
The translation of Matthew 8:20 from 1000 shows examples of case endings (nominative plural, accusative plural, genitive singular) and a verb ending (present plural):
From the 8th to the 12th century, Old English gradually transformed through language contact into Middle English. Middle English is often arbitrarily defined as beginning with the conquest of England by William the Conqueror in 1066, but it developed further in the period from 1200 to 1450.
First, the waves of Norse colonisation of northern parts of the British Isles in the 8th and 9th centuries put Old English into intense contact with Old Norse, a North Germanic language. Norse influence was strongest in the north-eastern varieties of Old English spoken in the Danelaw area around York, which was the centre of Norse colonisation; today these features are still particularly present in Scots and Northern English. However, the centre of norsified English seems to have been in the Midlands around Lindsey, and after 920 CE when Lindsey was reincorporated into the Anglo-Saxon polity, Norse features spread from there into English varieties that had not been in direct contact with Norse speakers. An element of Norse influence that persists in all English varieties today is the group of pronouns beginning with th- (they, them, their) which replaced the Anglo-Saxon pronouns with h- (hie, him, hera).[45]
With the Norman Conquest of England in 1066, the now norsified Old English language was subject to contact with Old French, in particular with the Old Norman dialect. The Norman language in England eventually developed into Anglo-Norman.[10] Because Norman was spoken primarily by the elites and nobles, while the lower classes continued speaking Anglo-Saxon (English), the main influence of Norman was the introduction of a wide range of loanwords related to politics, legislation and prestigious social domains.[9] Middle English also greatly simplified the inflectional system, probably in order to reconcile Old Norse and Old English, which were inflectionally different but morphologically similar. The distinction between nominative and accusative cases was lost except in personal pronouns, the instrumental case was dropped, and the use of the genitive case was limited to indicating possession. The inflectional system regularised many irregular inflectional forms,[46] and gradually simplified the system of agreement, making word order less flexible.[47] In Wycliff'e Bible of the 1380s, the verse Matthew 8:20 was written: Foxis han dennes, and briddis of heuene han nestis.[48] Here the plural suffix -n on the verb have is still retained, but none of the case endings on the nouns are present. By the 12th century Middle English was fully developed, integrating both Norse and French features; it continued to be spoken until the transition to early Modern English around 1500. Middle English literature includes Geoffrey Chaucer's The Canterbury Tales, and Thomas Malory's Le Morte d'Arthur. In the Middle English period, the use of regional dialects in writing proliferated, and dialect traits were even used for effect by authors such as Chaucer.[49]
The Great Vowel Shift affected the stressed long vowels of Middle English. It was a chain shift, meaning that each shift triggered a subsequent shift in the vowel system. Mid and open vowels were raised, and close vowels were broken into diphthongs. For example, the word bite was originally pronounced as the word beet is today, and the second vowel in the word about was pronounced as the word boot is today. The Great Vowel Shift explains many irregularities in spelling since English retains many spellings from Middle English, and it also explains why English vowel letters have very different pronunciations from the same letters in other languages.[50][51]
By the late 18th century, the British Empire had spread English through its colonies and geopolitical dominance. Commerce, science and technology, diplomacy, art, and formal education all contributed to English becoming the first truly global language. English also facilitated worldwide international communication.[54][3] England continued to form new colonies, and these later developed their own norms for speech and writing. English was adopted in parts of North America, parts of Africa, Australasia, and many other regions. When they obtained political independence, some of the newly independent nations that had multiple indigenous languages opted to continue using English as the official language to avoid the political and other difficulties inherent in promoting any one indigenous language above the others.[55][56][57] In the 20th century the growing economic and cultural influence of the United States and its status as a superpower following the Second World War has, along with worldwide broadcasting in English by the BBC[58] and other broadcasters, caused the language to spread across the planet much faster.[59][60] In the 21st century, English is more widely spoken and written than any language has ever been.[61]
As Modern English developed, explicit norms for standard usage were published, and spread through official media such as public education and state-sponsored publications. In 1755 Samuel Johnson published his A Dictionary of the English Language, which introduced standard spellings of words and usage norms. In 1828, Noah Webster published the American Dictionary of the English language to try to establish a norm for speaking and writing American English that was independent of the British standard. Within Britain, non-standard or lower class dialect features were increasingly stigmatised, leading to the quick spread of the prestige varieties among the middle classes.[62]
In modern English, the loss of grammatical case is almost complete (it is now only found in pronouns, such as he and him, she and her, who and whom), and SVO word order is mostly fixed.[62] Some changes, such as the use of do-support, have become universalised. (Earlier English did not use the word "do" as a general auxiliary as Modern English does; at first it was only used in question constructions, and even then was not obligatory.[63] Now, do-support with the verb have is becoming increasingly standardised.) The use of progressive forms in -ing, appears to be spreading to new constructions, and forms such as had been being built are becoming more common. Regularisation of irregular forms also slowly continues (e.g. dreamed instead of dreamt), and analytical alternatives to inflectional forms are becoming more common (e.g. more polite instead of politer). British English is also undergoing change under the influence of American English, fuelled by the strong presence of American English in the media and the prestige associated with the US as a world power.[64][65][66]
The countries where English is spoken can be grouped into different categories according to how English is used in each country. The "inner circle"[69] countries with many native speakers of English share an international standard of written English and jointly influence speech norms for English around the world. English does not belong to just one country, and it does not belong solely to descendants of English settlers. English is an official language of countries populated by few descendants of native speakers of English. It has also become by far the most important language of international communication when people who share no native language meet anywhere in the world.
The Indian linguist Braj Kachru distinguished countries where English is spoken with a three circles model.[69] In his model,
Kachru based his model on the history of how English spread in different countries, how users acquire English, and the range of uses English has in each country. The three circles change membership over time.[70]
Those countries have millions of native speakers of dialect continua ranging from an English-based creole to a more standard version of English. They have many more speakers of English who acquire English as they grow up through day-to-day use and listening to broadcasting, especially if they attend schools where English is the medium of instruction. Varieties of English learned by non-native speakers born to English-speaking parents may be influenced, especially in their grammar, by the other languages spoken by those learners.[79] Most of those varieties of English include words little used by native speakers of English in the inner-circle countries,[79] and they may show grammatical and phonological differences from inner-circle varieties as well. The standard English of the inner-circle countries is often taken as a norm for use of English in the outer-circle countries.[79]
In the three-circles model, countries such as Poland, China, Brazil, Germany, Japan, Indonesia, Egypt, and other countries where English is taught as a foreign language, make up the "expanding circle".[87] The distinctions between English as a first language, as a second language, and as a foreign language are often debatable and may change in particular countries over time.[86] For example, in the Netherlands and some other countries of Europe, knowledge of English as a second language is nearly universal, with over 80 percent of the population able to use it,[88] and thus English is routinely used to communicate with foreigners and often in higher education. In these countries, although English is not used for government business, its widespread use puts them at the boundary between the "outer circle" and "expanding circle". English is unusual among world languages in how many of its users are not native speakers but speakers of English as a second or foreign language.[89]
Many users of English in the expanding circle use it to communicate with other people from the expanding circle, so that interaction with native speakers of English plays no part in their decision to use the language.[90] Non-native varieties of English are widely used for international communication, and speakers of one such variety often encounter features of other varieties.[91] Very often today a conversation in English anywhere in the world may include no native speakers of English at all, even while including speakers from several different countries. This is particularly true of the shared vocabulary of mathematics and the sciences.[92]
Pie chart showing the percentage of native English speakers living in "inner circle" English-speaking countries. Native speakers are now substantially outnumbered worldwide by second-language speakers of English (not counted in this chart).
English is a pluricentric language, which means that no one national authority sets the standard for use of the language.[93][94][95][96] Spoken English, for example English used in broadcasting, generally follows national pronunciation standards that are established by custom rather than by regulation. International broadcasters are usually identifiable as coming from one country rather than another through their accents,[97] but newsreader scripts are also composed largely in international standard written English. The norms of standard written English are maintained purely by the consensus of educated English-speakers around the world, without any oversight by any government or international organisation.[98]
American listeners generally readily understand most British broadcasting, and British listeners readily understand most American broadcasting. Most English speakers around the world can understand radio programmes, television programmes, and films from many parts of the English-speaking world.[99] Both standard and non-standard varieties of English can include both formal or informal styles, distinguished by word choice and syntax and use both technical and non-technical registers.[100]
The settlement history of the English-speaking inner circle countries outside Britain helped level dialect distinctions and produce koineised forms of English in South Africa, Australia, and New Zealand.[101] The majority of immigrants to the United States without British ancestry rapidly adopted English after arrival. Now the majority of the United States population are monolingual English speakers,[71][102] and English has been given official or co-official status by 30 of the 50 state governments, as well as all five territorial governments of the US, though there has never been an official language at the federal level.[103][104]
English has ceased to be an "English language" in the sense of belonging only to people who are ethnically English.[105][106] Use of English is growing country-by-country internally and for international communication. Most people learn English for practical rather than ideological reasons.[107] Many speakers of English in Africa have become part of an "Afro-Saxon" language community that unites Africans from different countries.[108]
As decolonisation proceeded throughout the British Empire in the 1950s and 1960s, former colonies often did not reject English but rather continued to use it as independent countries setting their own language policies.[56][57][109] For example, the view of the English language among many Indians has gone from associating it with colonialism to associating it with economic progress, and English continues to be an official language of India.[110] English is also widely used in media and literature, and the number of English language books published annually in India is the third largest in the world after the US and UK.[111] However English is rarely spoken as a first language, numbering only around a couple hundred-thousand people, and less than 5% of the population speak fluent English in India.[112][113] David Crystal claimed in 2004 that, combining native and non-native speakers, India now has more people who speak or understand English than any other country in the world,[114] but the number of English speakers in India is uncertain, with most scholars concluding that the United States still has more speakers of English than India.[115]
Modern English, sometimes described as the first global lingua franca,[59][116] is also regarded as the first world language.[117][118] English is the world's most widely used language in newspaper publishing, book publishing, international telecommunications, scientific publishing, international trade, mass entertainment, and diplomacy.[118] English is, by international treaty, the basis for the required controlled natural languages[119] Seaspeak and Airspeak, used as international languages of seafaring[120] and aviation.[121] English used to have parity with French and German in scientific research, but now it dominates that field.[122] It achieved parity with French as a language of diplomacy at the Treaty of Versailles negotiations in 1919.[123] By the time of the foundation of the United Nations at the end of World War II, English had become pre-eminent[124] and is now the main worldwide language of diplomacy and international relations.[125] It is one of six official languages of the United Nations.[126] Many other worldwide international organisations, including the International Olympic Committee, specify English as a working language or official language of the organisation.
Many regional international organisations such as the European Free Trade Association, Association of Southeast Asian Nations (ASEAN),[60] and Asia-Pacific Economic Cooperation (APEC) set English as their organisation's sole working language even though most members are not countries with a majority of native English speakers. While the European Union (EU) allows member states to designate any of the national languages as an official language of the Union, in practice English is the main working language of EU organisations.[127]
Although in most countries English is not an official language, it is currently the language most often taught as a foreign language.[59][60] In the countries of the EU, English is the most widely spoken foreign language in nineteen of the twenty-five member states where it is not an official language (that is, the countries other than Ireland and Malta). In a 2012 official Eurobarometer poll (conducted when the UK was still a member of the EU), 38 percent of the EU respondents outside the countries where English is an official language said they could speak English well enough to have a conversation in that language. The next most commonly mentioned foreign language, French (which is the most widely known foreign language in the UK and Ireland), could be used in conversation by 12 percent of respondents.[128]
A working knowledge of English has become a requirement in a number of occupations and professions such as medicine[130] and computing. English has become so important in scientific publishing that more than 80 percent of all scientific journal articles indexed by Chemical Abstracts in 1998 were written in English, as were 90 percent of all articles in natural science publications by 1996 and 82 percent of articles in humanities publications by 1995.[131]
International communities such as international business people may use English as an auxiliary language, with an emphasis on vocabulary suitable for their domain of interest. This has led some scholars to develop the study of English as an auxiliary language. The trademarked Globish uses a relatively small subset of English vocabulary (about 1500 words, designed to represent the highest use in international business English) in combination with the standard English grammar.[132] Other examples include Simple English.
The increased use of the English language globally has had an effect on other languages, leading to some English words being assimilated into the vocabularies of other languages. This influence of English has led to concerns about language death,[133] and to claims of linguistic imperialism,[134] and has provoked resistance to the spread of English; however the number of speakers continues to increase because many people around the world think that English provides them with opportunities for better employment and improved lives.[135]
Although some scholars[who?] mention a possibility of future divergence of English dialects into mutually unintelligible languages, most think a more likely outcome is that English will continue to function as a koineised language in which the standard form unifies speakers from around the world.[136] English is used as the language for wider communication in countries around the world.[137] Thus English has grown in worldwide use much more than any constructed language proposed as an international auxiliary language, including Esperanto.[138][139]
The phonetic symbols used below are from the International Phonetic Alphabet (IPA).[141][142][143]
The pronunciation of vowels varies a great deal between dialects and is one of the most detectable aspects of a speaker's accent. The table below lists the vowel phonemes in Received Pronunciation (RP) and General American (GA), with examples of words in which they occur from lexical sets compiled by linguists. The vowels are represented with symbols from the International Phonetic Alphabet; those given for RP are standard in British dictionaries and other publications.[150]
Stress plays an important role in English. Certain syllables are stressed, while others are unstressed. Stress is a combination of duration, intensity, vowel quality, and sometimes changes in pitch. Stressed syllables are pronounced longer and louder than unstressed syllables, and vowels in unstressed syllables are frequently reduced while vowels in stressed syllables are not.[158] Some words, primarily short function words but also some modal verbs such as can, have weak and strong forms depending on whether they occur in stressed or non-stressed position within a sentence.
In terms of rhythm, English is generally described as a stress-timed language, meaning that the amount of time between stressed syllables tends to be equal.[163] Stressed syllables are pronounced longer, but unstressed syllables (syllables between stresses) are shortened. Vowels in unstressed syllables are shortened as well, and vowel shortening causes changes in vowel quality: vowel reduction.[164]
Varieties of English vary the most in pronunciation of vowels. The best known national varieties used as standards for education in non-English-speaking countries are British (BrE) and American (AmE). Countries such as Canada, Australia, Ireland, New Zealand and South Africa have their own standard varieties which are less often used as standards for education internationally. Some differences between the various dialects are shown in the table "Varieties of Standard English and their features".[165]
English has undergone many historical sound changes, some of them affecting all varieties, and others affecting only a few. Most standard varieties are affected by the Great Vowel Shift, which changed the pronunciation of long vowels, but a few dialects have slightly different results. In North America, a number of chain shifts such as the Northern Cities Vowel Shift and Canadian Shift have produced very different vowel landscapes in some regional accents.[166]
General American and Received Pronunciation vary in their pronunciation of historical /r/ after a vowel at the end of a syllable (in the syllable coda). GA is a rhotic dialect, meaning that it pronounces /r/ at the end of a syllable, but RP is non-rhotic, meaning that it loses /r/ in that position. English dialects are classified as rhotic or non-rhotic depending on whether they elide /r/ like RP or keep it like GA.[170]
As is typical of an Indo-European language, English follows accusative morphosyntactic alignment. Unlike other Indo-European languages though, English has largely abandoned the inflectional case system in favour of analytic constructions. Only the personal pronouns retain morphological case more strongly than any other word class. English distinguishes at least seven major word classes: verbs, nouns, adjectives, adverbs, determiners (including articles), prepositions, and conjunctions. Some analyses add pronouns as a class separate from nouns, and subdivide conjunctions into subordinators and coordinators, and add the class of interjections.[173] English also has a rich set of auxiliary verbs, such as have and do, expressing the categories of mood and aspect. Questions are marked by do-support, wh-movement (fronting of question words beginning with wh-) and word order inversion with some verbs.[174]
Some traits typical of Germanic languages persist in English, such as the distinction between irregularly inflected strong stems inflected through ablaut (i.e. changing the vowel of the stem, as in the pairs speak/spoke and foot/feet) and weak stems inflected through affixation (such as love/loved, hand/hands).[175] Vestiges of the case and gender system are found in the pronoun system (he/him, who/whom) and in the inflection of the copula verb to be.[175]
English nouns are only inflected for number and possession. New nouns can be formed through derivation or compounding. They are semantically divided into proper nouns (names) and common nouns. Common nouns are in turn divided into concrete and abstract nouns, and grammatically into count nouns and mass nouns.[177]
Most count nouns are inflected for plural number through the use of the plural suffix -s, but a few nouns have irregular plural forms. Mass nouns can only be pluralised through the use of a count noun classifier, e.g. one loaf of bread, two loaves of bread.[178]
Possession can be expressed either by the possessive enclitic -s (also traditionally called a genitive suffix), or by the preposition of. Historically the -s possessive has been used for animate nouns, whereas the of possessive has been reserved for inanimate nouns. Today this distinction is less clear, and many speakers use -s also with inanimates. Orthographically the possessive -s is separated from a singular noun with an apostrophe. If the noun is plural formed with -s the apostrophe follows the -s.[174]
Nouns can form noun phrases (NPs) where they are the syntactic head of the words that depend on them such as determiners, quantifiers, conjunctions or adjectives.[179] Noun phrases can be short, such as the man, composed only of a determiner and a noun. They can also include modifiers such as adjectives (e.g. red, tall, all) and specifiers such as determiners (e.g. the, that). But they can also tie together several nouns into a single long NP, using conjunctions such as and, or prepositions such as with, e.g. the tall man with the long red trousers and his skinny wife with the spectacles (this NP uses conjunctions, prepositions, specifiers, and modifiers). Regardless of length, an NP functions as a syntactic unit.[174] For example, the possessive enclitic can, in cases which do not lead to ambiguity, follow the entire noun phrase, as in The President of India's wife, where the enclitic follows India and not President.
The class of determiners is used to specify the noun they precede in terms of definiteness, where the marks a definite noun and a or an an indefinite one. A definite noun is assumed by the speaker to be already known by the interlocutor, whereas an indefinite noun is not specified as being previously known. Quantifiers, which include one, many, some and all, are used to specify the noun in terms of quantity or number. The noun must agree with the number of the determiner, e.g. one man (sg.) but all men (pl.). Determiners are the first constituents in a noun phrase.[180]
English adjectives are words such as good, big, interesting, and Canadian that most typically modify nouns, denoting characteristics of their referents (e.g., a red car). As modifiers, they come before the nouns they modify and after determiners.[181] English adjectives also function as predicative complements (e.g., the child is happy).
In Modern English, adjectives are not inflected so as to agree in form with the noun they modify, as adjectives in most other Indo-European languages do. For example, in the phrases the slender boy, and many slender girls, the adjective slender does not change form to agree with either the number or gender of the noun.
Some adjectives are inflected for degree of comparison, with the positive degree unmarked, the suffix -er marking the comparative, and -est marking the superlative: a small boy, the boy is smaller than the girl, that boy is the smallest. Some adjectives have irregular suppletive comparative and superlative forms, such as good, better, and best. Other adjectives have comparatives formed by periphrastic constructions, with the adverb more marking the comparative, and most marking the superlative: happier or more happy, the happiest or most happy.[182] There is some variation among speakers regarding which adjectives use inflected or periphrastic comparison, and some studies have shown a tendency for the periphrastic forms to become more common at the expense of the inflected form.[183]
English determiners are words such as the, each, many, some, and which, occurring most typically in noun phrases before the head nouns and any modifiers and marking the noun phrase as definite or indefinite.[184] They often agree with the noun in number. They do not typically inflect for degree of comparison.
English pronouns conserve many traits of case and gender inflection. The personal pronouns retain a difference between subjective and objective case in most persons (I/me, he/him, she/her, we/us, they/them) as well as an animateness distinction in the third person singular (distinguishing it from the three sets of animate third person singular pronouns) and an optional gender distinction in the animate third person singular (distinguishing between she/her [feminine], they/them [epicene], and he/him [masculine]).[185][186] The subjective case corresponds to the Old English nominative case, and the objective case is used in the sense both of the previous accusative case (for a patient, or direct object of a transitive verb), and of the Old English dative case (for a recipient or indirect object of a transitive verb).[187][188] The subjective is used when the pronoun is the subject of a finite clause, otherwise the objective is used.[189] While grammarians such as Henry Sweet[190] and Otto Jespersen[191] noted that the English cases did not correspond to the traditional Latin-based system, some contemporary grammars, for example Huddleston & Pullum (2002), retain traditional labels for the cases, calling them nominative and accusative cases respectively.
Possessive pronouns exist in dependent and independent forms; the dependent form functions as a determiner specifying a noun (as in my chair), while the independent form can stand alone as if it were a noun (e.g. the chair is mine).[192] The English system of grammatical person no longer has a distinction between formal and informal pronouns of address (the old second person singular familiar pronoun thou acquired a pejorative or inferior tinge of meaning and was abandoned).
Both the second and third persons share pronouns between the plural and singular:
English verbs are inflected for tense and aspect and marked for agreement with present-tense third-person singular subject. Only the copula verb to be is still inflected for agreement with the plural and first and second person subjects.[182] Auxiliary verbs such as have and be are paired with verbs in the infinitive, past, or progressive forms. They form complex tenses, aspects, and moods. Auxiliary verbs differ from other verbs in that they can be followed by the negation, and in that they can occur as the first constituent in a question sentence.[197][198]
Most verbs have six inflectional forms. The primary forms are a plain present, a third-person singular present, and a preterite (past) form. The secondary forms are a plain form used for the infinitive, a gerund-participle and a past participle.[199] The copula verb to be is the only verb to retain some of its original conjugation, and takes different inflectional forms depending on the subject. The first-person present-tense form is am, the third person singular form is is, and the form are is used in the second-person singular and all three plurals. The only verb past participle is been and its gerund-participle is being.
English has two primary tenses, past (preterite) and non-past. The preterite is inflected by using the preterite form of the verb, which for the regular verbs includes the suffix -ed, and for the strong verbs either the suffix -t or a change in the stem vowel. The non-past form is unmarked except in the third person singular, which takes the suffix -s.[197]
English does not have future verb forms.[200] The future tense is expressed periphrastically with one of the auxiliary verbs will or shall.[201] Many varieties also use a near future constructed with the phrasal verb be going to ("going-to future").[202]
Further aspectual distinctions are shown by auxiliary verbs, primarily have and be, which show the contrast between a perfect and non-perfect past tense (I have run vs. I was running), and compound tenses such as preterite perfect (I had been running) and present perfect (I have been running).[203]
For the expression of mood, English uses a number of modal auxiliaries, such as can, may, will, shall and the past tense forms could, might, would, should. There are also subjunctive and imperative moods, both based on the plain form of the verb (i.e. without the third person singular -s), for use in subordinate clauses (e.g. subjunctive: It is important that he run every day; imperative Run!).[201]
An infinitive form, that uses the plain form of the verb and the preposition to, is used for verbal clauses that are syntactically subordinate to a finite verbal clause. Finite verbal clauses are those that are formed around a verb in the present or preterite form. In clauses with auxiliary verbs, they are the finite verbs and the main verb is treated as a subordinate clause.[204] For example, he has to go where only the auxiliary verb have is inflected for time and the main verb to go is in the infinitive, or in a complement clause such as I saw him leave, where the main verb is see, which is in a preterite form, and leave is in the infinitive.
English also makes frequent use of constructions traditionally called phrasal verbs, verb phrases that are made up of a verb root and a preposition or particle that follows the verb. The phrase then functions as a single predicate. In terms of intonation the preposition is fused to the verb, but in writing it is written as a separate word. Examples of phrasal verbs are to get up, to ask out, to back up, to give up, to get together, to hang out, to put up with, etc. The phrasal verb frequently has a highly idiomatic meaning that is more specialised and restricted than what can be simply extrapolated from the combination of verb and preposition complement (e.g. lay off meaning terminate someone's employment).[205] In spite of the idiomatic meaning, some grammarians, including Huddleston & Pullum (2002:274), do not consider this type of construction to form a syntactic constituent and hence refrain from using the term "phrasal verb". Instead, they consider the construction simply to be a verb with a prepositional phrase as its syntactic complement, i.e. he woke up in the morning and he ran up in the mountains are syntactically equivalent.
The function of adverbs is to modify the action or event described by the verb by providing additional information about the manner in which it occurs.[174] Many adverbs are derived from adjectives by appending the suffix -ly. For example, in the phrase the woman walked quickly, the adverb quickly is derived in this way from the adjective quick. Some commonly used adjectives have irregular adverbial forms, such as good, which has the adverbial form well.
Modern English syntax language is moderately analytic.[206] It has developed features such as modal verbs and word order as resources for conveying meaning. Auxiliary verbs mark constructions such as questions, negative polarity, the passive voice and progressive aspect.
In most sentences, English only marks grammatical relations through word order.[208] The subject constituent precedes the verb and the object constituent follows it. The example below demonstrates how the grammatical roles of each constituent are marked only by the position relative to the verb:
An exception is found in sentences where one of the constituents is a pronoun, in which case it is doubly marked, both by word order and by case inflection, where the subject pronoun precedes the verb and takes the subjective case form, and the object pronoun follows the verb and takes the objective case form.[209] The example below demonstrates this double marking in a sentence where both object and subject are represented with a third person singular masculine pronoun:
Indirect objects (IO) of ditransitive verbs can be placed either as the first object in a double object construction (S V IO O), such as I gave Jane the book or in a prepositional phrase, such as I gave the book to Jane.[210]
In English a sentence may be composed of one or more clauses, that may, in turn, be composed of one or more phrases (e.g. Noun Phrases, Verb Phrases, and Prepositional Phrases). A clause is built around a verb and includes its constituents, such as any NPs and PPs. Within a sentence, there is always at least one main clause (or matrix clause) whereas other clauses are subordinate to a main clause. Subordinate clauses may function as arguments of the verb in the main clause. For example, in the phrase I think (that) you are lying, the main clause is headed by the verb think, the subject is I, but the object of the phrase is the subordinate clause (that) you are lying. The subordinating conjunction that shows that the clause that follows is a subordinate clause, but it is often omitted.[211] Relative clauses are clauses that function as a modifier or specifier to some constituent in the main clause: For example, in the sentence I saw the letter that you received today, the relative clause that you received today specifies the meaning of the word letter, the object of the main clause. Relative clauses can be introduced by the pronouns who, whose, whom and which as well as by that (which can also be omitted.)[212] In contrast to many other Germanic languages there are no major differences between word order in main and subordinate clauses.[213]
Negation is done with the adverb not, which precedes the main verb and follows an auxiliary verb. A contracted form of not -n't can be used as an enclitic attaching to auxiliary verbs and to the copula verb to be. Just as with questions, many negative constructions require the negation to occur with do-support, thus in Modern English I don't know him is the correct answer to the question Do you know him?, but not *I know him not, although this construction may be found in older English.[215]
Passive constructions also use auxiliary verbs. A passive construction rephrases an active construction in such a way that the object of the active phrase becomes the subject of the passive phrase, and the subject of the active phrase is either omitted or demoted to a role as an oblique argument introduced in a prepositional phrase. They are formed by using the past participle either with the auxiliary verb to be or to get, although not all varieties of English allow the use of passives with get. For example, putting the sentence she sees him into the passive becomes he is seen (by her), or he gets seen (by her).[216]
While English is a subject-prominent language, at the discourse level it tends to use a topic-comment structure, where the known information (topic) precedes the new information (comment). Because of the strict SVO syntax, the topic of a sentence generally has to be the grammatical subject of the sentence. In cases where the topic is not the grammatical subject of the sentence, it is often promoted to subject position through syntactic means. One way of doing this is through a passive construction, the girl was stung by the bee. Another way is through a cleft sentence where the main clause is demoted to be a complement clause of a copula sentence with a dummy subject such as it or there, e.g. it was the girl that the bee stung, there was a girl who was stung by a bee.[218] Dummy subjects are also used in constructions where there is no grammatical subject such as with impersonal verbs (e.g., it is raining) or in existential clauses (there are many cars on the street). Through the use of these complex sentence constructions with informationally vacuous subjects, English is able to maintain both a topic-comment sentence structure and a SVO syntax.
Focus constructions emphasise a particular piece of new or salient information within a sentence, generally through allocating the main sentence level stress on the focal constituent. For example, the girl was stung by a bee (emphasising it was a bee and not, for example, a wasp that stung her), or The girl was stung by a bee (contrasting with another possibility, for example that it was the boy).[219] Topic and focus can also be established through syntactic dislocation, either preposing or postposing the item to be focused on relative to the main clause. For example, That girl over there, she was stung by a bee, emphasises the girl by preposition, but a similar effect could be achieved by postposition, she was stung by a bee, that girl over there, where reference to the girl is established as an "afterthought".[220]
Cohesion between sentences is achieved through the use of deictic pronouns as anaphora (e.g. that is exactly what I mean where that refers to some fact known to both interlocutors, or then used to locate the time of a narrated event relative to the time of a previously narrated event).[221] Discourse markers such as oh, so or well, also signal the progression of ideas between sentences and help to create cohesion. Discourse markers are often the first constituents in sentences. Discourse markers are also used for stance taking in which speakers position themselves in a specific attitude towards what is being said, for example, no way is that true! (the idiomatic marker no way! expressing disbelief), or boy! I'm hungry (the marker boy expressing emphasis). While discourse markers are particularly characteristic of informal and spoken registers of English, they are also used in written and formal registers.[222]
Due to its status as an international language, English adopts foreign words quickly, and borrows vocabulary from many other sources. Early studies of English vocabulary by lexicographers, the scholars who formally study vocabulary, compile dictionaries, or both, were impeded by a lack of comprehensive data on actual vocabulary in use from good-quality linguistic corpora,[225] collections of actual written texts and spoken passages. Many statements published before the end of the 20th century about the growth of English vocabulary over time, the dates of first use of various words in English, and the sources of English vocabulary will have to be corrected as new computerised analysis of linguistic corpus data becomes available.[224][226]
English forms new words from existing words or roots in its vocabulary through a variety of processes. One of the most productive processes in English is conversion,[227] using a word with a different grammatical role, for example using a noun as a verb or a verb as a noun. Another productive word-formation process is nominal compounding,[224][226] producing compound words such as babysitter or ice cream or homesick.[227] A process more common in Old English than in Modern English, but still productive in Modern English, is the use of derivational suffixes (-hood, -ness, -ing, -ility) to derive new words from existing words (especially those of Germanic origin) or stems (especially for words of Latin or Greek origin).
Formation of new words, called neologisms, based on Greek and/or Latin roots (for example television or optometry) is a highly productive process in English and in most modern European languages, so much so that it is often difficult to determine in which language a neologism originated. For this reason, American lexicographer Philip Gove attributed many such words to the "international scientific vocabulary" (ISV) when compiling Webster's Third New International Dictionary (1961). Another active word-formation process in English are acronyms,[228] words formed by pronouncing as a single word abbreviations of longer phrases, e.g. NATO, laser.
English, besides forming new words from existing words and their roots, also borrows words from other languages. This adoption of words from other languages is commonplace in many world languages, but English has been especially open to borrowing of foreign words throughout the last 1,000 years.[230] The most commonly used words in English are West Germanic.[231] The words in English learned first by children as they learn to speak, particularly the grammatical words that dominate the word count of both spoken and written texts, are mainly the Germanic words inherited from the earliest periods of the development of Old English.[224]
But one of the consequences of long language contact between French and English in all stages of their development is that the vocabulary of English has a very high percentage of "Latinate" words (derived from French, especially, and also from other Romance languages and Latin). French words from various periods of the development of French now make up one-third of the vocabulary of English.[232] Linguist Anthony Lacoudre estimated that over 40,000 English words are of French origin and may be understood without orthographical change by French speakers.[233] Words of Old Norse origin have entered the English language primarily from the contact between Old Norse and Old English during colonisation of eastern and northern England. Many of these words are part of English core vocabulary, such as egg and knife.[234]
English has also borrowed many words directly from Latin, the ancestor of the Romance languages, during all stages of its development.[226][224] Many of these words had earlier been borrowed into Latin from Greek. Latin or Greek are still highly productive sources of stems used to form vocabulary of subjects learned in higher education such as the sciences, philosophy, and mathematics.[235] English continues to gain new loanwords and calques ("loan translations") from languages all over the world, and words from languages other than the ancestral Anglo-Saxon language make up about 60% of the vocabulary of English.[236]
English has formal and informal speech registers; informal registers, including child-directed speech, tend to be made up predominantly of words of Anglo-Saxon origin, while the percentage of vocabulary that is of Latinate origin is higher in legal, scientific, and academic texts.[237][238]
English has had a strong influence on the vocabulary of other languages.[232][239] The influence of English comes from such factors as opinion leaders in other countries knowing the English language, the role of English as a world lingua franca, and the large number of books and films that are translated from English into other languages.[240] That pervasive use of English leads to a conclusion in many places that English is an especially suitable language for expressing new ideas or describing new technologies. Among varieties of English, it is especially American English that influences other languages.[241] Some languages, such as Chinese, write words borrowed from English mostly as calques, while others, such as Japanese, readily take in English loanwords written in sound-indicating script.[242] Dubbed films and television programmes are an especially fruitful source of English influence on languages in Europe.[242]
Since the ninth century, English has been written in a Latin alphabet (also called Roman alphabet). Earlier Old English texts in Anglo-Saxon runes are only short inscriptions. The great majority of literary works in Old English that survive to today are written in the Roman alphabet.[37] The modern English alphabet contains 26 letters of the Latin script: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z (which also have capital forms: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z).
The spelling system, or orthography, of English is multi-layered and complex, with elements of French, Latin, and Greek spelling on top of the native Germanic system.[243] Further complications have arisen through sound changes with which the orthography has not kept pace.[50] Compared to European languages for which official organisations have promoted spelling reforms, English has spelling that is a less consistent indicator of pronunciation, and standard spellings of words that are more difficult to guess from knowing how a word is pronounced.[244] There are also systematic spelling differences between British and American English. These situations have prompted proposals for spelling reform in English.[245]
Although letters and speech sounds do not have a one-to-one correspondence in standard English spelling, spelling rules that take into account syllable structure, phonetic changes in derived words, and word accent are reliable for most English words.[246] Moreover, standard English spelling shows etymological relationships between related words that would be obscured by a closer correspondence between pronunciation and spelling, for example the words photograph, photography, and photographic,[246] or the words electricity and electrical. While few scholars agree with Chomsky and Halle (1968) that conventional English orthography is "near-optimal",[243] there is a rationale for current English spelling patterns.[247] The standard orthography of English is the most widely used writing system in the world.[248] Standard English spelling is based on a graphomorphemic segmentation of words into written clues of what meaningful units make up each word.[249]
For the vowel sounds of the English language, however, correspondences between spelling and pronunciation are more irregular. There are many more vowel phonemes in English than there are single vowel letters (a, e, i, o, u, w, y). As a result, some "long vowels" are often indicated by combinations of letters (like the oa in boat, the ow in how, and the ay in stay), or the historically based silent e (as in note and cake).[247]
The consequence of this complex orthographic history is that learning to read and write can be challenging in English. It can take longer for school pupils to become independently fluent readers of English than of many other languages, including Italian, Spanish, and German.[251] Nonetheless, there is an advantage for learners of English reading in learning the specific sound-symbol regularities that occur in the standard English spellings of commonly used words.[246] Such instruction greatly reduces the risk of children experiencing reading difficulties in English.[252][253] Making primary school teachers more aware of the primacy of morpheme representation in English may help learners learn more efficiently to read and write English.[254]
English writing also includes a system of punctuation marks that is similar to those used in most alphabetic languages around the world. The purpose of punctuation is to mark meaningful grammatical relationships in sentences to aid readers in understanding a text and to indicate features important for reading a text aloud.[255]
Dialectologists identify many English dialects, which usually refer to regional varieties that differ from each other in terms of patterns of grammar, vocabulary, and pronunciation. The pronunciation of particular areas distinguishes dialects as separate regional accents. The major native dialects of English are often divided by linguists into the two extremely general categories of British English (BrE) and North American English (NAE).[256] There also exists a third common major grouping of English varieties: Southern Hemisphere English, the most prominent being Australian and New Zealand English.
Since the English language first evolved in Britain and Ireland, the archipelago is home to the most diverse dialects, particularly in England. Within the United Kingdom, the Received Pronunciation (RP), an educated dialect of South East England, is traditionally used as the broadcast standard and is considered the most prestigious of the British dialects. The spread of RP (also known as BBC English) through the media has caused many traditional dialects of rural England to recede, as youths adopt the traits of the prestige variety instead of traits from local dialects. At the time of the Survey of English Dialects, grammar and vocabulary differed across the country, but a process of lexical attrition has led most of this variation to disappear.[257]
English in England can be divided into four major dialect regions, Southwest English, South East English, Midlands English, and Northern English. Within each of these regions several local subdialects exist: Within the Northern region, there is a division between the Yorkshire dialects and the Geordie dialect spoken in Northumbria around Newcastle, and the Lancashire dialects with local urban dialects in Liverpool (Scouse) and Manchester (Mancunian). Having been the centre of Danish occupation during the Viking Invasions, Northern English dialects, particularly the Yorkshire dialect, retain Norse features not found in other English varieties.[261]
Scots is today considered a separate language from English, but it has its origins in early Northern Middle English[266] and developed and changed during its history with influence from other sources, particularly Scots Gaelic and Old Norse. Scots itself has a number of regional dialects. And in addition to Scots, Scottish English comprises the varieties of Standard English spoken in Scotland; most varieties are Northern English accents, with some influence from Scots.[267]
In Ireland, various forms of English have been spoken since the Norman invasions of the 11th century. In County Wexford, in the area surrounding Dublin, two extinct dialects known as Forth and Bargy and Fingallian developed as offshoots from Early Middle English, and were spoken until the 19th century. Modern Irish English, however, has its roots in English colonisation in the 17th century. Today Irish English is divided into Ulster English, the Northern Ireland dialect with strong influence from Scots, and various dialects of the Republic of Ireland. Like Scottish and most North American accents, almost all Irish accents preserve the rhoticity which has been lost in the dialects influenced by RP.[22][268]
North American English has been regarded as fairly homogeneous compared to British English but this has been disputed.[269] Today, American accent variation is often increasing at the regional level and decreasing at the very local level,[270] though most Americans still speak within a phonological continuum of similar accents,[271] known collectively as General American (GA), with differences hardly noticed even among Americans themselves (such as Midland and Western American English).[272][273][274] In most American and Canadian English dialects, rhoticity (or r-fulness) is dominant, with non-rhoticity (r-dropping) becoming associated with lower prestige and social class especially after World War II; this contrasts with the situation in England, where non-rhoticity has become the standard.[275]
Today spoken primarily by working- and middle-class African Americans, African-American Vernacular English (AAVE) is also largely non-rhotic and likely originated among enslaved Africans and African Americans influenced primarily by the non-rhotic, non-standard older Southern dialects. A minority of linguists,[284] contrarily, propose that AAVE mostly traces back to African languages spoken by the slaves who had to develop a pidgin or Creole English to communicate with slaves of other ethnic and linguistic origins.[285] AAVE's important commonalities with Southern accents suggests it developed into a highly coherent and homogeneous variety in the 19th or early 20th century. AAVE is commonly stigmatised in North America as a form of "broken" or "uneducated" English, as are white Southern accents, but linguists today recognise both as fully developed varieties of English with their own norms shared by a large speech community.[286][287]
Since 1788, English has been spoken in Oceania, and Australian English has developed as a first language of the vast majority of the inhabitants of the Australian continent, its standard accent being General Australian. The English of neighbouring New Zealand has to a lesser degree become an influential standard variety of the language.[288] Australian and New Zealand English are each other's closest relatives with few differentiating characteristics, followed by South African English and the English of southeastern England, all of which have similarly non-rhotic accents, aside from some accents in the South Island of New Zealand. Australian and New Zealand English stand out for their innovative vowels: many short vowels are fronted or raised, whereas many long vowels have diphthongised. Australian English also has a contrast between long and short vowels, not found in most other varieties. Australian English grammar aligns closely to British and American English; like American English, collective plural subjects take on a singular verb (as in the government is rather than are).[289][290] New Zealand English uses front vowels that are often even higher than in Australian English.[291][292][293]
The first significant exposure of the Philippines to the English language occurred in 1762 when the British occupied Manila during the Seven Years' War, but this was a brief episode that had no lasting influence. English later became more important and widespread during American rule between 1898 and 1946, and remains an official language of the Philippines. Today, the use of English is ubiquitous in the Philippines, from street signs and marquees, government documents and forms, courtrooms, the media and entertainment industries, the business sector, and other aspects of daily life. One such usage that is also prominent in the country is in speech, where most Filipinos from Manila would use or have been exposed to Taglish, a form of code-switching between Tagalog and English. A similar code-switching method is used by urban native speakers of Bisayan languages called Bislish.
Several varieties of English are also spoken in the Caribbean islands that were colonial possessions of Britain, including Jamaica, and the Leeward and Windward Islands and Trinidad and Tobago, Barbados, the Cayman Islands, and Belize. Each of these areas is home both to a local variety of English and a local English-based creole, combining English and African languages. The most prominent varieties are Jamaican English and Jamaican Creole. In Central America, English-based creoles are spoken in on the Caribbean coasts of Nicaragua and Panama.[299] Locals are often fluent both in the local English variety and the local creole languages and code-switching between them is frequent, indeed another way to conceptualise the relationship between Creole and Standard varieties is to see a spectrum of social registers with the Creole forms serving as "basilect" and the more RP-like forms serving as the "acrolect", the most formal register.[300]
Article 1 of the Universal Declaration of Human Rights in English:[305]
Click on a date/time to view the file as it appeared at that time.
More than 100 pages use this file.
The following list shows the first 100 pages that use this file only.
A full list is available.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
The species is found on many Japanese islands, including Honshu, Shikoku, and Kyushu. Their habitats include both natural and artificial bodies of water, as well as forests and grasslands. They breed from spring to the beginning of summer, both sexes producing pheromones when ready to mate. Eggs are laid separately, hatching after about three weeks. They grow from larval to juvenile form in between five and six months. Juveniles eat soil-dwelling prey, and adults eat a wide variety of insects, tadpoles, and the eggs of their own species. They have several adaptations to avoid predators, although which they use depends on where they live. Several aspects of their biology have been studied, including their ability to regrow missing body parts.
The Japanese fire-bellied newt first diverged from its closest relative in the Middle Miocene, before splitting into four distinct varieties, each with a mostly separate range, although all four are formally recognized as composing a single species. Currently, their population is declining, and they face threats from disease and the pet trade. They can be successfully kept in captivity.
The Integrated Taxonomic Information System lists sixteen synonyms for Cynops pyrrhogaster.[9] Common names of the species include Japanese fire-bellied newt,[1] red-bellied newt,[10] and Japanese fire-bellied salamander.[11] Studies examining morphological and geographic variation had formerly recognized six races: Tohoku, Kanto, Atsumi, intermediate, Sasayama, and Hiroshima,[12] one of which, the Sasayama, was described as a subspecies in 1969 by Robert Mertens as Triturus pyrrhogaster sasayamae, which is now considered a synonym of C. pyrrhogaster.[2] Modern molecular analysis supports the division of C. pyrrhogaster into four clades instead.[12] In particular, the validity of the Sasayama and intermediate races has never been proven, with one study finding no behavioral differences between the two supposed forms.[13]
On the newt's upper body, the skin is dark brown, approaching black, and covered in wartlike bumps. The underbelly and the underside of its tail are bright red, with black spots.[4] Younger juveniles have creamy coloration instead of red, although most larger juveniles have some red present.[15] Adults from smaller islands tend to have more red on their ventral (belly) regions than those from larger islands, sometimes with extremely small spots or none at all. In general males tend to have more red than females.[16] Males can also be distinguished from females by their flat, wide tails and swelling around the ventral region.[17] An entirely red variant exists: that coloration is believed to be inherited and recessive. This variant is not confined to any single population, but is more common in the western half of Japan overall.[18]
Of the four clades, the northern is found in the districts of Tohoku and Kanto. This does not overlap with the range of the central clade, which is found in Chubu, northern Kansai, and eastern Chugoku. The central's range has a small amount of overlap with the western, which is found in southern Kinki, western Chugoku, Shikoku, and central Kyushu. The western also has some overlap with the southern clade, which is found in western and southern Kyushu.[12]
Courtship begins when the male approaches the female, sniffing its sides or cloaca. The male then brings its tail to the female and rapidly vibrates it. The female responds by pushing the male's neck with its snout. At this point, the male slowly moves away, undulating its tail, and the female follows, touching the tail with its snout when close enough. The male then deposits two to four spermatophores, one at a time, moving several centimeters after each, which the female attempts to pick up with its cloaca, sometimes unsuccessfully.[24] Females lay eggs separately on underwater objects, such as leaves and submerged grass roots, fertilized one by one from the spermatophores they carry. They can lay up to 40 eggs in one session, and 100 to 400 eggs in a breeding season.[24]
Newts in Mainland Japan have different antipredator behavior than newts on smaller islands. Individuals on smaller islands (for instance, Fukue Island) generally use a maneuver called the unken reflex, where they expose their bright red underbelly to attackers. As their main predators are birds, which are capable of distinguishing the color red, this technique is effective. In Mainland Japan the newts must also avoid mammalian predators, which cannot distinguish colors as well as avian hunters. This leads these populations to use the maneuver less, as it can result in death if attempted.[16]
Against snakes, newts from Fukue Island tend to perform tail-wagging displays, designed to bring a predator's attention to their replaceable tail rather than their more valuable head; those from Nagasaki Prefecture in Mainland Japan tend to simply flee. Snakes are present in both areas. This is likely because those from the mainland are adapted to escape from mammalian hunters, which are less likely to be repelled by such a display.[28]
Wild Japanese fire-bellied newts contain high levels of the neurotoxin tetrodotoxin (TTX).[29] This toxin inhibits the activity of sodium channels in most vertebrates, discouraging predation by both birds and mammals.[28] Experiments have shown the toxin is almost entirely derived from the newt's diet. When raised in captivity with no source of TTX, 36- to 70-week-old juveniles did not contain detectable levels, but wild specimens from the same original habitat had high toxicity. In younger captive-reared newts some TTX was still detected, which was inferred to have been transferred by adult females to their eggs.[29] In a follow-up experiment by the same team captive-reared newts were given food containing the neurotoxin. They readily consumed TTX-laced bloodworms when offered, not showing any symptoms after ingesting the poison. It was detectable in their bodies afterward, further indicating food to be the source of the toxin. No TTX-producing organisms are known from their habitat, but their existence is likely, and would explain the origin of TTX in wild newts.[30]
The International Union for the Conservation of Nature (IUCN) has ranked it as near-threatened. This assessment was made in 2020,[1] a shift from 2004 when it was rated least-concern.[31] It successfully reproduces in Australian zoos.[1] One major threat that C. pyrrhogaster faces is collection for the pet trade. The IUCN states that this trade needs to be ended immediately. Their population is decreasing, particularly near areas of human habitation.[1]
Japanese fire-bellied newts with mysterious skin lesions at Lake Biwa in Japan's Shiga Prefecture were found to be suffering from infections caused by a single-celled eukaryote in the order Dermocystida. The lesions contained cysts, which were filled with spores. Nearly all the lesions were external, although one was found on the liver. Globally, diseases are one of the causes for declining amphibian populations. There is concern that this affliction could spread to other nearby species, including Zhangixalus arboreus and Hynobius vandenburghi.[32]
A variety, believed to be found exclusively on the Atsumi Peninsula, was thought to have become extinct in the 1960s. Then, in 2016, a trio of researchers discovered that newts on the Chita Peninsula were very likely the same variant due to their similar morphological traits. Both groups share a preference for cooler temperature and have smooth and soft bodies, pale dorsal regions, and yellowish undersides. Even if still alive, this form is highly threatened and will soon be wiped out without immediate protection.[33]
Japanese fire-bellied newts serve as a highly useful model organism in laboratory settings, but they become more difficult to care for after metamorphosis. An experiment supported by the Japan Society for the Promotion of Science found that thiourea (TU) can prevent this process from occurring, allowing the animals to stay in their pre-metamorphosis form for as long as two years, while still capable of metamorphosizing when removed from the TU solution. This did not have any impact on their regeneration capabilities.[25]
Japanese fire-bellied newts produce motilin, a peptide that stimulates gastrointestinal contractions, identified in many vertebrates. It is created in the upper small intestine and pancreas. The discovery of the latter was the first time pancreatic motilin had been observed. The organ also produces insulin. These results represented the first discovery of motilin in amphibians, suggesting that it has a similar role for them as it does for birds and mammals. The existence of pancreatic motilin also indicated another, unknown function.[34]
This species, as well as other Urodele amphibians, is capable of regrowing missing body parts, including limbs with functional joints and the lower jaw.[35][36] When this process occurs, the regenerated tissue tends to mirror intact tissue in form.[35] It is also able to regrow missing lenses, taking thirty days to do so as a larva and eighty days as an adult. The difference in time is purely due to the size of the eye, and regenerative ability does not change; the discovery of this fact contradicted a popular claim that juvenile animals are quicker to regenerate than adults.[37]
Doctor of Veterinary Medicine Lianne McLeod described them as "low-maintenance", noting that captive newts enjoy bloodworms, brine shrimp, glass shrimp, Daphnia, and, for larger individuals, guppies.
In biology, a species is the basic unit of classification and a taxonomic rank of an organism, as well as a unit of biodiversity. A species is often defined as the largest group of organisms in which any two individuals of the appropriate sexes or mating types can produce fertile offspring, typically by sexual reproduction. Other ways of defining species include their karyotype, DNA sequence, morphology, behaviour or ecological niche. In addition, paleontologists use the concept of the chronospecies since fossil reproduction cannot be examined.
The most recent rigorous estimate for the total number of species of eukaryotes is between 8 and 8.7 million.[1][2][3] However, only about 14% of these had been described by 2011.[3]
All species (except viruses) are given a two-part name, a "binomial". The first part of a binomial is the genus to which the species belongs. The second part is called the specific name or the specific epithet (in botanical nomenclature, also sometimes in zoological nomenclature). For example, Boa constrictor is one of the species of the genus Boa, with constrictor being the species' epithet.
While the definitions given above may seem adequate at first glance, when looked at more closely they represent problematic species concepts. For example, the boundaries between closely related species become unclear with hybridisation, in a species complex of hundreds of similar microspecies, and in a ring species. Also, among organisms that reproduce only asexually, the concept of a reproductive species breaks down, and each clone is potentially a microspecies. Although none of these are entirely satisfactory definitions, and while the concept of species may not be a perfect model of life, it is still an incredibly useful tool to scientists and conservationists for studying life on Earth, regardless of the theoretical difficulties. If species were fixed and clearly distinct from one another, there would be no problem, but evolutionary processes cause species to change. This obliges taxonomists to decide, for example, when enough change has occurred to declare that a lineage should be divided into multiple chronospecies, or when populations have diverged to have enough distinct character states to be described as cladistic species.
Species were seen from the time of Aristotle until the 18th century as fixed categories that could be arranged in a hierarchy, the great chain of being. In the 19th century, biologists grasped that species could evolve given sufficient time. Charles Darwin's 1859 book On the Origin of Species explained how species could arise by natural selection. That understanding was greatly extended in the 20th century through genetics and population ecology. Genetic variability arises from mutations and recombination, while organisms themselves are mobile, leading to geographical isolation and genetic drift with varying selection pressures. Genes can sometimes be exchanged between species by horizontal gene transfer; new species can arise rapidly through hybridisation and polyploidy; and species may become extinct for a variety of reasons. Viruses are a special case, driven by a balance of mutation and selection, and can be treated as quasispecies.
Biologists and taxonomists have made many attempts to define species, beginning from morphology and moving towards genetics. Early taxonomists such as Linnaeus had no option but to describe what they saw: this was later formalised as the typological or morphological species concept. Ernst Mayr emphasised reproductive isolation, but this, like other species concepts, is hard or even impossible to test.[4][5] Later biologists have tried to refine Mayr's definition with the recognition and cohesion concepts, among others.[6] Many of the concepts are quite similar or overlap, so they are not easy to count: the biologist R. L. Mayden recorded about 24 concepts,[7] and the philosopher of science John Wilkins counted 26.[4] Wilkins further grouped the species concepts into seven basic kinds of concepts: (1) agamospecies for asexual organisms (2) biospecies for reproductively isolated sexual organisms (3) ecospecies based on ecological niches (4) evolutionary species based on lineage (5) genetic species based on gene pool (6) morphospecies based on form or phenotype and (7) taxonomic species, a species as determined by a taxonomist.[8]
A typological species is a group of organisms in which individuals conform to certain fixed properties (a type), so that even pre-literate people often recognise the same taxon as do modern taxonomists.[10][11] The clusters of variations or phenotypes within specimens (such as longer or shorter tails) would differentiate the species. This method was used as a "classical" method of determining species, such as with Linnaeus, early in evolutionary theory. However, different phenotypes are not necessarily different species (e.g. a four-winged Drosophila born to a two-winged mother is not a different species). Species named in this manner are called morphospecies.[12][13]
In the 1970s, Robert R. Sokal, Theodore J. Crovello and Peter Sneath proposed a variation on the morphological species concept, a phenetic species, defined as a set of organisms with a similar phenotype to each other, but a different phenotype from other sets of organisms.[14] It differs from the morphological species concept in including a numerical measure of distance or similarity to cluster entities based on multivariate comparisons of a reasonably large number of phenotypic traits.[15]
A mate-recognition species is a group of sexually reproducing organisms that recognise one another as potential mates.[16][17] Expanding on this to allow for post-mating isolation, a cohesion species is the most inclusive population of individuals having the potential for phenotypic cohesion through intrinsic cohesion mechanisms; no matter whether populations can hybridise successfully, they are still distinct cohesion species if the amount of hybridisation is insufficient to completely mix their respective gene pools.[18] A further development of the recognition concept is provided by the biosemiotic concept of species.[19]
The average nucleotide identity method quantifies genetic distance between entire genomes, using regions of about 10,000 base pairs. With enough data from genomes of one genus, algorithms can be used to categorize species, as for Pseudomonas avellanae in 2013,[22] and for all sequenced bacteria and archaea since 2020.[23]
DNA barcoding has been proposed as a way to distinguish species suitable even for non-specialists to use.[24] One of the barcodes is a region of mitochondrial DNA within the gene for cytochrome c oxidase. A database, Barcode of Life Data System, contains DNA barcode sequences from over 190,000 species.[25][26] However, scientists such as Rob DeSalle have expressed concern that classical taxonomy and DNA barcoding, which they consider a misnomer, need to be reconciled, as they delimit species differently.[27] Genetic introgression mediated by endosymbionts and other vectors can further make barcodes ineffective in the identification of species.[28]
An evolutionary species, suggested by George Gaylord Simpson in 1951, is "an entity composed of organisms which maintains its identity from other such entities through time and over space, and which has its own independent evolutionary fate and historical tendencies".[7][41] This differs from the biological species concept in embodying persistence over time. Wiley and Mayden stated that they see the evolutionary species concept as "identical" to Willi Hennig's species-as-lineages concept, and asserted that the biological species concept, "the several versions" of the phylogenetic species concept, and the idea that species are of the same kind as higher taxa are not suitable for biodiversity studies (with the intention of estimating the number of species accurately). They further suggested that the concept works for both asexual and sexually-reproducing species.[42] A version of the concept is Kevin de Queiroz's "General Lineage Concept of Species".[43]
An ecological species is a set of organisms adapted to a particular set of resources, called a niche, in the environment. According to this concept, populations form the discrete phenetic clusters that we recognise as species because the ecological and evolutionary processes controlling how resources are divided up tend to produce those clusters.[44]
A genetic species as defined by Robert Baker and Robert Bradley is a set of genetically isolated interbreeding populations. This is similar to Mayr's Biological Species Concept, but stresses genetic rather than reproductive isolation.[45] In the 21st century, a genetic species can be established by comparing DNA sequences, but other methods were available earlier, such as comparing karyotypes (sets of chromosomes) and allozymes (enzyme variants).[46]
An evolutionarily significant unit (ESU) or "wildlife species"[47] is a population of organisms considered distinct for purposes of conservation.[48]
In palaeontology, with only comparative anatomy (morphology) from fossils as evidence, the concept of a chronospecies can be applied. During anagenesis (evolution, not necessarily involving branching), palaeontologists seek to identify a sequence of species, each one derived from the phyletically extinct one before through continuous, slow and more or less uniform change. In such a time sequence, palaeontologists assess how much change is required for a morphologically distinct form to be considered a different species from its ancestors.[49][50][51][52]
Most modern textbooks make use of Ernst Mayr's 1942 definition,[60][61] known as the Biological Species Concept as a basis for further discussion on the definition of species. It is also called a reproductive or isolation concept. This defines a species as[62]
groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.[62]
It has been argued that this definition is a natural consequence of the effect of sexual reproduction on the dynamics of natural selection.[63][64][65][66] Mayr's use of the adjective "potentially" has been a point of debate; some interpretations exclude unusual or artificial matings that occur only in captivity, or that involve animals capable of mating but that do not normally do so in the wild.[62]
It is difficult to define a species in a way that applies to all organisms.[67] The debate about species concepts is called the species problem.[62][68][69][70] The problem was recognised even in 1859, when Darwin wrote in On the Origin of Species:
No one definition has satisfied all naturalists; yet every naturalist knows vaguely what he means when he speaks of a species. Generally the term includes the unknown element of a distinct act of creation.[71]
A simple textbook definition, following Mayr's concept, works well for most multi-celled organisms, but breaks down in several situations:
Species identification is made difficult by discordance between molecular and morphological investigations; these can be categorised as two types: (i) one morphology, multiple lineages (e.g. morphological convergence, cryptic species) and (ii) one lineage, multiple morphologies (e.g. phenotypic plasticity, multiple life-cycle stages).[81] In addition, horizontal gene transfer (HGT) makes it difficult to define a species.[82] All species definitions assume that an organism acquires its genes from one or two parents very like the "daughter" organism, but that is not what happens in HGT.[83] There is strong evidence of HGT between very dissimilar groups of prokaryotes, and at least occasionally between dissimilar groups of eukaryotes,[82] including some crustaceans and echinoderms.[84]
there is no easy way to tell whether related geographic or temporal forms belong to the same or different species. Species gaps can be verified only locally and at a point of time. One is forced to admit that Darwin's insight is correct: any local reality or integrity of species is greatly reduced over large geographic ranges and time periods.[18]
Wilkins writes that biologists such as the botanist Brent Mishler[85] have argued that the species concept is not valid, and that "if we were being true to evolution and the consequent phylogenetic approach to taxa, we should replace it with a 'smallest clade' idea" (a phylogenetic species concept).[86] Wilkins states that he concurs[87] with this approach, while noting the difficulties it would cause to taxonomy. He cites the ichthyologist Charles Tate Regan's early 20th century remark that "a species is whatever a suitably qualified biologist chooses to call a species".[86] Wilkins notes that the philosopher Philip Kitcher called this the "cynical species concept",[88] and arguing that far from being cynical, it usefully leads to an empirical taxonomy for any given group, based on taxonomists' experience.[86]
Blackberries belong to any of hundreds of microspecies of the Rubus fruticosus species aggregate.
Natural hybridisation presents a challenge to the concept of a reproductively isolated species, as fertile hybrids permit gene flow between two populations. For example, the carrion crow Corvus corone and the hooded crow Corvus cornix appear and are classified as separate species, yet they can hybridise where their geographical ranges overlap.[97]
Seven "species" of Larus gulls interbreed in a ring around the Arctic.
Opposite ends of the ring: a herring gull (Larus argentatus) (front) and a lesser black-backed gull (Larus fuscus) in Norway
Presumed evolution of five "species"  of greenish warblers around the Himalayas
The commonly used names for kinds of organisms are often ambiguous: "cat" could mean the domestic cat, Felis catus, or the cat family, Felidae. Another problem with common names is that they often vary from place to place, so that puma, cougar, catamount, panther, painter and mountain lion all mean Puma concolor in various parts of America, while "panther" may also mean the jaguar (Panthera onca) of Latin America or the leopard (Panthera pardus) of Africa and Asia. In contrast, the scientific names of species are chosen to be unique and universal; they are in two parts used together: the genus as in Puma, and the specific epithet as in concolor.[106][107]
A species is given a taxonomic name when a type specimen is described formally, in a publication that assigns it a unique scientific name. The description typically provides means for identifying the new species, which may not be based solely on morphology[108] (see cryptic species), differentiating it from other previously described and related or confusable species and provides a validly published name (in botany) or an available name (in zoology) when the paper is accepted for publication. The type material is usually held in a permanent repository, often the research collection of a major museum or university, that allows independent verification and the means to compare specimens.[109][110][111] Describers of new species are asked to choose names that, in the words of the International Code of Zoological Nomenclature, are "appropriate, compact, euphonious, memorable, and do not cause offence".[112]
Books and articles sometimes intentionally do not identify species fully, using the abbreviation "sp." in the singular or "spp." (standing for species pluralis, Latin for "multiple species") in the plural in place of the specific name or epithet (e.g. Canis sp.). This commonly occurs when authors are confident that some individuals belong to a particular genus but are not sure to which exact species they belong, as is common in paleontology.[113]
Authors may also use "spp." as a short way of saying that something applies to many species within a genus, but not to all. If scientists mean that something applies to all species within a genus, they use the genus name without the specific name or epithet. The names of genera and species are usually printed in italics. However, abbreviations such as "sp." should not be italicised.[113]
When a species's identity is not clear, a specialist may use "cf." before the epithet to indicate that confirmation is required. The abbreviations "nr." (near) or "aff." (affine) may be used when the identity is unclear but when the species appears to be similar to the species mentioned after.[113]
With the rise of online databases, codes have been devised to provide identifiers for species that are already defined, including:
The naming of a particular species, including which genus (and higher taxa) it is placed in, is a hypothesis about the evolutionary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be corroborated or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual organisms later identified as the same species. When two species names are discovered to apply to the same species, the older species name is given priority and usually retained, and the newer name considered as a junior synonym, a process called synonymy. Dividing a taxon into multiple, often new, taxa is called splitting. Taxonomists are often referred to as "lumpers" or "splitters" by their colleagues, depending on their personal approach to recognising differences or commonalities between organisms.[118][119][113] The circumscription of taxa, considered a taxonomic decision at the discretion of cognizant specialists, is not governed by the Codes of Zoological or Botanical Nomenclature.
The nomenclatural codes that guide the naming of species, including the ICZN for animals and the ICN for plants, do not make rules for defining the boundaries of the species. Research can change the boundaries, also known as circumscription, based on new evidence. Species may then need to be distinguished by the boundary definitions used, and in such cases the names may be qualified with sensu stricto ("in the narrow sense") to denote usage in the exact meaning given by an author such as the person who named the species, while the antonym sensu lato ("in the broad sense") denotes a wider usage, for instance including other subspecies. Other abbreviations such as "auct." ("author"), and qualifiers such as "non" ("not") may be used to further clarify the sense in which the specified authors delineated or described the species.[113][120][121]
Species are subject to change, whether by evolving into new species,[122] exchanging genes with other species,[123] merging with other species or by becoming extinct.[124]
Horizontal gene transfer between organisms of different species, either through hybridisation, antigenic shift, or reassortment, is sometimes an important source of genetic variation. Viruses can transfer genes between species. Bacteria can exchange plasmids with bacteria of other species, including some apparently distantly related ones in different phylogenetic domains, making analysis of their relationships difficult, and weakening the concept of a bacterial species.[129][82][130][123]
Louis-Marie Bobay and Howard Ochman suggest, based on analysis of the genomes of many types of bacteria, that they can often be grouped "into communities that regularly swap genes", in much the same way that plants and animals can be grouped into reproductively isolated breeding populations. Bacteria may thus form species, analogous to Mayr's biological species concept, consisting of asexually reproducing populations that exchange genes by homologous recombination.[131][132]
A species is extinct when the last individual of that species dies, but it may be functionally extinct well before that moment. It is estimated that over 99 percent of all species that ever lived on Earth, some five billion species, are now extinct. Some of these were in mass extinctions such as those at the ends of the Ordovician, Devonian, Permian, Triassic and Cretaceous periods. Mass extinctions had a variety of causes including volcanic activity, climate change, and changes in oceanic and atmospheric chemistry, and they in turn had major effects on Earth's ecology, atmosphere, land surface and waters.[133][134] Another form of extinction is through the assimilation of one species by another through hybridization. The resulting single species has been termed as a "compilospecies".[135]
Biologists and conservationists need to categorise and identify organisms in the course of their work. Difficulty assigning organisms reliably to a species constitutes a threat to the validity of research results, for example making measurements of how abundant a species is in an ecosystem moot. Surveys using a phylogenetic species concept reported 48% more species and accordingly smaller populations and ranges than those using nonphylogenetic concepts; this was termed "taxonomic inflation",[136] which could cause a false appearance of change to the number of endangered species and consequent political and practical difficulties.[137][138] Some observers claim that there is an inherent conflict between the desire to understand the processes of speciation and the need to identify and to categorise.[138]
Conservation laws in many countries make special provisions to prevent species from going extinct. Hybridization zones between two species, one that is protected and one that is not, have sometimes led to conflicts between lawmakers, land owners and conservationists. One of the classic cases in North America is that of the protected northern spotted owl which hybridises with the unprotected California spotted owl and the barred owl; this has led to legal debates.[139] It has been argued that the species problem is created by the varied uses of the concept of species, and that the solution is to abandon it and all other taxonomic ranks, and use unranked monophyletic groups instead. It has been argued, too, that since species are not comparable, counting them is not a valid measure of biodiversity; alternative measures of phylogenetic biodiversity have been proposed.[140][141]
When observers in the Early Modern period began to develop systems of organization for living things, they placed each kind of animal or plant into a context. Many of these early delineation schemes would now be considered whimsical: schemes included consanguinity based on colour (all plants with yellow flowers) or behaviour (snakes, scorpions and certain biting ants). John Ray, an English naturalist, was the first to attempt a biological definition of species in 1686, as follows:
In the 18th century, the Swedish scientist Carl Linnaeus classified organisms according to shared physical characteristics, and not simply based upon differences.[144] He established the idea of a taxonomic hierarchy of classification based upon observable characteristics and intended to reflect natural relationships.[145][146] At the time, however, it was still widely believed that there was no organic connection between species, no matter how similar they appeared. This view was influenced by European scholarly and religious education, which held that the categories of life are dictated by God, forming an Aristotelian hierarchy, the scala naturae or great chain of being. However, whether or not it was supposed to be fixed, the scala (a ladder) inherently implied the possibility of climbing.[147]
In viewing evidence of hybridisation, Linnaeus recognised that species were not fixed and could change; he did not consider that new species could emerge and maintained a view of divinely fixed species that may alter through processes of hybridisation or acclimatisation.[148] By the 19th century, naturalists understood that species could change form over time, and that the history of the planet provided enough time for major changes. Jean-Baptiste Lamarck, in his 1809 Zoological Philosophy, described the transmutation of species, proposing that a species could change over time, in a radical departure from Aristotelian thinking.[149]
In 1859, Charles Darwin and Alfred Russel Wallace provided a compelling account of evolution and the formation of new species. Darwin argued that it was populations that evolved, not individuals, by natural selection from naturally occurring variation among individuals.[150] This required a new definition of species. Darwin concluded that species are what they appear to be: ideas, provisionally useful for naming groups of interacting individuals, writing:
The term "mainland Japan" is used to distinguish the large islands of the Japanese archipelago from the remote, smaller islands; it refers to the main islands of Hokkaido, Honshu, Kyushu and Shikoku.[6] From 1943 until the end of the Pacific War, Karafuto Prefecture was designated part of the mainland.
The term "home islands" was used at the end of World War II to define the area where Japanese sovereignty and constitutional rule of its emperor would be restricted.[citation needed] The term is also commonly used today to distinguish the archipelago from Japan's colonies and other territories.[7]
Japanese archipelago, Sea of Japan and surrounding part of continental East Asia in Middle Pliocene to Late Pliocene (3.5-2 Ma)
The archipelago consists of 6,852 islands[8] (here defined as land more than 100 m in circumference), of which 430 are inhabited.[9] The five main islands, from north to south, are Hokkaido, Honshu, Shikoku, Kyushu, and Okinawa.[6] Honshu is the largest and referred to as the Japanese mainland.[10]
The Ryukyu Islands, which stretch towards Taiwan, are administered by Kagoshima Prefecture and Okinawa Prefecture
Seabed relief map, showing surface and underwater terrain and islands such as Minami-Tori-Shima, Benten-jima, Okinotorishima, and Yonaguni.
In ecology, the term habitat summarises the array of resources, physical and biotic factors that are present in an area, such as to support the survival and reproduction of a particular species. A species habitat can be seen as the physical manifestation of its ecological niche. Thus "habitat" is a species-specific term, fundamentally different from concepts such as environment or vegetation assemblages, for which the term "habitat-type" is more appropriate.[2]
The physical factors may include (for example): soil, moisture, range of temperature, and light intensity. Biotic factors will include the availability of food and the presence or absence of predators. Every species has particular habitat requirements, with habitat generalist species able to thrive in a wide array of environmental conditions while habitat specialist species requiring a very limited set of factors to survive. The habitat of a species is not necessarily found in a geographical area, it can be the interior of a stem, a rotten log, a rock or a clump of moss; a parasitic organism has as its habitat the body of its host, part of the host's body (such as the digestive tract), or a single cell within the host's body.[3]
The chief environmental factors affecting the distribution of living organisms are temperature, humidity, climate, soil and light intensity, and the presence or absence of all the requirements that the organism needs to sustain it. Generally speaking, animal communities are reliant on specific types of plant communities.[7]
Some plants and animals have habitat requirements which are met in a wide range of locations. The small white butterfly Pieris rapae for example is found on all the continents of the world apart from Antarctica. Its larvae feed on a wide range of Brassicas and various other plant species, and it thrives in any open location with diverse plant associations.[8] The large blue butterfly Phengaris arion is much more specific in its requirements; it is found only in chalk grassland areas, its larvae feed on Thymus species and because of complex lifecycle requirements it inhabits only areas in which Myrmica ants live.[9]
Disturbance is important in the creation of biodiverse habitat types. In the absence of disturbance, a climax vegetation cover develops that prevents the establishment of other species. Wildflower meadows are sometimes created by conservationists but most of the flowering plants used are either annuals or biennials and disappear after a few years in the absence of patches of bare ground on which their seedlings can grow.[10] Lightning strikes and toppled trees in tropical forests allow species richness to be maintained as pioneering species move in to fill the gaps created.[11] Similarly coastal habitat types can become dominated by kelp until the seabed is disturbed by a storm and the algae swept away, or shifting sediment exposes new areas for colonisation. Another cause of disturbance is when an area may be overwhelmed by an invasive introduced species which is not kept under control by natural enemies in its new habitat.[12]
Terrestrial habitat types include forests, grasslands, wetlands and deserts. Within these broad biomes are more specific habitat types with varying climate types, temperature regimes, soils, altitudes and vegetation. Many of these habitat types grade into each other and each one has its own typical communities of plants and animals. A habitat-type may suit a particular species well, but its presence or absence at any particular location depends to some extent on chance, on its dispersal abilities and its efficiency as a colonizer.[13]
Arid habitats are those where there is little available water. The most extreme arid habitats are deserts. Desert animals have a variety of adaptations to survive the dry conditions. Some frogs live in deserts, creating moist habitat types underground and hibernating while conditions are adverse. Couch's spadefoot toad (Scaphiopus couchii) emerges from its burrow when a downpour occurs and lays its eggs in the transient pools that form; the tadpoles develop with great rapidity, sometimes in as little as nine days, undergo metamorphosis, and feed voraciously before digging a burrow of their own.[14]
Other organisms cope with the drying up of their aqueous habitat in other ways. Vernal pools are ephemeral ponds that form in the rainy season and dry up afterwards. They have their specially-adapted characteristic flora, mainly consisting of annuals, the seeds of which survive the drought, but also some uniquely adapted perennials.[15] Animals adapted to these extreme habitat types also exist; fairy shrimps can lay "winter eggs" which are resistant to desiccation, sometimes being blown about with the dust, ending up in new depressions in the ground. These can survive in a dormant state for as long as fifteen years.[16] Some killifish behave in a similar way; their eggs hatch and the juvenile fish grow with great rapidity when the conditions are right, but the whole population of fish may end up as eggs in diapause in the dried up mud that was once a pond.[17]
Freshwater habitat types include rivers, streams, lakes, ponds, marshes and bogs.[18] Although some organisms are found across most of these habitat types, the majority have more specific requirements. The water velocity, its temperature and oxygen saturation are important factors, but in river systems, there are fast and slow sections, pools, bayous and backwaters which provide a range of habitat types. Similarly, aquatic plants can be floating, semi-submerged, submerged or grow in permanently or temporarily saturated soils besides bodies of water. Marginal plants provide important habitat for both invertebrates and vertebrates, and submerged plants provide oxygenation of the water, absorb nutrients and play a part in the reduction of pollution.[19]
Marine habitats include brackish water, estuaries, bays, the open sea, the intertidal zone, the sea bed, reefs and deep / shallow water zones.[18] Further variations include rock pools, sand banks, mudflats, brackish lagoons, sandy and pebbly beaches, and seagrass beds, all supporting their own flora and fauna. The benthic zone or seabed provides a home for both static organisms, anchored to the substrate, and for a large range of organisms crawling on or burrowing into the surface. Some creatures float among the waves on the surface of the water, or raft on floating debris, others swim at a range of depths, including organisms in the demersal zone close to the seabed, and myriads of organisms drift with the currents and form the plankton.[20]
Many animals and plants have taken up residence in urban environments. They tend to be adaptable generalists and use the town's features to make their homes. Rats and mice have followed man around the globe, pigeons, peregrines, sparrows, swallows and house martins use the buildings for nesting, bats use roof space for roosting, foxes visit the garbage bins and squirrels, coyotes, raccoons and skunks roam the streets. About 2,000 coyotes are thought to live in and around Chicago.[21] A survey of dwelling houses in northern European cities in the twentieth century found about 175 species of invertebrate inside them, including 53 species of beetle, 21 flies, 13 butterflies and moths, 13 mites, 9 lice, 7 bees, 5 wasps, 5 cockroaches, 5 spiders, 4 ants and a number of other groups.[22] In warmer climates, termites are serious pests in the urban habitat; 183 species are known to affect buildings and 83 species cause serious structural damage.[23]
A microhabitat is the small-scale physical requirements of a particular organism or population. Every habitat includes large numbers of microhabitat types with subtly different exposure to light, humidity, temperature, air movement, and other factors. The lichens that grow on the north face of a boulder are different from those that grow on the south face, from those on the level top, and those that grow on the ground nearby; the lichens growing in the grooves and on the raised surfaces are different from those growing on the veins of quartz. Lurking among these miniature "forests" are the microfauna, species of invertebrate, each with its own specific habitat requirements.[24]
There are numerous different microhabitat types in a wood; coniferous forest, broad-leafed forest, open woodland, scattered trees, woodland verges, clearings, and glades; tree trunk, branch, twig, bud, leaf, flower, and fruit; rough bark, smooth bark, damaged bark, rotten wood, hollow, groove, and hole; canopy, shrub layer, plant layer, leaf litter, and soil; buttress root, stump, fallen log, stem base, grass tussock, fungus, fern, and moss.[25] The greater the structural diversity in the wood, the greater the number of microhabitat types that will be present. A range of tree species with individual specimens of varying sizes and ages, and a range of features such as streams, level areas, slopes, tracks, clearings, and felled areas will provide suitable conditions for an enormous number of biodiverse plants and animals. For example, in Britain it has been estimated that various types of rotting wood are home to over 1700 species of invertebrate.[25]
For a parasitic organism, its habitat is the particular part of the outside or inside of its host on or in which it is adapted to live. The life cycle of some parasites involves several different host species, as well as free-living life stages, sometimes within vastly different microhabitat types.[26] One such organism is the trematode (flatworm) Microphallus turgidus, present in brackish water marshes in the southeastern United States. Its first intermediate host is a snail and the second, a glass shrimp. The final host is the waterfowl or mammal that consumes the shrimp.[27]
Although the vast majority of life on Earth lives in mesophyllic (moderate) environments, a few organisms, most of them microbes, have managed to colonise extreme environments that are unsuitable for more complex life forms. There are bacteria, for example, living in Lake Whillans, half a mile below the ice of Antarctica; in the absence of sunlight, they must rely on organic material from elsewhere, perhaps decaying matter from glacier melt water or minerals from the underlying rock.[28] Other bacteria can be found in abundance in the Mariana Trench, the deepest place in the ocean and on Earth; marine snow drifts down from the surface layers of the sea and accumulates in this undersea valley, providing nourishment for an extensive community of bacteria.[29]
Besides providing locomotion opportunities for winged animals and a conduit for the dispersal of pollen grains, spores and seeds, the atmosphere can be considered to be a habitat-type in its own right. There are metabolically active microbes present that actively reproduce and spend their whole existence airborne, with hundreds of thousands of individual organisms estimated to be present in a cubic meter of air. The airborne microbial community may be as diverse as that found in soil or other terrestrial environments, however these organisms are not evenly distributed, their densities varying spatially with altitude and environmental conditions. Aerobiology has not been studied much, but there is evidence of nitrogen fixation in clouds, and less clear evidence of carbon cycling, both facilitated by microbial activity.[39]
Whether from natural processes or the activities of man, landscapes and their associated habitat types change over time. There are the slow geomorphological changes associated with the geologic processes that cause tectonic uplift and subsidence, and the more rapid changes associated with earthquakes, landslides, storms, flooding, wildfires, coastal erosion, deforestation and changes in land use.[48] Then there are the changes in habitat types brought on by alterations in farming practices, tourism, pollution, fragmentation and climate change.[49]
Loss of habitat is the single greatest threat to any species. If an island on which an endemic organism lives becomes uninhabitable for some reason, the species will become extinct. Any type of habitat surrounded by a different habitat is in a similar situation to an island. If a forest is divided into parts by logging, with strips of cleared land separating woodland blocks, and the distances between the remaining fragments exceeds the distance an individual animal is able to travel, that species becomes especially vulnerable. Small populations generally lack genetic diversity and may be threatened by increased predation, increased competition, disease and unexpected catastrophe.[49] At the edge of each forest fragment, increased light encourages secondary growth of fast-growing species and old growth trees are more vulnerable to logging as access is improved. The birds that nest in their crevices, the epiphytes that hang from their branches and the invertebrates in the leaf litter are all adversely affected and biodiversity is reduced.[49] Habitat fragmentation can be ameliorated to some extent by the provision of wildlife corridors connecting the fragments. These can be a river, ditch, strip of trees, hedgerow or even an underpass to a highway. Without the corridors, seeds cannot disperse and animals, especially small ones, cannot travel through the hostile territory, putting populations at greater risk of local extinction.[50]
Habitat disturbance can have long-lasting effects on the environment. Bromus tectorum is a vigorous grass from Europe which has been introduced to the United States where it has become invasive. It is highly adapted to fire, producing large amounts of flammable detritus and increasing the frequency and intensity of wildfires. In areas where it has become established, it has altered the local fire regimen to such an extant that native plants cannot survive the frequent fires, allowing it to become even more dominant.[51] A marine example is when sea urchin populations "explode" in coastal waters and destroy all the macroalgae present. What was previously a kelp forest becomes an urchin barren that may last for years and this can have a profound effect on the food chain. Removal of the sea urchins, by disease for example, can result in the seaweed returning, with an over-abundance of fast-growing kelp.[52]
Habitat destruction (also termed habitat loss and habitat reduction) is the process by which a natural habitat becomes incapable of supporting its native species. The organisms that previously inhabited the site are displaced or dead, thereby reducing biodiversity and species abundance.[57][58] Habitat destruction is the leading cause of biodiversity loss.[59] Fragmentation and loss of habitat have become one of the most important topics of research in ecology as they are major threats to the survival of endangered species.[60]
Activities such as harvesting natural resources, industrial production and urbanization are human contributions to habitat destruction. Pressure from agriculture is the principal human cause. Some others include mining, logging, trawling, and urban sprawl. Habitat destruction is currently considered the primary cause of species extinction worldwide.[61] Environmental factors can contribute to habitat destruction more indirectly. Geological processes, climate change,[58] introduction of invasive species, ecosystem nutrient depletion, water and noise pollution are some examples. Loss of habitat can be preceded by an initial habitat fragmentation.
The protection of habitat types is a necessary step in the maintenance of biodiversity because if habitat destruction occurs, the animals and plants reliant on that habitat suffer. Many countries have enacted legislation to protect their wildlife. This may take the form of the setting up of national parks, forest reserves and wildlife reserves, or it may restrict the activities of humans with the objective of benefiting wildlife. The laws may be designed to protect a particular species or group of species, or the legislation may prohibit such activities as the collecting of bird eggs, the hunting of animals or the removal of plants. A general law on the protection of habitat types may be more difficult to implement than a site specific requirement. A concept introduced in the United States in 1973 involves protecting the critical habitat of endangered species, and a similar concept has been incorporated into some Australian legislation.[63]
International treaties may be necessary for such objectives as the setting up of marine reserves. Another international agreement, the Convention on the Conservation of Migratory Species of Wild Animals, protects animals that migrate across the globe and need protection in more than one country.[64] Even where legislation protects the environment, a lack of enforcement often prevents effective protection. However, the protection of habitat types needs to take into account the needs of the local residents for food, fuel and other resources. Faced with hunger and destitution, a farmer is likely to plough up a level patch of ground despite it being the last suitable habitat for an endangered species such as the San Quintin kangaroo rat, and even kill the animal as a pest.[65] In the interests of ecotourism it is desirable that local communities are educated on the uniqueness of their flora and fauna.[66]
The larva's appearance is generally very different from the adult form (e.g. caterpillars and butterflies) including different unique structures and organs that do not occur in the adult form. Their diet may also be considerably different.
Larvae are frequently adapted to different environments than adults. For example, some larvae such as tadpoles live almost exclusively in aquatic environments, but can live outside water as adult frogs. By living in a distinct environment, larvae may be given shelter from predators and reduce competition for resources with the adult population.
Animals in the larval stage will consume food to fuel their transition into the adult form. In some organisms like polychaetes and barnacles, adults are immobile but their larvae are mobile, and use their mobile larval form to distribute themselves.[1][2] These larvae used for dispersal are either planktotrophic (feeding) or lecithotrophic (non-feeding).
Some larvae are dependent on adults to feed them. In many eusocial Hymenoptera species, the larvae are fed by female workers. In Ropalidia marginata (a paper wasp) the males are also capable of feeding larvae but they are much less efficient, spending more time and getting less food to the larvae.[3]
The larvae of some organisms (for example, some newts) can become pubescent and do not develop further into the adult form. This is a type of neoteny.[4]
It is a misunderstanding that the larval form always reflects the group's evolutionary history. This could be the case, but often the larval stage has evolved secondarily, as in insects.[5][6] In these cases the larval form may differ more than the adult form from the group's common origin.[7]
Within Insects, only Endopterygotes show complete metamorphosis, including a distinct larval stage.[9][10] Several classifications have been suggested by many entomologists,[11][12] and following classification is based on Antonio Berlese classification in 1913. There are four main types of endopterygote larvae types:[13][14]
A juvenile is an individual organism (especially an animal) that has not yet reached its adult form, sexual maturity or size. Juveniles can look very different from the adult form, particularly in colour, and may not fill the same niche as the adult form.[1] In many organisms the juvenile has a different name from the adult (see List of animal names).
Many invertebrates, on reaching the adult stage, are fully mature and their development and growth stops. Their juveniles are larvae or nymphs.
In vertebrates and some invertebrates (e.g. spiders), larval forms (e.g. tadpoles) are usually considered a development stage of their own, and "juvenile" refers to a post-larval stage that is not fully grown and not sexually mature. In amniotes, the embryo represents the larval stage. Here, a "juvenile" is an individual in the time between hatching/birth/germination and reaching maturity.
This developmental biology article is a stub. You can help Wikipedia by expanding it.
Insects (from Latin insectum) are pancrustacean hexapod invertebrates of the class Insecta. They are the largest group within the arthropod phylum. Insects have a chitinous exoskeleton, a three-part body (head, thorax and abdomen), three pairs of jointed legs, compound eyes and one pair of antennae. Their blood is not totally contained in vessels; some circulates in an open cavity known as the haemocoel. Insects are the most diverse group of animals; they include more than a million described species and represent more than half of all known living organisms.[1][2] The total number of extant species is estimated at between six and ten million;[1][3][4] potentially over 90% of the animal life forms on Earth are insects.[4][5] Insects may be found in nearly all environments, although only a small number of species reside in the oceans, which are dominated by another arthropod group, crustaceans, which recent research has indicated insects are nested within.
Adult insects typically move about by walking, flying, or sometimes swimming. As it allows for rapid yet stable movement, many insects adopt a tripedal gait in which they walk with their legs touching the ground in alternating triangles, composed of the front and rear on one side with the middle on the other side. Insects are the only invertebrate group with members able to achieve sustained powered flight, and all flying insects derive from one common ancestor. Many insects spend at least part of their lives under water, with larval adaptations that include gills, and some adult insects are aquatic and have adaptations for swimming. Some species, such as water striders, are capable of walking on the surface of water. Insects are mostly solitary, but some, such as certain bees, ants and termites, are social and live in large, well-organized colonies. Some insects, such as earwigs, show maternal care, guarding their eggs and young. Insects can communicate with each other in a variety of ways. Male moths can sense the pheromones of female moths over great distances. Other species communicate with sounds: crickets stridulate, or rub their wings together, to attract a mate and repel other males. Lampyrid beetles communicate with light.
Humans regard certain insects as pests, and attempt to control them using insecticides, and a host of other techniques. Some insects damage crops by feeding on sap, leaves, fruits, or wood. Some species are parasitic, and may vector diseases. Some insects perform complex ecological roles; blow-flies, for example, help consume carrion but also spread diseases. Insect pollinators are essential to the life cycle of many flowering plant species on which most organisms, including humans, are at least partly dependent; without them, the terrestrial portion of the biosphere would be devastated.[7] Many insects are considered ecologically beneficial as predators and a few provide direct economic benefit. Silkworms produce silk and honey bees produce honey, and both have been domesticated by humans. Insects are consumed as food in 80% of the world's nations, by people in roughly 3000 ethnic groups.[8][9] Human activities also have effects on insect biodiversity.
In common parlance, insects are also called bugs, though this term usually includes all terrestrial arthropods.[a] The term is also occasionally extended to colloquial names for freshwater or marine crustaceans (e.g. Balmain bug, Moreton Bay bug, mudbug) and used by physicians and bacteriologists for disease-causing germs (e.g. superbugs), but entomologists to some extent reserve this term for a narrow category of "true bugs", insects of the order Hemiptera, such as cicadas and shield bugs.[16]
The precise definition of the taxon Insecta and the equivalent English name "insect" varies; three alternative definitions are shown in the table.
Although traditionally grouped with millipedes and centipedes,[22] more recent analysis indicates closer evolutionary ties with crustaceans. In the Pancrustacea theory, insects, together with Entognatha, Remipedia, and Cephalocarida, form a clade, the Pancrustacea.[23] Insects form a single clade, closely related to crustaceans and myriapods.[24]
Other terrestrial arthropods, such as centipedes, millipedes, scorpions, spiders, woodlice, mites, and ticks are sometimes confused with insects since their body plans can appear similar, sharing (as do all arthropods) a jointed exoskeleton. However, upon closer examination, their features differ significantly; most noticeably, they do not have the six-legged characteristic of adult insects.[25]
A phylogenetic tree of the arthropods places the insects in the context of other hexapods and the crustaceans, and the more distantly-related myriapods and chelicerates.[26]
Four large-scale radiations of insects have occurred: beetles (from about 300 million years ago), flies (from about 250 million years ago), moths and wasps (both from about 150 million years ago).[27] These four groups account for the majority of described species. 
The origins of insect flight remain obscure, since the earliest winged insects currently known appear to have been capable fliers. Some extinct insects had an additional pair of winglets attaching to the first segment of the thorax, for a total of three pairs. As of 2009, no evidence suggests the insects were a particularly successful group of animals before they evolved to have wings.[28]
The remarkably successful Hymenoptera (wasps, bees, and ants) appeared as long as 200 million years ago in the Triassic period, but achieved their wide diversity more recently in the Cenozoic era, which began 66 million years ago. Some highly successful insect groups evolved in conjunction with flowering plants, a powerful illustration of coevolution.[34]
The internal phylogeny is based on the works of Sroka, Staniczek & Bechly 2014,[35] Prokop et al. 2017[36] and Wipfler et al. 2019.[37]
Insects can be divided into two groups historically treated as subclasses: wingless insects, known as Apterygota, and winged insects, known as Pterygota. The Apterygota consisted of the primitively wingless orders Archaeognatha (jumping bristletails) and Zygentoma (silverfish). However, Apterygota is not a monophyletic group, as Archaeognatha are the sister group to all other insects, based on the arrangement of their mandibles, while Zygentoma and Pterygota are grouped together as Dicondylia. It was originally believed that Archaeognatha possessed a single phylogenetically primitive condyle each (thus the name "Monocondylia"), where all more derived insects have two, but this has since been shown to be incorrect; all insects, including Archaeognatha, have dicondylic mandibles, but archaeognaths possess two articulations that are homologous to those in other insects, though slightly different.[40] The Zygentoma themselves possibly are not monophyletic, with the family Lepidotrichidae being a sister group to the Dicondylia (Pterygota and the remaining Zygentoma).[41][42][clarification needed]
Paleoptera and Neoptera are the winged orders of insects differentiated by the presence of hardened body parts called sclerites, and in the Neoptera, muscles that allow their wings to fold flatly over the abdomen. Neoptera can further be divided into incomplete metamorphosis-based (Polyneoptera and Paraneoptera) and complete metamorphosis-based groups. It has proved difficult to clarify the relationships between the orders in Polyneoptera because of constant new findings calling for revision of the taxa. For example, the Paraneoptera have turned out to be more closely related to the Endopterygota than to the rest of the Exopterygota. The recent molecular finding that the traditional louse orders Mallophaga and Anoplura are derived from within Psocoptera has led to the new taxon Psocodea.[43] Phasmatodea and Embiidina have been suggested to form the Eukinolabia.[44] Mantodea, Blattodea, and Isoptera are thought to form a monophyletic group termed Dictyoptera.[45]
The Exopterygota likely are paraphyletic in regard to the Endopterygota. The Neuropterida are often lumped or split on the whims of the taxonomist. Fleas are now thought to be closely related to boreid mecopterans.[46] Many questions remain in the basal relationships among endopterygote orders, particularly the Hymenoptera.
Insects are prey for a variety of organisms, including terrestrial vertebrates. The earliest vertebrates on land existed 400 million years ago and were large amphibious piscivores. Through gradual evolutionary change, insectivory was the next diet type to evolve.[47]
Insects were among the earliest terrestrial herbivores and acted as major selection agents on plants.[34] Plants evolved chemical defenses against this herbivory and the insects, in turn, evolved mechanisms to deal with plant toxins. Many insects make use of these toxins to protect themselves from their predators. Such insects often advertise their toxicity using warning colors.[48] This successful evolutionary pattern has also been used by mimics. Over time, this has led to complex groups of coevolved species. Conversely, some interactions between plants and insects, like pollination, are beneficial to both organisms. Coevolution has led to the development of very specific mutualisms in such systems.
Estimates of the total number of insect species, or those within specific orders, often vary considerably. Globally, averages of these estimates suggest there are around 1.5 million beetle species and 5.5 million insect species, with about 1 million insect species currently found and described.[49] E. O. Wilson has estimated that the number of insects living at any one time are around 10 quintillion (10 billion billion).[50]
Between 950,000 and 1,000,000 of all described species are insects, so over 50% of all described eukaryotes (1.8 million) are insects (see illustration). With only 950,000 known non-insects, if the actual number of insects is 5.5 million, they may represent over 80% of the total. As only about 20,000 new species of all organisms are described each year, most insect species may remain undescribed, unless the rate of species descriptions greatly increases. Of the 24 orders of insects, four dominate in terms of numbers of described species; at least 670,000 identified species belong to Coleoptera, Diptera, Hymenoptera or Lepidoptera.
As of 2017, at least 66 insect species extinctions had been recorded in the previous 500 years, generally on oceanic islands.[52] Declines in insect abundance have been attributed to artificial lighting,[53] land use changes such as urbanization or agricultural use,[54][55] pesticide use,[56] and invasive species.[57] Studies summarized in a 2019 review suggested that a large proportion of insect species is threatened with extinction in the 21st century.[58] The ecologist Manu Sanders notes that the 2019 review was biased by mostly excluding data showing increases or stability in insect population, with the studies limited to specific geographic areas and specific groups of species.[59] A larger 2020 meta-study, analyzing data from 166 long-term surveys, suggested that populations of terrestrial insects are decreasing rapidly, by about 9% per decade.[60][61] Claims of pending mass insect extinctions or "insect apocalypse" based on a subset of these studies have been popularized in news reports, but often extrapolate beyond the study data or hyperbolize study findings.[62] Other areas have shown increases in some insect species, although trends in most regions are currently unknown. It is difficult to assess long-term trends in insect abundance or diversity because historical measurements are generally not known for many species. Robust data to assess at-risk areas or species is especially lacking for arctic and tropical regions and a majority of the southern hemisphere.[62]
The thorax is a tagma composed of three sections, the prothorax, mesothorax and the metathorax. The anterior segment, closest to the head, is the prothorax, with the major features being the first pair of legs and the pronotum. The middle segment is the mesothorax, with the major features being the second pair of legs and the anterior wings. The third and most posterior segment, abutting the abdomen, is the metathorax, which features the third pair of legs and the posterior wings. Each segment is delineated by an intersegmental suture. Each segment has four basic regions. The dorsal surface is called the tergum (or notum) to distinguish it from the abdominal terga.[38] The two lateral regions are called the pleura (singular: pleuron) and the ventral aspect is called the sternum. In turn, the notum of the prothorax is called the pronotum, the notum for the mesothorax is called the mesonotum and the notum for the metathorax is called the metanotum. Continuing with this logic, the mesopleura and metapleura, as well as the mesosternum and metasternum, are used.[64]
During growth insects goes through a various number of instars where the old exoskeleton is shed, but once they reach sexual maturity, they stop molting. The exceptions are apterygote (ancestrally wingless) insects. Mayflies are the only insects with a sexually immature instar with functional wings, called subimago.[65]
Having their muscles attached to their exoskeletons is efficient and allows more muscle connections.
The thoracic segments have one ganglion on each side, which are connected into a pair, one pair per segment. This arrangement is also seen in the abdomen but only in the first eight segments. Many species of insects have reduced numbers of ganglia due to fusion or reduction.[66] Some cockroaches have just six ganglia in the abdomen, whereas the wasp Vespa crabro has only two in the thorax and three in the abdomen. Some insects, like the house fly Musca domestica, have all the body ganglia fused into a single large thoracic ganglion. [67]
At least some insects have nociceptors, cells that detect and transmit signals responsible for the sensation of pain.[68][failed verification][69] This was discovered in 2003 by studying the variation in reactions of larvae of the common fruit-fly Drosophila to the touch of a heated probe and an unheated one. The larvae reacted to the touch of the heated probe with a stereotypical rolling behavior that was not exhibited when the larvae were touched by the unheated probe.[70] Although nociception has been demonstrated in insects, there is no consensus that insects feel pain consciously[71]
An insect uses its digestive system to extract nutrients and other substances from the food it consumes.[73] Most of this food is ingested in the form of macromolecules and other complex substances like proteins, polysaccharides, fats and nucleic acids. These macromolecules must be broken down by catabolic reactions into smaller molecules like amino acids and simple sugars before being used by cells of the body for energy, growth, or reproduction. This break-down process is known as digestion.
There is extensive variation among different orders, life stages, and even castes in the digestive system of insects.[74] This is the result of extreme adaptations to various lifestyles. The present description focuses on a generalized composition of the digestive system of an adult orthopteroid insect, which is considered basal to interpreting particularities of other groups.
Digestion starts in buccal cavity (mouth) as partially chewed food is broken down by saliva from the salivary glands. As the salivary glands produce fluid and carbohydrate-digesting enzymes (mostly amylases), strong muscles in the pharynx pump fluid into the buccal cavity, lubricating the food like the salivarium does, and helping blood feeders, and xylem and phloem feeders.
In the hindgut (element 16 in numbered diagram), or proctodaeum, undigested food particles are joined by uric acid to form fecal pellets. The rectum absorbs 90% of the water in these fecal pellets, and the dry pellet is then eliminated through the anus (element 17), completing the process of digestion. Envaginations at the anterior end of the hindgut form the Malpighian tubules, which form the main excretory system of insects.
Insect respiration is accomplished without lungs. Instead, the insect respiratory system uses a system of internal tubes and sacs through which gases either diffuse or are actively pumped, delivering oxygen directly to tissues that need it via their trachea (element 8 in numbered diagram). In most insects, air is taken in through openings on the sides of the abdomen and thorax called spiracles.
Some insects use parthenogenesis, a process in which the female can reproduce and give birth without having the eggs fertilized by a male. Many aphids undergo a form of parthenogenesis, called cyclical parthenogenesis, in which they alternate between one or many generations of asexual and sexual reproduction.[90][91] In summer, aphids are generally female and parthenogenetic; in the autumn, males may be produced for sexual reproduction. Other insects produced by parthenogenesis are bees, wasps and ants, in which they spawn males. However, overall, most individuals are female, which are produced by fertilization. The males are haploid and the females are diploid.[6]
Insect life-histories show adaptations to withstand cold and dry conditions. Some temperate region insects are capable of activity during winter, while some others migrate to a warmer climate or go into a state of torpor.[92] Still other insects have evolved mechanisms of diapause that allow eggs or pupae to survive these conditions.[93]
Metamorphosis in insects is the biological process of development all insects must undergo. There are two forms of metamorphosis: incomplete metamorphosis and complete metamorphosis.
Immature insects that go through incomplete metamorphosis are called nymphs or in the case of dragonflies and damselflies, also naiads. Nymphs are similar in form to the adult except for the presence of wings, which are not developed until adulthood. With each molt, nymphs grow larger and become more similar in appearance to adult insects.
Some insects display a rudimentary sense of numbers,[98] such as the solitary wasps that prey upon a single species. The mother wasp lays her eggs in individual cells and provides each egg with a number of live caterpillars on which the young feed when hatched. Some species of wasp always provide five, others twelve, and others as high as twenty-four caterpillars per cell. The number of caterpillars is different among species, but always the same for each sex of larva. The male solitary wasp in the genus Eumenes is smaller than the female, so the mother of one species supplies him with only five caterpillars; the larger female receives ten caterpillars in her cell.
A few insects, such as members of the families Poduridae and Onychiuridae (Collembola), Mycetophilidae (Diptera) and the beetle families Lampyridae, Phengodidae, Elateridae and Staphylinidae are bioluminescent. The most familiar group are the fireflies, beetles of the family Lampyridae. Some species are able to control this light generation to produce flashes. The function varies with some species using them to attract mates, while others use them to lure prey. Cave dwelling larvae of Arachnocampa (Mycetophilidae, fungus gnats) glow to lure small flying insects into sticky strands of silk.[99] Some fireflies of the genus Photuris mimic the flashing of female Photinus species to attract males of that species, which are then captured and devoured.[100] The colors of emitted light vary from dull blue (Orfelia fultoni, Mycetophilidae) to the familiar greens and the rare reds (Phrixothrix tiemanni, Phengodidae).[101]
Most insects, except some species of cave crickets, are able to perceive light and dark. Many species have acute vision capable of detecting minute movements. The eyes may include simple eyes or ocelli as well as compound eyes of varying sizes. Many species are able to detect light in the infrared, ultraviolet and visible light wavelengths. Color vision has been demonstrated in many species and phylogenetic analysis suggests that UV-green-blue trichromacy existed from at least the Devonian period between 416 and 359 million years ago.[102]
The individual lenses in compound eyes are immobile, and it was therefore presumed that insects were not able to focus. But research on fruit flies, which is the only insects studied so far, has shown that photoreceptor cells underneath each lens move rapidly in and out of focus in a series of movements called photoreceptor microsaccades. This gives them a much clearer image of the world than previously assumed.[103]
Very low sounds are also produced in various species of Coleoptera, Hymenoptera, Lepidoptera, Mantodea and Neuroptera. These low sounds are simply the sounds made by the insect's movement. Through microscopic stridulatory structures located on the insect's muscles and joints, the normal sounds of the insect moving are amplified and can be used to warn or communicate with other insects. Most sound-making insects also have tympanal organs that can perceive airborne sounds. Some species in Hemiptera, such as the corixids (water boatmen), are known to communicate via underwater sounds.[111] Most insects are also able to sense vibrations transmitted through surfaces.
Communication using surface-borne vibrational signals is more widespread among insects because of size constraints in producing air-borne sounds.[112] Insects cannot effectively produce low-frequency sounds, and high-frequency sounds tend to disperse more in a dense environment (such as foliage), so insects living in such environments communicate primarily using substrate-borne vibrations.[113] The mechanisms of production of vibrational signals are just as diverse as those for producing sound in insects.
Some species use vibrations for communicating within members of the same species, such as to attract mates as in the songs of the shield bug Nezara viridula.[114] Vibrations can also be used to communicate between entirely different species; lycaenid (gossamer-winged butterfly) caterpillars, which are myrmecophilous (living in a mutualistic association with ants) communicate with ants in this way.[115] The Madagascar hissing cockroach has the ability to press air through its spiracles to make a hissing noise as a sign of aggression;[116] the death's-head hawkmoth makes a squeaking noise by forcing air out of their pharynx when agitated, which may also reduce aggressive worker honey bee behavior when the two are close.[117]
Chemical communications in animals rely on a variety of aspects including taste and smell. Chemoreception is the physiological response of a sense organ (i.e. taste or smell) to a chemical stimulus where the chemicals act as signals to regulate the state or activity of a cell. A semiochemical is a message-carrying chemical that is meant to attract, repel, and convey information. Types of semiochemicals include pheromones and kairomones. One example is the butterfly Phengaris arion which uses chemical signals as a form of mimicry to aid in predation.[118]
The eusocial insects build nests, guard eggs, and provide food for offspring full-time.
Most insects, however, lead short lives as adults, and rarely interact with one another except to mate or compete for mates. A small number exhibit some form of parental care, where they will at least guard their eggs, and sometimes continue guarding their offspring until adulthood, and possibly even feeding them. Another simple form of parental care is to construct a nest (a burrow or an actual construction, either of which may be simple or complex), store provisions in it, and lay an egg upon those provisions. The adult does not contact the growing offspring, but it nonetheless does provide food. This sort of care is typical for most species of bees and various types of wasps.[123]
Insect flight has been a topic of great interest in aerodynamics due partly to the inability of steady-state theories to explain the lift generated by the tiny wings of insects. But insect wings are in motion, with flapping and vibrations, resulting in churning and eddies, and the misconception that physics says "bumblebees can't fly" persisted throughout most of the twentieth century.
Unlike birds, many small insects are swept along by the prevailing winds[127] although many of the larger insects are known to make migrations. Aphids are known to be transported long distances by low-level jet streams.[128] As such, fine line patterns associated with converging winds within weather radar imagery, like the WSR-88D radar network, often represent large groups of insects.[129] Radar can also be deliberately used to monitor insects.[130]
Many adult insects use six legs for walking, with an alternating tripod gait. This allows for rapid walking while always having a stable stance; it has been studied extensively in cockroaches and ants. For the first step, the middle right leg and the front and rear left legs are in contact with the ground and move the insect forward, while the front and rear right leg and the middle left leg are lifted and moved forward to a new position. When they touch the ground to form a new stable triangle the other legs can be lifted and brought forward in turn and so on.[131] The purest form of the tripedal gait is seen in insects moving at high speeds. However, this type of locomotion is not rigid and insects can adapt a variety of gaits. For example, when moving slowly, turning, avoiding obstacles, climbing or slippery surfaces, four (tetrapod) or more feet (wave-gait[132]) may be touching the ground. Insects can also adapt their gait to cope with the loss of one or more limbs.
Cockroaches are among the fastest insect runners and, at full speed, adopt a bipedal run to reach a high velocity in proportion to their body size. As cockroaches move very quickly, they need to be video recorded at several hundred frames per second to reveal their gait. More sedate locomotion is seen in the stick insects or walking sticks (Phasmatodea). A few insects have evolved to walk on the surface of the water, especially members of the Gerridae family, commonly known as water striders. A few species of ocean-skaters in the genus Halobates even live on the surface of open oceans, a habitat that has few insect species.[133]
Insect walking is of particular interest as practical form of robot locomotion. The study of insects and bipeds has a significant impact on possible robotic methods of transport. This may allow new hexapod robots to be designed that can traverse terrain that robots with wheels may be unable to handle.[131]
A large number of insects live either part or the whole of their lives underwater. In many of the more primitive orders of insect, the immature stages are spent in an aquatic environment. Some groups of insects, like certain water beetles, have aquatic adults as well.[80]
Many of these species have adaptations to help in under-water locomotion. Water beetles and water bugs have legs adapted into paddle-like structures. Dragonfly naiads use jet propulsion, forcibly expelling water out of their rectal chamber.[134] Some species like the water striders are capable of walking on the surface of water. They can do this because their claws are not at the tips of the legs as in most insects, but recessed in a special groove further up the leg; this prevents the claws from piercing the water's surface film.[80] Other insects such as the Rove beetle Stenus are known to emit pygidial gland secretions that reduce surface tension making it possible for them to move on the surface of water by Marangoni propulsion (also known by the German term Entspannungsschwimmen).[135][136]
Insects are mostly soft bodied, fragile and almost defenseless compared to other, larger lifeforms. The immature stages are small, move slowly or are immobile, and so all stages are exposed to predation and parasitism. Insects then have a variety of defense strategies to avoid being attacked by predators or parasitoids. These include camouflage, mimicry, toxicity and active defense.[140]
Camouflage is an important defense strategy, which involves the use of coloration or shape to blend into the surrounding environment.[141] This sort of protective coloration is common and widespread among beetle families, especially those that feed on wood or vegetation, such as many of the leaf beetles (family Chrysomelidae) or weevils. In some of these species, sculpturing or various colored scales or hairs cause the beetle to resemble bird dung or other inedible objects. Many of those that live in sandy environments blend in with the coloration of the substrate.[140] Most phasmids are known for effectively replicating the forms of sticks and leaves, and the bodies of some species (such as O. macklotti and Palophus centaurus) are covered in mossy or lichenous outgrowths that supplement their disguise. Very rarely, a species may have the ability to change color as their surroundings shift (Bostra scabrinota). In a further behavioral adaptation to supplement crypsis, a number of species have been noted to perform a rocking motion where the body is swayed from side to side that is thought to reflect the movement of leaves or twigs swaying in the breeze. Another method by which stick insects avoid predation and resemble twigs is by feigning death (catalepsy), where the insect enters a motionless state that can be maintained for a long period. The nocturnal feeding habits of adults also aids Phasmatodea in remaining concealed from predators.[142]
Pollination is the process by which pollen is transferred in the reproduction of plants, thereby enabling fertilisation and sexual reproduction. Most flowering plants require an animal to do the transportation. While other animals are included as pollinators, the majority of pollination is done by insects.[145] Because insects usually receive benefit for the pollination in the form of energy rich nectar it is a grand example of mutualism. The various flower traits (and combinations thereof) that differentially attract one type of pollinator or another are known as pollination syndromes. These arose through complex plant-animal adaptations. Pollinators find flowers through bright colorations, including ultraviolet, and attractant pheromones. The study of pollination by insects is known as anthecology.
Many insects are parasites of other insects such as the parasitoid wasps. These insects are known as entomophagous parasites. They can be beneficial due to their devastation of pests that can destroy crops and other resources. Many insects have a parasitic relationship with humans such as the mosquito. These insects are known to spread diseases such as malaria and yellow fever and because of such, mosquitoes indirectly cause more deaths of humans than any other animal.
Despite the large amount of effort focused at controlling insects, human attempts to kill pests with insecticides can backfire. If used carelessly, the poison can kill all kinds of organisms in the area, including insects' natural predators, such as birds, mice and other insectivores. The effects of DDT's use exemplifies how some insecticides can threaten wildlife beyond intended populations of pest insects.[148][149]
The economic value of pollination by insects has been estimated to be about $34 billion in the US alone.[153]
Products made by insects. Insects also produce useful substances such as honey, wax, lacquer and silk. Honey bees have been cultured by humans for thousands of years for honey, although contracting for crop pollination is becoming more significant for beekeepers. The silkworm has greatly affected human history, as silk-driven trade established relationships between China and the rest of the world.
Medical uses. Insects are also used in medicine, for example fly larvae (maggots) were formerly used to treat wounds to prevent or stop gangrene, as they would only consume dead flesh. This treatment is finding modern usage in some hospitals. Recently insects have also gained attention as potential sources of drugs and other medicinal substances.[157] Adult insects, such as crickets and insect larvae of various kinds, are also commonly used as fishing bait.[158]
Insects play important roles in biological research. For example, because of its small size, short generation time and high fecundity, the common fruit fly Drosophila melanogaster is a model organism for studies in the genetics of eukaryotes. D. melanogaster has been an essential part of studies into principles like genetic linkage, interactions between genes, chromosomal genetics, development, behavior and evolution. Because genetic systems are well conserved among eukaryotes, understanding basic cellular processes like DNA replication or transcription in fruit flies can help to understand those processes in other eukaryotes, including humans.[159] The genome of D. melanogaster was sequenced in 2000, reflecting the organism's important role in biological research. It was found that 70% of the fly genome is similar to the human genome, supporting the evolution theory.[160]
Because of the abundance of insects and a worldwide concern of food shortages, the Food and Agriculture Organization of the United Nations considers that the world may have to, in the future, regard the prospects of eating insects as a food staple. Insects are noted for their nutrients, having a high content of protein, minerals and fats and are eaten by one-third of the global population.[164]
Several insect species such as the black soldier fly or the housefly in their maggot forms, as well as beetle larvae such as mealworms can be processed and used as feed for farmed animals such as chicken, fish and pigs.[165]
Black soldier fly larvae can provide protein, fats for use in cosmetics,[166] and chitin.
Also, insect cooking oil, insect butter and fatty alcohols can be made from such insects as the superworm (Zophobas morio).[167][168]
Many species of insects are sold and kept as pets. There are special hobbyist magazines such as "Bugs" (now discontinued).[169]
A tadpole is the larval stage in the biological life cycle of an amphibian. Most tadpoles are fully aquatic, though some species of amphibians have tadpoles that are terrestrial. Tadpoles have some fish-like features that may not be found in adult amphibians such as a lateral line, gills and swimming tails. As they undergo metamorphosis, they start to develop functional lungs for breathing air, and the diet of tadpoles changes drastically.
Tadpoles are eaten as human food in some parts of the world and are mentioned in various folk tales from around the world.
The name tadpole is from Middle English taddepol, made up of the elements tadde, 'toad', and pol, 'head' (modern English poll). Similarly, pollywog / polliwog is from Middle English polwygle, made up of the same pol, 'head', and wiglen, 'to wiggle'.[1]
The life cycle of all amphibians involves a larval stage that is intermediate between embryo and adult. In most cases this larval stage is a limbless free-living organism that has a tail and is referred to as a tadpole, although in a few cases (e.g., in the Breviceps and Probreviceps genera of frogs) direct development occurs in which the larval stage is confined within the egg. Tadpoles of frogs are mostly herbivorous, while tadpoles of salamanders and caecilians are carnivorous.
Tadpoles of frogs and toads are usually globular, with a laterally compressed tail with which they swim by lateral undulation. When first hatched, anuran tadpoles have external gills that are eventually covered by skin, forming an opercular chamber with internal gills vented by spiracles. Depending on the species, there can be two spiracles on both sides of the body, a single spiracle on the underside near the vent, or a single spiracle on the left side of the body.[2] Newly hatched tadpoles are also equipped with a cement gland which allows them to attach to objects. The tadpoles have a cartilaginous skeleton and a notochord which eventually develops into a proper spinal cord. 
Anuran tadpoles are usually herbivorous, feeding on soft decaying plant matter. The gut of most tadpoles is long and spiral-shaped to efficiently digest organic matter and can be seen through the bellies of many species. Though many tadpoles will feed on dead animals if available to them, only a few species of frog have strictly carnivorous tadpoles, an example being the frogs of the family Ceratophryidae, their cannibalistic tadpoles having wide gaping mouths with which they devour other organisms, including other tadpoles. Another example is the tadpoles of the New Mexico spadefoot toad (Spea multiplicata) which will develop a carnivorous diet along with a broader head, larger jaw muscles, and a shorter gut if food is scarce, allowing them to consume fairy shrimp and their smaller herbivorous siblings.[3] A few genera such as Pipidae and Microhylidae have species whose tadpoles are filter feeders that swim through the water column feeding on plankton. Megophrys tadpoles feed at the water surface using unusual funnel-shaped mouths.[4]
As a frog tadpole matures it gradually develops its limbs, with the back legs growing first and the front legs second. The tail is absorbed into the body using apoptosis. Lungs develop around the time as the legs start growing, and tadpoles at this stage will often swim to the surface and gulp air. During the final stages of metamorphosis, the tadpole's mouth changes from a small, enclosed mouth at the front of the head to a large mouth the same width as the head. The intestines shorten as they transition from a herbivorous diet to the carnivorous diet of adult frogs.
While most anuran tadpoles inhabit wetlands, ponds, vernal pools, and other small bodies of water with slow moving water, a few species are adapted to different environments. Some frogs have terrestrial tadpoles, such as the family Ranixalidae, whose tadpoles are found in wet crevices near streams. The tadpoles of Micrixalus herrei are adapted to a fossorial lifestyle, with a muscular body and tail, eyes covered by a layer of skin, and reduced pigment.[8] Several frogs have stream dwelling tadpoles equipped with a strong oral sucker that allows them to hold onto rocks in fast flowing water, two examples being the Indian purple frog (Nasikabatrachus sahyadrensis) and the tailed frogs (Ascaphus) of Western North America. Although there are no marine tadpoles, the tadpoles of the crab-eating frog can cope with brackish water.[9]
Some anurans will provide parental care towards their tadpoles. Frogs of the genus Afrixalus will lay their eggs on leaves above water, folding the leaves around the eggs for protection. Female Pipa frogs will embed the eggs into their backs where they get covered by a thin layer of skin. The eggs will hatch underneath her skin and grow, eventually leaving as either large tadpoles (such as in Pipa parva) or as fully formed froglets (Pipa pipa). Female marsupial frogs (Hemiphractidae) will carry eggs on her back for various amounts of time, with it going as far as letting the tadpoles develop into tiny froglets in a pouch. Male African bullfrogs (Pyxicephalus adspersus) will keep watch over their tadpoles, attacking anything that might be a potential threat, even though he may eat some of the tadpoles himself.[10]
Males of the Emei mustache toads (Leptobrachium boringii) will construct nests along riverbanks where they breed with females and keep watch over the eggs, losing as much as 7.3% of their body mass in the time they spend protecting the nest.[11] Male midwife toads (Alytes) will carry eggs between their legs to protect them from predators, eventually releasing them into a body of water when they are ready to hatch. Poison dart frogs (Dendrobatidae) will carry their tadpoles to various locations, usually phytotelma, where they remain until metamorphosis. Some female dart frogs such as the strawberry poison dart frog (Oophaga pumilio) will regularly lay unfertilized eggs for the developing tadpoles to feed on.[12]
According to Sir George Scott, in the origin myths of the Wa people in China and Myanmar, the first Wa originated from two female ancestors Ya Htawm and Ya Htai, who spent their early phase as tadpoles ("rairoh") in a lake in the Wa country known as Nawng Hkaeo.[18]
In the Ancient Egyptian numerals, a hieroglyphic representing a tadpole was used to denote the value of 100,000.
Collective intelligence
Collective action
Self-organized criticality
Herd mentality
Phase transition
Agent-based modelling
Synchronization
Ant colony optimization
Particle swarm optimization
Swarm behaviour
Evolutionary computation
Genetic algorithms
Genetic programming
Artificial life
Machine learning
Evolutionary developmental biology
Artificial intelligence
Evolutionary robotics
In biology, adaptation has three related meanings. Firstly, it is the dynamic evolutionary process of natural selection that fits organisms to their environment, enhancing their evolutionary fitness. Secondly, it is a state reached by the population during that process. Thirdly, it is a phenotypic trait or adaptive trait, with a functional role in each individual organism, that is maintained and has evolved through natural selection. 
Historically, adaptation has been described from the time of the ancient Greek philosophers such as Empedocles and Aristotle. In 18th and 19th century natural theology, adaptation was taken as evidence for the existence of a deity. Charles Darwin proposed instead that it was explained by natural selection.
Adaptation is a major topic in the philosophy of biology, as it concerns function and purpose (teleology). Some biologists try to avoid terms which imply purpose in adaptation, not least because it suggests a deity's intentions, but others note that adaptation is necessarily purposeful.
Adaptation is an observable fact of life accepted by philosophers and natural historians from ancient times, independently of their views on evolution, but their explanations differed. Empedocles did not believe that adaptation required a final cause (a purpose), but thought that it "came about naturally, since such things survived." Aristotle did believe in final causes, but assumed that species were fixed.[1]
In natural theology, adaptation was interpreted as the work of a deity and as evidence for the existence of God.[2] William Paley believed that organisms were perfectly adapted to the lives they led, an argument that shadowed Gottfried Wilhelm Leibniz, who had argued that God had brought about "the best of all possible worlds." Voltaire's satire Dr. Pangloss[3] is a parody of this optimistic idea, and David Hume also argued against design.[4] Charles Darwin broke with the tradition by emphasising the flaws and limitations which occurred in the animal and plant worlds.[5]
Jean-Baptiste Lamarck proposed a tendency for organisms to become more complex, moving up a ladder of progress, plus "the influence of circumstances," usually expressed as use and disuse.[6] This second, subsidiary element of his theory is what is now called Lamarckism, a proto-evolutionary hypothesis of the inheritance of acquired characteristics, intended to explain adaptations by natural means.[7]
Other natural historians, such as Buffon, accepted adaptation, and some also accepted evolution, without voicing their opinions as to the mechanism. This illustrates the real merit of Darwin and Alfred Russel Wallace, and secondary figures such as Henry Walter Bates, for putting forward a mechanism whose significance had only been glimpsed previously. A century later, experimental field studies and breeding experiments by people such as E. B. Ford and Theodosius Dobzhansky produced evidence that natural selection was not only the 'engine' behind adaptation, but was a much stronger force than had previously been thought.[8][9][10]
The significance of an adaptation can only be understood in relation to the total biology of the species.
Adaptation is primarily a process rather than a physical form or part of a body.[12] An internal parasite (such as a liver fluke) can illustrate the distinction: such a parasite may have a very simple bodily structure, but nevertheless the organism is highly adapted to its specific environment. From this we see that adaptation is not just a matter of visible traits: in such parasites critical adaptations take place in the life cycle, which is often quite complex.[13] However, as a practical term, "adaptation" often refers to a product: those features of a species which result from the process. Many aspects of an animal or plant can be correctly called adaptations, though there are always some features whose function remains in doubt. By using the term adaptation for the evolutionary process, and adaptive trait for the bodily part or function (the product), one may distinguish the two different senses of the word.[14][15][16][17]
Adaptation is one of the two main processes that explain the observed diversity of species, such as the different species of Darwin's finches. The other process is speciation, in which new species arise, typically through reproductive isolation.[18][19] An example widely used today to study the interplay of adaptation and speciation is the evolution of cichlid fish in African lakes, where the question of reproductive isolation is complex.[20][21]
Adaptation is not always a simple matter where the ideal phenotype evolves for a given environment. An organism must be viable at all stages of its development and at all stages of its evolution. This places constraints on the evolution of development, behaviour, and structure of organisms. The main constraint, over which there has been much debate, is the requirement that each genetic and phenotypic change during evolution should be relatively small, because developmental systems are so complex and interlinked. However, it is not clear what "relatively small" should mean, for example polyploidy in plants is a reasonably common large genetic change.[22] The origin of eukaryotic endosymbiosis is a more dramatic example.[23]
All adaptations help organisms survive in their ecological niches. The adaptive traits may be structural, behavioural or physiological. Structural adaptations are physical features of an organism, such as shape, body covering, armament, and internal organization. Behavioural adaptations are inherited systems of behaviour, whether inherited in detail as instincts, or as a neuropsychological capacity for learning. Examples include searching for food, mating, and vocalizations. Physiological adaptations permit the organism to perform special functions such as making venom, secreting slime, and phototropism, but also involve more general functions such as growth and development, temperature regulation, ionic balance and other aspects of homeostasis. Adaptation affects all aspects of the life of an organism.[24]
The following definitions are given by the evolutionary biologist Theodosius Dobzhansky:
Adaptation differs from flexibility, acclimatization, and learning, all of which are changes during life which are not inherited. Flexibility deals with the relative capacity of an organism to maintain itself in different habitats: its degree of specialization. Acclimatization describes automatic physiological adjustments during life;[30] learning means improvement in behavioural performance during life.[31]
If humans move to a higher altitude, respiration and physical exertion become a problem, but after spending time in high altitude conditions they acclimatize to the reduced partial pressure of oxygen, such as by producing more red blood cells. The ability to acclimatize is an adaptation, but the acclimatization itself is not. The reproductive rate declines, but deaths from some tropical diseases also go down. Over a longer period of time, some people are better able to reproduce at high altitudes than others. They contribute more heavily to later generations, and gradually by natural selection the whole population becomes adapted to the new conditions. This has demonstrably occurred, as the observed performance of long-term communities at higher altitude is significantly better than the performance of new arrivals, even when the new arrivals have had time to acclimatize.[35]
There is a relationship between adaptedness and the concept of fitness used in population genetics. Differences in fitness between genotypes predict the rate of evolution by natural selection. Natural selection changes the relative frequencies of alternative phenotypes, insofar as they are heritable.[36] However, a phenotype with high adaptedness may not have high fitness. Dobzhansky mentioned the example of the Californian redwood, which is highly adapted, but a relict species in danger of extinction.[25] Elliott Sober commented that adaptation was a retrospective concept since it implied something about the history of a trait, whereas fitness predicts a trait's future.[37]
Sewall Wright proposed that populations occupy adaptive peaks on a fitness landscape. To evolve to another, higher peak, a population would first have to pass through a valley of maladaptive intermediate stages, and might be "trapped" on a peak that is not optimally adapted.[41]
Before Darwin, adaptation was seen as a fixed relationship between an organism and its habitat. It was not appreciated that as the climate changed, so did the habitat; and as the habitat changed, so did the biota. Also, habitats are subject to changes in their biota: for example, invasions of species from other areas. The relative numbers of species in a given habitat are always changing. Change is the rule, though much depends on the speed and degree of the change.
When the habitat changes, three main things may happen to a resident population: habitat tracking, genetic change or extinction. In fact, all three things may occur in sequence. Of these three effects only genetic change brings about adaptation.
When a habitat changes, the resident population typically moves to more suitable places; this is the typical response of flying insects or oceanic organisms, which have wide (though not unlimited) opportunity for movement.[43] This common response is called habitat tracking. It is one explanation put forward for the periods of apparent stasis in the fossil record (the punctuated equilibrium theory).[44]
Without mutation, the ultimate source of all genetic variation, there would be no genetic changes and no subsequent adaptation through evolution by natural selection. Genetic change occurs in a population when mutation increases or decreases in its initial frequency followed by random genetic drift, migration, recombination or natural selection act on this genetic variation.[45] One example is that the first pathways of enzyme-based metabolism at the very origin of life on Earth may have been co-opted components of the already-existing purine nucleotide metabolism, a metabolic pathway that evolved in an ancient RNA world. The co-option requires new mutations and through natural selection, the population then adapts genetically to its present circumstances.[10] Genetic changes may result in entirely new or gradual change to visible structures, or they may adjust physiological activity in a way that suits the habitat. The varying shapes of the beaks of Darwin's finches, for example, are driven by adaptive mutations in the ALX1 gene.[46] The coat color of different wild mouse species matches their environments, whether black lava or light sand, owing to adaptive mutations in the melanocortin 1 receptor and other melanin pathway genes.[47][48] Physiological resistance to the heart poisons (cardiac glycosides) that monarch butterflies store in their bodies to protect themselves from predators[49][50] are driven by adaptive mutations in the target of the poison, the sodium pump, resulting in target site insensitivity.[51][52][53] These same adaptive mutations and similar changes at the same amino acid sites were found to evolve in a parallel manner in distantly related insects that feed on the same plants, and even in a bird that feeds on monarchs through convergent evolution, a hallmark of adaptation.[54][55] Convergence at the gene-level across distantly related species can arise because of evolutionary constraint.[56]
Habitats and biota do frequently change over time and space. Therefore, it follows that the process of adaptation is never fully complete.[57] Over time, it may happen that the environment changes little, and the species comes to fit its surroundings better and better, resulting in stabilizing selection. On the other hand, it may happen that changes in the environment occur suddenly, and then the species becomes less and less well adapted. The only way for it to climb back up that fitness peak is via the introduction of new genetic variation for natural selection to act upon. Seen like this, adaptation is a genetic tracking process, which goes on all the time to some extent, but especially when the population cannot or does not move to another, less hostile area. Given enough genetic change, as well as specific demographic conditions, an adaptation may be enough to bring a population back from the brink of extinction in a process called evolutionary rescue. Adaptation does affect, to some extent, every species in a particular ecosystem.[58][59]
Leigh Van Valen thought that even in a stable environment, because of antagonistic species interactions and limited resources, a species must constantly had to adapt to maintain its relative standing. This became known as the Red Queen hypothesis, as seen in host-parasite interactions.[60]
Existing genetic variation and mutation were the traditional sources of material on which natural selection could act. In addition, horizontal gene transfer is possible between organisms in different species, using mechanisms as varied as gene cassettes, plasmids, transposons and viruses such as bacteriophages.[61][62][63]
In coevolution, where the existence of one species is tightly bound up with the life of another species, new or 'improved' adaptations which occur in one species are often followed by the appearance and spread of corresponding features in the other species. In other words, each species triggers reciprocal natural selection in the other. These co-adaptational relationships are intrinsically dynamic, and may continue on a trajectory for millions of years, as has occurred in the relationship between flowering plants and pollinating insects.[64][65]
It is a profound truth that Nature does not know best; that genetical evolution... is a story of waste, makeshift, compromise and blunder.
All adaptations have a downside: horse legs are great for running on grass, but they can't scratch their backs; mammals' hair helps temperature, but offers a niche for ectoparasites; the only flying penguins do is under water. Adaptations serving different functions may be mutually destructive. Compromise and makeshift occur widely, not perfection. Selection pressures pull in different directions, and the adaptation that results is some kind of compromise.[72]
Since the phenotype as a whole is the target of selection, it is impossible to improve simultaneously all aspects of the phenotype to the same degree.
Consider the antlers of the Irish elk, (often supposed to be far too large; in deer antler size has an allometric relationship to body size). Obviously, antlers serve positively for defence against predators, and to score victories in the annual rut. But they are costly in terms of resources. Their size during the last glacial period presumably depended on the relative gain and loss of reproductive capacity in the population of elks during that time.[74] As another example, camouflage to avoid detection is destroyed when vivid coloration is displayed at mating time. Here the risk to life is counterbalanced by the necessity for reproduction.[75]
Stream-dwelling salamanders, such as Caucasian salamander or Gold-striped salamander have very slender, long bodies, perfectly adapted to life at the banks of fast small rivers and mountain brooks. Elongated body protects their larvae from being washed out by current. However, elongated body increases risk of desiccation and decreases dispersal ability of the salamanders; it also negatively affects their fecundity. As a result, fire salamander, less perfectly adapted to the mountain brook habitats, is in general more successful, have a higher fecundity and broader geographic range.[76]
The peacock's ornamental train (grown anew in time for each mating season) is a famous adaptation. It must reduce his maneuverability and flight, and is hugely conspicuous; also, its growth costs food resources. Darwin's explanation of its advantage was in terms of sexual selection: "This depends on the advantage which certain individuals have over other individuals of the same sex and species, in exclusive relation to reproduction."[77] The kind of sexual selection represented by the peacock is called 'mate choice,' with an implication that the process selects the more fit over the less fit, and so has survival value.[78] The recognition of sexual selection was for a long time in abeyance, but has been rehabilitated.[79]
Pre-adaptation occurs when a population has characteristics which by chance are suited for a set of conditions not previously experienced. For example, the polyploid cordgrass Spartina townsendii is better adapted than either of its parent species to their own habitat of saline marsh and mud-flats.[86] Among domestic animals, the White Leghorn chicken is markedly more resistant to vitamin B1 deficiency than other breeds; on a plentiful diet this makes no difference, but on a restricted diet this preadaptation could be decisive.[87]
Features that now appear as adaptations sometimes arose by co-option of existing traits, evolved for some other purpose. The classic example is the ear ossicles of mammals, which we know from paleontological and embryological evidence originated in the upper and lower jaws and the hyoid bone of their synapsid ancestors, and further back still were part of the gill arches of early fish.[91][92] The word exaptation was coined to cover these common evolutionary shifts in function.[93] The flight feathers of birds evolved from the much earlier feathers of dinosaurs,[94] which might have been used for insulation or for display.[95][96]
Animals including earthworms, beavers and humans use some of their adaptations to modify their surroundings, so as to maximize their chances of surviving and reproducing. Beavers create dams and lodges, changing the ecosystems of the valleys around them. Earthworms, as Darwin noted, improve the topsoil in which they live by incorporating organic matter. Humans have constructed extensive civilizations with cities in environments as varied as the Arctic and hot deserts.
In all three cases, the construction and maintenance of ecological niches helps drive the continued selection of the genes of these animals, in an environment that the animals have modified.[97]
Some traits do not appear to be adaptive as they have a neutral or deleterious effect on fitness in the current environment. Because genes often have pleiotropic effects, not all traits may be functional: they may be what Stephen Jay Gould and Richard Lewontin called spandrels, features brought about by neighbouring adaptations, on the analogy with the often highly decorated triangular areas between pairs of arches in architecture, which began as functionless features.[98]
Another possibility is that a trait may have been adaptive at some point in an organism's evolutionary history, but a change in habitats caused what used to be an adaptation to become unnecessary or even maladapted. Such adaptations are termed vestigial. Many organisms have vestigial organs, which are the remnants of fully functional structures in their ancestors. As a result of changes in lifestyle the organs became redundant, and are either not functional or reduced in functionality. Since any structure represents some kind of cost to the general economy of the body, an advantage may accrue from their elimination once they are not functional. Examples: wisdom teeth in humans; the loss of pigment and functional eyes in cave fauna; the loss of structure in endoparasites.[99]
If a population cannot move or change sufficiently to preserve its long-term viability, then obviously, it will become extinct, at least in that locale. The species may or may not survive in other locales. Species extinction occurs when the death rate over the entire species exceeds the birth rate for a long enough period for the species to disappear. It was an observation of Van Valen that groups of species tend to have a characteristic and fairly regular rate of extinction.[100]
Just as there is co-adaptation, there is also coextinction, the loss of a species due to the extinction of another with which it is coadapted, as with the extinction of a parasitic insect following the loss of its host, or when a flowering plant loses its pollinator, or when a food chain is disrupted.[101][102]
The first stage in the evolution of life on earth is often hypothesized to be the RNA world in which short self-replicating RNA molecules proliferated before the evolution of DNA and proteins. By this hypothesis, life started when RNA chains began to self-replicate, initiating the three mechanisms of Darwinian selection: heritability, variation of type, and competition for resources. The fitness of an RNA replicator (its per capita rate of increase) would likely have been a function of its intrinsic adaptive capacities, determined by its nucleotide sequence, and the availability of resources.[103][104] The three primary adaptive capacities may have been: (1) replication with moderate fidelity, giving rise to heritability while allowing variation of type, (2) resistance to decay, and (3) acquisition of resources.[103][104] These adaptive capacities would have been determined by the folded configurations of the RNA replicators resulting from their nucleotide sequences.
Tetrodotoxin (TTX) is a potent neurotoxin. Its name derives from Tetraodontiformes, an order that includes pufferfish, porcupinefish, ocean sunfish, and triggerfish; several of these species carry the toxin. Although tetrodotoxin was discovered in these fish and found in several other animals (e.g., in blue-ringed octopuses, rough-skinned newts, and moon snails), it is actually produced by certain infecting or symbiotic bacteria like Pseudoalteromonas, Pseudomonas, and Vibrio as well as other species found in animals.[1][2]
Tetrodotoxin is a sodium channel blocker. It inhibits the firing of action potentials in neurons by binding to the voltage-gated sodium channels in nerve cell membranes and blocking the passage of sodium ions (responsible for the rising phase of an action potential) into the neuron. This prevents the nervous system from carrying messages and thus muscles from contracting in response to nervous stimulation.[3]
Its mechanism of action, selective blocking of the sodium channel, was shown definitively in 1964 by Toshio Narahashi and John W. Moore at Duke University, using the sucrose gap voltage clamp technique.[4]
Apart from their bacterial species of most likely ultimate biosynthetic origin (see below), tetrodotoxin has been isolated from widely differing animal species, including:[1]
Tarichatoxin was shown to be identical to TTX in 1964 by Mosher et al.,[10][11] and the identity of maculotoxin and TTX was reported in Science in 1978,[12] and the synonymity of these two toxins is supported in modern reports (e.g., at Pubchem[13] and in modern toxicology textbooks[14]) though historic monographs questioning this continue in reprint.[15]
The toxin is variously used by metazoans as a defensive biotoxin to ward off predation, or as both a defensive and predatory venom (e.g., in octopuses, chaetognaths, and ribbon worms).[16] Even though the toxin acts as a defense mechanism, some predators such as the common garter snake have developed insensitivity to TTX, which allows them to prey upon toxic newts.[17]
The association of TTX with consumed, infecting, or symbiotic bacterial populations within the metazoan species from which it is isolated is relatively clear;[1] presence of TTX-producing bacteria within a metazoan's microbiome is determined by culture methods, the presence of the toxin by chemical analysis, and the association of the bacteria with TTX production by toxicity assay of media in which suspected bacteria are grown.[2] As Lago et al. note, "there is good evidence that uptake of bacteria producing TTX is an important element of TTX toxicity in marine metazoans that present this toxin."[2] TTX-producing bacteria include Actinomyces, Aeromonas, Alteromonas, Bacillus, Pseudomonas, and Vibrio species;[2] in the following animals, specific bacterial species have been implicated:[1]
Tetrodotoxin binds to what is known as site 1 of the fast voltage-gated sodium channel.[24]  Site 1 is located at the extracellular pore opening of the ion channel. The binding of any molecules to this site will temporarily disable the function of the ion channel, thereby blocking the passage of sodium ions into the nerve cell (which is ultimately necessary for nerve conduction); neosaxitoxin and several of the conotoxins also bind the same site.
TTX and its analogs have historically been important agents for use as chemical tool compounds, for use in channel characterization and in fundamental studies of channel function.[27][28] The prevalence of TTX-s Na+ channels in the central nervous system makes tetrodotoxin a valuable agent for the silencing of neural activity within a cell culture.
The toxin can enter the body of a victim by ingestion, injection, or inhalation, or through abraded skin.[42]
Poisoning occurring as a consequence of consumption of fish from the order Tetraodontiformes is extremely serious. The organs (e.g. liver) of the pufferfish can contain levels of tetrodotoxin sufficient to produce the described paralysis of the diaphragm and corresponding death due to respiratory failure.[43] Toxicity varies between species and at different seasons and geographic localities, and the flesh of many pufferfish may not be dangerously toxic.[3]
The mechanism of toxicity is through the blockage of fast voltage-gated sodium channels, which are required for the normal transmission of signals between the body and brain.[44] As a result, TTX causes loss of sensation, and paralysis of voluntary muscles including the diaphragm and intercostal muscles, stopping breathing.[45]
The German physician Engelbert Kaempfer, in his "A History of Japan" (translated and published in English in 1727), described how well known the toxic effects of the fish were, to the extent that it would be used for suicide and that the Emperor specifically decreed that soldiers were not permitted to eat it.[47] There is also evidence from other sources that knowledge of such toxicity was widespread throughout southeast Asia and India.[27]
The first recorded cases of TTX poisoning affecting Westerners are from the logs of Captain James Cook from 7 September 1774.[43] On that date Cook recorded his crew eating some local tropic fish (pufferfish), then feeding the remains to the pigs kept on board. The crew experienced numbness and shortness of breath, while the pigs were all found dead the next morning. In hindsight, it is clear that the crew survived a mild dose of tetrodotoxin, while the pigs ate the pufferfish body parts that contain most of the toxin, thus being fatally poisoned.
The toxin was first isolated and named in 1909 by Japanese scientist Dr. Yoshizumi Tahara.[2][48][43]  It was one of the agents studied by Japan's Unit 731, which evaluated biological weapons on human subjects in the 1930s.[49]
The diagnosis of pufferfish poisoning is based on the observed symptomatology and recent dietary history.[50]
Symptoms typically develop within 30 minutes of ingestion, but may be delayed by up to four hours; however, if the dose is fatal, symptoms are usually present within 17 minutes of ingestion.[43] Paresthesia of the lips and tongue is followed by developing paresthesia in the extremities, hypersalivation, sweating, headache, weakness, lethargy, incoordination, tremor, paralysis, cyanosis, aphonia, dysphagia, and seizures. The gastrointestinal symptoms are often severe and include nausea, vomiting, diarrhea, and abdominal pain; death is usually secondary to respiratory failure.[45][50] There is increasing respiratory distress, speech is affected, and the victim usually exhibits dyspnea, mydriasis, and hypotension. Paralysis increases, and convulsions, mental impairment, and cardiac arrhythmia may occur. The victim, although completely paralyzed, may be conscious and in some cases completely lucid until shortly before death, which generally occurs within 4 to 6 hours (range ~20 minutes to ~8 hours). However, some victims enter a coma.[45][51]
If the patient survives 24 hours, recovery without any residual effects will usually occur over a few days.[50]
Therapy is supportive and based on symptoms, with aggressive early airway management.[43] If ingested, treatment can consist of emptying the stomach, feeding the victim activated charcoal to bind the toxin, and taking standard life-support measures to keep the victim alive until the effect of the poison has worn off.[43] Alpha adrenergic agonists are recommended in addition to intravenous fluids to combat hypotension; anticholinesterase agents "have been proposed as a treatment option but have not been tested adequately".[51]
No antidote has been developed and approved for human use, but a primary research report (preliminary result) indicates that a monoclonal antibody specific to tetrodotoxin is in development by USAMRIID that was effective, in the one study, for reducing toxin lethality in tests on mice.[52]
Poisonings from tetrodotoxin have been almost exclusively associated with the consumption of pufferfish from waters of the Indo-Pacific Ocean regions. Pufferfishes from other regions are much less commonly eaten. Several reported cases of poisonings, including fatalities, involved pufferfish from the Atlantic Ocean, Gulf of Mexico, and Gulf of California. There have been no confirmed cases of tetrodotoxicity from the Atlantic pufferfish, Sphoeroides maculatus, but in three studies, extracts from fish of this species were highly toxic in mice. Several recent intoxications from these fishes in Florida were due to saxitoxin, which causes paralytic shellfish poisoning with very similar symptoms and signs. The trumpet shell Charonia sauliae has been implicated in food poisonings, and evidence suggests it contains a tetrodotoxin derivative. There have been several reported poisonings from mislabelled pufferfish, and at least one report of a fatal episode in Oregon when an individual swallowed a rough-skinned newt Taricha granulosa.[53]
In 2009, a major scare in the Auckland Region of New Zealand was sparked after several dogs died eating Pleurobranchaea maculata (grey side-gilled seaslug) on beaches.[54] Children and pet owners were asked to avoid beaches, and recreational fishing was also interrupted for a time. After exhaustive analysis, it was found that the sea slugs must have ingested tetrodotoxin.[55]
Genetic background is not a factor in susceptibility to tetrodotoxin poisoning. This toxicosis may be avoided by not consuming animal species known to contain tetrodotoxin, principally pufferfish; other tetrodotoxic species are not usually consumed by humans.
Poisoning from tetrodotoxin is of particular public health concern in Japan, where "fugu" is a traditional delicacy. It is prepared and sold in special restaurants where trained and licensed chefs carefully remove the viscera to reduce the danger of poisoning.[61] There is potential for misidentification and mislabelling, particularly of prepared, frozen fish products.
The mouse bioassay developed for paralytic shellfish poisoning (PSP) can be used to monitor tetrodotoxin in pufferfish and is the current method of choice. An HPLC method with post-column reaction with alkali and fluorescence has been developed to determine tetrodotoxin and its associated toxins. The alkali degradation products can be confirmed as their trimethylsilyl derivatives by gas chromatography/mass spectrometry.[citation needed]
Tetrodotoxin may be quantified in serum, whole blood or urine to confirm a diagnosis of poisoning in hospitalized patients or to assist in the forensic investigation of a case of fatal overdosage. Most analytical techniques involve mass spectrometric detection following gas or liquid chromatographic separation.[62]
Tetrodotoxin has been investigated as a possible treatment for cancer-associated pain. Early clinical trials demonstrate significant pain relief in some patients.[63][64]
In addition to the cancer pain application mentioned, mutations in one particular TTX-sensitive Na+ channel are associated with some migraine headaches,[65] although it is unclear as to whether this has any therapeutic relevance for most people with migraine.[66]
Tetrodotoxin has been used clinically to relieve the headache associated with heroin withdrawal.[67]
Tetrodotoxin serves as a plot device for characters to fake death, as in the films Hello Again (1987), The Serpent and the Rainbow (1988), The A-Team (2010) and Captain America: The Winter Soldier (2014), War (2019), and in episodes of "Jane the Virgin", Miami Vice (1985),[70] Nikita, MacGyver Season 7, Episode 6, where the antidote is Datura stramonium leaf, CSI: NY (Season 4, episode 9 "Boo") and Chuck. In Law Abiding Citizen (2009) and Alex Cross (2012), its paralysis is presented as a method of assisting torture. The toxin was also referenced in "synthetic form" in the S1E2 of the series "FBI". The toxin is used as a weapon in both the second season of Archer, in Covert Affairs and in the Inside No. 9 episode "The Riddle of the Sphinx".[71][72]
Based on the presumption that tetrodotoxin is not always fatal, but at near-lethal doses can leave a person extremely unwell with the person remaining conscious,[50] tetrodotoxin has been alleged to result in zombieism, and has been suggested as an ingredient in Haitian Vodou preparations.[73] This idea first appeared in the 1938 non-fiction book Tell My Horse by Zora Neale Hurston in which there were multiple accounts of purported tetrodotoxin poisoning in Haiti by a voodoo sorcerer called the Bokor.[74] These stories were later popularized by Harvard-trained ethnobotanist Wade Davis[73] in his 1985 book and Wes Craven's 1988 film, both titled The Serpent and the Rainbow. James Ellroy includes "blowfish toxin" as an ingredient in Haitian Vodou preparations to produce zombieism and poisoning deaths in his dark, disturbing, violent novel Blood's a Rover.  But this theory has been questioned by the scientific community since the 1990s based on analytical chemistry-based tests of multiple preparations and review of earlier reports (see above).[58][59][60]
Neurotoxins are toxins that are destructive to nerve tissue (causing neurotoxicity).[3] Neurotoxins are an extensive class of exogenous chemical neurological insults[4] that can adversely affect function in both developing and mature nervous tissue.[5] The term can also be used to classify endogenous compounds, which, when abnormally contacted, can prove neurologically toxic.[4] Though neurotoxins are often neurologically destructive, their ability to specifically target neural components is important in the study of nervous systems.[6] Common examples of neurotoxins include lead,[7] ethanol (drinking alcohol),[8] glutamate,[9] nitric oxide,[10] botulinum toxin (e.g. Botox),[11] tetanus toxin,[12] and tetrodotoxin.[6] Some substances such as nitric oxide and glutamate are in fact essential for proper function of the body and only exert neurotoxic effects at excessive concentrations.
Neurotoxins inhibit neuron control over ion concentrations across the cell membrane,[6] or communication between neurons across a synapse.[13] Local pathology of neurotoxin exposure often includes neuron excitotoxicity or apoptosis[14] but can also include glial cell damage.[15] Macroscopic manifestations of neurotoxin exposure can include widespread central nervous system damage such as intellectual disability,[5] persistent memory impairments,[16] epilepsy, and dementia.[17] Additionally, neurotoxin-mediated peripheral nervous system damage such as neuropathy or myopathy is common. Support has been shown for a number of treatments aimed at attenuating neurotoxin-mediated injury, such as antioxidant[8] and antitoxin[18] administration.
Exposure to neurotoxins in society is not new,[19] as civilizations have been exposed to neurologically destructive compounds for thousands of years. One notable example is the possible significant lead exposure during the Roman Empire resulting from the development of extensive plumbing networks and the habit of boiling vinegared wine in lead pans to sweeten it, the process generating lead acetate, known as "sugar of lead".[20] In part, neurotoxins have been part of human history because of the fragile and susceptible nature of the nervous system, making it highly prone to disruption.
The nervous tissue found in the brain, spinal cord, and periphery comprises an extraordinarily complex biological system that largely defines many of the unique traits of individuals. As with any highly complex system, however, even small perturbations to its environment can lead to significant functional disruptions. Properties leading to the susceptibility of nervous tissue include a high surface area of neurons, a high lipid content which retains lipophilic toxins, high blood flow to the brain inducing increased effective toxin exposure, and the persistence of neurons through an individual's lifetime, leading to compounding of damages.[21] As a result, the nervous system has a number of mechanisms designed to protect it from internal and external assaults, including the blood brain barrier.
This barrier creates a tight hydrophobic layer around the capillaries in the brain, inhibiting the transport of large or hydrophilic compounds. In addition to the BBB, the choroid plexus provides a layer of protection against toxin absorption in the brain. The choroid plexuses are vascularized layers of tissue found in the third, fourth, and lateral ventricles of the brain, which through the function of their ependymal cells, are responsible for the synthesis of cerebrospinal fluid (CSF).[23] Importantly, through selective passage of ions and nutrients and trapping heavy metals such as lead, the choroid plexuses maintain a strictly regulated environment which contains the brain and spinal cord.[22][23]
By being hydrophobic and small, or inhibiting astrocyte function, some compounds including certain neurotoxins are able to penetrate into the brain and induce significant damage. In modern times, scientists and physicians have been presented with the challenge of identifying and treating neurotoxins, which has resulted in a growing interest in both neurotoxicology research and clinical studies.[24] Though clinical neurotoxicology is largely a burgeoning field, extensive inroads have been made in the identification of many environmental neurotoxins leading to the classification of 750 to 1000 known potentially neurotoxic compounds.[21] Due to the critical importance of finding neurotoxins in common environments, specific protocols have been developed by the United States Environmental Protection Agency (EPA) for testing and determining neurotoxic effects of compounds (USEPA 1998). Additionally, in vitro systems have increased in use as they provide significant improvements over the more common in vivo systems of the past. Examples of improvements include tractable, uniform environments, and the elimination of contaminating effects of systemic metabolism.[24] In vitro systems, however, have presented problems as it has been difficult to properly replicate the complexities of the nervous system, such as the interactions between supporting astrocytes and neurons in creating the BBB.[25] To even further complicate the process of determining neurotoxins when testing in-vitro, neurotoxicity and cytotoxicity may be difficult to distinguish as exposing neurons directly to compounds may not be possible in-vivo, as it is in-vitro. Additionally, the response of cells to chemicals may not accurately convey a distinction between neurotoxins and cytotoxins, as symptoms like oxidative stress or skeletal modifications may occur in response to either.[26]
In an effort to address this complication, neurite outgrowths (either axonal or dendritic) in response to applied compounds have recently been proposed as a more accurate distinction between true neurotoxins and cytotoxins in an in-vitro testing environment. Due to the significant inaccuracies associated with this process, however, it has been slow in gaining widespread support.[27] Additionally, biochemical mechanisms have become more widely used in neurotoxin testing, such that compounds can be screened for sufficiency to induce cell mechanism interference, like the inhibition of acetylcholinesterase capacity of organophosphates (includes DDT and sarin gas).[28] Though methods of determining neurotoxicity still require significant development, the identification of deleterious compounds and toxin exposure symptoms has undergone significant improvement.
Though diverse in chemical properties and functions, neurotoxins share the common property that they act by some mechanism leading to either the disruption or destruction of necessary components within the nervous system. Neurotoxins, however, by their very design can be very useful in the field of neuroscience. As the nervous system in most organisms is both highly complex and necessary for survival, it has naturally become a target for attack by both predators and prey. As venomous organisms often use their neurotoxins to subdue a predator or prey very rapidly, toxins have evolved to become highly specific to their target channels such that the toxin does not readily bind other targets[29] (see Ion Channel toxins). As such, neurotoxins provide an effective means by which certain elements of the nervous system may be accurately and efficiently targeted. An early example of neurotoxin based targeting used radiolabeled tetrodotoxin to assay sodium channels and obtain precise measurements about their concentration along nerve membranes.[29] Likewise through isolation of certain channel activities, neurotoxins have provided the ability to improve the original Hodgkin-Huxley model of the neuron in which it was theorized that single generic sodium and potassium channels could account for most nervous tissue function.[29] From this basic understanding, the use of common compounds such as tetrodotoxin, tetraethylammonium, and bungarotoxins have led to a much deeper understanding of the distinct ways in which individual neurons may behave.
As neurotoxins are compounds which adversely affect the nervous system, a number of mechanisms through which they function are through the inhibition of neuron cellular processes. These inhibited processes can range from membrane depolarization mechanisms to inter-neuron communication. By inhibiting the ability for neurons to perform their expected intracellular functions, or pass a signal to a neighboring cell, neurotoxins can induce systemic nervous system arrest as in the case of botulinum toxin,[13] or even nervous tissue death.[30] The time required for the onset of symptoms upon neurotoxin exposure can vary between different toxins, being on the order of hours for botulinum toxin[18] and years for lead.[31]
Tetrodotoxin (TTX) is a poison produced by organisms belonging to the Tetraodontiformes order, which includes the puffer fish, ocean sunfish, and porcupine fish.[55] Within the puffer fish, TTX is found in the liver, gonads, intestines, and skin.[6][56] TTX can be fatal if consumed, and has become a common form of poisoning in many countries. Common symptoms of TTX consumption include paraesthesia (often restricted to the mouth and limbs), muscle weakness, nausea, and vomiting[55] and often manifest within 30 minutes of ingestion.[57] The primary mechanism by which TTX is toxic is through the inhibition of sodium channel function, which reduces the functional capacity of neuron communication. This inhibition largely affects a susceptible subset of sodium channels known as TTX-sensitive (TTX-s), which also happens to be largely responsible for the sodium current that drives the depolarization phase of neuron action potentials.[6] 
 TTX-resistant (TTX-r) is another form of sodium channel which has limited sensitivity to TTX, and is largely found in small diameter axons such as those found in nociception neurons.[6] When a significant level of TTX is ingested, it will bind sodium channels on neurons and reduce their membrane permeability to sodium. This results in an increased effective threshold of required excitatory signals in order to induce an action potential in a postsynaptic neuron.[6] The effect of this increased signaling threshold is a reduced excitability of postsynaptic neurons, and subsequent loss of motor and sensory function which can result in paralysis and death. Though assisted ventilation may increase the chance of survival after TTX exposure, there is currently no antitoxin. The use of the acetylcholinesterase inhibitor Neostigmine or the muscarinic acetylcholine antagonist atropine (which will inhibit parasympathetic activity), however, can increase sympathetic nerve activity enough to improve the chance of survival after TTX exposure.[55]
Tetraethylammonium (TEA) is a compound that, like a number of neurotoxins, was first identified through its damaging effects to the nervous system and shown to have the capacity of inhibiting the function of motor nerves and thus the contraction of the musculature in a manner similar to that of curare.[58] Additionally, through chronic TEA administration, muscular atrophy would be induced.[58] It was later determined that TEA functions in-vivo primarily through its ability to inhibit both the potassium channels responsible for the delayed rectifier seen in an action potential and some population of calcium-dependent potassium channels.[32] It is this capability to inhibit potassium flux in neurons that has made TEA one of the most important tools in neuroscience. It has been hypothesized that the ability for TEA to inhibit potassium channels is derived from its similar space-filling structure to potassium ions.[58] What makes TEA very useful for neuroscientists is its specific ability to eliminate potassium channel activity, thereby allowing the study of neuron response contributions of other ion channels such as voltage gated sodium channels.[59] In addition to its many uses in neuroscience research, TEA has been shown to perform as an effective treatment of Parkinson's disease through its ability to limit the progression of the disease.[60]
Chlorotoxin (Cltx) is the active compound found in scorpion venom, and is primarily toxic because of its ability to inhibit the conductance of chloride channels.[33] Ingestion of lethal volumes of Cltx results in paralysis through this ion channel disruption. Similar to botulinum toxin, Cltx has been shown to possess significant therapeutic value. Evidence has shown that Cltx can inhibit the ability for gliomas to infiltrate healthy nervous tissue in the brain, significantly reducing the potential invasive harm caused by tumors.[61][62]
Tetanus neurotoxin (TeNT)  is a compound that functionally reduces inhibitory transmissions in the nervous system resulting in muscular tetany. TeNT is similar to BTX, and is in fact highly similar in structure and origin; both belonging to the same category of clostridial neurotoxins.[12] Like BTX, TeNT inhibits inter-neuron communication by means of vesicular neurotransmitter (NT) release.[36] One notable difference between the two compounds is that while BTX inhibits muscular contractions, TeNT induces them. Though both toxins inhibit vesicle release at neuron synapses, the reason for this different manifestation is that BTX functions mainly in the peripheral nervous system (PNS) while TeNT is largely active in the central nervous system (CNS).[68] This is a result of TeNT migration through motor neurons to the inhibitory neurons of the spinal cord after entering through endocytosis.[69] This results in a loss of function in inhibitory neurons within the CNS resulting in systemic muscular contractions. Similar to the prognosis of a lethal dose of BTX, TeNT leads to paralysis and subsequent suffocation.[69]
Neurotoxic behavior of Aluminium is known to occur upon entry into the circulatory system, where it can migrate to the brain and inhibit some of the crucial functions of the blood brain barrier (BBB).[37] A loss of function in the BBB can produce significant damage to the neurons in the CNS, as the barrier protecting the brain from other toxins found in the blood will no longer be capable of such action. Though the metal is known to be neurotoxic, effects are usually restricted to patients incapable of removing excess ions from the blood, such as those experiencing renal failure.[70] Patients experiencing aluminium toxicity can exhibit symptoms such as impaired learning and reduced motor coordination.[71] Additionally, systemic aluminium levels are known to increase with age, and have been shown to correlate with Alzheimer's Disease, implicating it as a neurotoxic causative compound of the disease.[72] Despite its known toxicity in its ionic form, studies are divided on the potential toxicity of using aluminium in packaging and cooking appliances.
Mercury is capable of inducing CNS damage by migrating into the brain by crossing the BBB.[38] Mercury exists in a number of different compounds, though methylmercury (MeHg+), dimethylmercury and diethylmercury are the only significantly neurotoxic forms. Diethylmercury and dimethylmercury are considered some of the most potent neurotoxins ever discovered.[38] MeHg+ is usually acquired through consumption of seafood, as it tends to concentrate in organisms high on the food chain.[73] It is known that the mercuric ion inhibits amino acid (AA) and glutamate (Glu) transport, potentially leading to excitotoxic effects.[74]
Investigations into anatoxin-a, also known as "Very Fast Death Factor", began in 1961 following the deaths of cows that drank from a lake containing an algal bloom in Saskatchewan, Canada.[41][42] It is a cyanotoxin produced by at least four different genera of cyanobacteria, and has been reported in North America, Europe, Africa, Asia, and New Zealand.[75]
Toxic effects from anatoxin-a progress very rapidly because it acts directly on the nerve cells (neurons). The progressive symptoms of anatoxin-a exposure are loss of coordination, twitching, convulsions and rapid death by respiratory paralysis. The nerve tissues which communicate with muscles contain a receptor called the nicotinic acetylcholine receptor. Stimulation of these receptors causes a muscular contraction. The anatoxin-a molecule is shaped so it fits this receptor, and in this way it mimics the natural neurotransmitter normally used by the receptor, acetylcholine. Once it has triggered a contraction, anatoxin-a does not allow the neurons to return to their resting state, because it is not degraded by cholinesterase which normally performs this function. As a result, the muscle cells contract permanently, the communication between the brain and the muscles is disrupted and breathing stops.[76][77]
When it was first discovered, the toxin was called the Very Fast Death Factor (VFDF) because when it was injected into the body cavity of mice it induced tremors, paralysis and death within a few minutes. In 1977, the structure of VFDF was determined as a secondary, bicyclic amine alkaloid, and it was renamed anatoxin-a.[78][79] Structurally, it is similar to cocaine.[80] There is continued interest in anatoxin-a because of the dangers it presents to recreational and drinking waters, and because it is a particularly useful molecule for investigating acetylcholine receptors in the nervous system.[81] The deadliness of the toxin means that it has a high military potential as a toxin weapon.[82]
Caramboxin (CBX) is a toxin found in star fruit (Averrhoa carambola). Individuals with some types of kidney disease are susceptible to adverse neurological effects including intoxication, seizures and even death after eating star fruit or drinking juice made of this fruit.  Caramboxin is a new nonpeptide amino acid toxin that stimulate the glutamate receptors in neurons. Caramboxin is an agonist of both NMDA and AMPA glutamatergic ionotropic receptors with potent excitatory, convulsant, and neurodegenerative properties.[43]
The term "curare" is ambiguous because it has been used to describe a number of poisons which at the time of naming were understood differently from present day understandings. In the past the characterization has meant poisons used by South American tribes on arrows or darts, though it has matured to specify a specific categorization of poisons which act on the neuromuscular junction to inhibit signaling and thus induce muscle relaxation.[86] The neurotoxin category contains a number of distinct poisons, though all were originally purified from plants originating in South America.[86] The effect with which injected curare poison is usually associated is muscle paralysis and resultant death.[87] Curare notably functions to inhibit nicotinic acetylcholine receptors at the neuromuscular junction.  Normally, these receptor channels allow sodium ions into muscle cells to initiate an action potential that leads to muscle contraction.  By blocking the receptors, the neurotoxin is capable of significantly reducing neuromuscular junction signaling, an effect which has resulted in its use by anesthesiologists to produce muscular relaxation.[88]
Ammonia toxicity is often seen through two routes of administration, either through consumption or through endogenous ailments such as liver failure.[89][90] One notable case in which ammonia toxicity is common is in response to cirrhosis of the liver which results in hepatic encephalopathy, and can result in cerebral edema (Haussinger 2006). This cerebral edema can be the result of nervous cell remodeling. As a consequence of increased concentrations, ammonia activity in-vivo has been shown to induce swelling of astrocytes in the brain through increased production of cGMP (Cyclic Guanosine Monophosphate) within the cells which leads to Protein Kinase G-mediated (PKG) cytoskeletal modifications.[46] The resultant effect of this toxicity can be reduced brain energy metabolism and function. Importantly, the toxic effects of ammonia on astrocyte remodeling can be reduced through administration of L-carnitine.[89] This astrocyte remodeling appears to be mediated through ammonia-induced mitochondrial permeability transition. This mitochondrial transition is a direct result of glutamine activity a compound which forms from ammonia in-vivo.[91] Administration of antioxidants or glutaminase inhibitor can reduce this mitochondrial transition, and potentially also astrocyte remodeling.[91]
Arsenic is a neurotoxin commonly found concentrated in areas exposed to agricultural runoff, mining, and smelting sites (Martinez-Finley 2011). One of the effects of arsenic ingestion during the development of the nervous system is the inhibition of neurite growth[92] which can occur both in PNS and the CNS.[93] This neurite growth inhibition can often lead to defects in neural migration, and significant morphological changes of neurons during development,[94]) often leading to neural tube defects in neonates.[95] As a metabolite of arsenic, arsenite is formed after ingestion of arsenic and has shown significant toxicity to neurons within about 24 hours of exposure. The mechanism of this cytotoxicity functions through arsenite-induced increases in intracellular calcium ion levels within neurons, which may subsequently reduce mitochondrial transmembrane potential which activates caspases, triggering cell death.[94] Another known function of arsenite is its destructive nature towards the cytoskeleton through inhibition of neurofilament transport.[47] This is particularly destructive as neurofilaments are used in basic cell structure and support. Lithium administration has shown promise, however, in restoring some of the lost neurofilament motility.[96] Additionally, similar to other neurotoxin treatments, the administration of certain antioxidants has shown some promise in reducing neurotoxicity of ingested arsenic.[94]
Lead is a potent neurotoxin whose toxicity has been recognized for at least thousands of years.[97] Though neurotoxic effects for lead are found in both adults and young children, the developing brain is particularly susceptible to lead-induced harm, effects which can include apoptosis and excitotoxicity.[97] An underlying mechanism by which lead is able to cause harm is its ability to be transported by calcium ATPase pumps across the BBB, allowing for direct contact with the fragile cells within the central nervous system.[98] Neurotoxicity results from lead's ability to act in a similar manner to calcium ions, as concentrated lead will lead to cellular uptake of calcium which disrupts cellular homeostasis and induces apoptosis.[48] It is this intracellular calcium increase that activates protein kinase C (PKC), which manifests as learning deficits in children as a result of early lead exposure.[48] In addition to inducing apoptosis, lead inhibits interneuron signaling through the disruption of calcium-mediated neurotransmitter release.[99]
As a neurotoxin, ethanol has been shown to induce nervous system damage and affect the body in a variety of ways. Among the known effects of ethanol exposure are both transient and lasting consequences. Some of the lasting effects include long-term reduced neurogenesis in the hippocampus,[100][101] widespread brain atrophy,[102] and induced inflammation in the brain.[103] Of note, chronic ethanol ingestion has additionally been shown to induce reorganization of cellular membrane constituents, leading to a lipid bilayer marked by increased membrane concentrations of cholesterol and saturated fat.[50] This is important as neurotransmitter transport can be impaired through vesicular transport inhibition, resulting in diminished neural network function. One significant example of reduced inter-neuron communication is the ability for ethanol to inhibit NMDA receptors in the hippocampus, resulting in reduced long-term potentiation (LTP) and memory acquisition.[49] NMDA has been shown to play an important role in LTP and consequently memory formation.[104] With chronic ethanol intake, however, the susceptibility of these NMDA receptors to induce LTP increases in the mesolimbic dopamine neurons in an inositol 1,4,5-triphosphate (IP3) dependent manner.[105] This reorganization may lead to neuronal cytotoxicity both through hyperactivation of postsynaptic neurons and through induced addiction to continuous ethanol consumption. It has, additionally, been shown that ethanol directly reduces intracellular calcium ion accumulation through inhibited NMDA receptor activity, and thus reduces the capacity for the occurrence of LTP.[106]
In addition to the neurotoxic effects of ethanol in mature organisms, chronic ingestion is capable of inducing severe developmental defects. Evidence was first shown in 1973 of a connection between chronic ethanol intake by mothers and defects in their offspring.[107] This work was responsible for creating the classification of fetal alcohol syndrome, a disease characterized by common morphogenesis aberrations such as defects in craniofacial formation, limb development, and cardiovascular formation. The magnitude of ethanol neurotoxicity in fetuses leading to fetal alcohol syndrome has been shown to be dependent on antioxidant levels in the brain such as vitamin E.[108] As the fetal brain is relatively fragile and susceptible to induced stresses, severe deleterious effects of alcohol exposure can be seen in important areas such as the hippocampus and cerebellum. The severity of these effects is directly dependent upon the amount and frequency of ethanol consumption by the mother, and the stage in development of the fetus.[109] It is known that ethanol exposure results in reduced antioxidant levels, mitochondrial dysfunction (Chu 2007),  and subsequent neuronal death, seemingly as a result of increased generation of reactive oxidative species (ROS).[30] This is a plausible mechanism, as there is a reduced presence in the fetal brain of antioxidant enzymes such as catalase and peroxidase.[110] In support of this mechanism, administration of high levels of dietary vitamin E results in reduced or eliminated ethanol-induced neurotoxic effects in fetuses.[8]
n-Hexane is a neurotoxin which has been responsible for the poisoning of several workers in Chinese electronics factories in recent years.[111][112][113][51]
Unlike most common sources of neurotoxins which are acquired by the body through ingestion, endogenous neurotoxins both originate from and exert their effects in-vivo. Additionally, though most venoms and exogenous neurotoxins will rarely possess useful in-vivo capabilities, endogenous neurotoxins are commonly used by the body in useful and healthy ways, such as nitric oxide which is used in cell communication.[114] It is often only when these endogenous compounds become highly concentrated that they lead to dangerous effects.[9]
Though nitric oxide (NO) is commonly used by the nervous system in inter-neuron communication and signaling, it can be active in mechanisms leading to ischemia in the cerebrum (Iadecola 1998). The neurotoxicity of NO is based on its importance in glutamate excitotoxicity, as NO is generated in a calcium-dependent manner in response to glutamate mediated NMDA activation, which occurs at an elevated rate in glutamate excitotoxicity.[52] Though NO facilitates increased blood flow to potentially ischemic regions of the brain, it is also capable of increasing oxidative stress,[115] inducing DNA damage and apoptosis.[116] Thus an increased presence of NO in an ischemic area of the CNS can produce significantly toxic effects.
Glutamate, like nitric oxide, is an endogenously produced compound used by neurons to perform normally, being present in small concentrations throughout the gray matter of the CNS.[9] One of the most notable uses of endogenous glutamate is its functionality as an excitatory neurotransmitter.[53] When concentrated, however, glutamate becomes toxic to surrounding neurons. This toxicity can be both a result of direct lethality of glutamate on neurons and a result of induced calcium flux into neurons leading to swelling and necrosis.[53] Support has been shown for these mechanisms playing significant roles in diseases and complications such as Huntington's disease, epilepsy, and stroke.[9]
Biology is the scientific study of life.[1][2][3] It is a natural science with a broad scope but has several unifying themes that tie it together as a single, coherent field.[1][2][3] For instance, all organisms are made up of cells that process hereditary information encoded in genes, which can be transmitted to future generations. Another major theme is evolution, which explains the unity and diversity of life.[1][2][3] Energy processing is also important to life as it allows organisms to move, grow, and reproduce.[1][2][3] Finally, all organisms are able to regulate their own internal environments.[1][2][3][4][5]
Biologists are able to study life at multiple levels of organization,[1] from the molecular biology of a cell to the anatomy and physiology of plants and animals, and evolution of populations.[1][6] Hence, there are multiple subdisciplines within biology, each defined by the nature of their research questions and the tools that they use.[7][8][9] Like other scientists, biologists use the scientific method to make observations, pose questions, generate hypotheses, perform experiments, and form conclusions about the world around them.[1]
Life on Earth, which emerged more than 3.7 billion years ago,[10] is immensely diverse. Biologists have sought to study and classify the various forms of life, from prokaryotic organisms such as archaea and bacteria to eukaryotic organisms such as protists, fungi, plants, and animals. These various organisms contribute to the biodiversity of an ecosystem, where they play specialized roles in the cycling of nutrients and energy through their biophysical environment.
Biology began to quickly develop with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop techniques of microscopic dissection and staining.[17] Advances in microscopy had a profound impact on biological thinking. In the early 19th century, biologists pointed to the central importance of the cell. In 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells, continuing to support spontaneous generation. However, Robert Remak and Rudolf Virchow were able to reify the third tenet, and by the 1860s most biologists accepted all three tenets which consolidated into cell theory.[18][19]
Serious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who presented a coherent theory of evolution.[23] The British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions.[24][25]
The basis for modern genetics began with the work of Gregor Mendel in 1865.[26] This outlined the principles of biological inheritance.[27] However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics.[28] In the 1940s and early 1950s, a series of experiments by Alfred Hershey and Martha Chase pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double-helical structure of DNA by James Watson and Francis Crick in 1953, marked the transition to the era of molecular genetics. From the 1950s onwards, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. The Human Genome Project was launched in 1990 to map the human genome.[29]
All organisms are made up of chemical elements;[30] oxygen, carbon, hydrogen, and nitrogen account for most (96%) of the mass of all organisms, with calcium, phosphorus, sulfur, sodium, chlorine, and magnesium constituting essentially all the remainder. Different elements can combine to form compounds such as water, which is fundamental to life.[30] Biochemistry is the study of chemical processes within and relating to living organisms. Molecular biology is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms, and interactions.
The simplest form of an organic molecule is the hydrocarbon, which is a large family of organic compounds that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other elements such as oxygen (O), hydrogen (H), phosphorus (P), and sulfur (S), which can change the chemical behavior of that compound.[31] Groups of atoms that contain these elements (O-, H-, P-, and S-) and are bonded to a central carbon atom or skeleton are called functional groups.[31] There are six prominent functional groups that can be found in organisms: amino group, carboxyl group, carbonyl group, hydroxyl group, phosphate group, and sulfhydryl group.[31]
In 1953, the Miller-Urey experiment showed that organic compounds could be synthesized abiotically within a closed system mimicking the conditions of early Earth, thus suggesting that complex organic molecules could have arisen spontaneously in early Earth (see abiogenesis).[33][31]
Macromolecules are large molecules made up of smaller subunits or monomers.[34] Monomers include  sugars, amino acids, and nucleotides.[35] Carbohydrates include monomers and polymers of sugars.[36] 
Lipids are the only class of macromolecules that are not made up of polymers. They include steroids, phospholipids, and fats,[35] largely nonpolar and hydrophobic (water-repelling) substances.[37] 
Proteins are the most diverse of the macromolecules. They include enzymes, transport proteins, large signaling molecules, antibodies, and structural proteins. The basic unit (or monomer) of a protein is an amino acid.[34] Twenty amino acids are used in proteins.[34] 
Nucleic acids are polymers of nucleotides.[38] Their function is to store, transmit, and express hereditary information.[35]
Every cell is enclosed within a cell membrane that separates its cytoplasm from the extracellular space.[41] A cell membrane consists of a lipid bilayer, including cholesterols that sit between phospholipids to maintain their fluidity at various temperatures. Cell membranes are semipermeable, allowing small molecules such as oxygen, carbon dioxide, and water to pass through while restricting the movement of larger molecules and charged particles such as ions.[42] Cell membranes also contains membrane proteins, including integral membrane proteins that go across the membrane serving as membrane transporters, and peripheral proteins that loosely attach to the outer side of the cell membrane, acting as enzymes shaping the cell.[43] Cell membranes are involved in various cellular processes such as cell adhesion, storing electrical energy, and cell signalling and serve as the attachment surface for several extracellular structures such as a cell wall, glycocalyx, and cytoskeleton.
Cellular respiration is a set of metabolic reactions and processes that take place in cells to convert chemical energy from nutrients into adenosine triphosphate (ATP), and then release waste products.[46] The reactions involved in respiration are catabolic reactions, which break large molecules into smaller ones, releasing energy. Respiration is one of the key ways a cell releases chemical energy to fuel cellular activity. The overall reaction occurs in a series of biochemical steps, some of which are redox reactions. Although cellular respiration is technically a combustion reaction, it clearly does not resemble one when it occurs in a cell because of the slow, controlled release of energy from the series of reactions.
Sugar in the form of glucose is the main nutrient used by animal and plant cells in respiration. Cellular respiration involving oxygen is called aerobic respiration, which has four stages: glycolysis, citric acid cycle (or Krebs cycle), electron transport chain, and oxidative phosphorylation.[47] Glycolysis is a metabolic process that occurs in the cytoplasm whereby glucose is converted into two pyruvates, with two net molecules of ATP being produced at the same time.[47] Each pyruvate is then oxidized into acetyl-CoA by the pyruvate dehydrogenase complex, which also generates NADH and carbon dioxide. Acetyl-Coa enters the citric acid cycle, which takes places inside the mitochondrial matrix. At the end of the cycle, the total yield from 1 glucose (or 2 pyruvates) is 6 NADH, 2 FADH2, and 2 ATP molecules. Finally, the next stage is oxidative phosphorylation, which in eukaryotes, occurs in the mitochondrial cristae. Oxidative phosphorylation comprises the electron transport chain, which is a series of four protein complexes that transfer electrons from one complex to another, thereby releasing energy from NADH and FADH2 that is coupled to the pumping of protons (hydrogen ions) across the inner mitochondrial membrane (chemiosmosis), which generates a proton motive force.[47] Energy from the proton motive force drives the enzyme ATP synthase to synthesize more ATPs by phosphorylating ADPs. The transfer of electrons terminates with molecular oxygen being the final electron acceptor.
If oxygen were not present, pyruvate would not be metabolized by cellular respiration but undergoes a process of fermentation. The pyruvate is not transported into the mitochondrion but remains in the cytoplasm, where it is converted to waste products that may be removed from the cell. This serves the purpose of oxidizing the electron carriers so that they can perform glycolysis again and removing the excess pyruvate. Fermentation oxidizes NADH to NAD+ so it can be re-used in glycolysis.  In the absence of oxygen, fermentation prevents the buildup of NADH in the cytoplasm and provides NAD+ for glycolysis.  This waste product varies depending on the organism. In skeletal muscles, the waste product is lactic acid. This type of fermentation is called lactic acid fermentation. In strenuous exercise, when energy demands exceed energy supply, the respiratory chain cannot process all of the hydrogen atoms joined by NADH. During anaerobic glycolysis, NAD+ regenerates when pairs of hydrogen combine with pyruvate to form lactate. Lactate formation is catalyzed by lactate dehydrogenase in a reversible reaction. Lactate can also be used as an indirect precursor for liver glycogen. During recovery, when oxygen becomes available, NAD+ attaches to hydrogen from lactate to form ATP. In yeast, the waste products are ethanol and carbon dioxide. This type of fermentation is known as alcoholic or ethanol fermentation. The ATP generated in this process is made by substrate-level phosphorylation, which does not require oxygen.
Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organism's metabolic activities via cellular respiration. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water.[48][49][50] In most cases, oxygen is released as a waste product. Most plants, algae, and cyanobacteria perform photosynthesis, which is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth.[51]
Photosynthesis has four stages: Light absorption, electron transport, ATP synthesis, and carbon fixation.[47] Light absorption is the initial step of photosynthesis whereby light energy is absorbed by chlorophyll pigments attached to proteins in the thylakoid membranes. The absorbed light energy is used to remove electrons from a donor (water) to a primary electron acceptor, a quinone designated as Q. In the second stage, electrons move from the quinone primary electron acceptor through a series of electron carriers until they reach a final electron acceptor, which is usually the oxidized form of NADP+, which is reduced to NADPH, a process that takes place in a protein complex called photosystem I (PSI). The transport of electrons is coupled to the movement of protons (or hydrogen) from the stroma to the thylakoid membrane, which forms a pH gradient across the membrane as hydrogen becomes more concentrated in the lumen than in the stroma. This is analogous to the proton-motive force generated across the inner mitochondrial membrane in aerobic respiration.[47]
During the third stage of photosynthesis, the movement of protons down their concentration gradients from the thylakoid lumen to the stroma through the ATP synthase is coupled to the synthesis of ATP by that same ATP synthase.[47] The NADPH and ATPs generated by the light-dependent reactions in the second and third stages, respectively, provide the energy and electrons to drive the synthesis of glucose by fixing atmospheric carbon dioxide into existing organic carbon compounds, such as ribulose bisphosphate (RuBP) in a sequence of light-independent (or dark) reactions called the Calvin cycle.[52]
Cell signaling (or communication) is the ability of cells to receive, process, and transmit signals with its environment and with itself.[53][54] Signals can be non-chemical such as light, electrical impulses, and heat, or chemical signals (or ligands) that interact with receptors, which can be found embedded in the cell membrane of another cell or located deep inside a cell.[55][54] There are generally four types of chemical signals: autocrine, paracrine, juxtacrine, and hormones.[55] In autocrine signaling, the ligand affects the same cell that releases it. Tumor cells, for example, can reproduce uncontrollably because they release signals that initiate their own self-division. In paracrine signaling, the ligand diffuses to nearby cells and affects them. For example, brain cells called neurons release ligands called neurotransmitters that diffuse across a synaptic cleft to bind with a receptor on an adjacent cell such as another neuron or muscle cell. In juxtacrine signaling, there is direct contact between the signaling and responding cells. Finally, hormones are ligands that travel through the circulatory systems of animals or vascular systems of plants to reach their target cells. Once a ligand binds with a receptor, it can influence the behavior of another cell, depending on the type of receptor. For instance, neurotransmitters that bind with an inotropic receptor can alter the excitability of a target cell. Other types of receptors include protein kinase receptors (e.g., receptor for the hormone insulin) and G protein-coupled receptors. Activation of G protein-coupled receptors can initiate second messenger cascades. The process by which a chemical or physical signal is transmitted through a cell as a series of molecular events is called signal transduction
Prokaryotes (i.e., archaea and bacteria) can also undergo cell division (or binary fission). Unlike the processes of mitosis and meiosis in eukaryotes, binary fission takes in prokaryotes takes place without the formation of a spindle apparatus on the cell. Before binary fission, DNA in the bacterium is tightly coiled. After it has uncoiled and duplicated, it is pulled to the separate poles of the bacterium as it increases the size to prepare for splitting. Growth of a new cell wall begins to separate the bacterium (triggered by FtsZ polymerization and "Z-ring" formation)[60] The new cell wall (septum) fully develops, resulting in the complete split of the bacterium. The new daughter cells have tightly coiled DNA rods, ribosomes, and plasmids.
Genetics is the scientific study of inheritance.[61][62][63] Mendelian inheritance, specifically, is the process by which genes and traits are passed on from parents to offspring.[27] It has several principles. The first is that genetic characteristics, alleles, are discrete and have alternate forms (e.g., purple vs. white or tall vs. dwarf), each inherited from one of two parents. Based on the law of dominance and uniformity, which states that some alleles are dominant while others are recessive; an organism with at least one dominant allele will display the phenotype of that dominant allele. During gamete formation, the alleles for each gene segregate, so that each gamete carries only one allele for each gene. Heterozygotic individuals produce gametes with an equal frequency of two alleles. Finally, the law of independent assortment, states that genes of different traits can segregate independently during the formation of gametes, i.e., genes are unlinked. An exception to this rule would include traits that are sex-linked. Test crosses can be performed to experimentally determine the underlying genotype of an organism with a dominant phenotype.[64] A Punnett square can be used to predict the results of a test cross. The chromosome theory of inheritance, which states that genes are found on chromosomes, was supported by Thomas Morgans's experiments with fruit flies, which established the sex linkage between eye color and sex in these insects.[65]
A gene is a unit of heredity that corresponds to a region of deoxyribonucleic acid (DNA) that carries genetic information that controls form or function of an organism. DNA is composed of two polynucleotide chains that coil around each other to form a double helix.[66] It is found as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell is collectively known as its genome. In eukaryotes, DNA is mainly in the cell nucleus.[67] In prokaryotes, the DNA is held within the nucleoid.[68] The genetic information is held within genes, and the complete assemblage in an organism is called its genotype.[69]
DNA replication is a semiconservative process whereby each strand serves as a template for a new strand of DNA.[66] Mutations are heritable changes in DNA.[66] They can arise spontaneously as a result of replication errors that were not corrected by proofreading or can be induced by an environmental mutagen such as a chemical (e.g., nitrous acid, benzopyrene) or radiation (e.g., x-ray, gamma ray, ultraviolet radiation, particles emitted by unstable isotopes).[66] Mutations can lead to phenotypic effects such as loss-of-function, gain-of-function, and conditional mutations.[66]
Some mutations are beneficial, as they are a source of genetic variation for evolution.[66] Others are harmful if they were to result in a loss of function of genes needed for survival.[66] Mutagens such as carcinogens are typically avoided as a matter of public health policy goals.[66]
Gene expression is the molecular process by which a genotype encoded in DNA gives rise to an observable phenotype in the proteins of an organism's body. This process is summarized by the central dogma of molecular biology, which was formulated by Francis Crick in 1958.[70][71][72] According to the Central Dogma, genetic information flows from DNA to RNA to protein. There are two gene expression processes: transcription (DNA to RNA) and translation (RNA to protein).[73]
The regulation of gene expression by environmental factors and during different stages of development can occur at each step of the process such as transcription, RNA splicing, translation, and post-translational modification of a protein.[74] Gene expression can be influenced by positive or negative regulation, depending on which of the two types of regulatory proteins called transcription factors bind to the DNA sequence close to or at a promoter.[74] A cluster of genes that share the same promoter is called an operon, found mainly in prokaryotes and some lower eukaryotes (e.g., Caenorhabditis elegans).[74][75] In positive regulation of gene expression, the activator is the transcription factor that stimulates transcription when it binds to the sequence near or at the promoter. Negative regulation occurs when another transcription factor called a repressor binds to a DNA sequence called an operator, which is part of an operon, to prevent transcription. Repressors can be inhibited by compounds called inducers (e.g., allolactose), thereby allowing transcription to occur.[74] Specific genes that can be activated by inducers are called inducible genes, in contrast to constitutive genes that are almost constantly active.[74] In contrast to both, structural genes encode proteins that are not involved in gene regulation.[74] In addition to regulatory events involving the promoter, gene expression can also be regulated by epigenetic changes to chromatin, which is a complex of DNA and protein found in eukaryotic cells.[74]
Development is the process by which a multicellular organism (plant or animal) goes through a series of changes, starting from a single cell, and taking on various forms that are characteristic of its life cycle.[76] There are four key processes that underlie development: Determination, differentiation, morphogenesis, and growth. Determination sets the developmental fate of a cell, which becomes more restrictive during development. Differentiation is the process by which specialized cells from less specialized cells such as stem cells.[77][78] Stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell.[79] Cellular differentiation dramatically changes a cell's size, shape, membrane potential, metabolic activity, and responsiveness to signals, which are largely due to highly controlled modifications in gene expression and epigenetics.  With a few exceptions, cellular differentiation almost never involves a change in the DNA sequence itself.[80] Thus, different cells can have very different physical characteristics despite having the same genome. Morphogenesis, or the development of body form, is the result of spatial differences in gene expression.[76] A small fraction of the genes in an organism's genome called the developmental-genetic toolkit control the development of that organism. These toolkit genes are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body plan and the number, identity, and pattern of body parts. Among the most important toolkit genes are the Hox genes. Hox genes determine where repeating parts, such as the many vertebrae of snakes, will grow in a developing embryo or larva.[81]
Evolution is a central organizing concept in biology. It is the change in heritable characteristics of populations over successive generations.[82][83] In artificial selection, animals were selectively bred for specific traits.
[84] Given that traits are inherited, populations contain a varied mix of traits, and reproduction is able to increase any population, Darwin argued that in the natural world, it was nature that played the role of humans in selecting for specific traits.[84] Darwin inferred that individuals who possessed heritable traits better adapted to their environments are more likely to survive and produce more offspring than other individuals.[84] He further inferred that this would lead to the accumulation of favorable traits over successive generations, thereby increasing the match between the organisms and their environment.[85][86][87][84][88]
A phylogeny is an evolutionary history of a specific group of organisms or their genes.[90] It can be represented using a phylogenetic tree, a diagram showing lines of descent among organisms or their genes. Each line drawn on the time axis of a tree represents a lineage of descendants of a particular species or population. When a lineage divides into two, it is represented as a fork or split on the phylogenetic tree.[90] Phylogenetic trees are the basis for comparing and grouping different species.[90] Different species that share a feature inherited from a common ancestor are described as having homologous features (or synapomorphy).[91][92][90] Phylogeny provides the basis of biological classification.[90] This classification system is rank-based, with the highest rank being the domain followed by kingdom, phylum, class, order, family, genus, and species.[90] All organisms can be classified as belonging to one of three domains: Archaea (originally Archaebacteria); bacteria (originally eubacteria), or eukarya (includes the protist, fungi, plant, and animal kingdoms).[93]
The history of life on Earth traces how organisms have evolved from the earliest emergence of life to present day. Earth formed about 4.5 billion years ago and all life on Earth, both living and extinct, descended from a last universal common ancestor that lived about 3.5 billion years ago.[94][95] Geologists have developed a geologic time scale that divides the history of the Earth into major divisions, starting with four eons (Hadean, Archean, Proterozoic, and Phanerozoic), the first three of which are collectively known as the Precambrian, which lasted approximately 4 billion years.[96] Each eon can be divided into eras, with the Phanerozoic eon that began 539 million years ago[97] being subdivided into Paleozoic, Mesozoic, and Cenozoic eras.[96] These three eras together comprise eleven periods (Cambrian, Ordovician, Silurian, Devonian, Carboniferous, Permian, Triassic, Jurassic, Cretaceous, Tertiary, and Quaternary).[96]
The similarities among all known present-day species indicate that they have diverged through the process of evolution from their common ancestor.[98] Biologists regard the ubiquity of the genetic code as evidence of universal common descent for all bacteria, archaea, and eukaryotes.[99][10][100][101] Microbal mats of coexisting bacteria and archaea were the dominant form of life in the early Archean epoch and many of the major steps in early evolution are thought to have taken place in this environment.[102] The earliest evidence of eukaryotes dates from 1.85 billion years ago,[103][104] and while they may have been present earlier, their diversification accelerated when they started using oxygen in their metabolism. Later, around 1.7 billion years ago, multicellular organisms began to appear, with differentiated cells performing specialised functions.[105]
Algae-like multicellular land plants are dated back even to about 1 billion years ago,[106] although evidence suggests that microorganisms formed the earliest terrestrial ecosystems, at least 2.7 billion years ago.[107] Microorganisms are thought to have paved the way for the inception of land plants in the Ordovician period. Land plants were so successful that they are thought to have contributed to the Late Devonian extinction event.[108]
Bacteria are a type of cell that constitute a large domain of prokaryotic microorganisms. Typically a few micrometers in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste,[118] and the deep biosphere of the earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised, and only about 27 percent of the bacterial phyla have species that can be grown in the laboratory.[119]
Archaea constitute the other domain of prokaryotic cells and were initially classified as bacteria, receiving the name archaebacteria (in the Archaebacteria kingdom), a term that has fallen out of use.[120] Archaeal cells have unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as the flat and square cells of Haloquadratum walsbyi.[121] Despite this morphological similarity to bacteria, archaea possess genes and several metabolic pathways that are more closely related to those of eukaryotes, notably for the enzymes involved in transcription and translation. Other aspects of archaeal biochemistry are unique, such as their reliance on ether lipids in their cell membranes,[122] including archaeols. Archaea use more energy sources than eukaryotes: these range from organic compounds, such as sugars, to ammonia, metal ions or even hydrogen gas. Salt-tolerant archaea (the Haloarchaea) use sunlight as an energy source, and other species of archaea fix carbon, but unlike plants and cyanobacteria, no known species of archaea does both. Archaea reproduce asexually by binary fission, fragmentation, or budding; unlike bacteria, no known species of Archaea form endospores.
The first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms. Improved molecular detection tools led to the discovery of archaea in almost every habitat, including soil, oceans, and marshlands. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet.
Archaea are a major part of Earth's life. They are part of the microbiota of all organisms. In the human microbiome, they are important in the gut, mouth, and on the skin.[123] Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example.[124]
Eukaryotes are hypothesized to have split from archaea, which was followed by their endosymbioses with bacteria (or symbiogenesis) that gave rise to mitochondria and chloroplasts, both of which are now part of modern-day eukaryotic cells.[125] The major lineages of eukaryotes diversified in the Precambrian about 1.5 billion years ago and can be classified into eight major clades: alveolates, excavates, stramenopiles, plants, rhizarians, amoebozoans, fungi, and animals.[125] Five of these clades are collectively known as protists, which are mostly microscopic eukaryotic organisms that are not plants, fungi, or animals.[125] While it is likely that protists share a common ancestor (the last eukaryotic common ancestor),[126] protists by themselves do not constitute a separate clade as some protists may be more closely related to plants, fungi, or animals than they are to other protists. Like groupings such as algae, invertebrates, or protozoans, the protist grouping is not a formal taxonomic group but is used for convenience.[125][127] Most protists are unicellular; these are called microbial eukaryotes.[125]
Plants are mainly multicellular organisms, predominantly photosynthetic eukaryotes of the kingdom Plantae, which would exclude fungi and some algae. Plant cells were derived by endosymbiosis of a cyanobacterium into an early eukaryote about one billion years ago, which gave rise to chloroplasts.[128] The first several clades that emerged following primary endosymbiosis were aquatic and most of the aquatic photosynthetic eukaryotic organisms are collectively described as algae, which is a term of convenience as not all algae are closely related.[128] Algae comprise several distinct clades such as glaucophytes, which are microscopic freshwater algae that may have resembled in form to the early unicellular ancestor of Plantae.[128] Unlike glaucophytes, the other algal clades such as red and green algae are multicellular. Green algae comprise three major clades: chlorophytes, coleochaetophytes, and stoneworts.[128]
Fungi are eukaryotes that digest foods outside their bodies,[129] secreting digestive enzymes that break down large food molecules before absorbing them through their cell membranes. Many fungi are also saprobes, feeding on dead organic matter, making them important decomposers in ecological systems.[129]
Viruses are submicroscopic infectious agents that replicate inside the cells of organisms.[131] Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea.[132][133] More than 6,000 virus species have been described in detail.[134] Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity.[135][136]
Ecology is the study of the distribution and abundance of life, the interaction between organisms and their environment.[140]
The community of living (biotic) organisms in conjunction with the nonliving (abiotic) components (e.g., water, light, radiation, temperature, humidity, atmosphere, acidity, and soil) of their environment is called an ecosystem.[141][142][143] These biotic and abiotic components are linked together through nutrient cycles and energy flows.[144] Energy from the sun enters the system through photosynthesis and is incorporated into plant tissue. By feeding on plants and on one another, animals move matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes.[145]
A population is the group of organisms of the same species that occupies an area and reproduce from generation to generation.[146][147][148][149][150] Population size can be estimated by multiplying population density by the area or volume. The carrying capacity of an environment is the maximum population size of a species that can be sustained by that specific environment, given the food, habitat, water, and other resources that are available.[151] The carrying capacity of a population can be affected by changing environmental conditions such as changes in the availability resources and the cost of maintaining them. In human populations, new technologies such as the Green revolution have helped increase the Earth's carrying capacity for humans over time, which has stymied the attempted predictions of impending population decline, the most famous of which was by Thomas Malthus in the 18th century.[146]
In the global ecosystem or biosphere, matter exists as different interacting compartments, which can be biotic or abiotic as well as accessible or inaccessible, depending on their forms and locations.[159] For example, matter from terrestrial autotrophs are both biotic and accessible to other organisms whereas the matter in rocks and minerals are abiotic and inaccessible. A biogeochemical cycle is a pathway by which specific elements of matter are turned over or moved through the biotic (biosphere) and the abiotic (lithosphere, atmosphere, and hydrosphere) compartments of Earth. There are biogeochemical cycles for nitrogen, carbon, and water.
Conservation biology is the study of the conservation of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions.[160][161][162] It is concerned with factors that influence the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity.[163][164][165][166] The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years,[167] which has contributed to poverty, starvation, and will reset the course of evolution on this planet.[168][169] Biodiversity affects the functioning of ecosystems, which provide a variety of services upon which people depend. Conservation biologists research and educate on the trends of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Organizations and citizens are responding to the current biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales.[170][163][164][165]
In biology, regeneration is the process of renewal, restoration, and tissue growth that makes genomes, cells, organisms, and ecosystems resilient to natural fluctuations or events that cause disturbance or damage.[1] Every species is capable of regeneration, from bacteria to humans.[2][3] Regeneration can either be complete[4] where the new tissue is the same as the lost tissue,[4] or incomplete[5] where after the necrotic tissue comes fibrosis.[5]
At its most elementary level, regeneration is mediated by the molecular processes of gene regulation and involves the cellular processes of cell proliferation, morphogenesis and cell differentiation.[6][7] Regeneration in biology, however, mainly refers to the morphogenic processes that characterize the phenotypic plasticity of traits allowing multi-cellular organisms to repair and maintain the integrity of their physiological and morphological states. Above the genetic level, regeneration is fundamentally regulated by asexual cellular processes.[8] Regeneration is different from reproduction. For example, hydra perform regeneration but reproduce by the method of budding.
The hydra and the planarian flatworm have long served as model organisms for their highly adaptive regenerative capabilities.[9] Once wounded, their cells become activated and restore the organs back to their pre-existing state.[10] The Caudata ("urodeles"; salamanders and newts), an order of tailed amphibians, is possibly the most adept vertebrate group at regeneration, given their capability of regenerating limbs, tails, jaws, eyes and a variety of internal structures.[2] The regeneration of organs is a common and widespread adaptive capability among metazoan creatures.[9] In a related context, some animals are able to reproduce asexually through fragmentation, budding, or fission.[8] A planarian parent, for example, will constrict, split in the middle, and each half generates a new end to form two clones of the original.[11]
Echinoderms (such as the sea star), crayfish, many reptiles, and amphibians exhibit remarkable examples of tissue regeneration. The case of autotomy, for example, serves as a defensive function as the animal detaches a limb or tail to avoid capture. After the limb or tail has been autotomized, cells move into action and the tissues will regenerate.[12][13][14] In some cases a shed limb can itself regenerate a new individual.[15] Limited regeneration of limbs occurs in most fishes and salamanders, and tail regeneration takes place in larval frogs and toads (but not adults). The whole limb of a salamander or a triton will grow again and again after amputation. In reptiles, chelonians, crocodilians and snakes are unable to regenerate lost parts, but many (not all) kinds of lizards, geckos and iguanas possess regeneration capacity in a high degree. Usually, it involves dropping a section of their tail and regenerating it as part of a defense mechanism. While escaping a predator, if the predator catches the tail, it will disconnect.[16]
Ecosystems can be regenerative. Following a disturbance, such as a fire or pest outbreak in a forest, pioneering species will occupy, compete for space, and establish themselves in the newly opened habitat. The new growth of seedlings and community assembly process is known as regeneration in ecology.[17][18]
Pattern formation in the morphogenesis of an animal is regulated by genetic induction factors that put cells to work after damage has occurred. Neural cells, for example, express growth-associated proteins, such as GAP-43, tubulin, actin, an array of novel neuropeptides, and cytokines that induce a cellular physiological response to regenerate from the damage.[19] Many of the genes that are involved in the original development of tissues are reinitialized during the regenerative process. Cells in the primordia of zebrafish fins, for example, express four genes from the homeobox msx family during development and regeneration.[20]
Many arthropods can regenerate limbs and other appendages following either injury or autotomy.[23] Regeneration capacity is constrained by the developmental stage and ability to molt.
Crustaceans, which continually molt, can regenerate throughout their lifetimes.[24] While molting cycles are generally hormonally regulated, limb amputation induces premature molting.[23][25]
Hemimetabolous insects such as crickets can regenerate limbs as nymphs, before their final molt.[26]
Holometabolous insects can regenerate appendages as larvae prior to the final molt and metamorphosis. Beetle larvae, for example, can regenerate amputated limbs. Fruit fly larvae do not have limbs but can regenerate their appendage primordia, imaginal discs.[27] In both systems, the regrowth of the new tissue delays pupation.[27][28]
Mechanisms underlying appendage limb regeneration in insects and crustaceans are highly conserved.[29] During limb regeneration species in both taxa form a blastema that proliferates and grows to repattern the missing tissue.[30]
Arachnids, including scorpions, are known to regenerate their venom, although the content of the regenerated venom is different from the original venom during its regeneration, as the venom volume is replaced before the active proteins are all replenished.[31]
The fruit fly Drosophila melanogaster is a useful model organism to understand the molecular mechanisms that control regeneration, especially gut and germline regeneration.[32] In these tissues, resident stem cells continually renew lost cells.[33] The Hippo signaling pathway was discovered in flies and was found to be required for midgut regeneration. Later, this conserved signaling pathway was also found to be essential for regeneration of many mammalian tissues, including heart, liver, skin, and lung, and intestine.[34]
Many annelids (segmented worms) are capable of regeneration.[35] For example, Chaetopterus variopedatus and Branchiomma nigromaculata can regenerate both anterior and posterior body parts after latitudinal bisection.[36] The relationship between somatic and germline stem cell regeneration has been studied at the molecular level in the annelid Capitella teleta.[37] Leeches, however, appear incapable of segmental regeneration.[38] Furthermore, their close relatives, the branchiobdellids, are also incapable of segmental regeneration.[38][35] However, certain individuals, like the lumbriculids, can regenerate from only a few segments.[38] Segmental regeneration in these animals is epimorphic and occurs through blastema formation.[38] Segmental regeneration has been gained and lost during annelid evolution, as seen in oligochaetes, where head regeneration has been lost three separate times.[38]
Along with epimorphosis, some polychaetes like Sabella pavonina experience morphallactic regeneration.[38][39] Morphallaxis involves the de-differentiation, transformation, and re-differentation of cells to regenerate tissues. How prominent morphallactic regeneration is in oligochaetes is currently not well understood. Although relatively under-reported, it is possible that morphallaxis is a common mode of inter-segment regeneration in annelids. Following regeneration in L. variegatus, past posterior segments sometimes become anterior in the new body orientation, consistent with morphallaxis.
Following amputation, most annelids are capable of sealing their body via rapid muscular contraction. Constriction of body muscle can lead to infection prevention. In certain species, such as Limnodrilus, autolysis can be seen within hours after amputation in the ectoderm and mesoderm. Amputation is also thought to cause a large migration of cells to the injury site, and these form a wound plug.
Regeneration research using Planarians began in the late 1800s and was popularized by T.H. Morgan at the beginning of the 20th century.[43] Alejandro Sanchez-Alvarado and Philip Newmark transformed planarians into a model genetic organism in the beginning of the 20th century to study the molecular mechanisms underlying regeneration in these animals.[45] Planarians exhibit an extraordinary ability to regenerate lost body parts. For example, a planarian split lengthwise or crosswise will regenerate into two separate individuals. In one experiment, T.H. Morgan found that a piece corresponding to 1/279th of a planarian[43] or a fragment with as few as 10,000 cells can successfully regenerate into a new worm within one to two weeks.[46] After amputation, stump cells form a blastema formed from neoblasts, pluripotent cells found throughout the planarian body.[47] New tissue grows from neoblasts with neoblasts comprising between 20 and 30% of all planarian cells.[46] Recent work has confirmed that neoblasts are totipotent since one single neoblast can regenerate an entire irradiated animal that has been rendered incapable of regeneration.[48] In order to prevent starvation a planarian will use their own cells for energy, this phenomenon is known as de-growth.[10]
Limb regeneration in the axolotl and newt has been extensively studied and researched. The nineteenth century studies of this subject are reviewed in Holland (2021).[49] Urodele amphibians, such as salamanders and newts, display the highest regenerative ability among tetrapods.[50][49] As such, they can fully regenerate their limbs, tail, jaws, and retina via epimorphic regeneration leading to functional replacement with new tissue.[51]  Salamander limb regeneration occurs in two main steps. First, the local cells dedifferentiate at the wound site into progenitor to form a blastema.[52] Second, the blastemal cells will undergo cell proliferation, patterning, cell differentiation and tissue growth using similar genetic mechanisms that deployed during embryonic development.[53] Ultimately, blastemal cells will generate all the cells for the new structure.[50]
In spite of the historically few researchers studying limb regeneration, remarkable progress has been made recently in establishing the neotenous amphibian the axolotl (Ambystoma mexicanum) as a model genetic organism. This progress has been facilitated by advances in genomics, bioinformatics, and somatic cell transgenesis in other fields, that have created the opportunity to investigate the mechanisms of important biological properties, such as limb regeneration, in the axolotl.[53] The Ambystoma Genetic Stock Center (AGSC) is a self-sustaining, breeding colony of the axolotl supported by the National Science Foundation as a Living Stock Collection. Located at the University of Kentucky, the AGSC is dedicated to supplying genetically well-characterized axolotl embryos, larvae, and adults to laboratories throughout the United States and abroad. An NIH-funded NCRR grant has led to the establishment of the Ambystoma EST database, the Salamander Genome Project (SGP) that has led to the creation of the first amphibian gene map and several annotated molecular data bases, and the creation of the research community web portal.[60] In 2022, a first spatiotemporal map revealed key insights about axolotl brain regeneration, also providing the interactive Axolotl Regenerative Telencephalon Interpretation via Spatiotemporal Transcriptomic Atlas .[61][62]
Anurans (frogs) can only regenerate their limbs during embryonic development.[63] Reactive oxygen species (ROS) appear to be required for a regeneration response in the anuran larvae.[64] ROS production is essential to activate the Wnt signaling pathway, which has been associated with regeneration in other systems.[64]
Once the limb skeleton has developed in frogs, regeneration does not occur (Xenopus can grow a cartilaginous spike after amputation).[63]  The adult Xenopus laevis is used as a model organism for regenerative medicine. In 2022, a cocktail of drugs and hormones (1,4-DPCA, BDNF, growth hormone, resolvin D5, and retinoic acid), in a single dose lasting 24 hours, was shown to trigger long-term leg regeneration in adult X. laevis. Instead of a single spike, a paddle-shaped growth is obtained at the end of the limb by 18 months.[65]
Hydra is a genus of freshwater polyp in the phylum Cnidaria with highly proliferative stem cells that gives them the ability to regenerate their entire body.[66] Any fragment larger than a few hundred epithelial cells that is isolated from the body has the ability to regenerate into a smaller version of itself.[66] The high proportion of stem cells in the hydra supports its efficient regenerative ability.[67]
Regeneration among hydra occurs as foot regeneration arising from the basal part of the body, and head regeneration, arising from the apical region.[66] Regeneration tissues that are cut from the gastric region contain polarity, which allows them to distinguish between regenerating a head in the apical end and a foot in the basal end so that both regions are present in the newly regenerated organism.[66] Head regeneration requires complex reconstruction of the area, while foot regeneration is much simpler, similar to tissue repair.[68] In both foot and head regeneration, however, there are two distinct molecular cascades that occur once the tissue is wounded: early injury response and a subsequent, signal-driven pathway of the regenerating tissue that leads to cellular differentiation.[67] This early-injury response includes epithelial cell stretching for wound closure, the migration of interstitial progenitors towards the wound, cell death, phagocytosis of cell debris, and reconstruction of the extracellular matrix.[67]
Regeneration in hydra has been defined as morphallaxis, the process where regeneration results from remodeling of existing material without cellular proliferation.[69][70] If a hydra is cut into two pieces, the remaining severed sections form two fully functional and independent hydra, approximately the same size as the two smaller severed sections.[66] This occurs through the exchange and rearrangement of soft tissues without the formation of new material.[67]
Owing to a limited literature on the subject, birds are believed to have very limited regenerative abilities as adults. Some studies[71] on roosters have suggested that birds can adequately regenerate some parts of the limbs and depending on the conditions in which regeneration takes place, such as age of the animal, the inter-relationship of the injured tissue with other muscles, and the type of operation, can involve complete regeneration of some musculoskeletal structure. Werber and Goldschmidt (1909) found that the goose and duck were capable of regenerating their beaks after partial amputation[71]  and Sidorova (1962) observed liver regeneration via hypertrophy in roosters.[72] Birds are also capable of regenerating the hair cells in their cochlea following noise damage or ototoxic drug damage.[73] Despite this evidence, contemporary studies suggest reparative regeneration in avian species is limited to periods during embryonic development. An array of molecular biology techniques have been successful in manipulating cellular pathways known to contribute to spontaneous regeneration in chick embryos.[74] For instance, removing a portion of the elbow joint in a chick embryo via window excision or slice excision and comparing joint tissue specific markers and cartilage markers showed that window excision allowed 10 out of 20 limbs to regenerate and expressed joint genes similarly to a developing embryo. In contrast, slice excision did not allow the joint to regenerate due to the fusion of the skeletal elements seen by an expression of cartilage markers.[75]
Similar to the physiological regeneration of hair in mammals, birds can regenerate their feathers in order to repair damaged feathers or to attract mates with their plumage. Typically, seasonal changes that are associated with breeding seasons will prompt a hormonal signal for birds to begin regenerating feathers. This has been experimentally induced using thyroid hormones in the Rhode Island Red Fowls.[76]
Mammals are capable of cellular and physiological regeneration, but have generally poor reparative regenerative ability across the group.[1][24] Examples of physiological regeneration in mammals include epithelial renewal (e.g., skin and intestinal tract), red blood cell replacement, antler regeneration and hair cycling.[77][78] Male deer lose their antlers annually during the months of January to April then through regeneration are able to regrow them as an example of physiological regeneration. A deer antler is the only appendage of a mammal that can be regrown every year.[79] While reparative regeneration is a rare phenomenon in mammals, it does occur.  A well-documented example is regeneration of the digit tip distal to the nail bed.[80] Reparative regeneration has also been observed in rabbits, pikas and African spiny mice.  In 2012, researchers discovered that two species of African Spiny Mice, Acomys kempi and Acomys percivali, were capable of completely regenerating the autotomically released or otherwise damaged tissue. These species can regrow hair follicles, skin, sweat glands, fur and cartilage.[81] In addition to these two species, subsequent studies demonstrated that Acomys cahirinus could regenerate skin and excised tissue in the ear pinna.[82][83]
Despite these examples, it is generally accepted that adult mammals have limited regenerative capacity compared to most vertebrate embryos/larvae, adult salamanders and fish.[84] But the regeneration therapy approach of Robert O. Becker, using electrical stimulation, has shown promising results for rats[85] and mammals in general.[86]
Some researchers have also claimed that the MRL mouse strain exhibits enhanced regenerative abilities. Work comparing the differential gene expression of scarless healing MRL mice and a poorly-healing C57BL/6 mouse strain, identified 36 genes differentiating the healing process between MRL mice and other mice.[87][88] Study of the regenerative process in these animals is aimed at discovering how to duplicate them in humans, such as deactivation of the p21 gene.[89][90] However, recent work has shown that MRL mice actually close small ear holes with scar tissue, rather than regeneration as originally claimed.[82]
MRL mice are not protected against myocardial infarction; heart regeneration in adult mammals (neocardiogenesis) is limited, because heart muscle cells are nearly all terminally differentiated. MRL mice show the same amount of cardiac injury and scar formation as normal mice after a heart attack.[91] However, recent studies provide evidence that this may not always be the case, and that MRL mice can regenerate after heart damage.[92]
The regrowth of lost tissues or organs in the human body is being researched. Some tissues such as skin regrow quite readily; others have been thought to have little or no capacity for regeneration, but ongoing research suggests that there is some hope for a variety of tissues and organs.[1][93] Human organs that have been regenerated include the bladder, vagina and the penis.[94]
As are all metazoans, humans are capable of physiological regeneration (i.e. the replacement of cells during homeostatic maintenance that does not necessitate injury). For example, the regeneration of red blood cells via erythropoiesis occurs through the maturation of erythrocytes from hematopoietic stem cells in the bone marrow, their subsequent circulation for around 90 days in the blood stream, and their eventual cell-death in the spleen.[95] Another example of physiological regeneration is the sloughing and rebuilding of a functional endometrium during each menstrual cycle in females in response to varying levels of circulating estrogen and progesterone.[96]
However, humans are limited in their capacity for reparative regeneration, which occurs in response to injury. One of the most studied regenerative responses in humans is the hypertrophy of the liver following liver injury.[97][98] For example, the original mass of the liver is re-established in direct proportion to the amount of liver removed following partial hepatectomy,[99] which indicates that signals from the body regulate liver mass precisely, both positively and negatively, until the desired mass is reached. This response is considered cellular regeneration (a form of compensatory hypertrophy) where the function and mass of the liver is regenerated through the proliferation of existing mature hepatic cells (mainly hepatocytes), but the exact morphology of the liver is not regained.[98] This process is driven by growth factor and cytokine regulated pathways.[97] The normal sequence of inflammation and regeneration does not function accurately in cancer. Specifically, cytokine stimulation of cells leads to expression of genes that change cellular functions and suppress the immune response.[100]
Adult neurogenesis is also a form of cellular regeneration. For example, hippocampal neuron renewal occurs in normal adult humans at an annual turnover rate of 1.75% of neurons.[101] Cardiac myocyte renewal has been found to occur in normal adult humans,[102] and at a higher rate in adults following acute heart injury such as infarction.[103] Even in adult myocardium following infarction, proliferation is only found in around 1% of myocytes around the area of injury, which is not enough to restore function of cardiac muscle. However, this may be an important target for regenerative medicine as it implies that regeneration of cardiomyocytes, and consequently of myocardium, can be induced.
Another example of reparative regeneration in humans is fingertip regeneration, which occurs after phalange amputation distal to the nail bed (especially in children)[104][105] and rib regeneration, which occurs following osteotomy for scoliosis treatment (though usually regeneration is only partial and may take up to one year).[106]
Yet another example of regeneration in humans is vas deferens regeneration, which occurs after a vasectomy and which results in vasectomy failure.[107]
The ability and degree of regeneration in reptiles differs among the various species, but the most notable and well-studied occurrence is tail-regeneration in lizards.[108][109][110] In addition to lizards, regeneration has been observed in the tails and maxillary bone of crocodiles and adult neurogenesis has also been noted.[108][111][112] Tail regeneration has never been observed in snakes.[108] Lizards possess the highest regenerative capacity as a group.[108][109][110][113] Following autotomous tail loss, epimorphic regeneration of a new tail proceeds through a blastema-mediated process that results in a functionally and morphologically similar structure.[108][109]
Rhodopsin regeneration has been studied in skates and rays. After complete photo-bleaching, rhodopsin can completely regenerate within 2 hours in the retina.[114]
White bamboo sharks can regenerate at least two-thirds of their liver and this has been linked to three micro RNAs, xtr-miR-125b, fru-miR-204, and has-miR-142-3p_R-. In one study, two-thirds of the liver was removed and within 24 hours more than half of the liver had undergone hypertrophy.[115]
Some sharks can regenerate scales and even skin following damage. Within two weeks of skin wounding, mucus is secreted into the wound and this initiates the healing process. One study showed that the majority of the wounded area was regenerated within 4 months, but the regenerated area also showed a high degree of variability.[116]
A disease is a particular abnormal condition that negatively affects the structure or function of all or part of an organism, and that is not immediately due to any external injury.[1][2] Diseases are often known to be medical conditions that are associated with specific signs and symptoms. A disease may be caused by external factors such as pathogens or by internal dysfunctions. For example, internal dysfunctions of the immune system can produce a variety of different diseases, including various forms of immunodeficiency, hypersensitivity, allergies and autoimmune disorders.
In humans, disease is often used more broadly to refer to any condition that causes pain, dysfunction, distress, social problems, or death to the person affected, or similar problems for those in contact with the person. In this broader sense, it sometimes includes injuries, disabilities, disorders, syndromes, infections, isolated symptoms, deviant behaviors, and atypical variations of structure and function, while in other contexts and for other purposes these may be considered distinguishable categories. Diseases can affect people not only physically, but also mentally, as contracting and living with a disease can alter the affected person's perspective on life.
Death due to disease is called death by natural causes. There are four main types of disease: infectious diseases, deficiency diseases, hereditary diseases (including both genetic diseases and non-genetic hereditary diseases), and physiological diseases. Diseases can also be classified in other ways, such as communicable versus non-communicable diseases. The deadliest diseases in humans are coronary artery disease (blood flow obstruction), followed by cerebrovascular disease and lower respiratory infections.[3] In developed countries, the diseases that cause the most sickness overall are neuropsychiatric conditions, such as depression and anxiety.
The study of disease is called pathology, which includes the study of etiology, or cause.
In many cases, terms such as disease, disorder, morbidity, sickness and illness are used interchangeably; however, there are situations when specific terms are considered preferable.[4]
In an infectious disease, the incubation period is the time between infection and the appearance of symptoms. The latency period is the time between infection and the ability of the disease to spread to another person, which may precede, follow, or be simultaneous with the appearance of symptoms. Some viruses also exhibit a dormant phase, called viral latency, in which the virus hides in the body in an inactive state. For example, varicella zoster virus causes chickenpox in the acute phase; after recovery from chickenpox, the virus may remain dormant in nerve cells for many years, and later cause herpes zoster (shingles).
Diseases may be classified by cause, pathogenesis (mechanism by which the disease is caused), or by symptom(s). Alternatively, diseases may be classified according to the organ system involved, though this is often complicated since many diseases affect more than one organ.
A chief difficulty in nosology is that diseases often cannot be defined and classified clearly, especially when cause or pathogenesis are unknown. Thus diagnostic terms often only reflect a symptom or set of symptoms (syndrome).
Classical classification of human disease derives from the observational correlation between pathological analysis and clinical syndromes. Today it is preferred to classify them by their cause if it is known.[23]
The most known and used classification of diseases is the World Health Organization's ICD. This is periodically updated. Currently, the last publication is the ICD-11.
Only some diseases such as influenza are contagious and commonly believed infectious. The microorganisms that cause these diseases are known as pathogens and include varieties of bacteria, viruses, protozoa, and fungi. Infectious diseases can be transmitted, e.g. by hand-to-mouth contact with infectious material on surfaces, by bites of insects or other carriers of the disease, and from contaminated water or food (often via fecal contamination), etc.[24] Also, there are sexually transmitted diseases. In some cases, microorganisms that are not readily spread from person to person play a role, while other diseases can be prevented or ameliorated with appropriate nutrition or other lifestyle changes.
Some diseases, such as most (but not all) forms of cancer, heart disease, and mental disorders, are non-infectious diseases. Many non-infectious diseases have a partly or completely genetic basis (see genetic disorder) and may thus be transmitted from one generation to another.
Social determinants of health are the social conditions in which people live that determine their health. Illnesses are generally related to social, economic, political, and environmental circumstances. Social determinants of health have been recognized by several health organizations such as the Public Health Agency of Canada and the World Health Organization to greatly influence collective and personal well-being. The World Health Organization's Social Determinants Council also recognizes Social determinants of health in poverty.
When the cause of a disease is poorly understood, societies tend to mythologize the disease or use it as a metaphor or symbol of whatever that culture considers evil. For example, until the bacterial cause of tuberculosis was discovered in 1882, experts variously ascribed the disease to heredity, a sedentary lifestyle, depressed mood, and overindulgence in sex, rich food, or alcohol, all of which were social ills at the time.[25]
When a disease is caused by a pathogenic organism (e.g., when malaria is caused by Plasmodium), one should not confuse the pathogen (the cause of the disease) with disease itself. For example, West Nile virus (the pathogen) causes West Nile fever (the disease). The misuse of basic definitions in epidemiology is frequent in scientific publications.[26]
Many diseases and disorders can be prevented through a variety of means. These include sanitation, proper nutrition, adequate exercise, vaccinations and other self-care and public health measures, such as obligatory face mask mandates[citation needed].
Medical therapies or treatments are efforts to cure or improve a disease or other health problems. In the medical field, therapy is synonymous with the word treatment. Among psychologists, the term may refer specifically to psychotherapy or "talk therapy". Common treatments include medications, surgery, medical devices, and self-care. Treatments may be provided by an organized health care system, or informally, by the patient or family members.
Preventive healthcare is a way to avoid an injury, sickness, or disease in the first place. A treatment or cure is applied after a medical problem has already started. A treatment attempts to improve or remove a problem, but treatments may not produce permanent cures, especially in chronic diseases. Cures are a subset of treatments that reverse diseases completely or end medical problems permanently. Many diseases that cannot be completely cured are still treatable. Pain management (also called pain medicine) is that branch of medicine employing an interdisciplinary approach to the relief of pain and improvement in the quality of life of those living with pain.[27]
Treatment for medical emergencies must be provided promptly, often through an emergency department or, in less critical situations, through an urgent care facility.
Epidemiology is the study of the factors that cause or encourage diseases. Some diseases are more common in certain geographic areas, among people with certain genetic or socioeconomic characteristics, or at different times of the year.
Epidemiology is considered a cornerstone methodology of public health research and is highly regarded in evidence-based medicine for identifying risk factors for diseases. In the study of communicable and non-communicable diseases, the work of epidemiologists ranges from outbreak investigation to study design, data collection, and analysis including the development of statistical models to test hypotheses and the documentation of results for submission to peer-reviewed journals. Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic. Epidemiologists rely on a number of other scientific disciplines such as biology (to better understand disease processes), biostatistics (the current raw information available), Geographic Information Science (to store data and map disease patterns) and social science disciplines (to better understand proximate and distal risk factors). Epidemiology can help identify causes as well as guide prevention efforts.
In studying diseases, epidemiology faces the challenge of defining them. Especially for poorly understood diseases, different groups might use significantly different definitions. Without an agreed-on definition, different researchers may report different numbers of cases and characteristics of the disease.[28]
Some morbidity databases are compiled with data supplied by states and territories health authorities, at national levels[29][30] or larger scale (such as European Hospital Morbidity Database (HMDB))[31] which may contain hospital discharge data by detailed diagnosis, age and sex. The European HMDB data was submitted by European countries to the World Health Organization Regional Office for Europe.
Disease burden is the impact of a health problem in an area measured by financial cost, mortality, morbidity, or other indicators.
There are several measures used to quantify the burden imposed by diseases on people. The years of potential life lost (YPLL) is a simple estimate of the number of years that a person's life was shortened due to a disease. For example, if a person dies at the age of 65 from a disease, and would probably have lived until age 80 without that disease, then that disease has caused a loss of 15 years of potential life. YPLL measurements do not account for how disabled a person is before dying, so the measurement treats a person who dies suddenly and a person who died at the same age after decades of illness as equivalent. In 2004, the World Health Organization calculated that 932 million years of potential life were lost to premature death.[32]
How a society responds to diseases is the subject of medical sociology.
A condition may be considered a disease in some cultures or eras but not in others. For example, obesity can represent wealth and abundance, and is a status symbol in famine-prone areas and some places hard-hit by HIV/AIDS.[34] Epilepsy is considered a sign of spiritual gifts among the Hmong people.[35]
Sickness confers the social legitimization of certain benefits, such as illness benefits, work avoidance, and being looked after by others. The person who is sick takes on a social role called the sick role. A person who responds to a dreaded disease, such as cancer, in a culturally acceptable fashion may be publicly and privately honored with higher social status.[36] In return for these benefits, the sick person is obligated to seek treatment and work to become well once more. As a comparison, consider pregnancy, which is not interpreted as a disease or sickness, even if the mother and baby may both benefit from medical care.
Most religions grant exceptions from religious duties to people who are sick. For example, one whose life would be endangered by fasting on Yom Kippur or during Ramadan is exempted from the requirement, or even forbidden from participating. People who are sick are also exempted from social duties. For example, ill health is the only socially acceptable reason for an American to refuse an invitation to the White House.[37]
The identification of a condition as a disease, rather than as simply a variation of human structure or function, can have significant social or economic implications. The controversial recognition of diseases such as repetitive stress injury (RSI) and post-traumatic stress disorder (PTSD) has had a number of positive and negative effects on the financial and other responsibilities of governments, corporations, and institutions towards individuals, as well as on the individuals themselves. The social implication of viewing aging as a disease could be profound, though this classification is not yet widespread.
Lepers were people who were historically shunned because they had an infectious disease, and the term "leper" still evokes social stigma. Fear of disease can still be a widespread social phenomenon, though not all diseases evoke extreme social stigma.
Social standing and economic status affect health. Diseases of poverty are diseases that are associated with poverty and low social status; diseases of affluence are diseases that are associated with high social and economic status. Which diseases are associated with which states vary according to time, place, and technology. Some diseases, such as diabetes mellitus, may be associated with both poverty (poor food choices) and affluence (long lifespans and sedentary lifestyles), through different mechanisms. The term lifestyle diseases describes diseases associated with longevity and that are more common among older people. For example, cancer is far more common in societies in which most members live until they reach the age of 80 than in societies in which most members die before they reach the age of 50.
An illness narrative is a way of organizing a medical experience into a coherent story that illustrates the sick individual's personal experience.
People use metaphors to make sense of their experiences with disease. The metaphors move disease from an objective thing that exists to an affective experience. The most popular metaphors draw on military concepts: Disease is an enemy that must be feared, fought, battled, and routed. The patient or the healthcare provider is a warrior, rather than a passive victim or bystander. The agents of communicable diseases are invaders; non-communicable diseases constitute internal insurrection or civil war. Because the threat is urgent, perhaps a matter of life and death, unthinkably radical, even oppressive, measures are society's and the patient's moral duty as they courageously mobilize to struggle against destruction. The War on Cancer is an example of this metaphorical use of language.[38] This language is empowering to some patients, but leaves others feeling like they are failures.[39]
Another class of metaphors describes the experience of illness as a journey: The person travels to or from a place of disease, and changes himself, discovers new information, or increases his experience along the way. He may travel "on the road to recovery" or make changes to "get on the right track" or choose "pathways".[38][39] Some are explicitly immigration-themed: the patient has been exiled from the home territory of health to the land of the ill, changing identity and relationships in the process.[40] This language is more common among British healthcare professionals than the language of physical aggression.[39]
Some metaphors are disease-specific. Slavery is a common metaphor for addictions: The alcoholic is enslaved by drink, and the smoker is captive to nicotine. Some cancer patients treat the loss of their hair from chemotherapy as a metonymy or metaphor for all the losses caused by the disease.[38]
Some diseases are used as metaphors for social ills: "Cancer" is a common description for anything that is endemic and destructive in society, such as poverty, injustice, or racism. AIDS was seen as a divine judgment for moral decadence, and only by purging itself from the "pollution" of the "invader" could society become healthy again.[38] More recently, when AIDS seemed less threatening, this type of emotive language was applied to avian flu and type 2 diabetes mellitus.[41] Authors in the 19th century commonly used tuberculosis as a symbol and a metaphor for transcendence. People with the disease were portrayed in literature as having risen above daily life to become ephemeral objects of spiritual or artistic achievement. In the 20th century, after its cause was better understood, the same disease became the emblem of poverty, squalor, and other social problems.[40]
Wildlife trade refers to the products that are derived from non-domesticated animals or plants usually extracted from their natural environment or raised under controlled conditions. It can involve the trade of living or dead individuals, tissues such as skins, bones or meat, or other products. Legal wildlife trade is regulated by the United Nations' Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES), which currently has 184 member countries called Parties.[1] Illegal wildlife trade is widespread and constitutes one of the major illegal economic activities, comparable to the traffic of drugs and weapons.[2]
Wildlife trade is a serious conservation problem, has a negative effect on the viability of many wildlife populations and is one of the major threats to the survival of vertebrate species.[3] The illegal wildlife trade has been linked to the emergence and spread of new infectious diseases in humans, including emergent viruses.[4][5] Global initiative like the United Nations Sustainable Development Goal 15 have a target to end the illegal supply of wildlife.[6]
Wildlife use is a general term for all uses of wildlife products, including ritual or religious uses, consumption of bushmeat and different forms of trade. Wildlife use is usually linked to hunting or poaching. Wildlife trade can be differentiated in legal and illegal trade, and both can have domestic (local or national) or international markets, but they might be often related with each other.[7]
The volume of international trade in wildlife commodities is immense and continues to rise. According to an analysis to the 2012 Harmonized System customs statistics, global import of wildlife products amounted to US$187 billion, of which fisheries commodities accounted for $113 billion; plants and forestry products for $71 billion; non-fishery animal for $3 billion including live animals, parts and derivatives.[8]
However, the global trade of wildlife commodities is ineffectively monitored and accounted for due to the constraint of the HS Code System used by the customs worldwide. The majority of international imports of wildlife are only recorded in extremely basic and general categories, such as 'plant' or 'animal products', with no further taxonomic detail. It is estimated that near 50% of the global import of plant and 70% of animal products are imported as general categories, with an exception for fisheries (ca. 5%), thanks to various multilateral fishery management agreements that requires taxon-specific fish catch reporting.[8]
Many jurisdictions rely on the declared HS Code of the consignments for detection and prosecution of illegal wildlife import. The lack of specificity of HS Code precludes effective monitoring and traceability of global wildlife trade. There is an increasing call for a reform of the HS Code to strengthen monitoring and enforcement of global wildlife trade.[9][10][11][12][13][14]
Different forms of wildlife trade or use (utilization, hunting, trapping, collection or over-exploitation) are the second major threat to endangered mammals and it also ranks among the first ten threats to birds, amphibians and cycads.[3]
Wildlife trade threatens the local ecosystem, and puts all species under additional pressure at a time when they are facing threats such as over-fishing, pollution, dredging, deforestation and other forms of habitat destruction.[citation needed]
In the food chain, species higher up on the ladder ensure that the species below them do not become too abundant (hence controlling the population of those below them). Animals lower on the ladder are often non-carnivorous (but instead herbivorous) and control the abundance of plant species in a region. Due to the very large amounts of species that are removed from the ecosystem, it is not inconceivable that environmental problems will result, similar to e.g. overfishing, which causes an overabundance of jellyfish.[citation needed]
According to the United Nations, World Health Organization and World Wildlife Foundation, the Coronavirus disease 2019 is linked to the destruction of nature, especially to deforestation, habitat loss in general and wildlife trade. The head of the UN convention on biological diversity stated: "We have seen many diseases emerge over the years, such as Zika, Aids, Sars and Ebola, and they all originated from animal populations under conditions of severe environmental pressures."[15]
Outbreaks of zoonotic diseases including COVID-19, H5N1 avian flu, severe acute respiratory syndrome (SARS), and monkeypox have been traced to live wildlife markets where the potential for zoonotic transmission is greatly increased.[16][17][18][19] Wildlife markets in China have been implicated in the 2002 SARS outbreak and the COVID-19 pandemic.[20][21] It is thought that the market environment provided optimal conditions for the coronaviruses of zoonotic origin that caused both outbreaks to mutate and subsequently spread to humans.[20][21]
In some instances; such as the sale of chameleons from Madagascar, organisms are transported by boat or via the air to consumers. The survival rate of these is extremely poor (only 1% survival rate).[23] This is undoubtedly caused by the illegal nature; vendors rather not risk that the chameleons were to be discovered and so do not ship them in plain view. Due to the very low survival rate, it also means that far higher amounts of organisms (in this case chameleons) are taken away from the ecosystem, to make up for the losses.
Interpol has estimated the extent of the illegal wildlife trade between $10 billion and $20 billion per year. While the trade is a global one, with routes extending to every continent, conservationists say the problem is most acute in Southeast Asia. There, trade linkages to key markets in China, the United States, and the European Union; lax law enforcement; weak border controls; and the perception of high profit and low risk contribute to large-scale commercial wildlife trafficking.[29] The ASEAN Wildlife Enforcement Network (ASEAN-WEN), supported by the U.S. Agency for International Development and external funders, is one response to the region's illegal wildlife trade networks. There is no clear relationship between the legality of wildlife trade and its sustainability; a species can be legally traded to extinction but it is also possible for illegal trade to be sustainable [30]
Notable trade hubs of the wildlife trade include Suvarnabhumi Airport in Bangkok, which offers smugglers direct jet service to Europe, the Middle East, North America and Africa. The Chatuchak weekend market in Bangkok is a known center of illicit wildlife trade, and the sale of lizards, primates, and other endangered species has been widely documented. Trade routes connecting in Southeast Asia link Madagascar to the United States (for the sale of turtles, lemurs, and other primates), Cambodia to Japan (for the sale of slow lorises as pets), and the sale of many species to China.
The trade also includes demand for exotic pets especially birds,[32] and consumption of wildlife for meat. Large volumes of fresh water tortoises and turtles, snakes, pangolins and monitor lizards are consumed as meat in Asia, including in specialty restaurants that feature wildlife as gourmet dining.
Related to the exotic pet trade, captive wildlife are held in sanctuaries which have been involved in illegal wildlife trade. In Thailand the Tiger Temple was closed in 2016 due to being accused of clandestine exchange of tigers.
Morocco has been identified as a transit country for wildlife moving from Africa to Europe due to its porous borders with Spain. Wildlife is present in the markets as photo props, sold for decoration, used in medicinal practices, sold as pets and used to decorate shops. Large numbers of reptiles are sold in the markets, especially spur-thighed tortoises. Although leopards have most likely been extirpated from Morocco, their skins can regularly be seen sold openly as medicinal products or decoration in the markets.[35]
Although the volume of animals traded may be greater in Southeast Asia, animal trading in Latin America is widespread as well.
In Venezuela more than 400 animal species are involved in subsistence hunting, domestic and international (illegal) trade. These activities are widespread and might overlap in many regions, although they are driven by different markets and target different species.[7]
In Brazil, the wildlife trade has grown over the years, as it one of the most biodiverse areas in the world. Mammals and amphibians are among the highest traded animals. In recent studies, non-native species of amphibians and mammals were identified in Brazil, with frogs and rodents, respectively, posing the greatest invasion risks.[36][37] The online trade of amphibians as exotic pets has risen almost six times since 2015.[38]
Through both deep web (password protected, encrypted) and dark web (special portal browsers) markets, participants can trade and transact illegal substances, including wildlife. However the amount of activity is still negligible compared to the amount on the open or surface web. As stated in an examination of search engine key words relating to wildlife trade in an article published by Conservation Biology, "This negligible level of activity related to the illegal trade of wildlife on the dark web relative to the open and increasing trade on the surface web may indicate a lack of successful enforcement against illegal wildlife trade on the surface web."[39]
A study conducted by the International Fund for Animal Welfare (Ifaw) in 2018 revealed online sales of endangered wildlife (on the list of the global Convention on the International Trade in Endangered Species) was pervasive across Europe. Ivory accounted for almost 20 percent of the items offered.[40]
Legal trade of wildlife has occurred for many species for a number of reasons, including commercial trade, pet trade as well as conservation attempts. Whilst most examples of legal trade of wildlife are as a result of large population numbers or pests, there is potential for the use of legal trade to reduce illegal trade threatening many species. Legalizing the trade of species can allow for more regulated harvesting of animals and prevent illegal over-harvesting.[citation needed]
Many environmentalists, scientists, and zoologists around the world are against legalizing pet trade of invasive or introduced species, as their release into the wild, be it intentional or not, could compete with indigenous species, and lead to their endangerment.[citation needed]
Trade of crocodiles in Australia has been largely successful. Saltwater crocodiles (Crocodylus porosus) and freshwater crocodiles (Crocodylus johnstoni) are listed under CITES Appendix II. Commercial harvesting of these crocodiles occurs in Northern Territory, Queensland and Western Australia, including harvesting from wild populations as well as approved captive breeding programs based on quotas set by the Australian government.[41]
Kangaroos are currently legally harvested for commercial trade and export in Australia. There are a number of species included in the trade including:
Harvesting of kangaroos for legal trade does not occur in National Parks and is determined by quotas set by state government departments.  Active kangaroo management has gained a commercial value in the trade of kangaroo meat, hides and other products.[42]
Alligators have been traded commercially in Florida and other American states as part of a management program.[43] The use of legal trade and quotas have allowed management of a species as well as economic incentive for sustaining habitat with greater ecological benefits.
Legalising the trade of products derived from endangered species is highly controversial.[44] Many researchers have proposed that a well regulated legal market could benefit some endangered species by either flooding the market with products that drive down the price of illegal products,[45] decreasing the incentive to illegally harvest, or by providing revenue that could fund the species's conservation.[46] However, laundering and corruption pose a major obstacle to implementing such policies, as illegal harvesters attempt to disguise illegal product as legal when trade is legalized.[47]
Under the Convention on International Trade of Endangered Species of Wild Fauna and Flora (CITES), species listed under Appendix I are threatened with extinction, and commercial trade in wild-caught specimens, or products derived from them, is prohibited.[48] This rule applies to all species threatened with extinction, except in exceptional circumstances.[49] Commercial trade of endangered species listed under Appendix II and III is not prohibited, although Parties must provide non-detriment finding to show that the species in the wild is not being unsustainably harvested for the purpose of trade. Specimens of Appendix I species that were bred in captivity for commercial purposes are treated as Appendix II. An example of this is captive-bred saltwater crocodiles, with some wild populations listed in Appendix I and others in Appendix II.
Many animals are kept for months in markets waiting to be sold. The welfare of animals in trade is almost universally poor, with the vast majority of animals failing to receive even the most basic freedom from pain, hunger, distress, discomfort, and few opportunities to express normal behaviour.[50]
Reptiles specifically endure tight living spaces, torn claws and dehydration during capturing and transportation. Sometimes, they are also crushed from being stacked on top of each other.[51]
The species is found on many Japanese islands, including Honshu, Shikoku, and Kyushu. Their habitats include both natural and artificial bodies of water, as well as forests and grasslands. They breed from spring to the beginning of summer, both sexes producing pheromones when ready to mate. Eggs are laid separately, hatching after about three weeks. They grow from larval to juvenile form in between five and six months. Juveniles eat soil-dwelling prey, and adults eat a wide variety of insects, tadpoles, and the eggs of their own species. They have several adaptations to avoid predators, although which they use depends on where they live. Several aspects of their biology have been studied, including their ability to regrow missing body parts.
The Japanese fire-bellied newt first diverged from its closest relative in the Middle Miocene, before splitting into four distinct varieties, each with a mostly separate range, although all four are formally recognized as composing a single species. Currently, their population is declining, and they face threats from disease and the pet trade. They can be successfully kept in captivity.
The Integrated Taxonomic Information System lists sixteen synonyms for Cynops pyrrhogaster.[9] Common names of the species include Japanese fire-bellied newt,[1] red-bellied newt,[10] and Japanese fire-bellied salamander.[11] Studies examining morphological and geographic variation had formerly recognized six races: Tohoku, Kanto, Atsumi, intermediate, Sasayama, and Hiroshima,[12] one of which, the Sasayama, was described as a subspecies in 1969 by Robert Mertens as Triturus pyrrhogaster sasayamae, which is now considered a synonym of C. pyrrhogaster.[2] Modern molecular analysis supports the division of C. pyrrhogaster into four clades instead.[12] In particular, the validity of the Sasayama and intermediate races has never been proven, with one study finding no behavioral differences between the two supposed forms.[13]
On the newt's upper body, the skin is dark brown, approaching black, and covered in wartlike bumps. The underbelly and the underside of its tail are bright red, with black spots.[4] Younger juveniles have creamy coloration instead of red, although most larger juveniles have some red present.[15] Adults from smaller islands tend to have more red on their ventral (belly) regions than those from larger islands, sometimes with extremely small spots or none at all. In general males tend to have more red than females.[16] Males can also be distinguished from females by their flat, wide tails and swelling around the ventral region.[17] An entirely red variant exists: that coloration is believed to be inherited and recessive. This variant is not confined to any single population, but is more common in the western half of Japan overall.[18]
Of the four clades, the northern is found in the districts of Tohoku and Kanto. This does not overlap with the range of the central clade, which is found in Chubu, northern Kansai, and eastern Chugoku. The central's range has a small amount of overlap with the western, which is found in southern Kinki, western Chugoku, Shikoku, and central Kyushu. The western also has some overlap with the southern clade, which is found in western and southern Kyushu.[12]
Courtship begins when the male approaches the female, sniffing its sides or cloaca. The male then brings its tail to the female and rapidly vibrates it. The female responds by pushing the male's neck with its snout. At this point, the male slowly moves away, undulating its tail, and the female follows, touching the tail with its snout when close enough. The male then deposits two to four spermatophores, one at a time, moving several centimeters after each, which the female attempts to pick up with its cloaca, sometimes unsuccessfully.[24] Females lay eggs separately on underwater objects, such as leaves and submerged grass roots, fertilized one by one from the spermatophores they carry. They can lay up to 40 eggs in one session, and 100 to 400 eggs in a breeding season.[24]
Newts in Mainland Japan have different antipredator behavior than newts on smaller islands. Individuals on smaller islands (for instance, Fukue Island) generally use a maneuver called the unken reflex, where they expose their bright red underbelly to attackers. As their main predators are birds, which are capable of distinguishing the color red, this technique is effective. In Mainland Japan the newts must also avoid mammalian predators, which cannot distinguish colors as well as avian hunters. This leads these populations to use the maneuver less, as it can result in death if attempted.[16]
Against snakes, newts from Fukue Island tend to perform tail-wagging displays, designed to bring a predator's attention to their replaceable tail rather than their more valuable head; those from Nagasaki Prefecture in Mainland Japan tend to simply flee. Snakes are present in both areas. This is likely because those from the mainland are adapted to escape from mammalian hunters, which are less likely to be repelled by such a display.[28]
Wild Japanese fire-bellied newts contain high levels of the neurotoxin tetrodotoxin (TTX).[29] This toxin inhibits the activity of sodium channels in most vertebrates, discouraging predation by both birds and mammals.[28] Experiments have shown the toxin is almost entirely derived from the newt's diet. When raised in captivity with no source of TTX, 36- to 70-week-old juveniles did not contain detectable levels, but wild specimens from the same original habitat had high toxicity. In younger captive-reared newts some TTX was still detected, which was inferred to have been transferred by adult females to their eggs.[29] In a follow-up experiment by the same team captive-reared newts were given food containing the neurotoxin. They readily consumed TTX-laced bloodworms when offered, not showing any symptoms after ingesting the poison. It was detectable in their bodies afterward, further indicating food to be the source of the toxin. No TTX-producing organisms are known from their habitat, but their existence is likely, and would explain the origin of TTX in wild newts.[30]
The International Union for the Conservation of Nature (IUCN) has ranked it as near-threatened. This assessment was made in 2020,[1] a shift from 2004 when it was rated least-concern.[31] It successfully reproduces in Australian zoos.[1] One major threat that C. pyrrhogaster faces is collection for the pet trade. The IUCN states that this trade needs to be ended immediately. Their population is decreasing, particularly near areas of human habitation.[1]
Japanese fire-bellied newts with mysterious skin lesions at Lake Biwa in Japan's Shiga Prefecture were found to be suffering from infections caused by a single-celled eukaryote in the order Dermocystida. The lesions contained cysts, which were filled with spores. Nearly all the lesions were external, although one was found on the liver. Globally, diseases are one of the causes for declining amphibian populations. There is concern that this affliction could spread to other nearby species, including Zhangixalus arboreus and Hynobius vandenburghi.[32]
A variety, believed to be found exclusively on the Atsumi Peninsula, was thought to have become extinct in the 1960s. Then, in 2016, a trio of researchers discovered that newts on the Chita Peninsula were very likely the same variant due to their similar morphological traits. Both groups share a preference for cooler temperature and have smooth and soft bodies, pale dorsal regions, and yellowish undersides. Even if still alive, this form is highly threatened and will soon be wiped out without immediate protection.[33]
Japanese fire-bellied newts serve as a highly useful model organism in laboratory settings, but they become more difficult to care for after metamorphosis. An experiment supported by the Japan Society for the Promotion of Science found that thiourea (TU) can prevent this process from occurring, allowing the animals to stay in their pre-metamorphosis form for as long as two years, while still capable of metamorphosizing when removed from the TU solution. This did not have any impact on their regeneration capabilities.[25]
Japanese fire-bellied newts produce motilin, a peptide that stimulates gastrointestinal contractions, identified in many vertebrates. It is created in the upper small intestine and pancreas. The discovery of the latter was the first time pancreatic motilin had been observed. The organ also produces insulin. These results represented the first discovery of motilin in amphibians, suggesting that it has a similar role for them as it does for birds and mammals. The existence of pancreatic motilin also indicated another, unknown function.[34]
This species, as well as other Urodele amphibians, is capable of regrowing missing body parts, including limbs with functional joints and the lower jaw.[35][36] When this process occurs, the regenerated tissue tends to mirror intact tissue in form.[35] It is also able to regrow missing lenses, taking thirty days to do so as a larva and eighty days as an adult. The difference in time is purely due to the size of the eye, and regenerative ability does not change; the discovery of this fact contradicted a popular claim that juvenile animals are quicker to regenerate than adults.[37]
Doctor of Veterinary Medicine Lianne McLeod described them as "low-maintenance", noting that captive newts enjoy bloodworms, brine shrimp, glass shrimp, Daphnia, and, for larger individuals, guppies.
The Lake Street Transfer station was a rapid transit station on the Chicago "L", serving as a transfer station between its Lake Street Elevated Railroad and the Logan Square branch of its Metropolitan West Side Elevated Railroad. Located where the Logan Square branch crossed over the Lake Street Elevated, it was in service from 1913 to 1951, when it was rendered obsolete by the construction of the Dearborn Street subway.
The transfer station was an amalgamation of two predecessor stations: Wood, on the Lake Street Elevated, was on Wood Street, one block west of the site of the future transfer, and had been constructed in 1893; the Metropolitan's Lake station, on the other hand, was on the site of the future transfer and had been built in 1895. These stations, and their lines, had been constructed by two different companies; when they and two more companies building what would become the "L" merged operations in the early 1910s, a condition for the merger was the construction of a transfer station between the Metropolitan and Lake Street Elevateds at their crossing, which in practice meant the replacement of Wood station with a new Lake Street one under the Metropolitan. Having already merged operations, the "L" companies formally united under the Chicago Rapid Transit Company (CRT) in 1924; the "L" became publicly owned when the Chicago Transit Authority (CTA) assumed operations in 1947.
Lake Street Transfer was double-decked, the Metropolitan's tracks and station located immediately above the Lake Street's tracks and station. Access to the eastbound Lake Street platform was by a station house at the street level; passengers would then use the platform to access the Metropolitan's platforms and Lake Street's westbound platform by additional stairways.
The Metropolitan West Side Elevated Railroad Company, another founding company of the Chicago "L", was granted a fifty-year franchise by the Chicago City Council on April 7, 1892.[11] Unlike the Lake Street Elevated, which operated a single line, the Metropolitan had a main line that proceeded west from downtown to Marshfield Junction, where it split into three branches: one northwestern branch to Logan Square (which in turn had a branch to Humboldt Park[d]), one branch due west to Garfield Park, and one southwestern branch to Douglas Park.[15] While the competing South Side and Lake Street Elevateds used steam traction, the Metropolitan never did; although it had originally intended to, and indeed had built much of its structure under the assumption that locomotives would be used,[16] it decided in May 1894 to have electrified tracks instead,[17] making it upon its opening the first revenue electric elevated railroad in the United States.[18]
The new CTA began experiments to streamline service on the "L"; among them was skip-stop, which began as an experiment on the Lake Street Elevated on April 5, 1948.[2] Stations in between Pulaski and the Loop, exclusive, became either "A" or "B" stations and were serviced by respective "A" or "B" trains during weekdays.[2] Despite being located in this area, Lake Street Transfer was exempt from this system and continued to be serviced by all Lake Street Elevated trains.[2] As part of the same plan to streamline Lake Street service, the Ashland station one block east of the transfer was closed but remained standing.[2] The Logan Square branch would not begin skip-stop until the opening of the Dearborn Street subway and the closing of the transfer in 1951.[20]
The subway's approval did not immediately imply the end of the old Logan Square branch; plans in 1939 included another proposed subway to connect the branch with the Ravenswood branch to the north and through-routing it with the Douglas Park branch to the south into a subway on Ashland Avenue to form a crosstown route.[35] Damen Tower serving the Humboldt Park branch divergence was rebuilt with the expectation that it also would switch trains between the subway and the elevated, much like the State Street subway connects with the earlier elevated North Side main line that remained standing after its construction,[36] and as late as 1949 commuters were promised such a setup that would have preserved the old Logan Square trackage.[37] However, the CTA had no interest in operating either the old Logan Square elevated or the Humboldt Park branch; the new Damen Tower would never be installed with switching equipment, and the Logan Square branch south of Damen would be closed after the Dearborn subway opened.[36]
World War II interrupted the construction of the Dearborn Street subway; although the federal government allowed the continued construction of the State Street subway, it did not do so for the Dearborn Street subway even though it was 82 percent completed by 1942.[20] After the war ended, work resumed on the Dearborn Street subway and it opened at the midnight beginning Sunday, February 25, 1951; at the same time, the Humboldt Park branch was restricted to a shuttle service to and from Damen on the Logan Square branch.[20] Having been rendered obsolete by the subway, the Lake Street Transfer station was closed and the Lake Street's Ashland station reopened.[3] The subway was predicted to reduce the travel time between Logan Square and downtown from 28 minutes to 15.[20] Since construction had not started on the Congress Line, trains in the Dearborn subway stopped at its southern terminus at LaSalle and turned back.[20] Despite its incomplete state, and complaints from riders no longer given a direct trip to the Near West Side,[38][39] the new subway had over sixty percent higher ridership than the old Logan Square branch by the end of the year.[40] The old Logan Square branch trackage south of its entrance to the subway became known as the Paulina Connector, connecting the branch with the rest of the "L" system.[41]
Construction on the Congress Line began in 1954, leaving the Douglas branch with the issue of how to connect with the Loop in the meantime.[42] The Paulina Connector south of Washington Boulevard (a block south of Lake Street) was reopened for the purpose,[42] but the Metropolitan's old tracks north of Washington were replaced in revenue service by a direct connection to the Lake Street's trackage known as Washington Junction, located adjacent to the abandoned station.[41] This junction contained an automatic interlocking mechanism, where Douglas Park trains carried an electric coil to switch them to the Connector that Lake Street trains lacked.[43] This connection was used until the Congress Line was completed in 1958, after which the Douglas branch connected directly with it to use the Dearborn Street subway to go downtown, creating the "West-Northwest Route" that was renamed the Blue Line in 1992.[44][45]
Before 1913 the Wood and Lake stations had two wooden side platforms each. The Wood station had two station houses, one on each platform, designed in a "gingerbread" Queen Anne style, similar to the other stations on the route and the surviving station houses at Ashland.[48] The station houses were heated by potbelly stoves, and while earlier plans had called for their ticket agent's booths to be placed on the sides of the station houses facing the street, they ended up being placed in alcoves adjacent to the platforms.[48] The construction of the Lake Street Elevated's stations was contracted to Frank L. Underwood of Kansas City and Willard R. Green of New York.[49] The Metropolitan's Lake station, which continued as its portion of the Lake Street Transfer, also had two wooden side platforms, but a station house located at street level on the north side of Lake Street. The station house, made of red pressed brick and white limestone trim, was designed similarly to other stations on the Logan Square branch, surviving examples of which are at California and Damen, with a corniced and dentiled front bay containing dual doors specifically marked "Entrance" and "Exit" and prolific use of terra cotta. Its wooden platforms had hipped roof tin canopies in the center and decorative cast-iron railings with diamond designs.[1][50] Unlike elsewhere on the "L", station houses on the Metropolitan had central heating and a basement.[51]
After the transfer was completed in 1913 the C&OP built new platforms; these platforms projected westward from the Metropolitan, with their eastern halves covered by arched canopies with lattice framing and their western halves open. Auxiliary exits onto Hermitage Avenue were located on the middle of the Lake Street platforms at the western ends of their canopies. On the Metropolitan's end, its platforms and canopies were extended southward to meet the southern Lake Street platform, and a new station house on the south side of Lake Street was constructed sometime before 1917, after which the original station house was used for storage. The final station was double-decked, with the Metropolitan's original two side platforms being augmented by the Lake Street Elevated's lower two side platforms. Access to the station was through stairwells from the station house to the Lake Street platforms, which had additional stairways to connect to the Metropolitan platforms; each Lake Street platform was connected to each Metropolitan platform, leading to four inter-platform stairwells in total. The station house presumably had direct access only to the southern eastbound Lake Street platform, with patrons wishing to access the Lake Street's northern westbound platform having to walk up to the Metropolitan platforms and walk down again.[1]
Throughout the stations' existence, the Lake Street[f] and Metropolitan Elevateds[15] had two tracks each in the vicinity, meaning that the transfer station had four tracks overall. Having had trouble constructing its trackage with two different companies and assembling much of its own infrastructure, the Lake Street Elevated contracted with Underwood and Green to construct its stations and the tracks west of Ashland.[53] The Metropolitan's tracks were constructed by the West Side Construction Company, a company with the same officers as the Metropolitan itself and the chief engineer of E. W. Elliot, with steel and iron from the Carnegie Steel Company.[15] Like the rest of the station, the tracks were double-decked in relation with one another.[1]
Prior to the construction of the transfer, the Metropolitan's Lake station had a ridership that hovered around 250,000 a year, peaking at 296,116 in 1905.[63] The Lake Street's Wood station had a similar ridership, but one which peaked at 441,045 in 1905.[64] Once the transfer was in place, the two lines' contributions to station ridership were roughly equal, with the Lake Street edging out the Metropolitan each year.[65]
His fortunes declined when the cultural life of Kharkiv was affected by decrees issued by Tsar Paul I of Russia. Lacking a patron, and with his music unable to be performed, he returned  home to Kyiv in 1798, and became a novice monk of the Kyiv Pechersk Lavra. The monastery's authorities discovered handwritten threats towards the Russian royal family, and accused Vedel of writing them. He was subsequently incarcerated as a mental patient, and forbidden to compose. After almost a decade, the authorities allowed him to return to his father's house to die.
Vedel's music was censored during the period that Ukraine was part of the Soviet Union. More than 80 of his works are known, including 31 choral concertos, but many of his compositions are lost. Most of his choral music uses texts taken from the Psalms. The style of Vedel's compositions reflects the changes taking place in classical music during his lifetime; he was influenced by Ukrainian Baroque traditions, but also by new Western European operatic and instrumental styles.
The character of Russian and Ukrainian worship derives from performances of the znamenny chant, which developed a tradition that was characterised by seamless melodies and a capacity to sustain pitch. The tradition reached its culmination during the 16th and 17th centuries, having taken on its own character in the Russian Empire some three centuries earlier.[2]
The tradition of Russian church music can be traced back to Dmitry Bortniansky, who revered the Russian liturgical musical tradition. The early part of the 19th century was a period that marked a low ebb in the fortunes of traditional Russian music. Bortniansky studied in Venice before eventually becoming the director of music at the court chapel in St Petersburg in 1801. Composing in an era when attempts were being made to suppress the Russian Empire's cultural heritage, Bortniansky's choral concertos, set to texts in Russian, were modelled on counterpoint, the concerto grosso and Italian instrumental music. Under him, the Imperial Court Chapel expanded its role so it influenced, and eventually controlled, church choral singing throughout the Russian Empire.[2][4] Vedel followed Bortniansky in combining the Italian Baroque style to ancient Russian hymnody,[2] at a time when classical influences were being introduced into Ukrainian choral music, such as four-voice polyphony, the soloist and the choir singing at different alternative times, and the employment of three or four sections in a work.[1]
The Vedelsky family adhered strictly to the Orthodox faith.[5] Lukyan Vlasovich Vedelsky  was a wealthy carver of wooden iconostases, who owned his own workshop. The name Vedel, probably an abbreviated form of Vedelsky, was how the composer signed his letters, and named himself in military documents. His father signed himself "Kyiv citizen Lukyan Vedelsky".[9][note 4]
Vedel was a boy chorister in the Eparchial (bishop's) choir in Kyiv.[14] He studied at the Kyiv-Mohyla Academy, where his teachers included the Italian Giuseppe Sarti,[15] who spent 18 years as an operatic composer in the Russian Empire.[16] By the end of the 18th century, most of the students attending the Kyiv-Mohyla Academy were preparing for the priesthood. It was at that time the oldest and most influential higher education institution in the Russian Empire; most of the country's leading academics were originally graduates of the academy.[17]
Vedel's talent was recognised by other musicians in Moscow.   He probably continued his musical studies at the university.[16] During this period, he had the opportunity to become more familiar with Russian and Western European musical cultures.[9] He did not stay in Moscow for long and, resigning his position, he returned home to Kyiv in the early 1790s.[16]
On 13 March 1796, Levanidov was appointed as Governor General of the Kharkiv Governorate.[20] The composer moved to Kharkiv, along with his best musicians. In Kharkov (now Kharkiv, Ukraine) Vedel organised a new gubernia (governorate) choir and orchestra, and taught singing and music at the Kharkiv Collegium,[8] which was second only to the Kyiv-Mohyla Academy in terms of its curriculum.[18] The music class at the Kharkiv Collegium was first recorded in 1798, when in January that year two canons and a choral concerto by Vedel were performed.[21]
Vedel did much of his composing during this period.[8] Works included the concerts "Resurrect God" and "Hear the Lord my voice" (dated 6 October 1796) and the two-choir concerto "The Lord passes me". The composer and his works were highly valued in Kharkiv; his concerts were studied and performed at the Kharkiv Collegium, and they were sung in churches. Bortniansky, who conducted the St. Petersburg State Academic Capella, praised the quality of Vedel's teaching. In September 1796, Vedel was promoted to become a senior adjutant, with the rank of captain.[9][19]
The tsar's decrees caused the cultural and artistic life of Kharkiv to decline. The city's theatre was closed, and its choirs and orchestras were dissolved. Performances of Vedel's works in churches were banned,[9] as the tsar had prohibited singing in churches of any form of music except during the Divine Liturgy.[24]
Early in 1799, frustrated by the lack of opportunities to compose and teach and possibly suffering from a form of mental illness, Vedel enrolled as a novice monk at the Kyiv Pechersk Lavra.[9][24] He was an active member of the community and was respected by the monks for his asceticism.[24]
According to Turcaninov's biography, the Metropolitan of Kyiv commissioned Vedel to write a song of praise in honour of a royal visit to Kyiv, but Vedel instead wrote a letter to the tsar, probably of a political nature. Vedel was arrested in Okhtyrka, pronounced insane, and returned to Kyiv.[24]
Vedel returned to live with his father in an attempt to regain his mental health. Back home in Kyiv, he was able to compose, read, and play the violin, and he may have returned to teach at the Kyiv Academy.[25] By leaving the monastery before his training was completed, Vedel may have angered Hierotheus, the Metropolitan bishop. When the monastery authorities discovered a book containing handwritten insults about the royal family, the Metropolitan accused Vedel of writing in the book. He dismissed Vedel's servants, and personally detained him. On 25 May 1799, Hierotheus declared that Vedel was mentally ill.[9]
After the death of Paul I in 1801, the new tsar, Alexander I, proclaimed an amnesty for unjustly imprisoned convicts, and many prisoners were released.[25] Alexander ordered that Vedel's case should be re-examined, but Vedel was again declared insane and remained an inmate.[10] The tsar wrote of Vedel on 15 May 1802: "... leave in the present captivity".[14]
In 1808, after nine years' imprisonment, and by now mortally ill, Vedel was allowed to return home to his father's house in Kyiv. Shortly before his death there on 14 July 1808, he is said to have stood and prayed in the garden.[10]
Vedel was almost entirely a liturgical composer of the a cappella choral music sung in Orthodox churches.[8][note 5] As of 2011[update], more than 80 of his compositions have been identified, including 31 choral concertos and six trios, two liturgies, an all-night vigil,[8][9] and three irmos cycles.[29]
An edition of Vedel's works was published by Mykola Hodbych and Tetiana Husarchuk in 2007.[8]
Many of Vedel's works have been lost.[14] The V.I. Vernadsky National Library of Ukraine holds the only existing autograph score by the composer, the Score of Divine Liturgy of Saint John Chrysostom and Other Compositions. The score consists of 12 choral concertos (composed between 1794 and 1798),[14] and the Liturgy of Saint John Chrysostom. The ink varies in colour, which suggests that Vedel worked on the compositions at different times.[30] It was acquired by Askochensky, who bequeathed it to the Kyiv Academy.[14][31]
The musicologists Ihor Sonevytsky and Marko Robert Stech consider Vedel to be the archetypal composer of Ukrainian music from the Baroque era.[8] An outstanding tenor singer, he was one of the best choral conductors of his time. He helped to raise the standard of choral singing in Ukraine to previously unknown levels.[1]
Vedel was considered during his lifetime to be a traditional and conservative composer, in contrast to his older contemporaries Berezovsky and Bortniansky. Unlike Vedel, they composed secular, non-spiritual works. He was a famous violinist, but no music by Vedel for the violin is documented. His works, perhaps even more than those of Berezovsky or Bortnyansky, represented a development in Ukrainian musical culture.[6] According to Koshetz, Vedel's music was based on Ukrainian folk melodies.[29]
Vedel's music was written at a time when Western music had largely emerged from the Renaissance and Baroque eras. The style of his compositions reflected two contrasting traditions. He was strongly influenced by the baroque traditions of the Ukrainian hetman culture, with its religious-mystical music linked with ideas about spiritual enlightenment, but was also influenced by developments in new operatic and instrumental styles emerging from Western Europe at that time.[32]
Performances of Vedel's music were censored and the publication of his scores was prohibited during most of the 19th century. Distributed in secret in manuscript form, they were however known and performed, despite the ban.[9] Hand-written variations of Vedel's music appeared,[7] as conductors amended the scores to make them more suitable for unauthorised performances. Tempi were changed and modal textures, the level of complexity of the music, and the formal structure were all altered.[33] The hand copying of Vedel's music led to the creation of versions that were notably different from his original scores.[25]
Vedel's compositions were rediscovered during the early 20th century by the conductor and composer Alexander Koshetz, at that time the leader of the Kyiv-Mohyla Academy's student choir, and himself a student.[14] They were first published in 1902.[34] Koshetz, one of the earliest conductors from Ukraine to attempt to revive performances of Vedel using the autograph scores, noted that "the great technical difficulties of solo parts... and the need for large choruses" made his works difficult to perform in public.[33] Koshetz toured Europe and America, conducting the Ukrainian Republican Chapel in performances of Vedel.[6]
Koshetz's revival of Vedel's music was banned by the Soviets after Ukraine was absorbed into the Soviet Union in 1922.[14] Unlike many of the sacred works written by Western composers, Orthodox sacred music is sung in the vernacular, and its religious nature is visible and tangible to Orthodox Christians. Because of this, Soviet anti-religious legislation prohibited Russian and Ukrainian sacred music from being performed in public from 1928 until well into the 1950s, when the Khrushchev Thaw occurred, and Vedel's works were once again heard by Soviet audiences.[35]
Vedel, Berezovsky and Bortniansky are recognised by modern scholars as the "Golden Three" composers of Ukrainian classical music during the end of the 18th century,[36] and the outstanding composers at a time when church music was reaching its peak in eastern Europe.[5] They composed some of the greatest choral music to emerge from the Russian Empire.[18]
Vedel made an important contribution in the music history of Ukraine,[37][38] and musicologists consider him to the archetypal composer of the baroque style in Ukrainian music.[8] Koshetz stated that Vedel should be seen as "the first and greatest spokesperson of the national substance in Ukrainian church music".[29] The musical culture that developed in Ukraine during the 19th century was founded in part on Vedel's choral compositions. According to the ethnomusicologist Taras Filenko, "His free command of contemporary techniques of choral writing, combined with innovations in adapting the particularities of Ukrainian melody, make Artem Vedel's works a unique phenomenon in the context of world musical culture."[39] According to Chekan, Vedel's texture is "at times monumental and at others subtly contrasted, strikingly showing the possibilities of the a cappella sound".[1]
The Germans attempted to delay the Allied advance until the onset of bad weather by denying access to ports and demolishing communications infrastructure in order to give their own forces time to recover. Between September and November, the American forces in Europe suffered from severe transportation problems. In September, Cherbourg was the only deep-water port in northwest Europe in Allied hands capable of handling Liberty ships, but it had been badly damaged, and took a long time to restore. Smaller ports could handle only small, shallow-draft coastal trading vessels known as "coasters". Two-thirds of the British coaster fleet, on which critical industries depended, was dedicated to the campaign. Over time, rough seas, enemy action and continuous use laid up a quarter of the coaster fleet for repairs. From September onwards, an increasing volume of supplies came directly from the United States. These were stowed in Liberty ships so as to make optimal use of their cargo space. The shipments frequently included heavy and bulky items that required dockside cranes to unload. The available port capacity was insufficient to unload the ships arriving. As the number of ships awaiting discharge in European waters climbed, turnaround times increased, and fewer ships reported back to port in the United States, precipitating a widespread shipping crisis.
In the first seven weeks after the commencement of Operation Overlord, the Allied invasion of Normandy on D-Day (6 June 1944), determined German opposition exploited the defensive value of the Normandy bocage country against American forces. At first, the Allied advance was slower than the Operation Overlord plan had anticipated.[1] The American Operation Cobra, which commenced on 25 July, effected a turnaround in the operational situation by achieving a breakout from the Normandy lodgment area.[2] The Germans were outmaneuvered and driven into a chaotic retreat.[3] The 12th Army Group became active on 1 August, under the command of Lieutenant General Omar N. Bradley. It initially consisted of the First Army, commanded by Lieutenant General Courtney Hodges, and the Third Army, under Lieutenant General George S. Patton Jr.[4] The Ninth Army, under Lieutenant General William H. Simpson, joined the 12th Army Group on 5 September.[5]
British General Sir Bernard Montgomery, the commander of the British 21st Army Group, remained in command of all ground forces, British and American,[4] until 1 September, when the Supreme Allied Commander, General Dwight D. Eisenhower, opened his Supreme Headquarters, Allied Expeditionary Force (SHAEF) in France, and assumed direct command of the ground forces.[5] This brought not just the 12th and 21st Army Groups under Eisenhower's direct command, but also Lieutenant General John C. H. Lee's Communications Zone (COMZ), which became operational on the continent on 7 August.[6]
Bradley had previously exercised control over the Advance Section (ADSEC) of COMZ as the senior American commander on the continent. As such, he had prescribed stock levels in the depots and priorities for the delivery of supplies, and apportioned service units between the armies and the Communications Zone. Bradley believed that as the senior operational commander he should exercise such authority, as was the case in the British forces.[7] Under the American organization, COMZ headquarters also functioned as the headquarters of the European Theater of Operations, United States Army (ETOUSA).[6]
The Allied commanders pushed their forces to the limits logistically to take advantage of the chaotic German retreat.[3] By 3 August Patton's Third Army was advancing into Brittany. Bradley ordered Patton to turn east, leaving only minimal forces behind.[8] This decision entailed grave risk, for under the Operation Overlord plan, the ports of Lorient and Quiberon Bay were to be developed for the logistical support of the American forces under the codename Operation Chastity.[9][10] This was the first in a series of critical decisions that subordinated logistical considerations to short-term operational advantage.[8]
In mid-August, Eisenhower decided to continue the pursuit of the retreating German forces beyond the Seine. This stretched the logistical system.[11][12] Between 25 August and 12 September the Allied armies advanced from the D plus 90 phase line, the position the Operation Overlord plan expected to be reached 90 days after D-Day, to the D plus 350 phase line, moving through 260 phase lines in just 19 days. Although the planners had estimated that no more than twelve divisions could be maintained beyond the Seine, by September sixteen were, albeit on reduced scales of rations and supplies.[13] Logistical forecasts were repeatedly shown to be overly pessimistic, imbuing a sense of confidence that difficulties could be overcome.[14]
Failure to follow proper procedures contributed to the waste and disorder. Dumps established by the armies were frequently turned over to COMZ with little or no paperwork, so supplies were unrecorded, unidentified and unlocatable, resulting in duplicate requisitions. This was exacerbated by the dispatch of filler cargoes of unwanted goods shipped solely to make maximum use of the available transport. The indenting system itself was imperfect and slow in responding to urgent demands. Logisticians at all levels strove to improvise, adapt and overcome difficulties, with considerable success, but short-term solutions frequently created longer-term problems. Hoarding, bartering, over-requisitioning, and cannibalizing vehicles for spare parts degraded the effectiveness of the supply system.[23]
The German strategy was to conduct a fighting withdrawal to the Siegfried Line (which they called the Westwall) while holding the ports as long as possible and conducting a scorched earth program to deny or destroy as much of the transportation infrastructure as possible. The hope was that these measures would restrict the Allies' operational capabilities, which relied heavily on logistical support, and thereby gain sufficient time to reconstitute the German forces. If six to eight weeks could be gained, then bad autumn weather would set in, further restricting the Allies' mobility, air operations and logistical support, and the German forces might be able to take the offensive again.[24]
The Overlord planners had identified a need for shallow-draft vessels, and the Combined Chiefs of Staff had allocated 625,000 deadweight tons (635,000 deadweight tonnes) of coasters for the first six weeks of the operation. This represented about two-thirds of the British coaster fleet, on which critical industries depended for the transport of iron, coal and other commodities. The allocation of so much coastal shipping to Overlord entailed temporarily shutting down a quarter of the UK's blast furnaces. The British therefore wanted the shipping returned as soon as possible. It was planned that after the first six weeks, the allocation to Overlord would be reduced to 250,000 deadweight tons (254,000 deadweight tonnes).[32]
Under the circumstances, the coasters could not be released. Some 560,000 deadweight tons (570,000 deadweight tonnes) of coasters were still engaged in the cross-Channel run in September, and this actually rose to over 600,000 deadweight tons (610,000 deadweight tonnes) in November. The crux of the problem was slow turnaround times. A survey of coaster movements in late October and early November found that 63 round trip voyages had required 1,422 ship-days instead of the planned 606. Diversions to alternate ports, increasingly bad weather, and vessels being laid up for repairs all contributed to this situation. Between 20 and 25 percent of the coaster fleet was laid up for repairs in November and December. Not until December, after the opening of the port of Antwerp, was it possible to release 50,000 deadweight tons (51,000 deadweight tonnes) of coasters, but that was from support of the 21st Army Group; the American allocation actually increased.[33]
It was planned that an increasing volume of supplies would come directly from the United States from September onwards. These ships were not combat loaded, but stowed so as to make optimal use of cargo space. Whereas vehicles had been brought across from the UK on motor transport (MT) vessels (ships specially outfitted to carry vehicles), landing ships, tank (LSTs) or landing craft, tank (LCTs), they now arrived in crates and boxes, with some assembly required. Nearly every ship would arrive with boxed vehicles or other bulky or heavy items. This kind of awkward cargo needed to be discharged at ports where large shore cranes were available; to discharge them over the beaches or at minor ports was difficult, although not impossible.[34] But the only major port in Allied hands on 25 August was Cherbourg.[35] An alternate was to discharge in the UK, assemble them there and transfer the vehicles to the continent in MT ships. SHAEF was allocated 258 MT ships in July, but this was cut to 62 in August, which was still 22 more than originally allocated.[36]
The shipping crisis in the ETO escalated into a global one. Merchant ship construction was lagging behind schedule, mainly due to a deficit of 35,000 skilled workers in the shipyards, as they were being lured away to work on the higher-priority amphibious cargo ships and Boeing B-29 Superfortress programs, where pay and conditions were better. The Allied merchant fleet was still growing at a rate of 500,000 deadweight tons (510,000 deadweight tonnes) per month, but the number of ships available for loading at US ports was shrinking. The problem was growing retentions of vessels by the theaters, of which the ETO was the worst but not the only offender.[42] The chairman of the Maritime Commission, Vice Admiral Emory S. Land, noted that 350 ships were being held idle in the theaters awaiting discharge, and 400 more WSA vessels were being retained for various uses by theater commanders.[43] This represented 7,000,000 deadweight tons (7,100,000 deadweight tonnes) of shipping, which was about 30 percent of all Allied-controlled tonnage.[44] As ships failed to return from overseas on time, supplies began piling up in ports, depots and railway sidings in the United States.[45]
Drastic measures were required. On 18 November the Joint Chiefs of Staff (JCS) approved and forwarded to President Franklin Roosevelt a memorandum from Somervell recommending cuts in non-military shipments.[46] Specifically, Somervell proposed eliminating the American contribution to the UK Import Program of 40 sailings per month, reducing Lend-Lease to the UK by 12 sailings per month, those to the USSR by 10, and cutting civilian relief to Europe by 34.[47] Conway enlisted Harry Hopkins, Roosevelt's chief advisor, in putting the WSA's case to the president: that it could not ask the British "to bear the brunt of our failure to utilize our ships properly."[48] Roosevelt instructed the WSA to negotiate a cut in the UK Import Program for December 1944, January 1945 and February 1945 with the British, asked the Office of War Mobilization and Reconversion to investigate the labor situation at the shipyards, and told the JCS to get the theaters to break up the pools of idle shipping and improve turnaround times.[49]
The port of Cherbourg was operated by the 4th Port, under the command of Colonel Cleland C. Sibley, which was augmented by Colonel August H. Schroeder's 12th Port.[66] From 16 August, the 4th Port was part of Colonel Theodore Wyman Jr.'s Normandy Base Section.[67] The operation of a major port requires a great deal of coordination, and much of this was worked out through trial and error. Bringing vessels into the port was a US Navy responsibility, but the Naval harbor master would take the Army's preferences into account in deciding what berth should be used. For example, the Army preferred vessels with cargo suitable for unloading by DUKW anchor in the Petit Rade to avoid long hauls from the Grande Rade. But communication between the office of the Naval harbor master and the headquarters of the 4th Port was unsatisfactory at first, and it took time to develop a smooth working relationship.[68]
Problems arose when ships arrived unexpectedly and there were no preparations to receive them. Advance warning of ship arrivals was necessary because unloading a ship was a complicated business. Sufficient stevedores had to be provided to work the hatches, and the required cargo handling equipment, such as cranes, had to be available. Trucks and railway cars had to be brought forward and spotted on the quays to allow cargo to be quickly cleared, and the depots and dumps had to be alerted to be ready to receive the supplies. Some arrived without manifests or stowage plans. In some cases the only way the contents of a ship could be determined was for the port personnel to board it and physically check. At first the Navy refused to allow ships without manifests to enter the port at all, but too often they were found to contain critical cargo.[68]
The 4th Port was handicapped by the slow arrival of its unit equipment, which had been brought from the UK on twelve ships but unloaded at Utah Beach instead of Cherbourg. The hatch crew found themselves lacking basic gear such as ropes, slings and cargo nets, and three DUKWs went around collecting gear from ships in the harbor. Cranes were also in short supply, and this was exacerbated by a dearth of well-trained crane operators. In the UK, where the 4th Port had worked the docks along the River Mersey, crane operating had been mostly carried out by experienced civilians. A training program was initiated at Cherbourg, where crane operators were instructed by a pair of sergeants who had learned crane operation in the UK. In the meantime, inexperienced operators caused avoidable damage both to cranes and to cargo. Insufficient numbers of skilled mechanics were available to repair the cranes, and there was a shortage of spare parts as well. The result was that at times half the cranes were out of action.[68]
Whenever there were a few days running of good weather, cargo built up at the port. SHAEF was dissatisfied with how the port was being run, and on 30 October Wyman was relieved of command of the Normandy Base Section and replaced by Major General Lucius D. Clay, on loan from ASF. Clay recognized that the heart of the problem was a lack of coordination between the port and rail operations, and he delegated the necessary authority over the railways to Crothers. This brought about an improvement in clearance tonnage in November. Clay remained for only a few weeks. On 26 November Colonel Eugene M. Caffey assumed temporary command of the Normandy Base Section until Major General Henry S. Aurand arrived on 17 December.[71] Unloading at Cherbourg reached its peak in November. With the opening of ports further north it declined in importance, although it remained an important port for the discharge of ammunition.[69]
Le Havre was operated by the 16th Major Port, under the command of Brigadier General William M. Hoge, who had commanded the Provisional Engineer Special Brigade Group on Omaha Beach. He was succeeded by Colonel Thomas J. Weed on 31 October.[54] It was joined by the 52nd Port in January.[78] The number of berths that the logisticians felt was necessary were never developed, so the port continued to depend on DUKWs and lighters.[55] In the first quarter of 1945, seven DUKW companies handled 35.2 percent of the cargo; quayside discharges accounted for 23.3 percent and lighters handled the rest.[59]
In addition to handling cargo Le Havre also became an important debarkation point for American troops. In January 1945 the 52nd Port was attached to the 16th Major Port and its commander, Colonel William J. Deyo, was given responsibility for troop movements. That month the Red Horse Staging Area was established in the Le Havre area. Troops debarked over a long steel ponton pier and a troopship berth at the Quai d'Escale. Arrivals peaked at 247,607 troops in March 1945.[59]
Both COMZ and the 12th Army Group urged SHAEF to allocate a portion of Antwerp's capacity to the support of the American forces. On considering the matter SHAEF concluded that Antwerp was indeed sufficiently large to serve the needs of both the British and the American forces.[82] Eisenhower rejected a proposal from Lee that it be operated jointly, as joint control of ports had been shown to not work well in the past; the port was to be run by the British.[88] Lee arranged a conference in Antwerp with representatives of COMZ and the 21st Army Group between 24 and 26 September. Tentative agreements were reached on the allocation of tonnage capacity, storage facilities and railway lines, and arrangements were made for the command and control of the port and its installations, and regarding responsibility for rehabilitation works.[82] Additional matters were discussed at a second conference in Brussels on 5 October, and the result was formalized in a Memorandum of Agreement that became known as the "Treaty of Antwerp", and was signed by Major General Miles Graham, the Major General Administration at 21st Army Group, and Colonel Fenton S. Jacobs, the commander of the Channel Base Section.[88][89]
The rehabilitation of the port was undertaken by the British, on the understanding that American resources could be called upon as required. The main US engineer unit assigned was the 358th Engineer General Service Regiment. Two of the ETO's five engineer port repair ships were also available. American engineers cleared rubble away from the quays, improved roads, repaired rail lines, rebuilt warehouses and constructed hardstands. The main American project was the repair of mine damage to the Kruisschans sluice, the longest of the four that connected the river with the wet basins, and the only one that provided access to the American wet basins. Work commenced on 6 November and was completed in time for the arrival of the first Liberty ship, SS James B. Weaver, on 28 November. By this time 219 of the 242 berths were cleared, all the port's cranes were in working order, and all the bridges needed to access the quays had been repaired.[89][93] The port's floating equipment was augmented by a small fleet of American harbor craft that included 17 small tugboats, 20 towboats and 6 floating cranes.[94]
The 13th Port was assigned to operate the American part of Antwerp, and began arriving from Plymouth in October 1944. It was joined by the 5th Major Port in November and December. The commander of the 13th Port, Colonel Doswell Gullatt, who had led the 5th Engineer Special Brigade at Omaha Beach on D-Day, was designated the overall commander. He had also been the district engineer in Mobile, Alabama, and had extensive experience in flood control and the construction of piers and docks.[95] Most of the work of unloading ships was carried out by Belgian stevedores, some 9,000 of whom were working on the American section of the port on an average day in December.[96] American personnel operated in a supervisory capacity. The biggest problem with the Belgian workers was transporting them to and from their homes, as German V-weapon and air attacks on the port forced many of them to move to the outskirts of the city.[94] The city suffered heavy damage from these attacks, and there were more than 10,000 casualties. The workers' living conditions deteriorated during the winter, and on 16 January they went on strike over shortages of food, clothing and coal. The strike lasted only one day, and it ended when the burgomaster promised the workers food and coal at regulated prices.[96] To keep the civilian workers at their jobs, the city of Antwerp granted workers in the port area a 25 percent increase in pay, but since the V-weapons were fairly inaccurate, this soon led to more industrial disputes when workers elsewhere demanded the same.[94]
COMZ attempted to obtain additional supplies of tires by restoring the local French and Belgian tire industries. This was hampered by a shortage of raw materials, which had to be imported, and the transportation systems and electricity grids, which had to be restored. The first tire made from American synthetic rubber was produced by the Goodrich factory in Paris on 4 January 1945, and by the end of the month it was turning out 4,000 tires per month and the Michelin plant was making 2,000. Meanwhile, since inspections had shown that 40 percent of tire losses were due to preventable causes such as underinflation and overloading, the theater launched a media campaign in the Stars and Stripes newspaper and on the Armed Forces Network radio. The War Department removed the tires from unserviceable vehicles in the United States, and brokered a moratorium on industrial disputes in the synthetic rubber industry to increase production.[112]
While motor transport was flexible, it lacked the capacity of rail transport to move large tonnages over long distances. It was upon the railways that the movement of supplies from the ports to the depots ultimately depended. The railway system in northern France was operated by the 2nd Military Railway Service,[113] under the command of Brigadier General Clarence L. Burpee, an Atlantic Coast Line Railroad executive who had commanded the 703rd Railway Grand Division in the North African campaign and the military railways in the Italian campaign.[114]
It was not intended that the railway network would be operated entirely by Allied service personnel for an extended period of time. The plan was that the lines would be progressively handed over to civilian operators once the fighting had passed through and the network was restored. The rapid Allied advance in August and September disrupted this plan, and resulted in the military personnel being spread far more widely and thinly than anticipated. This was complicated by the fact that while the railway operating battalions were formed around cadres drawn from American railroads, not all of their personnel had operating or supervisory experience. Difficulties also arose in dealing with the French civilian operators through differences of language, documentation and operating procedures.[119]
The most important limiting factor affecting the railways was the availability of rolling stock. The Operation Overlord planners had anticipated that much of the French rolling stock would be destroyed by Allied bombing, and that the retreating Germans would take what they could with them. This proved prescient: only fifty serviceable French locomotives were captured southwest of the Seine. It was estimated that the Allied forces would require 2,724 of the large 2-8-0 and 680 of the smaller 0-6-0 type, of which 1,800 and 470 respectively would be for American use, but only 1,358 2-8-0s and 362 0-6-0s were on hand by the end of June 1944.[119]
Due to cutbacks in American locomotive production, the War Department was unable to guarantee delivery of the remaining 2,000 locomotives, and suggested that ETOUSA obtain 450 locomotives that had been loaned to the British. The British refused to hand them over unless the Americans released their coasters. In December, 100,000 deadweight tons (100,000 deadweight tonnes) of coasters was released, and the British agreed to ship 150 locomotives in December, followed by 100 per month thereafter. By the end of the year 1,500 locomotives had been shipped to the continent, and 800 captured French, German and Italian ones had been repaired by French and American mechanics and restored to service.[119]
More than 57,000 railroad cars of various types, including boxcars, flatcars, refrigerator cars and tank cars were shipped to the continent. Of these, some 20,000 had been shipped from the US in the form of knock-down kits and had been assembled in the UK. These were augmented by captured rolling stock.[119] Nonetheless, serious shortages of rolling stock developed in November. Part of the problem was that the armies liked to keep a certain amount of supplies on wheels, using railroad cars as mobile warehouses, but a major factor was a decision to ship bulk supplies directly from the ports to the ADSEC and army depots to reduce port congestion. These depots were primarily points for issuing supplies, with limited unloading and storage capacity, and did not have sufficient resources to classify and segregate bulk supplies. By the end of November there were 11,000 loaded freight cars on the rails northeast of Paris, and ten days later this had increased to 14,000, but the depots could unload only about 2,000 cars per day. This resulted in increased railroad car turnaround times, with many cars taking 20 to 40 days to be unloaded.[120]
The Oise was restored to supply coal to Paris from the coalfields around Valenciennes. Its rehabilitation was undertaken by the 1057th Port Construction and Repair Group, which repaired several locks and removed 34 obstructions, mostly demolished bridges. The Seine was rehabilitated to bring civil relief supplies to Paris from Rouen and Le Havre. It too was obstructed by sunken bridges and damaged locks. The biggest task was the repair of the locks of the Tancarville Canal, which had been built to allow barges from Le Havre to reach the Seine without having to negotiate the Seine estuary, where there were strong tidal currents. The 1055th Port Construction and Repair Group finished this work in March 1945.[125]
Barge traffic on the Seine was obstructed by a pair of ponton bridges. They had to be opened to allow barges through, but this interrupted motor traffic. At Le Manoir there was a railway bridge built by the British that was too low to permit barge traffic. The bridge was raised in October, but then the river rose in November as the Seine flooded, reducing clearance below the minimum again. The river rose so high it was feared that the bridge would be washed away, and consideration was given to routing the trains hauling British supplies from Normandy via Paris instead. On 25 December the bridge was struck by a tugboat and put out of commission. It was then removed.[126]
Although logistical difficulties constituted a brake on combat operations, they were not the only factors that brought the Allied advance to a halt. The American forces also had to contend with rugged terrain, worsening weather and, above all, stiffening German resistance. American forces were widely dispersed and, with the logistical situation preying on his mind, a cautious Hodges ordered his corps commanders to halt when they encountered strong resistance.[130][131] Patton's intelligence officer, Colonel Oscar W. Koch, warned that the German Army had been defeated, but not routed, and was not on the brink of collapse. He forecast that fierce resistance and a last-ditch struggle could be expected.[132]
As American forces confronted the defenses of the Siegfried Line, priority shifted from fuel to ammunition.[133] The armies made little progress during the fighting in September and October. Although the logistical situation improved even before the opening of Antwerp, the effort to reach the Rhine in November was probably premature. Worsening weather and stubborn German resistance impeded the American advance as much as any logistical difficulties. The German recovery was sufficient to mount the Ardennes offensive in December. This placed immense strain on the American lines of communication, especially the newly opened port of Antwerp. By the new year though, the American transportation system was stronger and more robust than ever, and preparations were underway to support the final drive into the heart of Germany.[134]
Featured articles are considered to be some of the best articles Wikipedia has to offer, as determined by Wikipedia's editors. They are used by editors as examples for writing other articles. Before being listed here, articles are reviewed as featured article candidates for accuracy, neutrality, completeness, and style according to our featured article criteria. Many featured articles were previously good articles (which are reviewed with a less restrictive set of criteria).
There are 6,216 featured articles out of 6,623,683 articles on the English Wikipedia (about 0.09% or one out of every 1,060 articles). Articles that no longer meet the criteria can be proposed for improvement or removal at featured article review.
On non-mobile versions of our website, a small bronze star icon () on the top right corner of an article's page indicates that the article is featured. On most smartphones and tablets you can also select "Desktop" at the very bottom of the page  or "Request Desktop Site" in your browser's menu to see this line (do a search to find out how). Additionally, if the current article is featured in another language, a star will appear next to the corresponding entry in the language switcher to let you know.
A list of articles needing cleanup associated with this project is available. See also the tool's wiki page and the index of WikiProjects.
Featured articles (FAs) are some of the best articles in the English Wikipedia.  They are written by volunteers about subjects of their own choosing, and evaluated by other volunteers against the featured article criteria.  
Designated volunteers select the FA to become Today's featured article on Wikipedia's main page.  
Click on a date/time to view the file as it appeared at that time.
More than 100 pages use this file.
The following list shows the first 100 pages that use this file only.
A full list is available.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
Silver was an arena concert by Filipina entertainer Regine Velasquez held on November 16, 2012, at the Mall of Asia Arena in Pasay City. She became the first local artist to play at the venue since it opened that May. The show's concept and name is a reference to the twenty-fifth anniversary since Velasquez's professional debut in 1986.  The setlist contained songs predominantly taken from Velasquez's discography and various covers. The show was produced by iMusic Entertainment, with GMA Network as its broadcast partner. Ryan Cayabyab and Raul Mitra served as music directors, accompanied by the Manila Philharmonic Orchestra.
Originally scheduled as a one-night show, Velasquez suffered from a viral infection and was forced to cancel her performance halfway through the concert after temporarily losing her voice. It was later re-staged, titled Silver Rewind, on January 5, 2013, and featured Ogie Alcasid, Janno Gibbs, Gloc-9, La Diva, Rachelle Ann Go, Jaya, and Lani Misalucha as guest acts. The show was acclaimed by critics, receiving praise for Velasquez's vocal abilities and rapport with the audience, and was considered a vindication from its initial cancellation.
Filipina singer Regine Velasquez's career began with a record deal with OctoArts International and the release of her single "Love Me Again" in 1986.[1][2] After an appearance in the variety show The Penthouse Live!, she caught the attention of Ronnie Henares, a producer and talent manager who signed her to a management deal.[2][3] In 1993, she signed an international record deal with PolyGram Records,[4] and achieved commercial success in some Asian territories with her albums Listen Without Prejudice (1994), My Love Emotion (1995) and Retro (1996).[5] In April 1996, Velasquez staged a show, named Isang Pasasalamat, at UPD's Sunken Garden to commemorate her ten-year career.[2][6] A twentieth anniversary show was staged in October 2006 at the Araneta Coliseum titled Twenty.[7]
On October 23, 2012, the Philippine Entertainment Portal announced that Velasquez would headline a concert on November 16 at the Mall of Asia Arena in Pasay City, which was dubbed as her "25th anniversary concert" and titled Silver.[8] She became the first Filipino solo act to perform at the venue since it opened that May.[9][10] The show was a joint production by GMA Network and iMusic Entertainment, with Smart Communications, McDonald's Philippines, and Bench as sponsors.[11] Ryan Cayabyab and Raul Mitra were chosen as music directors, accompanied by the 60-member ensemble of the Manila Symphony Orchestra. Velasquez and her team selected Ogie Alcasid, Janno Gibbs, and Lani Misalucha as guest acts[12] During rehearsals and preparations for the show, she revealed that the repertoire included some songs from her older albums that have been transformed into a more upbeat production which she aimed to create an "element of surprise" for concertgoers and described it as a "new way of singing a particular song".[11] She further said that it was vital for her to have the core of the show be a celebration and appreciation of her fans.[11]
The show started with Velasquez, dressed in a silver gown with long train, appearing from the upper stage accompanied by background dancers while singing several lines of "Shine". The song was mashed with Rihanna's "Where Have You Been" incorporating the chorus lines' trance elements. Shortly after, she began a medley of "Hot Stuff" and "Shake Your Groove Thing". After the number, she spoke briefly to the audience, saying, "I learned a lesson last year. It's not about me and it's not even about you. It's about glorifying God and this whole show we dedicate to You."[17] Velasquez then sang a medley songs from her album Nineteen 90 (1990). She followed this with an orchestral arrangement of "Dadalhin", before being joined by Gloc-9 for a performance of "Sirena". The set list continued with an upbeat rendition of "You've Made Me Stronger". She closed the segment with a medley of her movie themes alongside Rachelle Ann Go, La Diva, and Jaya.[19]
For "On the Wings of Love", Velasquez  was lifted by fabrics for an aerial silk performance. This was followed by George Benson's "In Your Eyes" and Aerosmith's "I Don't Want to Miss a Thing". She was joined by Ogie Alcasid and Janno Gibbs  for a medley of OPM songs. After a costume change, she performed Blondie's "Call Me". She followed this with a tribute number for her father and son with the songs "God Gave Me You" and "Leader of the Band". "Love Me Again" was introduced with a brief speech about her career beginnings, before closing the show with "You'll Never Walk Alone" and "You Are My Song". Velasquez returned onstage for an encore performance of "What Kind of Fool Am I?".[19]
The concert was met with positive responses from critics. Jojo Panaligan of the Manila Bulletin praised Velasquez's wide vocal range and rapport with her audience. He emphasized how the singer managed to "regain her roar" in various musical numbers. Panaligan further remarked that the show was a "resounding redemption of reputation" after its initial cancellation.[20] The Philippine Daily Inquirer's Dolly Anne Carvajal viewed the concert as "lovelier the second time around", and concluded that Velasquez delivered on her promise of a show the fans deserved.[21] In a review by ABS-CBNnews.com, it noted that Velasquez showcased her powerful vocals without missing a note and a proof that "she still has what it takes".[22] A writer for the Philippine Entertainment Portal summarized the show's concept as "a promise fulfilled" and observed that spectators were impressed of her performances.[17]
Silver Rewind was aired as a television special on January 27, 2013, on GMA Network.[17] For the production, Velasquez received nominations for Concert of the Year and Best Female Major Concert Act at the 5th Star Awards for Music, winning the latter.[23]
This set list is adapted from the television special Silver Rewind.[19][a]
After signing an international record deal with PolyGram Records, Velasquez achieved commercial success in some Asian territories with her fifth album Listen Without Prejudice (1994), which sold more than 700,000 copies and became her highest-selling album to date, aided by its lead single "In Love with You". She experimented further with jazz and adult contemporary genres on My Love Emotion (1995), while she recorded covers on Retro (1997). After she left PolyGram to sign with Mark J. Feist's MJF Company in 1998, she released the R&B-influenced album Drawn. Velasquez's follow-up record, R2K (1999), was supported by remakes of "On the Wings of Love", "I'll Never Love This Way Again", and "I Don't Want to Miss a Thing", and was subsequently certified twelve-times platinum by the Philippine Association of the Record Industry (PARI).
We were very poor but we were happy. My parents made sure that we ate on time and that was enough for me. [My father] had scoliosis and he was working at a construction site; he wasn't earning enough. My mom was good with money. She was able to stretch whatever little money we had.
Velasquez started singing at age six;[8] she underwent intensive vocal training with her father, who immersed her neck-deep in the sea and had her go through vocal runs.[10][11] She credits this unorthodox method for strengthening her core and stomach muscles, and developing her lung capacity.[12] Velasquez placed third in her first singing competition on Betty Mendez Livioco's The Tita Betty's Children Show.[13]
When Velasquez was nine, her family moved to Balagtas, Bulacan, where she attended St. Lawrence Academy and competed for her school at the annual Bulacan Private Schools Association singing competition.[3] In 1984, at fourteen, Velasquez auditioned for the reality television series Ang Bagong Kampeon.[3] She qualified and became the show's senior division winner, defending her spot for eight consecutive weeks.[3] Velasquez won the competition and was signed to a record deal with OctoArts International.[3]
In 1986, Velasquez initially used the stage name Chona and released the single "Love Me Again",[3] which failed commercially.[14] At the recommendation of another OctoArts recording artist, Pops Fernandez, she appeared on The Penthouse Live![14] While rehearsing for the show, Velasquez caught the attention of Ronnie Henares, a producer and talent manager who signed her to a management deal.[14][15] Velasquez adopted the stage name Regine at the suggestion of Martin Nievera, Fernandez's husband and The Penthouse Live! co-host.[3][14]
Velasquez signed with Viva Records and released her debut album Regine in 1987.[16] Henares served as an executive producer and worked with songwriters Joaquin Francisco Sanchez and Vehnee Saturno.[17] Three singles were released in 1987: "Kung Maibabalik Ko Lang", " Urong Sulong", and "Isang Lahi".[3] During this period, Velasquez appeared on the ABS-CBN television shows Triple Treat and Teen Pan Alley.[18] Two years after Regine's release, Velasquez represented the Philippines in the 1989 Asia Pacific Singing Contest in Hong Kong and won, performing the songs "You'll Never Walk Alone" from the musical Carousel and "And I Am Telling You I'm Not Going" from the musical Dreamgirls.[3]
Velasquez released her second studio album, Nineteen 90, in 1990.[14] She worked with Louie Ocampo on the album's lead single "Narito Ako",[19] which was originally recorded and performed by Maricris Bermont and written by Nonong Pedero for the 1978 Metro Manila Popular Music Festival.[20] Later that year, she headlined her first major concert at the Folk Arts Theater.[21][22] She recorded "Please Be Careful with My Heart" with Jose Mari Chan, who released the track on his album Constant Change;[23] she also sang backing vocals on Gary Valenciano's "Each Passing Night", which appears on his album Faces of Love.[24][25]
In 1991, Velasquez made her North American concert debut at Carnegie Hall in New York City,[26] a first for an Asian solo artist.[27] British theatrical producer Cameron Mackintosh later invited Velasquez to audition for the West End production of the musical Miss Saigon.[28] She received a letter from the production offering to train her in London, which she declined: partly due to her lack of experience in musical theater, and also because she wished to remain with her family.[28]
Velasquez's third studio album Tagala Talaga was released in October 1991.[29] It includes cover versions of recordings by National Artist for Music recipients Ryan Cayabyab, Lucio San Pedro, and Levi Celerio.[30][31] The album's lead single, titled "Buhay Ng Buhay Ko", was originally recorded by Leah Navarro and was written by Pedero,[32][30] with whom Velasquez had worked on Nineteen 90.[19] Other notable singles from the album include  "Anak" and "Sa Ugoy Ng Duyan".[30]
PolyGram Far East announced a joint-venture licensing deal in the Philippines in July 1993 with the formation of its subsidiary PolyCosmic Records.[33] Velasquez recorded a duet titled "It's Hard to Say Goodbye" with Canadian singer Paul Anka, which became the new label's first release.[33][34] The single was later included on her fourth studio album Reason Enough.[34] David Gonzales of AllMusic described the album as "more attuned to international ears" and said Velasquez's vocals are "thin and unimpressive".[34] One of its singles, "Sana Maulit Muli", won the Awit Award for Best Performance by a Female Recording Artist in 1994.[35]
Velasquez released her fifth studio album Listen Without Prejudice in 1994.[36] She worked with songwriters, including Glenn Medeiros, Trina Belamide, and John Laudon.[37] The album was released in several countries in Southeast and East Asia, including China, Hong Kong, Indonesia, Malaysia, Singapore, Taiwan, and Thailand.[36][38] The album's lead single "In Love with You" features Cantonese singer Jacky Cheung.[38] Gonzales commended the record's themes and said, "Cheung's presence on the duet had much to do with the overseas success".[36] The album had sold more than 700,000 copies worldwide, including 100,000 in the Philippines,[38] making it the best-selling album of Velasquez's career to date.[27]
Velasquez's sixth studio album My Love Emotion was released in 1995.[39] The title track, which was written by Southern Sons vocalist Phil Buckle,[39][40] was described by Gonzales as a "triumph [and] an outstanding vehicle, containing a strong melody and hook in the chorus".[39] The album made a combined regional and domestic sales of 250,000 copies.[38]
For her seventh studio album Retro (1996), Velasquez recorded cover versions of popular music of the 1970s and 1980s from artists including Donna Summer, Foreigner, and The Carpenters.[41] The album's only original track, "Fly", is credited to Earth, Wind & Fire members Maurice White, Al McKay, and Allee Willis because the song interpolates the melody of their single "September".[42] Velasquez left PolyCosmic in 1998, and signed a six-album contract with the MJF Company.[43][44] That year, her ninth studio album Drawn was released.[44] MJF head Mark J. Feist wrote and produced most of the tracks, including the lead single "How Could You Leave".[44][45] Drawn sold more than 40,000 copies and was awarded a platinum certification within two weeks of its release.[46][a]
Velasquez produced most of her next album R2K,[47] which was released on November 27, 1999.[48] She recorded remakes of Jeffrey Osborne's "On the Wings of Love", Dionne Warwick's "I'll Never Love This Way Again", Aerosmith's "I Don't Want to Miss a Thing", and ABBA's "Dancing Queen", among others.[47] Gonzales criticized the record's "infatuation with Western popular music" and called Velasquez's singing "self-assured [but] also unimpressive".[49] Commercially, R2K sold more than 40,000 copies in its second week of release, earning a platinum certification,[48][a] and was certified four times platinum a year later.[50] R2K has since been certified twelve times platinum,[51] becoming the highest-selling album by a female artist in the Philippines.[52][53] On December 31, 1999, Velasquez was a featured musical act in 2000 Today,[51] a BBC millennium television special that attracted a worldwide audience of more than 800 million viewers with its core program broadcast across the world's time zones,[54] which began with Kiribati Line Islands and ended in American Samoa.[55][56]
Velasquez headlined and directed the R2K Concert at the Araneta Coliseum in April 2000,[57] which won her Best Female Major Concert Act at the 13th Aliw Awards.[58] Ricky Lo from The Philippine Star was generally impressed with the production and complimented Velasquez's "boundless energy and creativity".[57] She also performed a concert at the Westin Philippine Plaza that year, which spawned the release of her first live album Regine Live: Songbird Sings the Classics in December 2000.[59][60] Although it was criticized for its audio mixing,[60] the album was certified six times platinum.[61][a] 
Velasquez worked with Filipino songwriters for material on her eleventh studio album Reigne.[62] The album and its lead single "To Reach You" were released in December 2001.[63][64] Other singles were Tats Faustino's "Dadalhin" and Janno Gibbs' "Sa Aking Pag-iisa".[65] Gonzales called the album "an adventurous set" and praised the quality of the songwriting.[62]
Velasquez won the inaugural MTV Asia Award for Favorite Artist Philippines in February 2002.[66][67] She performed "Cry" with Mandy Moore to promote the theatrical release of Moore's film A Walk to Remember.[53] In March, Velasquez hosted the first season of Star for a Night, which is based on the British talent show of the same name.[68] In April, she headlined a benefit concert called One Night with Regine at the National Museum of the Philippines, which was a collaboration with ABS-CBN Foundation to benefit Bantay Bata Foundation's child abuse response fund.[69] The show won Best Musical Program at the 7th Asian Television Awards.[70]
At the 2003 MTV Asia Awards, Velasquez won her second consecutive award for Favorite Artist Philippines.[71] In May 2003, she embarked on the Martin & Regine: World Concert Tour with Nievera.[72] The following month, Velasquez returned to host the second season of Search for a Star.[73] That November, she had a concert residency named Songbird Sings Streisand, a tribute to American singer and actor Barbra Streisand, at Makati's Onstage Theatre.[74]
Later in November and December 2005, Velasquez had an eight-day concert residency named Reflections at the Aliw Theater.[1] The sequel album Covers, Vol. 2 was released in February 2006.[82] Unlike its predecessor, it contains songs by foreign artists, including Alanis Morissette's "Head Over Feet", Blondie's "Call Me", and Elvis Presley's "Blue Suede Shoes".[83] Manila Bulletin's Jojo Panaligan was generally impressed with Velasquez's "versatility" and the album tracks' "jazzy and blues-y interpretation".[82] In October 2006, she performed a concert titled Twenty at the Araneta Coliseum, which won her Best Female Major Concert Act and Entertainer of the Year award at the 20th Aliw Awards.[84][85] In 2007, she became co-host of the reality television show Celebrity Duets, an interactive music competition based on the eponymous original US show.[86]
Velasquez developed other television projects in 2008. She appeared in Songbird, a weekly late-night musical television show that featured performances by musical guests.[87] She also featured in the musical television special The Best of Me, which was filmed at her residence in Quezon City.[88][89] Velasquez signed a deal with Universal Records and released an album titled Low Key in December 2008.[90][91] The album consists of cover versions of international songs that she described as "more relaxed, laid-back and restrained".[90] It includes tracks such as Billy Joel's "She's Always a Woman", Dan Fogelberg's "Leader of the Band", and Janis Ian's "At Seventeen".[92] The Philippine Daily Inquirer praised the album's maturity and wrote, "[Velasquez] no longer shrieks and shouts as much as she used to".[93] The album sold more than 25,000 copies within two months of its release and was certified platinum.[94][a] In May 2009, she appeared on the television documentary Roots to Riches, which chronicles her personal and professional struggles, and includes musical performances filmed in her hometown Malolos.[95] Later that month, she hosted the television talent show Are You the Next Big Star?.[96]
Velasquez's next album, a double CD set called Fantasy, was released in December 2010.[97][98] The first disc is composed of Original Pilipino Music (OPM) recordings and the second includes covers of international singles such as Madonna's "Papa Don't Preach", Toronto's "What About Love", and the Eagles' "Love Will Keep Us Alive".[97][99] The Philippine Daily Inquirer called the album "vocally sumptuous" and was generally impressed with Velasquez's vocals and range.[97] Fantasy received a platinum certification[a] and earned three nominations at the 3rd Star Awards for Music.[98][100] After receiving the Magna Award at the Myx Music Awards 2011,[101] and the confirmation of her pregnancy, Velasquez took a hiatus from public engagements.[102] She returned to television on October 6, 2012, with Sarap Diva, a weekly lifestyle talk show.[103] On November 16, Velasquez performed a concert titled Silver at the Mall of Asia Arena, which was cut short after she lost her voice due to a viral infection.[104][105]
After Silver's cancellation, Velasquez restaged the concert on January 5, 2013.[106][107] The concert received generally favorable reviews; Manila Bulletin's Jojo Panaligan called it a "redemption of reputation",[108] while Dolly Anne Carvajal of the Philippine Daily Inquirer said Velasquez did not fail to make up for the initial cancellation of the show.[109] The following month, she co-headlined in Foursome alongside Alcasid, Fernandez, and Nievera.[110] For both shows, Velasquez received four nominations at the 5th Star Awards for Music,[111] winning Best Female Major Concert Act for "Silver" and Concert of the Year for "Foursome".[112]
In November 2013, Velasquez's album Hulog Ka Ng Langit was released;[113] it received a platinum certification for two-week sales of 15,000 copies.[114][a] She won Best Inspirational Record for "Nathaniel (Gift of God)" and Best Christmas Recording for "Hele ni Inay" at the 27th Awit Awards,[115] while Hulog Ka Ng Langit won Album Cover of the Year at the 6th Star Awards for Music.[116] In 2014, she worked with Nievera in a one-night show titled Voices of Love,[117] with Gloc-9 on "Takipsilim",[118] and with Vice Ganda on "Push Mo Yan Teh".[119]
In February 2015, Velasquez appeared alongside Nievera, Valenciano, and Lani Misalucha in a concert titled Ultimate at the Mall of Asia Arena.[120] She received accolades at the 47th Box Office Entertainment Awards,[121] 7th Star Awards for Music,[122] and 5th Edukcircle Awards for the production.[123] In the same year, Velasquez served as a judge on the sixth season of the reality talent television show StarStruck.[124] In November 2015, Velasquez headlined a four-date concert residency called Regine at the Theater, which featured songs from musicals.[125][126]
For a third consecutive year, Velasquez appeared in a co-headlining concert at the Mall of Asia Arena in February 2016.[127] The two-night show, Royals, reunited her with Nievera and also features Angeline Quinto and Erik Santos.[128] Due to the concert's positive critical reception,[129] Velasquez won Best Female Concert Performer at the 48th Box Office Entertainment Awards and Most Influential Concert Performer of the Year at the 6th Edukcircle Awards.[130][131] In December 2016, People Asia magazine included Velasquez on its annual People of the Year list.[132]
Velasquez hosted Full House Tonight,[133] which ran from February to May 2017.[134] The following month, she announced that she had returned to Viva Records and that she had begun production of a new studio album called R3.0.[135] In August 2017, a cover of Up Dharma Down's 2010 song "Tadhana" was released as a promotional single.[136] An original track called "Hugot" was released as the album's lead single the following month.[137] In November she headlined the R3.0 Concert at the Mall of Asia Arena, and two months later, with Alcasid, she played a four-date U.S. concert series titled "Mr. and Mrs. A."[138][139]
In 2018, Velasquez hosted the television talent show The Clash,[140] served as a judge on ABS-CBN's revival of the Idol franchise series Idol Philippines, and hosted the musical variety show ASAP Natin' To.[18] In November, she staged a three-date concert series titled "Regine at the Movies" at the New Frontier Theater.[141]
Sharon Cuneta and Velasquez co-headlined a concert, Iconic, in October 2019.[142] For the show, Velasquez won the awards for Best Collaboration in a Concert and Entertainer of the Year at the 32nd Aliw Awards,[143] having won the latter honor in 2007 and 2009.[144][145] The following month, she released a collaborative single with Moira Dela Torre called "Unbreakable", which was recorded for the soundtrack of the film of the same name.[146] Velasquez appeared as the face of Australian beauty brand BYS and released the promotional single "I Am Beautiful" for the brand's "Be Your Own Expert" campaign.[147][148] She released the soundtrack singles "Ikaw Ang Aking Mahal" for the action television series The General's Daughter (2019)[149] and "Mahal Ko O Mahal Ako" for the drama series Love Thy Woman (2020).[150]
Velasquez organized virtual benefit concerts in support of relief efforts during the COVID-19 pandemic in 2020. She curated One Night with Regine, a collaboration with ABS-CBN to support Bantay Bata Foundation's COVID-19 response fund in April,[151][152] and appeared in Joy From Home, which raised funds to support Jollibee Group's food aid program in June.[153] On February 28, 2021, she was featured in an online streaming concert titled Freedom.[154]
Velasquez made her cinema debut with a minor role in the 1988 comedy film The Untouchable Family.[155] Its soundtrack includes her single "Urong Sulong".[156] She continued to appear in a series of supporting roles in comedies, including Pik Pak Boom (1988) and Elvis and James 2 (1990).[141][156]
A key point in Velasquez's film career came when she was cast in Joyce Bernal's Kailangan Ko'y Ikaw (2000) opposite Robin Padilla.[58] Film critic Noel Vera criticized the film's formula as "the nth variation of Roman Holiday", but wrote that Velasquez "[brought] her own public persona and charisma and sense of humor to the role".[161] Her next film role was in Pangako Ikaw Lang (2001), which reunited her with Bernal and Muhlach.[162] Vera was impressed with the film's direction and writing, and described Velasquez's performance as "sunny good nature [with a] light comic touch".[163] Pangako Ikaw Lang became the highest-grossing Filipino film of 2001.[51][164] Velasquez was awarded the Box Office Queen title at the 32nd Box Office Entertainment Awards due to the film's commercial performance.[164]
Her next television appearance was in an episode of ABS-CBN's weekly drama series Maalaala Mo Kaya (2001),[165] playing a woman with autism.[166] The role won her the Best Actress award at the 16th Star Awards for Television.[167] She portrayed a mundane and undesirable mail sorter in the drama Ikaw Lamang Hanggang Ngayon (2002) opposite Richard Gomez,[168] while Pangarap Ko Ang Ibigin Ka (2003) reunited her with Christopher de Leon of Wanted: Perfect Mother,[158][169] premiering at the Manila Film Festival in July 2003.[169] In December, Velasquez next starred alongside Bong Revilla in the superhero film Captain Barbell.[170]
Although Velasquez did not make any film appearances in 2004, she made her prime time television debut in the drama series Forever in My Heart,[158] in which she was reunited with Gomez, and worked alongside Ariel Rivera.[158] She next starred in romantic dramas, reuniting with Padilla in Till I Met You (2006) and with Pascual in Paano Kita Iibigin (2007).[171][172] For the latter film, Velasquez received FAMAS and Luna nominations for Best Actress.[173][174] In 2008 she returned to television, playing the titular character in the comedy series Ako si Kim Samsoon, an adaption of a South Korean television show.[175] Velasquez also voiced the eponymous character in the animated film Urduja (2008).[176]
During 2009, Velasquez made cameo appearances in the comedies Kimmy Dora, OMG (Oh, My Girl!), and Yaya and Angelina: The Spoiled Brat Movie.[177][178] In March 2010, Velasquez appeared in the musical television series Diva as a facially disfigured ghost singer.[179][180] The following year, she collaborated with Dingdong Dantes in the television series I Heart You, Pare! (2011).[181] She left the show for health reasons and was replaced by Iza Calzado.[182]
Velasquez played a widow obsessed with a pop star in Nigel Santos' independent film Yours Truly, Shirley.[189] The film premiered at the 2019 Cinema One Originals Film Festival.[189] In January 2020, she briefly appeared in the iWant comedy series My Single Lady.[190]
As a child, Velasquez enjoyed listening to her father singing classic songs to lull her to sleep;[8] she was drawn to traditional songs rather than nursery rhymes because of this routine.[8] Since her childhood, Velasquez has considered Sharon Cuneta a role model and credits Cuneta as a key inspiration who led her to pursue a musical career.[191]
Velasquez's music is influenced by artists such as Sheena Easton, Angela Bofill, Whitney Houston, and Mariah Carey in her early years.[1][192] She admires Houston for her "style and R&B influence" and Carey's songwriting.[192] On several occasions, Velasquez has cited Barbra Streisand as her main influence and musical inspiration, saying, "I look up to [Streisand] not just because of her enormous talent, but because of her fearlessness and dedication to excellence, her willingness to take risks and to be different."[193] Streisand's music has frequently featured in Velasquez's repertoire throughout her career, including a series of concerts paying homage to Streisand, which Velasquez described as "a pleasure" to perform.[193][194] Velasquez has also been influenced by many Filipino artists; early in her career, she cited Kuh Ledesma, Joey Albert, Gary Valenciano, Martin Nievera, and Pops Fernandez as her role models.[1] She has also paid tribute to Filipino songwriters, including George Canseco, Rey Valera, Basil Valdez, Ryan Cayabyab, and Willy Cruz.[195]
Velasquez's early-career music includes elements of traditional OPM love songs.[196] She described how she developed her musical style, saying, "I was only 16 and people didn't know what to do with me. When they want me to sing love songs, they had to explain to me what it meant because I didn't know the feeling yet."[197] Her debut album Regine includes ballads and bubblegum pop love songs;[196] its themes revolve around feelings of "excitement and uncertainty", as well as "missed chances and regrets".[196] Elvin Luciano from CNN Philippines wrote: "During her [initial] phase, she proved that Filipino love songs don't have to come pre-packaged in the kundiman-rooted love ballad."[196] Her later releases, including Nineteen 90 and Tagala Talaga, capitalized on her popularity; they are dominated by Filipino love songs.[198] Velasquez began working with foreign songwriters while planning her first regional album Listen Without Prejudice,[36][37] which according to AllMusic is "oriented towards easy-listening love songs with adventurous, contemporary touches".[36] The album features tracks with syncopated backbeats and hip-hop influences.[36]
During the mid-1990s and the early 2000s, Velasquez's albums consisted primarily of cover versions of international material because of its commercial viability, and Filipinos' preference for American music.[41][49][196] According to CNN Philippines, "Regine has a knack for choosing songs which at first, may not fit her, but eventually become her own."[196] Many of her songs, particularly in Retro, Drawn, and R2K, contained R&B, soul, and hip-hop elements.[41][44][49] Reigne is an OPM album that she described as "songs influenced by the music, artists, and genres that I enjoy listening to,"[196] and included tracks that are melancholic, sensual, and poetic.[196] Her crossover to film saw significant use of contemporary love ballads in her catalog of soundtrack themes, describing the music as "straightforward, earnest, and lyrically simple".[196]
Velasquez is known for her use of vocal belting,[199] and credits the vocal training she received from her father as a child:
Velasquez is a soprano and is often praised for her range and technical ability.[27][200][201] Luciano of CNN Philippines complimented her "trademark and sometimes melismatic vocals"[196] while Gonzales adds her singing is "strong, emotive, and confident".[39] She has often been criticized, however, for the excessive use of belting and oversinging.[199] Gonzales described Velasquez's timbre as "thin, unimpressive and unappealing at times", and said her singing is "aiming for a higher [note], [which] she did all too often".[39] Velasquez said, "I don't mean to make any songs hard. It's just that when I'm on stage, with the adrenaline rush and all, you get excited. I do try to hold back [because] otherwise I'd be screaming the whole show, that's not good."[202]
Velasquez's music has broadly influenced a younger generation of performers from reality television talent shows; Sarah Geronimo has stated Velasquez made her realize the value of hard work[215] while Rachelle Ann Go and Angeline Quinto have both said Velasquez inspired them during their early years as aspiring singers.[216][217] American Idol finalists Ramiele Malubay, Thia Megia, and Jessica Sanchez have expressed a desire to emulate Velasquez.[218][219][220]
Velasquez has been involved with several charitable organizations. She became associated with the United Nations Children's Fund (UNICEF) in 2001 and worked on a documentary titled Speak Your Mind, which is about homeless children in Payatas, Quezon City, one of the Philippines' largest open dumpsites.[223][224] The program was nominated for the UNICEF Child Rights Award.[223]
One of Velasquez's highest-profile benefit concert appearances was in One Night with Regine,[69] which she performed at the National Museum of the Philippines in support of the Bantay Bata Foundation, a child welfare organization.[69] In 2005, Velasquez appeared in an episode of the lifestyle talk show Mel and Joey, and donated proceeds from an auction of her gowns to the GMA Kapuso Foundation's Christmas Give-a-Gift project.[225] In 2009, Velasquez headlined a benefit television special called After The Rain: A Hopeful Christmas in the aftermath of Typhoon Ketsana (Ondoy).[226] In October 2010, she became an ambassador for Operation Smile,[227] a nonprofit organization that provides cleft lip and palate repair surgery to children worldwide.[228] She recorded the theme "S.M.I.L.E.", which was written for the project and appears on her studio album Fantasy.[227][99] In November 2013, proceeds from the sales of her album Hulog Ka Ng Langit were donated to the Philippine Red Cross in support of the Typhoon Haiyan (Yolanda) relief.[229]
Velasquez announced her relationship with singer-songwriter Ogie Alcasid in an article published by Yes! magazine in June 2007.[234] On August 8, 2010, the couple announced their engagement,[235] and in December, they married in Nasugbu, Batangas.[236] She gave birth to their son, Nathaniel James, via caesarean section on November 8, 2011.[237]
Velasquez is a born-again Christian.[238] In March 2016, she revealed she had suffered a miscarriage prior to her marriage to Alcasid and stated it was her reason for converting.[238] Velasquez also said she had been attending Victory Christian Fellowship.[238]
Throughout her career, Velasquez has received many honors and awards, including MTV Asia's Favorite Artist Philippines in 2002[67] and 2003,[239] and the Aliw Awards' Entertainer of the Year in 2007, 2009, and 2019.[143][144][145] She has been the recipient of lifetime achievement awards, including the Awit Awards' Dangal ng Musikang Pilipino,[240] the Star Awards for Music's Pilita Corrales Lifetime Achievement[241] and Natatanging Alagad Ng Musika,[242] FAMAS Awards' Golden Artist,[243] and Myx Music's Magna Award.[101]
LaDainian Tarshane Tomlinson (born June 23, 1979) is an American former professional football player who was a running back in the National Football League (NFL) for 11 seasons. After a successful college career with the TCU Horned Frogs, the San Diego Chargers selected him as the fifth overall pick in the 2001 NFL Draft. He spent nine years with the Chargers, earning five Pro Bowl appearances, three Associated Press first-team All-Pro nominations, and two NFL rushing titles. Tomlinson was also voted the NFL Most Valuable Player (MVP) in 2006 after breaking the record for touchdowns in a single season. He played two further seasons with the New York Jets, before retiring. He was elected to the Pro Football Hall of Fame in 2017. 
A native of Rosebud, Texas, Tomlinson showed athletic promise while attending University High School in Waco. He was recruited by Texas Christian University (TCU). As a junior, Tomlinson rushed for 406 yards in a single game, a Division I record at the time. As a senior, he earned unanimous All-America honors, and won the Doak Walker Award as the best college running back. TCU retired his No. 5 in 2005, and he was elected to the College Football Hall of Fame in 2014.
The Chargers selected Tomlinson No. 5 overall after passing on the opportunity to select highly-rated quarterback Michael Vick. A starter in his rookie season, Tomlinson opened his career with the first of seven consecutive seasons with over 1,200 rushing yards, a streak achieved previously only by Eric Dickerson. He became a prolific scorer under Marty Schottenheimer, who coached the Chargers from 2002 to 2006. Tomlinson's output reached a peak in 2006, when he set numerous single-season records, including for most touchdowns scored (31). These feats won him the NFL MVP award, but San Diego suffered an upset defeat in their playoff opener, and Schottenheimer was fired shortly afterwards. Tomlinson became less central to the Charger offense in the following three seasons, and missed time through injury in key games. He was released following the 2009 season, played two seasons with the Jets, and retired.
Tomlinson is often known by his initials, L.T.. He works as an analyst on the NFL Network, and also serves as a special assistant to the Chargers' principal owner, Dean Spanos.
Tomlinson was born on June 23, 1979, to Loreane Chappelle and Oliver Tomlinson in Rosebud, Texas. His mother worked as a preacher; his father left the family when Tomlinson was seven years old.[1] Tomlinson did not see his father very often afterward.[2] He grew up with a brother and a sister and later, also a half-sister and three half-brothers.[3] At age nine, Tomlinson joined the Pop Warner Little Scholars football program and scored a touchdown the first time he touched the ball.[1]
Tomlinson was an avid Dallas Cowboys and Miami Hurricanes fan during his youth. He idolized Walter Payton and admired Emmitt Smith, Jim Brown, and Barry Sanders.[6][9] Tomlinson was able to meet Smith while attending a camp run by Dallas Cowboys tight end Jay Novacek.[3]
Tomlinson accepted an athletic scholarship at Texas Christian University in Fort Worth, Texas, then a member of the Western Athletic Conference (WAC). He played for the TCU Horned Frogs from 1997 to 2000.[10] Before Tomlinson's arrival, TCU had appeared in only one bowl game in the previous 12 seasons and two in the previous 31, losing both.[11] They had recently been downgraded to a minor conference (the WAC) after the breakup of the Southwest Conference.[12]
"What have we been playing college football, a hundred-and-something years and nobody has even been able to do what he did today."
TCU retired his No. 5 during halftime of a November 2005 game against UNLV.[48] He was their single-game, single-season, and career record holder in both rushing touchdowns and rushing yards, amongst other records.[49] In December of that year, Tomlinson fulfilled a promise to his mother by earning his degree in communications from TCU.[50] He was inducted into the College Football Hall of Fame on December 9, 2014.[51]
Tomlinson was a holdout through much of training camp, while his agent Tom Condon negotiated with the Chargers.[70] He eventually signed a six-year, $38 million contract on August 21.[71] He had missed the first two preseason games, and was kept on the bench for the third,[72] before featuring briefly in the final game, rushing five times for 14 yards in a defeat to the Arizona Cardinals.[73]
League-wide, Tomlinson finished ninth for rushing yards and tied-fourth for rushing touchdowns. However, his yards per carry of 3.6 was only 31st among players with at least 100 carries, and his eight fumbles, one of which was returned for a key touchdown in a loss to the Philadelphia Eagles,[84] were joint-most among non-quarterbacks.[85] Tomlinson led the league in touches (rushing attempts and receptions combined) with 398, and ranked ninth for yards from scrimmage.[86] He received 16 votes for the Associated Press Offensive Rookie of the Year award, finishing second to Anthony Thomas (22 votes).[87] Thomas and Tomlinson were the two running backs named to the Pro Football Writers Association All-Rookie team.[88]
A day after their final game of 2001, the Chargers fired head coach Mike Riley,[89] replacing him with Marty Schottenheimer, recently dismissed as Washington's head coach.[90] Schottenheimer brought with him a reputation for favoring the running game over the pass.[91] Tomlinson said of his new coach, "I think he knows how to win, and he's been doing it for a number of years. ... I think that is the kind of coach that we need."[92] At his Pro Football Hall of Fame enshrinement speech in 2017, Tomlinson would describe Schottenheimer as the best coach he ever had.[3]
"There is no question that a number of those runs, including that long one in the first half, was the product of his determination and heart that he isn't going on the ground."
Tomlinson was rewarded for his performances with his first Pro Bowl nomination (together with Junior Seau, he was one of only two Chargers so honored),[114] as well as being named an Associated Press (AP) 2nd-team All-Pro.[115]
During the 2003 offseason, San Diego signed Lorenzo Neal, a fullback who had blocked for 1,000-yard rushers in each of his previous six seasons, and was coming off his first Pro Bowl nomination.[116] Tomlinson would later describe Neal as vital to the progression of his career, and chose the fullback to introduce him on the day of his induction into the Hall of Fame.[117]
Tomlinson finished with 1,645 rushing yards, third-most in the league.[130] He averaged 5.3 yards per carry, the sixth-highest among backs with 100-plus carries; this would be the best average of his career.[107] His receiving numbers were career highs: 100 receptions, 725 receiving yards, and four receiving touchdowns.[107] The 100 receptions placed him fourth in the league; the rest of the top ten were all wide receivers.[131] He broke Tony Martin's franchise record of 90 receptions in 1995.[h] Tomlinson had 2,370 yards from scrimmage, leading the league;[133] it was the second-highest total in NFL history up to that point.[i] He had five games with at least 200 yards from scrimmage during the season, another league record.[135] Tomlinson scored 17 total touchdowns, tied for third in the league and another new career high.[136]
Tomlinson was not voted to the Pro Bowl in 2003, which was seen as a snub by multiple observers; Tomlinson himself expressed disappointment, saying, "I think all those guys deserve to be there, but are they better than me? Nope."[137] However, he was named a Second-team Associated Press All-Pro for the second season in a row,[138] and was runner-up to Jamal Lewis for the AP Offensive Player of the Year Award with eight votes.[139][140]
On August 14, Tomlinson signed an eight-year contract worth $60 million, with $21 million guaranteed. It was the richest contract for a running back up to that point.[141]
Tomlinson's yardage numbers were down from the previous season, with 1,335 rushing (7th in the league)[147] and 441 receiving, in part because he was rested in the regular season finale,[148] though his yards per carry dropped significantly to 3.9, and he had barely half as many receptions with 53.[107] However, he led the NFL in rushing touchdowns for the first time with 17.[147] His 1,776 scrimmage yards were tied for fifth in the league, while his 18 total touchdowns ranked second.[149] Tomlinson earned his second Pro Bowl nomination, and rewarded his offensive line with an expenses-paid trip to Hawaii, the site of the game.[150] He was voted Associated Press First-team All-Pro for the first time in his career.[151]
Tomlinson's rushing totals were 1,462 yards and 18 touchdowns, ranking sixth and third in the league respectively.[176] With two receiving touchdowns, he had 20 in total;[107] this broke Chuck Muncie's franchise record, set in 1981.[175] He ranked third in the league, behind Seattle Seahawks running back Shaun Alexander, who set a new NFL record with 28.[177][178] Tomlinson again made the Pro Bowl,[179] and was named an Associated Press second-team All-Pro.[180]
San Diego changed starting quarterbacks in 2006. Brees had injured his shoulder in the 2005 finale; after negotiations for a new contract with Chargers general manager A. J. Smith broke down, Brees was allowed to leave in free agency, paving the way for 2004 No. 4 overall draft pick Philip Rivers to take over.[181][182] Tomlinson spoke positively about Rivers in the leadup to the season, saying, "He's going to be a great quarterback because the intangibles he has are what the great ones have."[183] However, Tomlinson would suggest in a 2016 interview with ESPN that the switch cost San Diego a Super Bowl win, stating that Rivers was too inexperienced at the time.[184]
"When we're old and can't play this game anymore, them are the moments we are going to remember, that we'll be able to tell our kids, tell our grandchildren. We can talk about something special that we did. We made history today."
On January 5, 2007, Tomlinson was named the Associated Press NFL Most Valuable Player for his record-breaking season, receiving 44 of the 50 votes from a panel of nationwide sportswriters and broadcasters who cover the NFL.[224] Accepting the award, Tomlinson said that he'd had a great year on a great team, adding, "I would feel so much better about winning if we win the Super Bowl." He was the first Chargers player to win the award.[224] Other organizations to name Tomlinson the NFL MVP included the Pro Football Writers of America,[225] the Sporting News,[226] and the Maxwell Football Club via the Bert Bell Award.[227] The Associated Press also honored him as the Offensive Player of the Year and a unanimous 1st-Team All-Pro.[228] Tomlinson was also named co-winner of the Walter Payton Man of the Year Award alongside Brees, now quarterback of the New Orleans Saints,[229] and was voted to his fourth Pro Bowl.[230] On July 11, 2007, Tomlinson won four ESPY Awards including Male Athlete of the Year.[231]
Tomlinson and other Chargers defended Schottenheimer after their swift exit from the 2006 playoffs,[235]
but the coach was nonetheless fired by team president Dean Spanos on February 12, 2007.[236] A dysfunctional relationship between Schottenheimer and general manager A.J. Smith was among the reasons given by Spanos.[237] Norv Turner, who was the San Diego Chargers offensive coordinator in Tomlinson's rookie season, replaced Schottenheimer as head coach a week later. "Norv is the perfect fit for our team. He will know exactly what to do with our team," Tomlinson said of the hiring.[238]
Tomlinson was involved in a pair of sideline incidents with Rivers over the course of the season. In an early-season loss to the Packers, the two appeared to argue on the sideline; Tomlinson dismissed the interaction as "competitive talk".[253] Later, during the overtime win in Tennessee, Tomlinson got up and walked away immediately after Rivers sat near to him on the bench.[254] Both players downplayed the incident, with Tomlinson explaining that he had left because he had just finished a conversation with Neal.[255]
While Tomlinson's rushing statistics of 1,474 yards and 15 touchdowns were both well short of his 2006 performances, he still led the league in both areas,[256] and became the first player since Edgerrin James in 1999 and 2000 to win back-to-back rushing titles.[257] During the year, Tomlinson became the 23rd player to reach 10,000 rushing yards in NFL history, as well as the fourth fastest,[258] while his career-opening streak of seven consecutive seasons with at least 1,200 rushing yards had previously been achieved only by Eric Dickerson.[259] With 60 receptions for a further 475 yards and 3 touchdowns, Tomlinson ranked second in the NFL for both yards from scrimmage (1,949) and total touchdowns (18). He had zero fumbles for the first time in his career, despite a league-high 375 touches.[260] Tomlinson was nominated for his fifth and final Pro Bowl and, unanimously, his third and final AP 1st-Team All-Pro squad.[261][107] He was also awarded the Bart Starr Award for his work on and off the field.[262]
Tomlinson ended the regular season with career-lows in attempts (292) and rushing yards (1,110, ranking tenth in the league), while his 11 rushing touchdowns (seventh in the league) and 3.8 yards per carry were both the least since his rookie year.[287][107] His 344 touches, 1,536 scrimmage yards, and 12 total touchdowns also represented a clear drop from the previous season.[288] The Charger offense became more focused on Rivers, who led the league in touchdowns and passer rating while throwing for over 4,000 yards.[289][290]
"That's the class that he shows ... I wanted to come down here and show mine ... I'm happy that he did it. It makes it special, because he's a good human being. He's a class individual, and I hope in these later years y'all treat him that way."
The offseason began with contract negotiations for Tomlinson, as Smith and Spanos hoped to restructure his existing contract and free up more salary cap space.[297] Relations between Smith and Tomlinson were strained throughout the process. Smith was reported to have been angered when Tomlinson revealed the full extent of his injury before the Divisional Round game against Pittsburgh the prior season.[298] When Tomlinson release a statement expressing his desire to remain in San Diego, Smith appeared to mock him when he responded to an interview question using very similar wording.[299] Smith later apologized to Tomlinson, and the two sides came to an agreement on a restructured version of his three-year contract on March 10. Tomlinson said in another statement, "My heart has always been in San Diego. I couldn't imagine putting on another uniform."[300]
On January 31, Tomlinson was named to the NFL's 2000s All-Decade Team after leading the league with 12,490 rushing yards in the 2000s, 1,797 more than runner-up Edgerrin James.[324] His 138 rushing touchdowns during the decade set an NFL record for any decade, and were 38 more than any other player in the 2000s.[325][326] However, there was speculation as to whether Tomlinson would play for the Chargers again, with the player himself saying that he expected to be let go.[327]
"I believe he's got a lot more left. When you watch him out here bouncing around doing a great job with protections, running the football, he's still got that wiggle."
Tomlinson entered the free-agent market for the first time in his career, expressing excitement at the future and a desire to win a Super Bowl.[334] After the New York Jets and Minnesota Vikings emerged as Tomlinson's most likely destinations, he met with both teams and signed a two-year, $5.2 million contract with the Jets on March 14, 2010.[335] He was expected to back up second-year running back Shonn Greene;[336] the Jets had released five-time 1,000 yard rusher Thomas Jones the previous month when he refused to take a pay cut to serve as Greene's backup.[337] Tomlinson chose to sign with New York because of his familiarity with the system of offensive coordinator Brian Schottenheimer, his enthusiasm for the defense- and run-focussed philosophy of head coach Rex Ryan and because he felt that the team offered him the best chance to win a championship.[330] He added that he believed he and Green would form a successful partnership.[338]
While Tomlinson's output reduced over the remainder of the season,[343] he nonetheless improved upon his last year with the Chargers by rushing for 914 yards at 4.2 yards per carry, while catching 52 passes for 368 yards.[107] Tomlinson stayed injury-free, missing only the regular season finale when he was rested with the Jets assured of a wildcard appearance in the playoffs.[345] He did score less frequently than in any of his seasons in San Diego, with only six touchdowns.[107] Brought in to complement Greene, Tomlinson was the Jet's leading rusher, outgaining his backfield partner by 148 yards from 34 more carries. As a team, the Jets ranked fourth in the NFL for rushing yards (though they had been first in 2009) and 11th for total yardage (up from 20th in 2009).[346][347][348] He continued to move up the NFL's career rushing yardage leaderboard during the season, passing Tony Dorsett and Eric Dickerson to reach sixth place.[341][349] He received the Dennis Byrd Award as the Jets' most inspirational player after a vote of his teammates.[350]
Tomlinson finished his final season with 75 carries for 280 rushing yards and a single rushing touchdown, all career lows.[371] Greene, now the main running back, had over 1,000 yards, but the Jets rushing attack was ranked only 22nd in the league, while their offense as a whole was 25th.[372][373] Tomlinson's new pass-catching role yielded 42 catches for 449 receiving yards and two receiving touchdowns; he averaged 10.7 yards per reception, a career-high.[107] His teammates again voted him the winner of the Dennis Byrd Award.[374]
Tomlinson's contract with the Jets expired after the 2011 season. In the aftermath of the season-ending loss in Miami, he said that he would need three or four weeks to decide whether to retire or not.[369] On May 11, Tomlinson returned to Qualcomm Stadium in San Diego to speak at a memorial for former Charger Junior Seau, who had been a positive influence on Tomlinson during his rookie season.[375][211] On June 18, he signed a ceremonial one-day contract with the San Diego Chargers and then immediately announced his retirement.[376] Chargers president Dean Spanos said that no other Charger would ever wear Tomlinson's No. 21.[377]
Former teammates including Rivers, Gates and three offensive linemen from his 2006 season were present at Tomlinson's farewell press conference, as were his wife, mother and children. Recalling the words of Seau at his own retirement, Tomlinson described the act as graduating to the next phase of his life. Of his failure to win a Super Bowl, he said, "I'm OK with never winning a Super Bowl championship. I know we've got many memories that we can call championship days."
[377]
At the time of his retirement, Tomlinson ranked fifth in NFL history in career rushing attempts (3,174)[378] and yards (13,684), and second in career rushing touchdowns (145).[377] He had 47 100-yard rushing games, and three 100-yard receiving games.[124] He also ranked third for receptions by a running back, catching 624 passes for 4,772 yards and a further 17 touchdowns.[379] Overall, he ranked fourth in career touches (3,798), fifth in yards from scrimmage (18,456), and third in total touchdowns (162).[380][377] He was only the second player to rush for at least 13,000 yards and catch passes for at least 4,000 yards, following Payton.[381] Completing his reputation as a versatile back,[382] Tomlinson completed 8 of 12 passing attempts in his career, for seven touchdowns and no interceptions. Only Payton, with eight, had more touchdown passes among non-quarterbacks in the Super Bowl era.[383] His playoff performances were less impressive, as he was injured in 2007 and 2008 and only rushed for 100 yards once in ten postseason games.[384][124][385]
Tomlinson was noted for consistently playing well against the Oakland Raiders.[244][386] In nineteen games against them, he rushed for 2,055 yards, at an average of 108.2 yards per game, well above his career average of 80.5 yards per game. He also rushed for 22 touchdowns, caught four, and threw another three, in each case more than his total against any other single team.[387]
An elusive runner in the open field who would use stiff arms to break tackles,[388] Tomlinson was also effective as a power back on inside runs.[389] In goal-line situations, he would often leap directly over the line of scrimmage to score.[192] He wore a distinctive dark visor for the majority of his career, to prevent migraines caused by stadiums lights; this benefitted him as it prevented defenders from reading his eyes.[390] When scoring, Tomlinson would frequently perform his own "teardrop" celebration,[391] placing his left hand behind his head and flipping the ball with his right.[392] He was often known by his initials, L.T., a nickname he shared with Hall of Fame New York Giants linebacker Lawrence Taylor.[393]
In 2005, Schottenheimer described Tomlinson as the finest running back he'd seen, arguing that past greats such as Jim Brown and Gale Sayers hadn't had to contend with defenders of the same size and speed.[172] When Tomlinson's number was retired in 2015, a trio of analysts on NFL.com placed him 3rd, 7th, and 8th respectively on their lists of top running backs in the Super Bowl era.[394][395] In 2021, the statistical site Pro-Football-Reference.com ranked him as the fifth-best running back in NFL history.[396] An NFL Network show, The Top 100: NFL's Greatest Players, aired in 2010 and ranked Tomlinson No. 61 among all positions,[397] while a 2019 USA Today poll placed him at No. 54.[398] However, he was not among the ten running backs named to the NFL 100th Anniversary All-Time Team.[399]
The Chargers credit Tomlinson with numerous records. Career figures discount his two seasons with the New York Jets.
Tomlinson is a Christian.[430] Tomlinson was introduced to his future wife, LaTorsha Oakley, while the two were students at TCU.[431] The couple married on March 21, 2003,[432] and have two children: a son born in 2010 and a daughter in 2011.[433][358] In 2007, Tomlinson's father Oliver Tomlinson and brother-in-law Ronald McClain died in an auto accident.[434][435]
During his playing career, Tomlinson was featured in commercials for Nike,[436] Campbell Soup,[437] and Vizio.[438] In April 2007, CNBC reported that Tomlinson turned down a request to become the cover athlete for EA Sports' Madden NFL 08 video game, as the money offered was not enough to justify the promotional work involved.[439]
In August 2012, Tomlinson joined the cast of NFL Network's Sunday morning show "First on the Field" as an analyst.[440] As of 2022, he is still with the network.[441] He covers Chargers preseason games as an analyst with CBS.[442]
Tomlinson has a charitable foundation. The foundation helps high school and college students, provides meals for people who are homeless or poor, and raises money for after-school programs and other causes. It focuses its efforts in Los Angeles, San Diego, and Texas.[446] The charity was cited as a reason for Tomlinson receiving the Bart Starr Award in 2008.[262]
In 2017, the Los Angeles Chargers announced that Tomlinson was joining the team as a special assistant to ownership. The role involves attempting to build a new fanbase after the Chargers' move to Los Angeles.[447]
Tomlinson's nephew, Tre'Vius Hodges-Tomlinson, followed in his footsteps by playing at TCU, where he earned All-Big 12 honors three times playing cornerback and won the Jim Thorpe Award in 2022 as the nation's best defensive back.[448]
A touchdown (abbreviated as TD[1]) is a scoring play in gridiron football. Whether running, passing, returning a kickoff or punt, or recovering a turnover, a team scores a touchdown by advancing the ball into the opponent's end zone. In American football, a touchdown is worth six points and is followed by an extra point or two-point conversion attempt.
To score a touchdown, one team must take the football into the opposite end zone. In all gridiron codes, the touchdown is scored the instant the ball touches or "breaks" the plane of the front of the goal line (that is, if any part of the ball is in the space on, above, or across the goal line) while in the possession of a player whose team is trying to score in that end zone. This particular requirement of the touchdown differs from other sports in which points are scored by moving a ball or equivalent object into a goal where the whole of the relevant object must cross the whole of the goal line for a score to be awarded. The play is dead and the touchdown scored the moment the ball touches plane in possession of a player, or the moment the ball comes into possession of an offensive player in the end zone (having established possession by controlling the ball and having one or both feet depending on the rules of the league or another part of the body, excluding the hands, touch the ground). The slightest part of the ball touching or being directly over the goal line is sufficient for a touchdown to score. However, only the ball counts, not a player's helmet, foot, or any other part of the body. Touching one of the pylons at either end of the goal line with the ball constitutes "breaking the plane" as well.
Touchdowns are usually scored by the offense by running or passing the ball. The former is called a rushing touchdown, and in the latter, the quarterback throws a touchdown pass or passing touchdown to the receiver, who either catches the ball in the field of play and advances it into the end zone, or catches it while already being within the boundaries of the end zone; the result is a touchdown reception or touchdown catch. However, the defense can also score a touchdown if they have recovered a fumble or made an interception and return it to the opposing end zone. Special teams can score a touchdown on a kickoff or punt return, or on a return after a missed or blocked field goal attempt or blocked punt. In short, any play in which a player legally carries any part of the ball over or across the opponent's goal line scores a touchdown, as is any play in which a player legally gains possession of the ball while it is on or across his opponent's goal line and both the player and ball are legally in-bounds - beyond this, the manner in which he gained possession is inconsequential. In the NFL, a touchdown may be awarded by the referee as a penalty for a "palpably unfair act", such as a player coming off the bench during a play and tackling the runner, who would otherwise have scored.[2]
A touchdown is worth six points. The scoring team is also awarded the opportunity for an extra point or a two-point conversion.[3]  Afterwards, the team that scored the touchdown kicks off to the opposing team, if there is any time left in the half. In most codes, a conversion is not attempted if the touchdown ended the game and the conversion cannot affect the outcome.
Unlike a try scored in rugby, and contrary to the event's name, the ball does not need to touch the ground when the player and the ball are inside the end zone. The term touchdown is a holdover from gridiron's early days when the ball was required to be touched to the ground as in rugby, as rugby and gridiron were still extremely similar sports at this point. This rule was changed to the modern-day iteration in 1889.
The ability to score a touchdown on the point-after attempt (two-point conversion) was added to NCAA football in 1958 and also used in the American Football League during its ten-year run from 1960-69. It was subsequently adopted by high school football in 1969, the CFL in 1975 and the NFL in 1994.[5][6]  The short-lived World Football League, a professional American football league that operated in 1974 and 1975, gave touchdowns a seven-point value.
The National Football League (NFL) is a professional American football league that consists of 32 teams, divided equally between the American Football Conference (AFC) and the National Football Conference (NFC). The NFL is one of the major professional sports leagues in the United States and Canada and the highest professional level of American football in the world.[5] Each NFL season begins with a three-week preseason in August, followed by the 18-week regular season which runs from early September to early January, with each team playing 17 games and having one bye week. Following the conclusion of the regular season, seven teams from each conference (four division winners and three wild card teams) advance to the playoffs, a single-elimination tournament that culminates in the Super Bowl, which is contested in February and is played between the AFC and NFC conference champions. The league is headquartered in New York City.
The NFL was formed in 1920 as the American Professional Football Association (APFA) before renaming itself the National Football League for the 1922 season. After initially determining champions through end-of-season standings, a playoff system was implemented in 1933 that culminated with the NFL Championship Game until 1966. Following an agreement to merge the NFL with the rival American Football League (AFL), the Super Bowl was first held in 1967 to determine a champion between the best teams from the two leagues and has remained as the final game of each NFL season since the merger was completed in 1970.[6]
The NFL is the wealthiest professional sports league in the world by revenue[7] and the sports league with the most valuable teams.[8] The NFL also has the highest average attendance (67,591) of any professional sports league in the world[9] and is the most popular sports league in the United States.[10] The Super Bowl is also among the biggest club sporting events in the world,[11] with the individual games accounting for many of the most watched television programs in American history and all occupying the Nielsen's Top 5 tally of the all-time most watched U.S. television broadcasts by 2015.[12]
On August 20, 1920, a meeting was held by representatives of the Akron Pros, Canton Bulldogs, Cleveland Indians, and Dayton Triangles at the Jordan and Hupmobile auto showroom in Canton, Ohio.[13] This meeting resulted in the formation of the American Professional Football Conference (APFC), a group who, according to the Canton Evening Repository, intended to "raise the standard of professional football in every way possible, to eliminate bidding for players between rival clubs and to secure cooperation in the formation of schedules".[14]
Another meeting was held on September 17, 1920, with representatives from teams from four states: Akron, Canton, Cleveland, and Dayton from Ohio; the Hammond Pros and Muncie Flyers from Indiana; the Rochester Jeffersons from New York; and the Rock Island Independents, Decatur Staleys, and Racine (Chicago) Cardinals from Illinois.[15][16] The league was renamed to the American Professional Football Association (APFA).[14] The league elected Jim Thorpe as its first president, and consisted of 14 teams (the Buffalo All-Americans, Chicago Tigers, Columbus Panhandles and Detroit Heralds joined the league during the year). The Massillon Tigers from Massillon, Ohio was also at the September 17 meeting, but did not field a team in 1920. Only two of these teams, the Decatur Staleys (now the Chicago Bears) and the Chicago Cardinals (now the Arizona Cardinals), remain in the NFL.[17]
The NFL was always the largest professional football league in the United States; it nevertheless faced numerous rival professional leagues through the 1930s and 1940s. Rival leagues included at least three separate American Football Leagues and the All-America Football Conference (AAFC), on top of various regional leagues of varying caliber. Three NFL teams trace their histories to these rival leagues; the Los Angeles Rams who came from a 1936 iteration of the American Football League, and the Cleveland Browns and San Francisco 49ers, both from the AAFC. By the 1950s, the NFL had an effective monopoly on professional football in the United States; its only competition in North America was the professional Canadian football circuit, which formally became the Canadian Football League (CFL) in 1958. With Canadian football being a different football code than the American game, the CFL established a niche market in Canada and still survives as an independent league.
A new professional league, the fourth American Football League (AFL), began to play in 1960. The upstart AFL began to challenge the established NFL in popularity, gaining lucrative television contracts and engaging in a bidding war with the NFL for free agents and draft picks. The two leagues announced a merger on June 8, 1966, to take full effect in 1970. In the meantime, the leagues would hold a common draft and championship game. The game, the Super Bowl, was held four times before the merger, with the NFL winning Super Bowl I and Super Bowl II, and the AFL winning Super Bowl III and Super Bowl IV.[28] After the league merged, it was reorganized into two conferences: the National Football Conference (NFC), consisting of most of the pre-merger NFL teams, and the American Football Conference (AFC), consisting of all of the AFL teams as well as three pre-merger NFL teams.[29]
From 1920 to 1934, the NFL did not have a set number of games for teams to play, instead setting a minimum. The league mandated a twelve-game regular season for each team beginning in 1935, later shortening this to eleven games in 1937 and ten games in 1943, mainly due to World War II. After the war ended, the number of games returned to eleven games in 1946, and later back to twelve in 1947. The NFL went to a 14-game schedule in 1961, which it retained until switching to a 16-game schedule in 1978.[38] In March 2021, the NFL officially adopted a 17-game schedule after gaining the agreement of the National Football League Players Association (NFLPA).[39]
Having an odd number of games in the schedule will give half the teams nine games as the home team, while half the teams have only eight home games. To minimize the perceived benefit on competition of having more home games, the extra home game will be rotated between the two conferences each year. This is because playoff berths are allocated at the conference level, so all teams within the conference will have played the same number of home games.
The NFL operated in a two-conference system from 1933 to 1966, where the champions of each conference would meet in the NFL Championship Game. If two teams tied for the conference lead, they would meet in a one-game playoff to determine the conference champion. In 1967, the NFL expanded from 15 teams to 16 teams. Instead of just evening out the conferences by adding the expansion New Orleans Saints to the seven-member Western Conference, the NFL realigned the conferences and split each into two four-team divisions. The four division champions would meet in the NFL playoffs, a two-round playoff.[40] The NFL also operated the Playoff Bowl (officially the Bert Bell Benefit Bowl) from 1960 to 1969. Effectively, a third-place game, pitting the two conference runners-up against each other, the league considers Playoff Bowls to have been exhibitions rather than playoff games. The league discontinued the Playoff Bowl in 1970 due to its perception as a game for losers.[41]
Following the addition of the former AFL teams into the NFL in 1970, the NFL split into two conferences with three divisions each. The expanded league, now with twenty-six teams,[29] would also feature an expanded eight-team playoff, the participants being the three division champions from each conference as well as one 'wild card' team (the team with the best win percentage) from each conference. In 1978, the league added a second wild card team from each conference, bringing the total number of playoff teams to ten, and a further two wild card teams were added in 1990 to bring the total to twelve. When the NFL expanded to 32 teams in 2002, the league realigned, changing the division structure from three divisions in each conference to four divisions in each conference. As each division champion gets a playoff bid, the number of wild card teams from each conference dropped from three to two.[42] The playoffs expanded again in 2020, adding two more wild card teams to bring the total to 14 playoff teams.[43]
At the corporate level, the National Football League considers itself a trade association made up of and financed by its 32 member teams.[44] Up until 2015, the league was an unincorporated nonprofit 501(c)(6) association.[45] Section 501(c)(6) of the Internal Revenue Code provides an exemption from federal income taxation for "Business leagues, chambers of commerce, real-estate boards, boards of trade, or professional football leagues (whether or not administering a pension fund for football players), not organized for profit and no part of the net earnings of which inures to the benefit of any private shareholder or individual."[46] In contrast, each individual team (except the non-profit Green Bay Packers[47]) is subject to tax because they make a profit.[48]
The league has three defined officers: the commissioner, secretary, and treasurer. Each conference has one defined officer, the president, which is essentially an honorary position with few powers and mostly ceremonial duties, including awarding the conference championship trophy.
The commissioner is elected by the affirmative vote of two-thirds or eighteen (whichever is greater) of the members of the league, while the president of each conference is elected by an affirmative vote of three-fourths or 10 of the conference members.[50] The commissioner appoints the secretary and treasurer and has broad authority in disputes between clubs, players, coaches, and employees. He is the "principal executive officer"[51] of the NFL and also has authority in hiring league employees, negotiating television contracts, disciplining individuals that own part or all of an NFL team, clubs, or employed individuals of an NFL club if they have violated league by-laws or committed "conduct detrimental to the welfare of the League or professional football".[51] The commissioner can, in the event of misconduct by a party associated with the league, suspend individuals, hand down a fine of up to US$500,000, cancel contracts with the league, and award or strip teams of draft picks.[51]
In extreme cases, the commissioner can offer recommendations to the NFL's executive committee, up to and including the "cancellation or forfeiture"[51] of a club's franchise or any other action, he deems necessary. The commissioner can also issue sanctions up to and including a lifetime ban from the league if an individual connected to the NFL has bet on games or failed to notify the league of conspiracies or plans to bet on or fix games.[51] The current Commissioner of the National Football League is Roger Goodell, who was elected in 2006 after Paul Tagliabue, the previous commissioner, retired.[52]
The NFL consists of 32 clubs divided into two conferences of 16 teams each. Each conference is divided into four divisions of four clubs each. During the regular season, each team is allowed a maximum of 55 players on its roster; only 48 of these may be active (eligible to play) on game days.[53] Each team can also have a sixteen-player practice squad separate from its main roster.[54]
Each NFL club is granted a franchise, the league's authorization for the team to operate in its home city. This franchise covers 'Home Territory' (the 75 miles surrounding the city limits, or, if the team is within 100 miles of another league city, half the distance between the two cities) and 'Home Marketing Area' (Home Territory plus the rest of the state the club operates in, as well as the area the team operates its training camp in for the duration of the camp). Each NFL member has the exclusive right to host professional football games inside its Home Territory and the exclusive right to advertise, promote, and host events in its Home Marketing Area. There are a couple of exceptions to this rule, mostly relating to teams with close proximity to each other: teams that operate in the same city (e.g. New York City and Los Angeles) or the same state (e.g. California, Florida, and Texas) share the rights to the city's Home Territory and the state's Home Marketing Area, respectively.[55]
Every NFL team is based in the contiguous United States. Although no team is based in a foreign country, the Jacksonville Jaguars began playing one home game a year at Wembley Stadium in London, England, in 2013 as part of the NFL International Series.[56] The Jaguars' agreement with Wembley was originally set to expire in 2016 but was extended through 2020 prior to travel restrictions relating to the coronavirus.[57] The Las Vegas Raiders (then in Oakland) played one game each in the 2018 and 2019 seasons in London, while each of the Los Angeles teams (Rams, Chargers) played a game there from 2016 to 2019.[58][59][60]
The 32 teams are organized into eight geographic divisions of four teams each. These divisions are further organized into two conferences, the National Football Conference and the American Football Conference. The two-conference structure has its origins in a time when major American professional football was organized into two independent leagues, the National Football League and its younger rival, the American Football League. The leagues merged 1970, adopting the older league's name and reorganizing slightly to ensure the same number of teams in both conferences.
The NFL season format consists of a three-week preseason, an 18-week regular season (each team plays 17 games), and a 14-team single-elimination playoff culminating in the Super Bowl, the league's championship game.
The NFL preseason begins with the Pro Football Hall of Fame Game, played at Tom Benson Hall of Fame Stadium in Canton.[74] Each NFL team is required to schedule three preseason games. NFC teams must play at least two of these at home in odd numbered years and AFC teams must play at least two at home in even numbered years. However, the teams involved in the Hall of Fame game, as well as any team that played in an American Bowl game, play four preseason games.[75] Preseason games are exhibition matches and do not count towards regular-season totals.[76] Because the preseason does not count towards standings, teams generally do not focus on winning games; instead, they are used by coaches to evaluate their teams and by players to show their performance, both to their current team and to other teams if they get cut.[77] The quality of preseason games has been criticized by some fans, who dislike having to pay full price for exhibition games,[78] as well as by some players and coaches, who dislike the risk of injury the games have, while others have felt the preseason is a necessary part of the NFL season.[77][78]
Currently, the 14 opponents each team faces over the 17-game regular season schedule are set using a pre-determined formula:[79] The league runs an 18-week, 272-game regular season. Since 2021, the season has begun the week after Labor Day (the first Monday in September) and concluded the week after New Year.[80] The opening game of the season is normally a home game on a Thursday for the league's defending champion.[81]
Most NFL games are played on Sundays, with a Monday night game typically held at least once a week and Thursday night games occurring on most weeks as well.[81] NFL games are not normally played on Fridays or Saturdays until late in the regular season, as federal law prohibits professional football leagues from competing with college or high school football. Because high school and college teams typically play games on Friday and Saturday, respectively, the NFL cannot hold games on those days until the Friday before the third Saturday in December. While Saturday games late in the season are common, the league rarely holds Friday games, the most recent one being on Christmas Day in 2020.[82] NFL games are rarely scheduled for Tuesday or Wednesday, and those days have only been used three times since 1948: in 2010, when a Sunday game was rescheduled to Tuesday due to a blizzard; in 2012, when the Kickoff game was moved from Thursday to Wednesday to avoid conflict with the Democratic National Convention;[83][84] and in 2020, when a game was postponed from Sunday to Tuesday due to players testing positive for COVID-19.
Although a team's home and away opponents are known by the end of the previous year's regular season, the exact dates and times for NFL games are not determined until much later because the league has to account for, among other things, the Major League Baseball postseason and local events that could pose a scheduling conflict with NFL games. During the 2010 season, over 500,000 potential schedules were created by computers, 5,000 of which were considered "playable schedules" and were reviewed by the NFL's scheduling team. After arriving at what they felt was the best schedule out of the group, nearly 50 more potential schedules were developed to try to ensure that the chosen schedule would be the best possible one.[87]
The only other postseason event hosted by the NFL is the Pro Bowl, the league's all-star game. Since 2009, the Pro Bowl has been held the week before the Super Bowl; in previous years, the game was held the week following the Super Bowl, but in an effort to boost ratings, the game was moved to the week before.[89] Because of this, players from the teams participating in the Super Bowl are exempt from participating in the game. The Pro Bowl is not considered as competitive as a regular-season game because the biggest concern of teams is to avoid injuries to the players.[90]
The National Football League has used three different trophies to honor its champion over its existence. The first trophy, the Brunswick-Balke Collender Cup, was donated to the NFL (then APFA) in 1920 by the Brunswick-Balke Collender Corporation. The trophy, the appearance of which is only known by its description as a "silver loving cup", was intended to be a traveling trophy and not to become permanent until a team had won at least three titles. The league awarded it to the Akron Pros, champions of the inaugural 1920 season; however, the trophy was discontinued and its current whereabouts are unknown.[91]
A second trophy, the Ed Thorp Memorial Trophy, was issued by the NFL from 1934 to 1967. The trophy's namesake, Ed Thorp, was a referee in the league and a friend to many early league owners; upon his death in 1934, the league created the trophy to honor him. In addition to the main trophy, which would be in the possession of the current league champion, the league issued a smaller replica trophy to each champion, who would maintain permanent control over it. The current location of the Ed Thorp Memorial Trophy, long thought to be lost,[92] is believed to be possessed by the Green Bay Packers Hall of Fame.[93]
The current trophy of the NFL is the Vince Lombardi Trophy. The Super Bowl trophy was officially renamed in 1970 after Vince Lombardi, who as head coach led the Green Bay Packers to victories in the first two Super Bowls. Unlike the previous trophies, a new Vince Lombardi Trophy is issued to each year's champion, who maintains permanent control of it. Lombardi Trophies are made by Tiffany & Co. out of sterling silver and are worth anywhere from US$25,000 to US$300,000.[94] Additionally, each player on the winning team as well as coaches and personnel are awarded Super Bowl rings to commemorate their victory. The winning team chooses the company that makes the rings; each ring design varies, with the NFL mandating certain ring specifications (which have a degree of room for deviation), in addition to requiring the Super Bowl logo be on at least one side of the ring.[95] The losing team are also awarded rings, which must be no more than half as valuable as the winners' rings, but those are almost never worn.[96]
The conference champions receive trophies for their achievement. The champions of the NFC receive the George Halas Trophy,[97] named after Chicago Bears founder George Halas, who is also considered one of the co-founders of the NFL. The AFC champions receive the Lamar Hunt Trophy,[98] named after Lamar Hunt, the founder of the Kansas City Chiefs and the principal founder of the American Football League. Players on the winning team also receive a conference championship ring.[99][100]
The NFL recognizes a number of awards for its players and coaches at its annual NFL Honors presentation. The most prestigious award is the AP Most Valuable Player (MVP) award.[101] Other major awards include the AP Offensive Player of the Year, AP Defensive Player of the Year, AP Comeback Player of the Year, and the AP Offensive and Defensive Rookie of the Year awards.[102] Another prestigious award is the Walter Payton Man of the Year Award, which recognizes a player's off-field work in addition to his on-field performance.[103] The NFL Coach of the Year award is the highest coaching award.[104] The NFL also gives out weekly awards such as the FedEx Air & Ground NFL Players of the Week[105] and the Pepsi MAX NFL Rookie of the Week awards.[106]
In the United States, the National Football League has television contracts with five networks: ABC, CBS, ESPN, Fox, and NBC. Collectively, these contracts cover every regular season and postseason game. In general, CBS televises afternoon games in which the away team is an AFC team, and Fox carries afternoon games in which the away team belongs to the NFC. These afternoon games are not carried on all affiliates, as multiple games are being played at once; each network affiliate is assigned one game per time slot, according to a complicated set of rules.[107] Since 2011, the league has reserved the right to give Sunday games that, under the contract, would normally air on one network to the other network (known as "flexible scheduling").[108] The only way to legally watch a regionally televised game not being carried on the local network affiliates is to purchase NFL Sunday Ticket, the league's out-of-market sports package, which is only available to subscribers to the DirecTV satellite service. The league also provides RedZone, an omnibus telecast that cuts to the most relevant plays in each game, live as they happen.
In addition to the regional games, the league also has packages of telecasts, mostly in prime time, that are carried nationwide. NBC broadcasts the primetime Sunday Night Football package', which includes the Thursday NFL Kickoff game that starts the regular season and a primetime Thanksgiving Day game. ESPN carries all Monday Night Football games.[107] The NFL's own network, NFL Network, broadcasts a series titled Thursday Night Football, which was originally exclusive to the network, but which in recent years has had several games simulcast on CBS (since 2014) and NBC (since 2016) (except the Thanksgiving and kickoff games, which remain exclusive to NBC).[109] For the 2017 season, the NFL Network will broadcast 18 regular season games under its Thursday Night Football brand, 16 Thursday evening contests (10 of which are simulcast on either NBC or CBS) as well as one of the NFL International Series games on a Sunday morning and one of the 2017 Christmas afternoon games. In addition, 10 of the Thursday night games will be streamed live on Amazon Prime. In 2017, the NFL games occupied the top three rates for a 30-second advertisement: $699,602 for Sunday Night Football, $550,709 for Thursday Night Football (NBC), and $549,791 for Thursday Night Football (CBS).[110]
In addition to radio networks run by each NFL team, select NFL games are broadcast nationally by Westwood One (known as Dial Global for the 2012 season). These games are broadcast on over 500 networks, giving all NFL markets access to each primetime game. The NFL's deal with Westwood One was extended in 2012 and continued through 2017.[118] Other NFL games are nationally distributed by Compass Media Networks and Sports USA Radio Network under contracts with individual teams.
Some broadcasting innovations have either been introduced or popularized during NFL telecasts. Among them, the Skycam camera system was used for the first time in a live telecast, at a 1984 preseason NFL game in San Diego between the Chargers and 49ers, and televised by CBS.[119] Commentator John Madden famously used a telestrator during games between the early 1980s to the mid-2000s, boosting the device's popularity.[120]
The NFL, as a one-time experiment, distributed the October 25, 2015, International Series game from Wembley Stadium in London between the Buffalo Bills and Jacksonville Jaguars. The game was live streamed on the Internet exclusively via Yahoo!, except for over-the-air broadcasts on the local CBS-TV affiliates in the Buffalo and Jacksonville markets.[121][122][123]
In 2015, the NFL began sponsoring a series of public service announcements to bring attention to domestic abuse and sexual assault in response to what was seen as poor handling of incidents of violence by players.[124]
The NFL finished the new contract negotiation for the media rights deal worth over $110 billion on March 18, 2021. In this contract, ABC would be eligible to broadcast the Super Bowl on U.S. television for the first time since it broadcast Super Bowl XL after the end of the 2005 NFL season. Also in current agreement, Amazon would be the new home for Thursday Night Football starting in 2023.[125][126][127]
On August 25, 2021, the NFL sent a memo to all 32 teams that only fully vaccinated personnel, with a maximum of 50 people, will have access to locker rooms while players are present on game days. The memo also stated that non-club-affiliated media are not permitted in the locker room.[128]
On February 9, 2022, as part of efforts to increase the sport's international reach, the NFL announced that Munich will host its first regular-season game in Germany in 2022.[129]
Each April (excluding 2014 when it took place in May), the NFL holds a draft of college players. The draft consists of seven rounds, with each of the 32 clubs getting one pick in each round.[130] The draft order for non-playoff teams is determined by regular-season record; among playoff teams, teams are first ranked by the furthest round of the playoffs they reached, and then are ranked by regular-season record. For example, any team that reached the divisional round will be given a higher pick than any team that reached the conference championships, but will be given a lower pick than any team that did not make the divisional round. The Super Bowl champion always drafts last, and the losing team from the Super Bowl always drafts next-to-last.[131] All potential draftees must be at least three years removed from high school in order to be eligible for the draft.[132] Underclassmen that have met that criterion to be eligible for the draft must write an application to the NFL by January 15 renouncing their remaining college eligibility.[133] Clubs can trade away picks for future draft picks, but cannot trade the rights to players they have selected in previous drafts.[134]
Aside from the seven picks each club gets, compensatory draft picks are given to teams that have lost more compensatory free agents than they have gained. These are spread out from rounds 3 to 7, and a total of 32 are given.[135] Clubs are required to make their selection within a certain period of time, the exact time depending on which round the pick is made in. If they fail to do so on time, the clubs behind them can begin to select their players in order, but they do not lose the pick outright. This happened in the 2003 draft, when the Minnesota Vikings failed to make their selection on time. The Jacksonville Jaguars and Carolina Panthers were able to make their picks before the Vikings were able to use theirs.[136] Selected players are only allowed to negotiate contracts with the team that picked them, but if they choose not to sign they become eligible for the next year's draft.[137] Under the current collective bargaining contract, all contracts to drafted players must be four-year deals with a club option for a fifth. Contracts themselves are limited to a certain amount of money, depending on the exact draft pick the player was selected with.[138] Players who were draft eligible but not picked in the draft are free to sign with any club.[130]
The NFL operates several other drafts in addition to the NFL draft. The league holds a supplemental draft annually. Clubs submit emails to the league stating the player they wish to select and the round they will do so, and the team with the highest bid wins the rights to that player. The exact order is determined by a lottery held before the draft, and a successful bid for a player will result in the team forfeiting the rights to its pick in the equivalent round of the next NFL draft.[139] Players are only eligible for the supplemental draft after being granted a petition for special eligibility.[140] The league holds expansion drafts, the most recent happening in 2002 when the Houston Texans began play as an expansion team.[141] Other drafts held by the league include an allocation draft in 1950 to allocate players from several teams that played in the dissolved All-America Football Conference[142] and a supplemental draft in 1984 to give NFL teams the rights to players who had been eligible for the main draft but had not been drafted because they had signed contracts with the United States Football League or Canadian Football League.[143]
Like the other major sports leagues in the United States, the NFL maintains protocol for a disaster draft. In the event of a 'near disaster' (less than 15 players killed or disabled) that caused the club to lose a quarterback, they could draft one from a team with at least three quarterbacks. In the event of a 'disaster' (15 or more players killed or disabled) that results in a club's season being canceled, a restocking draft would be held. Neither of these protocols has ever had to be implemented.[144]
Free agents in the National Football League are divided into restricted free agents, who have three accrued seasons and whose current contract has expired, and unrestricted free agents, who have four or more accrued seasons and whose contract has expired. An accrued season is defined as "six or more regular-season games on a club's active/inactive, reserved/injured or reserve/physically unable to perform lists".[145] Restricted free agents are allowed to negotiate with other clubs besides their former club, but the former club has the right to match any offer. If they choose not to, they are compensated with draft picks. Unrestricted free agents are free to sign with any club, and no compensation is owed if they sign with a different club.[145]
Clubs are given one franchise tag to offer to any unrestricted free agent. The franchise tag is a one-year deal that pays the player 120% of his previous contract or no less than the average of the five highest-paid players at his position, whichever is greater. There are two types of franchise tags: exclusive tags, which do not allow the player to negotiate with other clubs, and non-exclusive tags, which allow the player to negotiate with other clubs but gives his former club the right to match any offer and two first-round draft picks if they decline to match it.[146]
Members of clubs' practice squads, despite being paid by and working for their respective clubs, are also simultaneously a kind of free agent and are able to sign to any other club's active roster (provided their new club is not their previous club's next opponent within a set number of days) without compensation to their previous club; practice squad players cannot be signed to other clubs' practice squads, however, unless released by their original club first.[151]
Altria Group, Inc. (previously known as Philip Morris Companies, Inc.) is an American corporation and one of the world's largest producers and marketers of tobacco, cigarettes and related products. It operates worldwide and is headquartered in Henrico County, Virginia, just outside the city of Richmond.
Altria is the parent company of Philip Morris USA (producer of Marlboro cigarettes), John Middleton, Inc., U.S. Smokeless Tobacco Company, Inc., and Philip Morris Capital Corporation. Altria also maintains large minority stakes in Belgium-based brewer AB InBev, the Canadian cannabis company Cronos Group, and the e-cigarette maker Juul. It is a component of the S&P 500 and was a component of the Dow Jones Industrial Average from 1985 to 2008, dropping due to spin-offs of Kraft Foods Inc. in 2007 and Philip Morris International in 2008.[3]
The company's branding consultants, the Wirthlin Group, said: "The name change alternative offers the possibility of masking the negatives associated with the tobacco business", thus enabling the company to improve its image and raise its profile without sacrificing tobacco profits.[8]
Philip Morris executives thought a name change would insulate the larger corporation and its other operating companies from the political pressures on tobacco.[8]
The rebranding took place amidst social, legal, and financially troubled circumstances.[vague][9] In 2003 Altria was ranked Fortune number 11, and has steadily declined since. In 2010 Altria Group (MO) ranked at Fortune number 137, whereas its former asset, Philip Morris International, was ranked 94th.[10]
In 2006, a United States court found that Philip Morris "publicly ... disputed scientific findings linking smoking and disease knowing their assertions were false."[11] In a 2006 ruling, a federal court found that Altria, along with R. J. Reynolds Tobacco, Lorillard and Philip Morris were found guilty of misleading the public about the dangers of smoking.[12] Within this ruling, it was noted that "defendants altered the chemical form of nicotine delivered in mainstream cigarette smoke for the purpose of improving nicotine transfer efficiency and increasing the speed with which nicotine is absorbed by smokers."[13] This was done by manipulating smoke pH with ammonia. Adding ammonia increases the smoke pH, in a process called "freebasing" which causes smokers to be "exposed to higher internal nicotine doses and become more addicted to the product."[14]
On March 30, 2007, Altria's 88.1% stake in Kraft Foods was spun off, through a distribution of the remaining stake of shares (88.1%) to Altria shareholders. That same year, Altria began selling all its shares of Philip Morris International to Altria stockholders, a spin-off that was completed on March 28, 2008. Again in 2007 the company began the acquisition of cigar manufacturer John Middleton Co. from Bradford Holdings, which was complete in 2008. After Philip Morris International spun off, the former international subsidiaries halted the purchase of tobacco from America, which was a major factor in the closing of a newly renovated plant in North Carolina, an approximately 50% reduction in manufacturing, large-scale layoffs, and induced early retirements.[15]
In 2008, Altria officially moved its headquarters from New York City to Richmond, Virginia, after Philip Morris sold its downtown offices in New York City a decade earlier. With a few exceptions, all manufacturing, commercial, and executive employees had long been based in and around Richmond. Currently the company is headquartered in an unincorporated area within Henrico County, less than five miles west of the city limits of Richmond and less than ten miles from its downtown Richmond campus.
In 2009, Altria finalized its purchase of UST Inc., whose products included smokeless tobacco (made by U.S. Smokeless Tobacco Company) and wine (made by Chateau Ste. Michelle).[16] This ended a short era of competition between the new Marlboro smokeless tobacco products such as snus, and those produced by UST Inc.
On December 8, 2018, Altria announced its intent to acquire a 45% stake in Cronos Group for $1.8 billion.[17]
On December 20, 2018, Altria finalized the acquisition of a 35% stake in JUUL Labs, an e-cigarette company based out of San Francisco, California, for $12.8 billion.[18] On November 3, 2019, it was reported that Altria was taking a $4.5 billion writedown on its stake in Juul, 35% of its original value.[19] On July 28, 2022, it was reported that Altria's investment in Juul is now worth only 5% of the original amount of $12.8 billion. Despite the losses, Altria has announced that it will continue to support Juul and avoid investing in competing products.[20]
Altria and Japan Tobacco announced a joint venture called Horizon Innovations LLC on October 27, 2022. Horizon, owned 75 percent by Altria and 25 percent by Japan Tobacco, intends to sell Ploom heated tobacco sticks in the United States. FDA approval was expected to take until 2025, with customers able to buy Ploom by 2027.[21]
For the fiscal year 2020, Altria reported earnings of US$4.45 billion, with an annual revenue of US$26.15 billion. Altria's shares traded at over $66 per share, and its market capitalization was valued at over US$118.5 billion in October 2018.[22] As of 2018, the company ranked 154th on the Fortune 500 list of the largest United States corporations by revenue.[23]
Members of the board of directors of Altria Group as of February 2013 were:[25]
Prior to being based in Virginia, Philip Morris had its headquarters in Midtown Manhattan, New York City.[26] In 2003, Philip Morris announced that it would move its headquarters to Virginia. The company said that it planned to keep around 750 employees in its former headquarters. Brendan McCormick, a spokesperson for Philip Morris, said that the company estimated that the move would save the company over $60 million each year.[27] The company now has its head offices in an unincorporated area of Henrico County, Virginia, near Richmond.[28] In addition, the company has a 450,000-square-foot, $350 million Center for Research and Technology located in downtown Richmond at the Virginia BioTechnology Research Park that employs approximately 600 scientists, engineers and support staff.
According to the Center for Public Integrity, Altria spent around $101 million on lobbying the United States government between 1998 and 2004, the second-highest such figure for any organization in the nation.[29][30]
Altria also funded The Advancement of Sound Science Coalition which lobbied against the scientific consensus on anthropogenic climate change.[31]
Daniel Smith, representing Altria, sits on the Private Enterprise Board of the American Legislative Exchange Council.[32]
In August 2006, the Altria group was found guilty of civil fraud and racketeering.[33][34] The lawsuit claimed that Altria's marketing of "light" and "low tar" cigarettes constituted fraudulent misrepresentations under the Maine Unfair Trade Practices Act (MUTPA) because it deceived smokers into thinking the products are safer than regular cigarettes.
Since its launch, Capri-Sun has been packaged in laminated foil vacuum Doy-N-Pack pouches, with which the brand has become strongly associated. In the United States, these pouches were innovative as the first single-serving fruit juice containers. The pouch design has stayed largely the same, but changes in some markets have included transparent bottoms and paper straws. Capri-Sun is available in varying ranges of flavors in different countries, targeting different national flavor profiles. Globally, its best-known flavor is Orange.
Capri-Sun's main products are high in sugar content, although lower than many competitors. Characterizations of the juice drinks as "all-natural" have led to conflict in several countries between consumer advocates who highlight the high sugar content and low juice percentage, and Capri-Sun and its licensees, who in most cases have maintained that the term correctly describes the ingredients. In the United States, Capri Sun Inc.[b] was owned by tobacco conglomerate Philip Morris Cos. (now[update] Altria) from 1991 to 2007. Prohibited from selling tobacco to children, Philip Morris executives applied their experience to sugary drinks including Capri Sun,[3] adding the pouches to Lunchables and advertising them through a "California cool" aesthetic. Kraft continued to use Philip Morris's techniques after it became independent in 2007.[4]
Capri Sun is one of the most favorably-rated brands in the United States among Generation Z. About a third of American parents view the drink as healthy. In France, Capri-Sun has figured prominently in rap songs and has been noted as a drink of choice in poor suburban areas. Disputes over sugar content, "all-natural" status, and marketing to children have led to two class actions in the United States (one of which led to the abandonment of "all-natural" for a time), the removal of Capri-Sun from Tesco shelves in the United Kingdom, and a negative award from Foodwatch for deceptive marketing to children.
Rudolf Wild of Heidelberg founded Rudolf Wild & Co. (better known as Wild) in the German Reich in 1931. After World War II, Rudolf Wild created Libella, which the Lexington Herald-Leader in 1998 described as "the first all-natural soda made with real fruit juice". This led him to several new products, including Capri-Sonne,[5] which was developed in the 1960s.[6] Restrictions on color additives at the time in West Germany led to less visually appealing soft drinks, incentivizing opaque packaging.[7] Rudolf Wild & Co. engaged with Thimonnier, a French company that primarily manufactured sewing machines, for rights to use their patented pouch design and patented pouch-making machines.[8] According to Rudolf's son Hans-Peter Wild, Rudolf Wild & Co. did not obtain exclusive rights from Thimonnier, but rather by ordering all of the machines they made.[9] After initial issues with spoilage and stains were resolved,[2] the product debuted in West Germany in 1969.[10] The name references the Italian island of Capri due to its status as a vacation destination.[11]
SiSi-Werke changed its name to Capri Sun GmbH in December 2018. Capri Sun GmbH is organized and headquartered in Germany and is a subsidiary of Swiss companies Capri Sun AG and Capri Sun Group Holding AG[b] and German company Wild;[16] this makes it unusual in a largely American-led beverage industry. Wild licenses the brand to different companies, which as of 2009[update] bottle Capri-Sun in 18 countries; Wild subsidiary INDAG supplies the various bottling plants.[2]
In 1979, Shasta Beverages (then a part of Consolidated Foods) began to license the drink from Wild under the name Capri Sun.[18] After promising test runs in Buffalo, New York, and Atlanta, Georgia,[19] Shasta began a rolling expansion, starting with the Midwestern and Southeastern United States in 1980 and 1981.[20]
When Shasta introduced the product in the United States, its single-serving packaging was unusual in contrast with the 46-fluid-ounce (1.4L) aluminum cans that dominated the fruit juice market. Despite initial issues on rollout, the packaging carried the advantage of being light, durable, blunt, long-lasting, freezable, and insular. The patented design, trademarked under the name Doy-N-Pack and exclusively licensed by Shasta from Wild, soon faced a competitor in aseptic "brick packaging" like Tetra Pak. Both proved popular in stores, and Doy-N-Pack would usher in the use of pouches for single-serving food and beverage containers in the United States.[21]
Kraft's acquisition of Capri Sun Inc. included marketing rights of Capri Sun in Canada, Mexico, and the U.S. territory of Puerto Rico.[36] Kraft announced in 2000 that it would be launching Capri Sun in Mexico, imported from the U.S.;[37] in 2013, Jumex, rather than Kraft, announced plans to bring Capri-Sun to the Mexican market for the first time, with Capri Sun AG and sister company WILD-INDAG providing equipment and support.[38] Capri Sun was being advertised in Canada since by March 1991,[39] continuing after Kraft's purchase;[40] a 2009 CNN Money article noted Kraft as the distributor there.[41] As of 2022[update], Kraft licenses the Capri Sun brand in the United States, Canada, and Puerto Rico.[42]
In January 2007, a Florida woman, backed by the Center for Science in the Public Interest, sued Kraft for deceptive packaging, alleging that its usage of high-fructose corn syrup (HFCS) made its claimed "all-natural" status inaccurate. Kraft announced a day later that they would cease labeling Capri Sun that way as part of a planned reformulation and repackaging, replacing the words with "no artificial colors, flavors, or preservatives".[43] In 2015, facing declining sales, Kraft switched the main Capri Sun line from HFCS to sugar and switched the Roarin' Waters flavored water brand from HFCS and sucralose to sugar and Stevia.[44]
In 2017, Capri Sun GmbH sued American Beverage, claiming that the company's use of pouches to market SunnyD constituted trademark infringement. The U.S. District Judge, Paul A. Engelmayer, found conflicting evidence as to whether the pouch design was regularly associated with Capri Sun by consumers, with Capri Sun GmbH asserting the notion and American Beverage disputing it. Capri Sun GmbH has sued other companies for infringement on the design in the past, securing a $650,000 licensing fee from Faribault Foods in one such lawsuit. As of 2022[update], the case was scheduled for a jury trial.[45]
In the 2020s, Capri Sun has been noted for its marketing to parents, still in the hope that they would give the drink to their children. In 2020, Capri Sun donated 5 million pouches of water labelled "we're sorry it's not juice" to schools in Granite City, Illinois, and the Chicago metropolitan area.[46] The accompanying ad campaign, according to Ad Age, was targeted towards parents in the area who were concerned about COVID-19 pandemic safety restrictions shutting down water fountains.[47] In 2022, the company released an advertisement more directly targeted at parents, starring a character modeled after the male leads of romance novels designed to disinterest children, before changing tack to pitch Capri Sun.[48]
In January 2023, a class action was filed against Capri Sun in the United States District Court for the Southern District of New York over its branded apple juice; Eboni Forbes, the class representative, claimed that its use of citric acid as an ingredient contravened its claim of having "no artificial colors, flavors or preservatives".[49]
Capri-Sun established a factory in Nigeria in 1980.[50] As of 2023[update], Capri-Sun is the most popular fruit juice drink among children in Nigeria, where it is licensed by Chi Limited, a Coca-Cola Company subsidiary.[51]
SiSi-Werke attempted to secure trademarks for eight of its pouch designs in the European Union; the European Court of Justice, which has consistently rejected trademarks based on product shape, turned the request down in 2006.[63] On 21 February 2017, SiSi-Werke announced that it would be renaming Capri-Sonne to Capri-Sun in Germany, the last country to retain the original name. The brand faced some criticism for the change, some of it lighthearted in tone; it did not rule out reintroducing the name at a later date.[6]
A Capri-Sun executive told China Daily in 2016 that they produce 27 flavors worldwide.[60] The best-known flavor globally is Orange.[64] In the United States, an initial roster of Orange, Apple, Lemonade, and Fruit Punch[65] has expanded to include Mountain Cooler (apple and raspberry), Strawberry Kiwi, Wild Cherry, Pacific Cooler (apple, cherry, pineapple, and raspberry), Tropical Punch (strawberry, orange, pineapple, lemon, and lime),[66] and Grape Geyser.[67] Options vary by country: In France, for instance, only Classic (orange), Crush, and Bio flavors are sold;[68] the United Kingdom has Orange, Tropical, Blackcurrant, and Cherry;[69] and the Chinese market sees Orange, White Grape, Pear, and Peach Apple.[70] Flavor profile varies as well: German fruit flavors are riper than French ones. Flavors in China, Mexico, and the United Arab Emirates are sweeter than those used in Europe, which Capri-Sun says is to cater to local tastes; likewise Hungarian cherry flavors are sourer than elsewhere.[71]
In addition to the main line of fruit-juice-based beverages, American Capri Sun products have included a 100% juice variant[72] and Roarin' Waters, a line of flavored waters.[73] The United Kingdom has its own flavored water line, Fruity Water,[74] as well as squash (concentrated syrup).[75]
Capri-Sun drinks are canned in laminate vacuum pouches.[77] In 2021, a TikTok video went viral after a father discovered mold in his daughter's Capri Sun through the package's clear bottom; Capri Sun stated that finding contaminants are the purpose of the clear bottom,[78] that the mold is naturally occurring, and that testing of the packaging showed that it was not sealed properly. A year later, a recall was announced on over 5,000 Capri Sun Wild Cherry pouches after it was discovered that some were contaminated with cleaning supplies.[79] Capri Sun also faced criticism in the United Kingdom for introducing paper straws to its pouches, a move it said was environmentally friendly; consumers complained about the straw's inability to pierce the pouch in opening the drink, as well as the still-existing plastic wrapping on the straw, which Capri-Sun said was required under British law.[80]
In addition to the well-known Doy-N-Pak pouch, Capri-Sun comes in other packaging in various markets, including a resealable pouch with safety cap used on some products.[81] The squash lines are sold in one-liter plastic bottles.[75]
A 2022 review of fruit punch drinks in the Marin Independent Journal gave Capri Sun All Natural Fruit Punch two stars, noting its lower sugar content compared to other listed sugary drinks but criticizing its taste as "watery" and non-evocative of the fruits depicted on the label.[84] A 2017 review of "your kids' lunch box favorites" by Brooke Jackson-Glidden in the Statesman Journal noted the Capri Sun Strawberry Kiwi's 13 grams of sugar, praising its moderately sweet taste and small size. The paper's "resident intern", Young Cooper, commented that it was "definitely not the best flavor of Capri Sun."[85] Marnie Shure of The Takeout, reviewing Capri Sun fruit punch after the switch to monk fruit, wrote that the sweetness had now become the primary flavor, rather than notes of actual fruit as before, and assessed a perhaps 5% increase in tartness, but complimented the lack of aftertaste she associated with most sweeteners.[83] Chad Eschman of Vinepair reviewed Capri Sun flavors as they relate to creating mixers; reviews included positively rating the combination of gin and Pacific Cooler as tasting like a large white gummy bear and negatively rating the combination of tequila and Tropical Cooler as "we've made a huge mistake".[66]
Roarin' Waters faced early criticism for its sugariness and lack of juice.[73] Gannett News Service's Kim Painter characterized it as "a reduced-calorie fruit drink, apparently made for children who expect all drinks, even water, to be sweet",[86] while James Lileks of the Minneapolist Star Tribune wrote that his daughter thought it tasted like Easy Mac.[87]
"Go ahead, stick the straw, stick the straw in the Capri-Sun."
A 2015 study in Public Health Nutrition on American parents' attitudes towards sugary drinks found that 36% of surveyed parents with children between the ages of 2 and 17 rated Capri Sun as "somewhat" or "very healthy", with 48% saying that they gave the drink to their children in that age. Black and Hispanic parents were significantly more likely to rate Capri Sun as healthy than white parents, and the rating was higher than sugary fruit drinks as a category, which only 30% of parents gave the same rating. Regarding Roarin' Waters, 39% rated the same, but only 16% said that they give their children the drink. Hispanic parents were significantly more likely than white parents to rate the product as healthy, although black parents were not. Roarin' Waters was one of a few products to be rated as less healthy than its category overall; 48% rated flavored water drinks as healthy. The study concluded that those parents may have selected Capri Sun Roarin' Waters because they consider it a healthier option.[91]
A 2013 online poll from Foodwatch, a European consumer protection group, resulted in Capri-Sun receiving a "Golden Windbag" award for perceived deceptive advertising to children. Capri-Sun denied that its advertising was targeted towards children.[92]
A 2022 Morning Consult survey of American Generation Z adults ranked Capri Sun in 17th place out of over 4,000 on a list of their most favored brands, with 77% rating the brand favorably.[95] Capri Sun was also one of the brands with the largest differential between Gen Z and older peers; the brand's favorability rating with Gen Z was 16 percentage points higher than the U.S. adult population at large, 19th highest in the brands surveyed, and 7 percentage points higher than the Millennial respondents, for 16th place.[96]
Decades of political controversy over slavery were brought to a head by the victory in the 1860 U.S. presidential election of Abraham Lincoln, who opposed slavery's expansion into the western territories. An initial seven southern slave states responded to Lincoln's victory by seceding from the United States and, in February 1861, forming the Confederacy. The Confederacy seized U.S. forts and other federal assets within their borders. Led by Confederate President Jefferson Davis, the Confederacy asserted control over about a third of the U.S. population in eleven of the 34 U.S. states that then existed. Four years of intense combat, mostly in the South, ensued.
A wave of Confederate surrenders followed. On April 14, just five days after Lee's surrender, Lincoln was assassinated. As a practical matter, the war ended with the May 26 surrender of the Department of the Trans-Mississippi but the conclusion of the American Civil War lacks a clear and precise historical end date. Confederate ground forces continued surrendering past the May 26 surrender date until June 23. By the end of the war, much of the South's infrastructure was destroyed, especially its railroads. The Confederacy collapsed, slavery was abolished, and four million enslaved black people were freed. The war-torn nation then entered the Reconstruction era in an attempt to rebuild the country, bring the former Confederate states back into the United States, and grant civil rights to freed slaves.
The Civil War is one of the most extensively studied and written about episodes in U.S. history. It remains the subject of cultural and historiographical debate. Of particular interest is the persisting myth of the Lost Cause of the Confederacy. The American Civil War was among the first wars to use industrial warfare. Railroads, the telegraph, steamships, the ironclad warship, and mass-produced weapons were all widely used during the war. In total, the war left between 620,000 and 750,000 soldiers dead, along with an undetermined number of civilian casualties, making the Civil War the deadliest military conflict in American history.[g] The technology and brutality of the Civil War foreshadowed the coming World Wars.
Disagreements among states about the future of slavery were the main cause of disunion and the war that followed.[20][21] Slavery had been controversial during the framing of the Constitution, which, because of compromises, ended up with proslavery and antislavery features.[22] The issue of slavery had confounded the nation since its inception and increasingly separated the United States into a slaveholding South and a free North. The issue was exacerbated by the rapid territorial expansion of the country, which repeatedly brought to the fore the question of whether new territory should be slaveholding or free. The issue had dominated politics for decades leading up to the war. Key attempts to resolve the matter included the Missouri Compromise and the Compromise of 1850, but these only postponed an inevitable showdown over slavery.[23]
The American Revolution and the cause of liberty added tremendous impetus to the abolitionist cause. Slavery, which had been around for thousands of years, was considered normal and was not a significant issue of public debate prior to the Revolution. The Revolution changed that and made it into an issue that had to be addressed. As a result, during and shortly after the Revolution, the Northern states quickly started outlawing slavery. Even in Southern states, laws were changed to limit slavery and facilitate manumission. The amount of indentured servitude dropped dramatically throughout the country. An Act Prohibiting Importation of Slaves sailed through Congress with little opposition. President Thomas Jefferson supported it, and it went into effect on January 1, 1808, which was the first day that the Constitution (Article I, section 9, clause 1) permitted Congress to prohibit the importation of slaves. Benjamin Franklin and James Madison each helped found manumission societies. Influenced by the Revolution, many slave owners freed their slaves, but some, such as George Washington, did so only in their wills. The number of free black people as a proportion of the black population in the upper South increased from less than 1 percent to nearly 10 percent between 1790 and 1810 as a result of these actions.[40][41][42][43][44][45]
In the decades leading up to the Civil War, abolitionists, such as Theodore Parker, Ralph Waldo Emerson, Henry David Thoreau and Frederick Douglass, repeatedly used the Puritan heritage of the country to bolster their cause. The most radical anti-slavery newspaper, The Liberator, invoked the Puritans and Puritan values over a thousand times. Parker, in urging New England congressmen to support the abolition of slavery, wrote, "The son of the Puritan . . . is sent to Congress to stand up for Truth and Right."[48][49] Literature served as a means to spread the message to common folks. Key works included Twelve Years a Slave, the Narrative of the Life of Frederick Douglass, American Slavery as It Is, and the most important: Uncle Tom's Cabin, the best-selling book of the 19th century aside from the Bible.[50][51][52]
A more unusual abolitionist than those named above was Hinton Rowan Helper, whose 1857 book, The Impending Crisis of the South: How to Meet It, "[e]ven more perhaps than Uncle Tom's Cabin ... fed the fires of sectional controversy leading up to the Civil War."[53] A Southerner and a virulent racist, Helper was nevertheless an abolitionist because he believed, and showed with statistics, that slavery "impeded the progress and prosperity of the South, ... dwindled our commerce, and other similar pursuits, into the most contemptible insignificance; sunk a large majority of our people in galling poverty and ignorance, ... [and] entailed upon us a humiliating dependence on the Free States...."[54]
By 1840 more than 15,000 people were members of abolitionist societies in the United States. Abolitionism in the United States became a popular expression of moralism, and led directly to the Civil War. In churches, conventions and newspapers, reformers promoted an absolute and immediate rejection of slavery.[55][56] Support for abolition among the religious was not universal though. As the war approached, even the main denominations split along political lines, forming rival Southern and Northern churches. For example, in 1845 the Baptists split into the Northern Baptists and Southern Baptists over the issue of slavery.[57][58]
Abolitionist sentiment was not strictly religious or moral in origin. The Whig Party became increasingly opposed to slavery because it saw it as inherently against the ideals of capitalism and the free market. Whig leader William H. Seward (who would serve as Lincoln's secretary of state) proclaimed that there was an "irrepressible conflict" between slavery and free labor, and that slavery had left the South backward and undeveloped.[59] As the Whig party dissolved in the 1850s, the mantle of abolition fell to its newly formed successor, the Republican Party.[60]
Manifest destiny heightened the conflict over slavery. Each new territory acquired had to face the thorny question of whether to allow or disallow the "peculiar institution".[61] Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. Pro- and anti-slavery forces collided over the territories west of the Mississippi.[62]
By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly.[70] The first of these theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a constitutional mandate. The failed Crittenden Compromise of 1860 was an expression of this view.[71]
The fourth doctrine was advocated by Mississippi Senator (and soon to be Confederate President) Jefferson Davis.[78] It was one of state sovereignty ("states' rights"),[79] also known as the "Calhoun doctrine",[80] named after the South Carolinian political theorist and statesman John C. Calhoun.[81] Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution.[82] These four doctrines comprised the dominant ideologies presented to the American public on the matters of slavery, the territories, and the U.S. Constitution before the 1860 presidential election.[83]
Historian James McPherson points out that even if Confederates genuinely fought over states' rights, it boiled down to states' right to slavery.[87] McPherson writes concerning states' rights and other non-slavery explanations:
While one or more of these interpretations remain popular among the Sons of Confederate Veterans and other Southern heritage groups, few professional historians now subscribe to them. Of all these interpretations, the states'-rights argument is perhaps the weakest. It fails to ask the question, states' rights for what purpose? States' rights, or sovereignty, was always more a means than an end, an instrument to achieve a certain goal more than a principle.[87]
States' rights was an ideology formulated and applied as a means of advancing slave state interests through federal authority.[89] As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of Federal power."[90][91] Before the Civil War, the Southern states supported the use of federal powers to enforce and extend slavery, as with the Fugitive Slave Act of 1850 and the Dred Scott v. Sandford decision.[92][93] The faction that pushed for secession often infringed on states' rights. Because of the overrepresentation of pro-slavery factions in the federal government, many Northerners, even non-abolitionists, feared the Slave Power conspiracy.[92][93] Some Northern states resisted the enforcement of the Fugitive Slave Act. Historian Eric Foner states that the act "could hardly have been designed to arouse greater opposition in the North. It overrode numerous state and local laws and legal procedures and 'commanded' individual citizens to assist, when called upon, in capturing runaways." He continues, "It certainly did not reveal, on the part of slaveholders, sensitivity to states' rights."[85] According to historian Paul Finkelman, "the southern states mostly complained that the northern states were asserting their states' rights and that the national government was not powerful enough to counter these northern claims."[86] The Confederate Constitution also "federally" required slavery to be legal in all Confederate states and claimed territories.[84][94]
Sectionalism resulted from the different economies, social structure, customs, and political values of the North and South.[95][96] Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention, which manifested Northern dissatisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence agriculture for poor whites. In the 1840s and 1850s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist, and Presbyterian churches) into separate Northern and Southern denominations.[97]
Historians have debated whether economic differences between the mainly industrial North and the mainly agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s, and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.[98][99]
Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entirety of the United States (called "Southern Unionists") and those loyal primarily to the Southern region and then the Confederacy.[105]
Perceived insults to Southern collective honor included the enormous popularity of Uncle Tom's Cabin and abolitionist John Brown's attempt to incite a slave rebellion in 1859.[106][107]
While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it.[108] The South ignored the warnings; Southerners did not realize how ardently the North would fight to hold the Union together.[109]
The election of Abraham Lincoln in November 1860 was the final trigger for secession.[110] Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction.[111] However, Lincoln would not be inaugurated until five months after the election, which gave the South time to secede and prepare for war in the winter and spring of 1861.[112]
According to Lincoln, the American people had shown that they had been successful in establishing and administering a republic, but a third challenge faced the nation: maintaining a republic based on the people's vote, in the face of an attempt to destroy it.[113]
The election of Lincoln provoked the legislature of South Carolina to call a state convention to consider secession. Before the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws, and even to secede from the United States. The convention unanimously voted to secede on December 20, 1860, and adopted a secession declaration. It argued for states' rights for slave owners in the South, but contained a complaint about states' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.[114]
As Southerners resigned their seats in the Senate and the House, Republicans were able to pass projects that had been blocked by Southern senators before the war. These included the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railroad Acts),[123] the National Bank Act, the authorization of United States Notes by the Legal Tender Act of 1862, and the ending of slavery in the District of Columbia. The Revenue Act of 1861 introduced the income tax to help finance the war.[124]
In December 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of the Southern states, but Lincoln and the Republicans rejected it.[125] Lincoln stated that any compromise that would extend slavery would in time bring down the Union.[126] A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise; it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.[127]
On March 4, 1861, Abraham Lincoln was sworn in as president. In his inaugural address, he argued that the Constitution was a more perfect union than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void".[128] He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of federal property,[128] including forts, arsenals, mints, and customhouses that had been seized by the Southern states.[129] The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify the armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.[128]
The Davis government of the new Confederacy sent three delegates to Washington to negotiate a peace treaty with the United States of America. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government.[130] Lincoln instead attempted to negotiate directly with the governors of individual seceded states, whose administrations he continued to recognize.[citation needed]
Complicating Lincoln's attempts to defuse the crisis were the actions of the new Secretary of State, William Seward. Seward had been Lincoln's main rival for the Republican presidential nomination. Shocked and embittered by this defeat, Seward agreed to support Lincoln's candidacy only after he was guaranteed the executive office that was considered at that time to be the most powerful and important after the presidency itself. Even in the early stages of Lincoln's presidency Seward still held little regard for the new chief executive due to his perceived inexperience, and therefore Seward viewed himself as the de facto head of government or "prime minister" behind the throne of Lincoln. In this role, Seward attempted to engage in unauthorized and indirect negotiations that failed.[130] However, President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy: Fort Monroe in Virginia, Fort Pickens, Fort Jefferson and Fort Taylor in Florida, and Fort Sumter in South Carolina.[131][citation needed]
The American Civil War began on April 12, 1861, when Confederate forces opened fire on the Union-held Fort Sumter. Fort Sumter is located in the middle of the harbor of Charleston, South Carolina.[132] Its status had been contentious for months. Outgoing President Buchanan had dithered in reinforcing the Union garrison in the harbor, which was under command of Major Robert Anderson. Anderson took matters into his own hands and on December 26, 1860, under the cover of darkness, sailed the garrison from the poorly placed Fort Moultrie to the stalwart island Fort Sumter.[133] Anderson's actions catapulted him to hero status in the North. An attempt to resupply the fort on January 9, 1861, failed and nearly started the war then and there. But an informal truce held.[134] On March 5, the newly sworn in Lincoln was informed that the Fort was running low on supplies.[135]
Fort Sumter proved to be one of the main challenges of the new Lincoln administration.[135] Back-channel dealing by Secretary of State Seward with the Confederates undermined Lincoln's decision-making; Seward wanted to pull out of the fort.[136] But a firm hand by Lincoln tamed Seward, and Seward became one of Lincoln's staunchest allies. Lincoln ultimately decided that holding the fort, which would require reinforcing it, was the only workable option. Thus, on April 6, Lincoln informed the Governor of South Carolina that a ship with food but no ammunition would attempt to supply the Fort. Historian McPherson describes this win-win approach as "the first sign of the mastery that would mark Lincoln's presidency"; the Union would win if it could resupply and hold onto the Fort, and the South would be the aggressor if it opened fire on an unarmed ship supplying starving men.[137] An April 9 Confederate cabinet meeting resulted in President Davis's ordering General P. G. T. Beauregard to take the Fort before supplies could reach it.[138]
Maryland, Delaware, Missouri, and Kentucky were slave states whose people had divided loyalties to Northern and Southern businesses and family members. Some men enlisted in the Union Army and others in the Confederate Army.[144] West Virginia separated from Virginia and was admitted to the Union on June 20, 1863.[145]
In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state (see also: Missouri secession). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.[152]
Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status while maintaining slavery. During a brief invasion by Confederate forces in 1861, Confederate sympathizers organized a secession convention, formed the shadow Confederate Government of Kentucky, inaugurated a governor, and gained recognition from the Confederacy. Its jurisdiction extended only as far as Confederate battle lines in the Commonwealth, and it went into exile after October 1862.[153]
A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.[159]
The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book The American Civil War, British historian John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". In many cases, without geographic objectives, the only target for each side was the enemy's soldier.[160]
As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias.[161] The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 soldiers for one year or the duration, and that was answered in kind by the U.S. Congress.[162][163][164]
When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft.[167] Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their services conscripted.[168]
In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war.[169] At least 100,000 Southerners deserted, or about 10 percent; Southern desertion was high because, according to one historian writing in 1991, the highly localized Southern identity meant that many Southern men had little investment in the outcome of the war, with individual soldiers caring more about the fate of their local area than any grand ideal.[170] In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.[171]
From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. Some European observers at the time dismissed them as amateur and unprofessional,[172] but historian John Keegan concluded that each outmatched the French, Prussian, and Russian armies of the time, and without the Atlantic, would have threatened any of them with defeat.[173]
At the start of the Civil War, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their army. They were paid, but they were not allowed to perform any military duties.[174] The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.[175]
Mary Edwards Walker, the only woman ever to receive the Medal of Honor, served in the Union Army and was given the medal for her efforts to treat the wounded during the war. Her name was deleted from the Army Medal of Honor Roll in 1917 (along with over 900 other Medal of Honor recipients); however, it was restored in 1977.[178][179]
The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 sailors in 1865, with 671 vessels, having a tonnage of 510,396.[180][181] Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy.[182] Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland. The U.S. Navy eventually gained control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers. In the East, the Navy shelled Confederate forts and provided support for coastal army operations.[183]
The Civil War occurred during the early stages of the industrial revolution. Many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union's naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries.[184] Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union's ironclad warships, they were unsuccessful.[185]
In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.[186]
By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible, which called for blockading the Confederacy and slowly suffocating the South to surrender.[190] Lincoln adopted parts of the plan, but chose to prosecute a more active vision of war.[191] In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.[192]
The Confederates began the war short on military supplies and in desperate need of large quantities of arms which the agrarian South could not provide. Arms manufactures in the industrial North were restricted by an arms embargo, keeping shipments of arms from going to the South, and ending all existing and future contracts. The Confederacy subsequently looked to foreign sources for their enormous military needs and sought out financiers and companies like S. Isaac, Campbell & Company and the London Armoury Company in Britain, who acted as purchasing agents for the Confederacy, connecting them with Britain's many arms manufactures, and ultimately becoming the Confederacy's main source of arms.[193][194]
To get the arms safely to the Confederacy, British investors built small, fast, steam-driven blockade runners that traded arms and supplies brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were lightweight and designed for speed and could only carry a relatively small amount of cotton back to England.[195] When the Union Navy seized a blockade runner, the ship and cargo were condemned as a prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British, and they were released.[196]
The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies.[197] Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.[197]
Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well.[198] The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade, so they stopped calling at Confederate ports.[199]
Although the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring the British and French governments in as mediators.[202][203] The Union, under Lincoln and Secretary of State William H. Seward, worked to block this and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe turned to Egypt and India for cotton, which they found superior, hindering the South's recovery after the war.[204][205]
Lincoln's administration initially failed to appeal to European public opinion. At first, diplomats explained that the United States was not committed to the ending of slavery, and instead repeated legalistic arguments about the unconstitutionality of secession. Confederate representatives, on the other hand, started off much more successful, by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy.[206] The European aristocracy was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."[206] However, there was still a European public with liberal sensibilities, that the U.S. sought to appeal to by building connections with the international press. As early as 1861, many Union diplomats such as Carl Schurz realized emphasizing the war against slavery was the Union's most effective moral asset in the struggle for public opinion in Europe. Seward was concerned that an overly radical case for reunification would distress the European merchants with cotton interests; even so, Seward supported a widespread campaign of public diplomacy.[206]
War loomed in late 1861 between the U.S. and Britain over the Trent affair, which began when U.S. Navy personnel boarded the British ship Trent and seized two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two men.[208] Prince Albert had left his deathbed to issue diplomatic instructions to Lord Lyons during the Trent affair. His request was honored, and, as a result, the British response to the United States was toned down and helped avert the British becoming involved in the war.[209] In 1862, the British government considered mediating between the Union and Confederacy, though even such an offer would have risked war with the United States. British Prime Minister Lord Palmerston reportedly read Uncle Tom's Cabin three times when deciding on what his decision would be.[208]
The Union victory in the Battle of Antietam caused the British to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Realizing that Washington could not intervene in Mexico as long as the Confederacy controlled Texas, France invaded Mexico in 1861. Washington repeatedly protested France's violation of the Monroe Doctrine. Despite sympathy for the Confederacy, France's seizure of Mexico ultimately deterred it from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers and ensured that they would remain neutral.[210]
Russia supported the Union, largely because it believed that the U.S. served as a counterbalance to its geopolitical rival, the United Kingdom. In 1863, the Russian Navy's Baltic and Pacific fleets wintered in the American ports of New York and San Francisco, respectively.[211]
The Eastern theater refers to the military operations east of the Appalachian Mountains, including the states of Virginia, West Virginia, Maryland, and Pennsylvania, the District of Columbia, and the coastal fortifications and seaports of North Carolina.[citation needed]
Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26, 1861 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. The 1862 Union strategy called for simultaneous advances along four axes:[212]
The primary Confederate force in the Eastern theater was the Army of Northern Virginia. The Army originated as the (Confederate) Army of the Potomac, which was organized on June 20, 1861, from all operational forces in Northern Virginia. On July 20 and 21, the Army of the Shenandoah and forces from the District of Harpers Ferry were added. Units from the Army of the Northwest were merged into the Army of the Potomac between March 14 and May 17, 1862. The Army of the Potomac was renamed Army of Northern Virginia on March 14. The Army of the Peninsula was merged into it on April 12, 1862.
When Virginia declared its secession in April 1861, Robert E. Lee chose to follow his home state, despite his desire for the country to remain intact and an offer of a senior Union command.
Lee's biographer, Douglas S. Freeman, asserts that the army received its final name from Lee when he issued orders assuming command on June 1, 1862.[213] However, Freeman does admit that Lee corresponded with Brigadier General Joseph E. Johnston, his predecessor in army command, before that date and referred to Johnston's command as the Army of Northern Virginia. Part of the confusion results from the fact that Johnston commanded the Department of Northern Virginia (as of October 22, 1861) and the name Army of Northern Virginia can be seen as an informal consequence of its parent department's name. Jefferson Davis and Johnston did not adopt the name, but it is clear that the organization of units as of March 14 was the same organization that Lee received on June 1, and thus it is generally referred to today as the Army of Northern Virginia, even if that is correct only in retrospect.
On July 4 at Harper's Ferry, Colonel Thomas J. Jackson assigned Jeb Stuart to command all the cavalry companies of the Army of the Shenandoah. He eventually commanded the Army of Northern Virginia's cavalry.
In one of the first highly visible battles, in July 1861, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces led by Gen. P. G. T. Beauregard near Washington was repulsed at the First Battle of Bull Run (also known as First Manassas).
The Union had the upper hand at first, nearly pushing confederate forces holding a defensive position into a rout, but Confederate reinforcements under Joseph E. Johnston arrived from the Shenandoah Valley by railroad, and the course of the battle quickly changed. A brigade of Virginians under the relatively unknown brigadier general from the Virginia Military Institute, Thomas J. Jackson, stood its ground, which resulted in Jackson receiving his famous nickname, "Stonewall".
Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. McClellan's army reached the gates of Richmond in the Peninsula Campaign.[214][215][216]
Johnston halted McClellan's advance at the Battle of Seven Pines, but he was wounded in the battle, and Robert E. Lee assumed his position of command. General Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.[217]
The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South.[218] McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops.[citation needed]
Emboldened by Second Bull Run, the Confederacy made its first invasion of the North with the Maryland Campaign. General Lee led 45,000 troops of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history.[217][219] Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.[220]
When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg[221] on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights.[222] After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker.[223]
Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, his Chancellorsville Campaign proved ineffective and he was humiliated in the Battle of Chancellorsville in May 1863.[224] Chancellorsville is known as Lee's "perfect battle" because his risky decision to divide his army in the presence of a much larger enemy force resulted in a significant Confederate victory. Gen. Stonewall Jackson was shot in the arm by accidental friendly fire during the battle and subsequently died of complications.[225] Lee famously said: "He has lost his left arm, but I have lost my right arm."[226]
Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863).[228] This was the bloodiest battle of the war and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000).[229]
The Western theater refers to military operations between the Appalachian Mountains and the Mississippi River, including the states of Alabama, Georgia, Florida, Mississippi, North Carolina, Kentucky, South Carolina, and Tennessee, as well as parts of Louisiana.[230]
The primary Union forces in the Western theater were the Army of the Tennessee and the Army of the Cumberland, named for the two rivers, the Tennessee River and Cumberland River. After Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant.[231][citation needed]
The primary Confederate force in the Western theater was the Army of Tennessee. The army was formed on November 20, 1862, when General Braxton Bragg renamed the former Army of Mississippi. While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West.[230]
The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry (February 6, 1862) and Donelson (February 11 to 16, 1862), earning him the nickname of "Unconditional Surrender" Grant, by which the Union seized control of the Tennessee and Cumberland Rivers.[232] Nathan Bedford Forrest rallied nearly 4,000 Confederate troops and led them to escape across the Cumberland. Nashville and central Tennessee thus fell to the Union, leading to attrition of local food supplies and livestock and a breakdown in social organization.[citation needed]
Leonidas Polk's invasion of Columbus ended Kentucky's policy of neutrality and turned it against the Confederacy. Grant used river transport and Andrew Foote's gunboats of the Western Flotilla to threaten the Confederacy's "Gibraltar of the West" at Columbus, Kentucky. Although rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their gunboats, were forced to retreat and the Union took control of western Kentucky and opened Tennessee in March 1862.[233]
One of the early Union objectives in the war was the capture of the Mississippi River, to cut the Confederacy in half. The Mississippi River was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee.[236]
In April 1862, the Union Navy captured New Orleans.[236] "The key to the river was New Orleans, the South's largest port [and] greatest industrial center."[237] U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South.[238] which allowed Union forces to begin moving up the Mississippi. Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.[239]
Bragg's second invasion of Kentucky in the Confederate Heartland Offensive included initial successes such as Kirby Smith's triumph at the Battle of Richmond and the capture of the Kentucky capital of Frankfort on September 3, 1862.[240] However, the campaign ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville. Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of logistical support and lack of infantry recruits for the Confederacy in that state.[241]
Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee, the culmination of the Stones River Campaign.[242]
Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at the Battle of Vicksburg in July 1863, which cemented Union control of the Mississippi River and is considered one of the turning points of the war.[243]
The one clear Confederate victory in the West was the Battle of Chickamauga. After Rosecrans' successful Tullahoma Campaign, Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas.[citation needed]
Rosecrans retreated to Chattanooga, which Bragg then besieged in the Chattanooga Campaign. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga,[244] eventually causing Longstreet to abandon his Knoxville Campaign and driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy.[245]
The Trans-Mississippi theater refers to military operations west of the Mississippi River, encompassing most of Missouri, Arkansas, most of Louisiana, and Indian Territory (now Oklahoma). The Trans-Mississippi District was formed by the Confederate Army to better coordinate Ben McCulloch's command of troops in Arkansas and Louisiana, Sterling Price's Missouri State Guard, as well as the portion of Earl Van Dorn's command that included the Indian Territory and excluded the Army of the West. The Union's command was the Trans-Mississippi Division, or the Military Division of West Mississippi.[246]
The first battle of the Trans-Mississippi theater was the Battle of Wilson's Creek (August 1861). The Confederates were driven from Missouri early in the war as a result of the Battle of Pea Ridge.[248]
Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control.[249] Roving Confederate bands such as Quantrill's Raiders terrorized the countryside, striking both military installations and civilian settlements.[250] The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged. By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union but Lincoln took 70 percent of the vote for re-election.[251]
Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Battle of Glorieta Pass was the decisive battle of the New Mexico Campaign. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy and smaller numbers for the Union.[252] The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.[253]
After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union, in turn, did not directly engage him.[254] Its 1864 Red River Campaign to take Shreveport, Louisiana, was a failure and Texas remained in Confederate hands throughout the war.[255]
The Lower Seaboard theater refers to military and naval operations that occurred near the coastal areas of the Southeast (Alabama, Florida, Louisiana, Mississippi, South Carolina, and Texas) as well as the southern part of the Mississippi River (Port Hudson and south). Union Naval activities were dictated by the Anaconda Plan.[256]
One of the earliest battles of the war was fought at Port Royal Sound (November 1861), south of Charleston. Much of the war along the South Carolina coast concentrated on capturing Charleston. In attempting to capture Charleston, the Union military tried two approaches: by land over James or Morris Islands or through the harbor. However, the Confederates were able to drive back each Union attack. One of the most famous of the land attacks was the Second Battle of Fort Wagner, in which the 54th Massachusetts Infantry took part. The Union suffered a serious defeat in this battle, losing 1,515 soldiers while the Confederates lost only 174.[257] However, the 54th was hailed for its valor in that battle, which encouraged the general acceptance of the recruitment of African American soldiers into the Union Army, which reinforced the Union's numerical advantage.
Fort Pulaski on the Georgia coast was an early target for the Union navy. Following the capture of Port Royal, an expedition was organized with engineer troops under the command of Captain Quincy A. Gillmore, forcing a Confederate surrender. The Union army occupied the fort for the rest of the war after repairing it.[258]
In April 1862, a Union naval task force commanded by Commander David D. Porter attacked Forts Jackson and St. Philip, which guarded the river approach to New Orleans from the south. While part of the fleet bombarded the forts, other vessels forced a break in the obstructions in the river and enabled the rest of the fleet to steam upriver to the city. A Union army force commanded by Major General Benjamin Butler landed near the forts and forced their surrender. Butler's controversial command of New Orleans earned him the nickname "Beast".[259]
The following year, the Union Army of the Gulf commanded by Major General Nathaniel P. Banks laid siege to Port Hudson for nearly eight weeks, the longest siege in US military history. The Confederates attempted to defend with the Bayou Teche Campaign but surrendered after Vicksburg. These two surrenders gave the Union control over the entire Mississippi.[260]
Several small skirmishes were fought in Florida, but no major battles. The biggest was the Battle of Olustee in early 1864.[citation needed]
The Pacific Coast theater refers to military operations on the Pacific Ocean and in the states and Territories west of the Continental Divide.[261]
At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war.[262] This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end."[263] Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.[264]
Grant's army set out on the Overland Campaign intending to draw Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides and forced Lee's Confederates to fall back repeatedly.[265] At the Battle of Yellow Tavern, the Confederates lost Jeb Stuart.[266]
An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though, unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.[267]
Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. vice president and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war and included a charge by teenage VMI cadets. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.[268]
Leaving Atlanta, and his base of supplies, Sherman's army marched, with no destination set, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia, in December 1864. Sherman's army was followed by thousands of freed slaves; there were no major battles along the march. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee's army.[271]
Lee's army, thinned by desertion and casualties, was now much smaller than Grant's. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler's Creek.[272]
Initially, Lee did not intend to surrender but planned to regroup at Appomattox Station, where supplies were to be waiting and then continue the war. Grant chased Lee and got in front of him so that when Lee's army reached the village of Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at "Wilmer McLean's farmhouse, located less than 100 yards west of the county courthouse",[275] now known as the McLean House.[276] In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller. His men were paroled, and a chain of Confederate surrenders began.[277]
On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Confederate sympathizer. Lincoln died early the next morning. Lincoln's vice president, Andrew Johnson, was unharmed, because his would-be assassin, George Atzerodt, lost his nerve, so Johnson was immediately sworn in as president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them.[278] On April 26, 1865, the same day Boston Corbett killed Booth at a tobacco barn, General Joseph E. Johnston surrendered nearly 90,000 troops of the Army of Tennessee to Major General William Tecumseh Sherman at Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces. On May 4, all remaining Confederate forces in Alabama, Louisiana east of the Mississippi River, and Mississippi under Lieutenant General Richard Taylor surrendered.[279]
The Confederate president, Jefferson Davis, was captured at Irwinsville, Georgia on May 10, 1865.[280]
On May 13, 1865, the last land battle of the war was fought at the Battle of Palmito Ranch in Texas.[281][282][283]
On May 26, 1865, Confederate Lt. Gen. Simon B. Buckner, acting for General Edmund Kirby Smith, signed a military convention surrendering the Confederate trans-Mississippi Department forces.[284][285] This date is often cited by contemporaries and historians as the end date of the American Civil War.[1][2] On June 2, 1865, with most of his troops having already gone home, technically deserted, a reluctant Kirby Smith had little choice but to sign the official surrender document.[286][287] On June 23, 1865, Cherokee leader and Confederate Brig. Gen. Stand Watie became the last Confederate general to surrender his forces.[288][289]
On June 19, 1865, Union Maj. Gen. Gordon Granger announced General Order No. 3, bringing the Emancipation Proclamation into effect in Texas and freeing the last slaves of the Confederacy.[290] The anniversary of this date is now celebrated as Juneteenth.[291]
The naval portion of the war ended more slowly. It had begun on April 11, 1865, two days after Lee's surrender, when President Lincoln proclaimed that foreign nations had no further "claim or pretense" to deny equality of maritime rights and hospitalities to U.S. warships and, in effect, that rights extended to Confederate ships to use neutral ports as safe havens from U.S. warships should end.[292][293] Having no response to Lincoln's proclamation, President Andrew Johnson issued a similar proclamation dated May 10, 1865, more directly stating the premise that the war was almost at an end ("armed resistance...may be regarded as virtually at an end") and that insurgent cruisers still at sea and prepared to attack U.S. ships should not have rights to do so through use of safe foreign ports or waters and warned nations which continued to do so that their government vessels would be denied access to U.S. ports. He also "enjoined" U.S. officers to arrest the cruisers and their crews so "that they may be prevented from committing further depredations on commerce and that the persons on board of them may no longer enjoy impunity for their crimes".[294] England finally responded on June 6, 1865, by transmitting a June 2, 1865 letter from England's Foreign Secretary John Russell, 1st Earl Russell to the Lords of the Admiralty (United Kingdom) withdrawing rights to Confederate warships to enter British ports and waters but with exceptions for a limited time to allow a captain to enter a port to "divest his vessel of her warlike character" and for U.S. ships to be detained in British ports or waters to allow Confederate cruisers twenty-four hours to leave first.[295] U.S. Secretary of State William Seward welcomed the withdrawal of concessions to the Confederates but objected to the exceptions.[296] Finally, on October 18, 1865, Russell advised the Admiralty that the time specified in his June 2, 1865 message had elapsed and "all measures of a restrictive nature on vessels of war of the United States in British ports, harbors, and waters, are now to be considered as at an end".[297] Nonetheless, the final Confederate surrender was in Liverpool, England where James Iredell Waddell, the captain of the CSS Shenandoah, surrendered the cruiser to British authorities on November 6, 1865.[298]
Legally, the war did not end until August 20, 1866, when President Andrew Johnson issued a proclamation that declared "that the said insurrection is at an end and that peace, order, tranquillity, and civil authority now exist in and throughout the whole of the United States of America".[299][300][301]
The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich Southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second-class citizenship of the freedmen and their poverty.[302]
Historians have debated whether the Confederacy could have won the war. Most scholars, including James M. McPherson, argue that Confederate victory was at least possible.[303] McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, it would have more easily been able to hold out long enough to exhaust the Union.[304]
Confederates did not need to invade and hold enemy territory to win but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win.[304] Lincoln was not a military dictator and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by outlasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads, who had wanted a negotiated peace with the Confederate States of America.[305]
Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers.[317] The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Great Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95% effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.[318]
Historian Don Doyle has argued that the Union victory had a major impact on the course of world history.[319] The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:
The North's victory decisively proved the durability of democratic government. Confederate independence, on the other hand, would have established an American model for reactionary politics and race-based repression that would likely have cast an international shadow into the twentieth century and perhaps beyond."[320]
Scholars have debated what the effects of the war were on political and economic power in the South.[321] The prevailing view is that the southern planter elite retained its powerful position in the South.[321] However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.[321]
Based on 1860 census figures, 8 percent of all white men aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South.[325][326] About 56,000 soldiers died in prison camps during the War.[327] An estimated 60,000 soldiers lost limbs in the war.[328]
Of the 359,528 Union army dead, amounting to 15 percent of the over two million who served:[7]
In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).[8]
The United States National Park Service uses the following figures in its official tally of war losses:[3]
While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, superintendent of the 1870 census, used census and surgeon general data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 census's undercounting, it was later found that the census was only off by 6.5% and that the data Walker used would be roughly accurate.[13]
Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war.[323] This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.[citation needed]
Abolishing slavery was not a Union war goal from the outset, but it quickly became one.[21] Lincoln's initial claims were that preserving the Union was the central goal of the war.[332] In contrast, the South saw itself as fighting to preserve slavery.[21] While not all Southerners saw themselves as fighting for slavery, most of the officers and over a third of the rank and file in Lee's army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery.[333] However, as the war dragged on, and it became clear that slavery was central to the conflict, and that emancipation was (to quote from the Emancipation Proclamation) "a fit and necessary war measure for suppressing [the] rebellion," Lincoln and his cabinet made ending slavery a war goal, culminating in the Emancipation Proclamation.[21][334] Lincoln's decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans.[334] By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the Northern state of Ohio when they tried to resurrect anti-black sentiment.[335]
During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. Lincoln's fears of making slavery a war issue were based on a harsh reality: abolition did not enjoy wide support in the west, the territories, and the border states.[339][340] In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game."[340] Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of the total war needed to save the Union.[341]
Lincoln's moderate approach succeeded in inducing the border states to remain in the Union and War Democrats to support the Union. The border states (Kentucky, Missouri, Maryland, Delaware) and Union-controlled regions around New Orleans, Norfolk, and elsewhere, were not covered by the Emancipation Proclamation. Nor was Tennessee, which had come under Union control.[351] Missouri and Maryland abolished slavery on their own; Kentucky and Delaware did not.[352] Still, the proclamation did not enjoy universal support. It caused much unrest in what were then considered western states, where racist sentiments led to a great fear of abolition. There was some concern that the proclamation would lead to the secession of western states, and its issuance prompted the stationing of Union troops in Illinois in case of rebellion.[339]
Since the Emancipation Proclamation was based on the President's war powers, it applied only in territory held by Confederates at the time it was issued. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty.[353] The Emancipation Proclamation greatly reduced the Confederacy's hope of being recognized or otherwise aided by Britain or France.[354] By late 1864, Lincoln was playing a leading role in getting the House of Representatives to vote for the Thirteenth Amendment to the United States Constitution, which mandated the ending of chattel slavery.[355]
President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865 when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They overrode Johnson's vetoes of civil rights legislation, and the House impeached him, although the Senate did not convict him. In 1868 and 1872, the Republican candidate Ulysses S. Grant won the presidency. In 1872, the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They chose Horace Greeley to head a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed further reconstruction. The Compromise of 1877 closed with a national consensus, except perhaps on the part of former slaves, that the Civil War had finally ended.[360] With the withdrawal of federal troops, however, whites retook control of every Southern legislature, and the Jim Crow era of disenfranchisement and legal segregation was ushered in.[361]
The Civil War would have a huge impact on American politics in the years to come. Many veterans on both sides were subsequently elected to political office, including five U.S. Presidents: General Ulysses Grant, Rutherford B. Hayes, James Garfield, Benjamin Harrison, and William McKinley.[362]
The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books, and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war's aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war.[363] The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.[364]
Professional historians have paid much more attention to the causes of the war than to the war itself. Military history has largely developed outside academia, leading to a proliferation of studies by non-scholars who nevertheless are familiar with the primary sources and pay close attention to battles and campaigns and who write for the general public. Bruce Catton and Shelby Foote are among the best known.[365][366] Practically every major figure in the war, both North and South, has had a serious biographical study.[367]
The memory of the war in the white South crystallized in the myth of the "Lost Cause": that the Confederate cause was just and heroic. The myth shaped regional identity and race relations for generations.[368] Alan T. Nolan notes that the Lost Cause was expressly a rationalization, a cover-up to vindicate the name and fame of those in rebellion. Some claims revolve around the insignificance of slavery as a cause of the war; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful.[369] Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing black American progress to white man's reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.[370] The Lost Cause myth was formalized by Charles A. Beard and Mary R. Beard, whose The Rise of American Civilization (1927) spawned "Beardian historiography". The Beards downplayed slavery, abolitionism, and issues of morality. Though this interpretation was abandoned by the Beards in the 1940s, and by historians generally by the 1950s, Beardian themes still echo among Lost Cause writers.[371][372]
The American Civil War has been commemorated in many capacities, ranging from the reenactment of battles to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. These commemorations occurred in greater numbers on the 100th and 150th anniversaries of the war.[378]
Hollywood's take on the war has been especially influential in shaping public memory, as in such film classics as The Birth of a Nation (1915), Gone with the Wind (1939), and Lincoln (2012). Ken Burns's PBS television series The Civil War (1990) is especially well-remembered, though criticized for its historical inaccuracy.[379][380]
Numerous technological innovations during the Civil War had a great impact on 19th-century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war.[381] New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel.[382][383] It was also in this war that aerial warfare, in the form of reconnaissance balloons, was first used.[384] It saw the first action involving steam-powered ironclad warships in naval warfare history.[385] Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare. The war also saw the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.[386]
The Civil War is one of the most studied events in American history, and the collection of cultural works around it is enormous.[387] This section gives an abbreviated overview of the most notable works.
Garnet and Delany collaborated with other Black New Yorkers[6] to establish the African Civilization Society (ACS) in September 1858.[2] The group envisioned Black colonists emigrating from the US to Yorubaland in West Africa, where they would spread Christianity and Western economic and political systems to Indigenous Africans.[8][2] In particular, the group was interested in undermining the Atlantic slave trade and the agricultural slave economies of the US and Caribbean. They sought to do this by training Africans to produce cotton and molasses to compete with slave-produced products in European and American textile manufacturing and other global markets.[5][8] They also promoted self-determination among all people of the African diaspora,[8] advocating in their constitution the "civilization and evangelization of Africa, and the descendants of African ancestors in any portion of the earth, wherever dispersed."[2] In 1859, Delany led a group along the Niger River in West Africa to explore possible sites for a colony. This expedition developed into a separate project called the Niger Valley Exploring Party.[9]
The American Civil War, particularly after the Emancipation Proclamation (1863), disrupted colonization projects like the ACS and caused many of their supporters to focus on mobilizing military support for the Union Army in order to end slavery in the US. Garnet joined the army as a chaplain and Delany as the major of a Colored Troops unit.[17] Under the direction of Presbyterian clergyman and newly-selected ACS president George W. LeVere, the organization shifted its focus from emigration to educating formerly enslaved people, called freedmen.[2] In 1863, they broadened their mission to include helping and educating folks recently freed from slavery in the American South, Central America, South America, the British West Indies, and Africa. The new constitution, adopted January 2, 1864, outlined a mission of ending the slave trade and civilizing, uplifting, and Christianizing Africa and all members of the African diaspora.[18] Their programming reflected Black nationalist ideals: helping Black Americans educate themselves, lead their own education programs, and create their own political and social institutions.[8] Between 1863 and 1867, they were the only Black-led organization opening Freedmen's Schools in the South.[2]
[8]
During the war, Black activist and educator Junius C. Morel claimed: "The African Civilization Society is fully in the field". They are "holding meetings, collecting clothes, books, paper" to support freedmen and "they are making arrangements to send colored teachers just as fast as they can find the means and persons qualified to go".[19] By 1866, the group employed 69 teachers with a collective student body of 2,000 throughout the Northeastern United States.[8] By 1868, they employed 129 teachers and supported schools in Virginia, Maryland, North Carolina, South Carolina, Georgia, Mississippi, Louisiana, and Washington, D.C., with a collective student body of 8,000 and an annual cost of $53,700.[b] Among those teachers were Maria W. Stewart, Laura Cardozo, Hezekiah Hunter, and his wife, Lizzie Hunter.[20]
In 1781 he was employed in a tour of survey of the north-east coast of England. He was sent to North America as commanding engineer in the province of Quebec from 1785 to 1791, served under the Duke of York in Holland in 1793, and in 1794 went back to the Canadas, where he remained till 1804, when he went home to England.   
He was made a colonel in 1797, colonel-commandant of his corps in 1805, lieutenant-general in 1810, and general in 1821. He was appointed inspector-general of fortifications in 1811, and held the office until death. Some of his plans for fortifying Canada are preserved in the British Library and Canada.
Gother Mann, second son of Cornelius Mann and Elizabeth Gother, was born at Plumstead, Kent, on 21 December 1747. His father, a first cousin of Sir Horace Mann, went to the West Indies in 1760, and died at Saint Kitts on 9 December 1776. Gother was left under the charge of his uncle, Mr. Wilks of Faversham, Kent.[1] After graduating from the Royal Military Academy, Woolwich, he obtained a commission as practitioner engineer and ensign in the Royal Engineers on 27 February 1763. He was employed in England on the defences of Sheerness and of the River Medway until 1775, having been promoted sub-engineer and lieutenant on 1 April 1771.[1]
Towards the end of 1775 Mann was posted to Dominica in the West Indies and while there was promoted engineer extraordinary and captain lieutenant on 2 March 1777. In the morning of 7 September 1778, the French landed a strong force on the island, beginning a surprise invasion of Dominica. The British garrison, which was small, prepared for resistance, and Mann was named to command a detachment of the militia stationed at the new battery at Guey's Hill (now called King's Hill), which he prepared to defend. The council of the island pressured Lieutenant-governor William Stuart to capitulate; he yielded, and the island was surrendered without an effort being made to retain it.[2]
Mann made a report to the Board of Ordnance dated 14 September 1778, giving full details of the attack. He was only detained for a few months as a prisoner of war, and on 19 August 1779 he was appointed to the engineer staff of Great Britain, and reported on the defences of the east coast of England. He was stationed at Chatham under Colonel Hugh Debbeig. In 1781 he was selected by Lord Amherst and Sir Charles Frederick to accompany Colonel Braham, the chief engineer, on a tour of survey of the north-east coast of England, to consider what defences were desirable, as seven corporations had submitted petitions on the subject.[1]
In 1785 Mann, age thirty-eight, was sent to the Province of Quebec as commanding engineer,[1] succeeding William Twiss, and accompanied by fellow engineer Ralph Henry Bruyeres.[3] Promoted captain on 16 September 1785, he was employed in every part of the country in both civil and military duties, erecting fortifications, improving ports, and laying out townships, such as Toronto and Sorel.[1] In 1788 the governor, Guy Carleton, Lord Dorchester, had him make an extensive examination of military posts, harbours and navigable waterways from Kingston to St. Marys River, Sault Ste. Marie, in which Mann laments the ruination and ill placement of the bases: many of which were on the United States' side of the border established by the Treaty of Paris in 1783, though the British did not quit them till two years after the signing of the Jay Treaty in 1794.[3]
Mann returned to England in 1791. He went to the Netherlands in 1792,[3] and, joining the British army under Prince Frederick, Duke of York in June 1793, took part in the Flanders campaign. He was present at the siege of Valenciennes, which capitulated to the Coalition forces on 28 July, at the siege of Dunkirk from 24 August to 9 September, and at the battle of Hondschoote or Menin from 12 to 15 September. He was promoted lieutenant-colonel on 5 December 1793.[1]
On his return to England in April 1794 Mann was briefly employed under the master-general of the ordnance in London, before being sent back to Lower Canada, as commanding engineer,[1] to prepare defences at Quebec, since invasion from the United States then seemed a possibility.[3] He became colonel in the Army on 26 January 1797, and colonel in the Royal Engineers on 18 August the same year.[1] He wrote several reports in favour of establishing new and permanent defence systems at Quebec, and building more fortifications.[3] In 1800 he made a report on the St. Lawrence River canals and pointed out needed repairs and proposed certain improvements to the locks.[4] He became major-general on 25 September 1803.[1] In the same year he received permission to return to England where his wife and children had remained, and he embarked in the spring of 1804.[3]
From 1805 until 1811 Mann was employed either on particular service in Ireland or on various committees in London. On 13 July 1805 he was made a colonel-commandant of the Corps of Royal Engineers, on 25 July 1810 lieutenant-general, and on 19 July 1821 general. On 23 July 1811 he succeeded General Robert Morse as inspector-general of fortifications, an office he held until his death,[1] and in that capacity he continued to write on Canadian defences, such as the construction of the Citadelle of Quebec.[3] He was appointed president of the committee to examine cadets for commissions on 19 May 1828.[1]
Gother Mann was the senior officer in the engineers when he died, at age eighty-two, on 27 March 1830.[3] He was buried in Plumstead churchyard, where a tombstone was erected to his memory.[1]
His services in Canada were rewarded by a grant, on 22 July 1805, of 22,859 acres (9,251 hectares) of land in the township of Acton in Lower Canada. He also received while holding the office of inspector-general of fortifications the offer of a baronetcy, which, for financial considerations, he declined.[1]
On 1 March 1768, at St. Nicholas's, Rochester, Kent, Ensign Gother Mann married Ann, second daughter of Peter Wade of Rushford Manor, Eythorne, Kent, rector of Cooling, vicar of Boughton Monchelsea, and minor canon of Rochester Cathedral.[4] By her he had five sons and three daughters. Of the sons, Gother was in the Royal Artillery, Cornelius in the Royal Engineers, John in the 28th Regiment of Foot, and Frederick William in the Royal Marines, and afterwards in the Royal Staff Corps.[1] William, son of Cornelius, was an astronomer.[4]
Three coloured miniatures of Mann came into the possession of his descendants. One, taken when he had just entered the Corps of Royal Engineers in 1763, was once owned by his grandson, Major-general James Robert Mann, C.M.G., Royal Engineers, son of Major-general Cornelius Mann, Royal Engineers. This is reproduced in Porter's History of the Corps of Royal Engineers, 1889.[5][1]
The following plans by Mann are in the British Library: 
The following drawn plans by Mann, formerly in the War Office, are now among the records of the government of Canada: 
Early on 7 September 1778, French forces landed on the southeastern coast of the island.  They rapidly took over some of the island's defenses, and eventually gained control of the high ground overlooking the island's capital, Roseau.  Lieutenant Governor William Stuart then surrendered the remaining forces.  Dominica remained in French hands until the end of the war, when it was returned to British control.
Following the pivotal Battles of Saratoga in October 1777 and the ensuing surrender of British General John Burgoyne's army, France decided to openly enter the American War of Independence as an ally of the young United States of America.  France's objectives in entering the war included the recovery of territories that had been lost to Britain in the Seven Years' War.  One key territory that was of particular interest was the West Indies island of Dominica, which lay between French-held Martinique and Guadeloupe, and had been captured by Britain in 1761.  Recapture of the island would improve communication among the islands, and deny the use of Dominican ports to privateers who preyed on French shipping.[2]
The ratification of the Fifteenth Amendment to the United States Constitution in 1870 meant that African American men in California finally had the right to vote.[3] However, equal access to education remained a critical issue for communities throughout the state.[3][4] Black newspapers such as the Pacific Appeal stated, "The proper education of our children is paramount to all other considerations."[4][5] Although the earliest school laws in California did not specifically mention race, segregated schools existed as early as 1854.[4]
In 1866, the California state legislature enacted a revised law requiring local districts to establish separate schools for children of African, Mongolian, or Indian descent,[a] if petitioned by ten or more parents or guardians of those children.[6][2][7] Where such schools did not exist, children of color should be permitted to enroll in school with white children, unless the parent of a white child objected in writing.[6] In practice, this meant that black children in rural areas often did not receive an elementary education.[3] For many families, paying for private school was not an option.[4] According to a professional teachers' organization, as of 1874, one in four black children did not attend school.[3] Where separate schools existed, black children were relegated to second-rate school facilities, funded in part through additional taxes paid by black parents,[3] and often had to walk long distances to school.[4] According to The San Francisco Elevator newspaper, black schools were receiving only two-thirds of the annual appropriation paid per-student to white schools.[5] Furthermore, The Elevator charged that no fewer than 20 counties had misappropriated public funds for colored schools and diverted them to white schools instead.[5]
In April 1870, the California legislature passed a law requiring all children of African and American Indian descent to attend separate schools; local school districts could no longer admit them to white schools using their discretion.[8] The African American community in California responded in anger, and mobilized to try to repeal the new law.[8]
In November 1871, an education convention was held in Stockton, California, at the church of educator Jeremiah B. Sanderson.[3] One of the resolutions they adopted was to petition the legislature to remove the words "children of African descent" from the law, so they could "be allowed educational facilities with other children."[8] Following the convention, Senator Seldon J. Finney of San Mateo County took up the cause, and introduced a bill in the California legislature to end segregation of schools.[3] The bill failed, and in 1872, African American leaders decided to pursue a test case in court.[3][9]
In April 1872, African American leaders announced that they had selected San Francisco attorney and former state assemblyman John W. Dwinelle to represent the interests of the black community,[10][3] after interviewing several candidates.[8] In the summer of 1872, they organized meetings in San Francisco, Sacramento, Stockton, and Maryville, to raise money to pay for legal fees, and hired Dwinelle.[3][9]
In 1872, San Francisco had two "colored schools" located at opposite ends of the city, one of which was a small room rented by the Board of Education.[3] Meanwhile, according to The Appeal newspaper, white children had access to "43 or more splendidly built school houses in the city suited or adapted to every neighborhood".[3]
On July 23, 1872, The San Francisco Chronicle reported that several African American parents had attempted to enroll their children in four different schools, but had been denied, and that John W. Dwinelle was planning legal proceedings to overturn these decisions.[8]
One of those parents was Harriet Ward, who had tried to register her eleven-year-old daughter, Mary Frances, at Broadway Grammar School, a "regular" public school for white children in San Francisco,[1][11] which was the closest to their home.[12] Harriet and A. J. Ward had been residents of San Francisco since 1859.[12] Principal Noah Flood refused to allow their daughter to enroll, advising Mrs. Ward that she should take Mary Frances to one of the "colored schools" as required by the San Francisco Board of Education, since she was black.[1]
Dwinelle chose Mary Frances Ward as the plaintiff for the case, represented by A. J. Ward as her father and guardian.[8] In September 1872, Dwinelle applied for a writ of mandate, requesting the California Supreme Court to order Flood to admit Mary Frances Ward to the school.[8] He submitted a written affidavit from Harriet Ward stating that the only reason Noah Flood had denied their request to enroll her daughter was due to her race and the school board policy.[8] Attorneys for the school board argued that the colored schools provided an equal education, and claimed that Mary Frances Ward had not completed the prerequisites to enter the lowest grade of the Broadway Grammar School.[8]
On November 22, 1872, the Wards' attorney, John W. Dwinelle appealed to the California Supreme Court, arguing that the existing school code violated both the Fourteenth and Fifteenth Amendments, as well as the Civil Rights Act of 1866.[4] One of his initial arguments, that the exclusion of Mary Frances Ward constituted a "badge of servitude" in violation of the Thirteenth Amendment, was rejected outright by the court, which noted that exclusion of a black child from a white school was not the same as forced slavery.[11]
Dwinelle's main argument was based on the equal protection clause of the Fourteenth Amendment, which had been enacted in 1868.[12] Echoing Charles Sumner's argument in Roberts v. City of Boston in 1850, Dwinelle contended that forcing black schoolchildren to attend separate schools marked them as "inferior" in the eyes of the rest of society, denying them equal protection under the law.[12]
When the case reached the California high court, Harriet Ward stated in her petition on behalf of her family, "We are all of African descent...residents of San Francisco...[and] have a right to be received...at the school nearest their residence."[5] Meanwhile, Noah Flood maintained that he had merely been following state law, and that the black school would provide Mary Frances with an education "equal" to the white school.[11]
Eighteen months later, the California Supreme Court ruled against Ward, citing both the Slaughter-House Cases and Roberts v. City of Boston as precedent.[13] The majority opinion held that the privileges and immunities of the Fourteenth Amendment only applied to federal laws, while California public schools were run by the state and were therefore a privilege held through state citizenship rather than U.S. citizenship.[13]
With regard to equal protection, the court ruled in Ward v. Flood that the state was not violating any law, as long as it provided similar educational opportunities to all its citizens.[13] The California Supreme Court cited an 1849 ruling by the Supreme Judicial Court of Massachusetts that segregating schools by race was no different to separating students by age, gender, or special needs.[11] Quoting from Roberts v. City of Boston, the court maintained that having separate schools was not the reason for the "odious caste distinctions" confronting black children.[12]
At the same time, the California high court clearly affirmed that all children had a right to public education, which had to be provided to them equally under state law.[11] Further, the state supreme court ruled that excluding black children from white schools would not be allowed unless separate schools were available.[12] If not, they had the right to attend white schools.[12][14]
In the wake of the Ward v. Flood decision, African Americans in San Francisco protested segregation in education,[2] by boycotting black schools.[15] Local districts also came under increased financial strain during the Long Depression, which started in 1873.[2] Thus, although the California State Supreme Court had upheld segregation, in practice, most school districts in California opted to enroll black students rather than fund two separate school systems,[16] including the Board of Education in San Francisco, which opened its white schools to black children in 1875.[15] Furthermore, in communities where no separate school for black children existed, schools were now required to enroll black children.[3] For many African American children living in rural California, it was their first opportunity to go to elementary school.[3] Between 1875 and 1880, the absentee rate for black students dropped from 40 percent to 17 percent.[15]
By 1880, the California State Legislature temporarily removed all references to race in the school code, but exclusion and segregation continued in some districts including San Francisco, particularly against families of Chinese immigrants.[2] In 1885, the legislature made a further change to the school code, establishing "separate schools for 'children of Mongoloid or Chinese descent".[14]
In 1896, when the Supreme Court of the United States ruled in Plessy v. Ferguson that racial segregation did not violate the United States Constitution, as long as "separate but equal" public facilities were available, it cited several state court decisions, including Ward v. Flood.[11]
Heaton was born in 1936 in Shillong, India, where his parents were Christian missionaries.[2] His family later moved to England, where Heaton attended Marlborough College before completing a Bachelor of Arts at the University of Cambridge. He went on to study medicine at Cambridge, with clinical placements in London at Middlesex Hospital and Central Middlesex Hospital. He married Susan O'Connor, a fellow medical student, in 1961, the year they both graduated.[1]
Heaton was a medical registrar at the Royal Free Hospital and the Bristol Royal Infirmary. During a year-long research fellowship at Duke University Medical Center in the United States, he developed an interest in bile salts;[1] he later wrote a book titled Bile Salts in Health and Disease.[2] He subscribed to the "Cleave hypothesis", first posed by Peter Cleave, that a number of diseases were due to the consumption of excessive processed foods to which the human gastrointestinal tract had not adapted. This influenced his research on irritable bowel syndrome (IBS), and in 1978 he co-authored a paper positing that a diagnosis of IBS could be made on the basis of symptoms alone. His research led to the development of the Rome criteria for diagnosis of functional gastrointestinal disorders including IBS.[1]
From 1968, Heaton worked as a consultant at the Bristol Royal Infirmary and as a lecturer (and eventually reader) in medicine at the University of Bristol.[1][3] With Steve Lewis, he developed the Bristol stool scale, an illustrated scale of faecal consistency that reflects intestinal transit time and can be used to assess bowel health and is used internationally by clinicians and researchers.[1][2][3]
The Bristol stool scale is a diagnostic medical tool designed to classify the form of human faeces into seven categories.[3] It is used in both clinical and experimental fields.[4][5][6]
It was developed at the Bristol Royal Infirmary as a clinical assessment tool in 1997,[7] and is widely used as a research tool to evaluate the effectiveness of treatments for various diseases of the bowel, as well as a clinical communication aid;[8][9] including being part of the diagnostic triad for irritable bowel syndrome.[10]
Types 1 and 2 indicate constipation, with 3 and 4 being the ideal stools as they are easy to defecate while not containing excess liquid, 5 indicating lack of dietary fiber, and 6 and 7 indicate diarrhoea.[12]
In the initial study, in the population examined in this scale, the type 1 and 2 stools were more prevalent in females, while the type 5 and 6 stools were more prevalent in males; furthermore, 80% of subjects who reported rectal tenesmus (sensation of incomplete defecation) had type 7. These and other data have allowed the scale to be validated.[11] The initial research did not include a pictorial chart with this being developed at a later point.[7]
The Bristol stool scale is also very sensitive to changes in intestinal transit time caused by medications, such as antidiarrhoeal loperamide, senna, or anthraquinone with laxative effect.[13]
People with irritable bowel syndrome (IBS) typically report that they suffer with abdominal cramps and constipation.
In some patients, chronic constipation is interspersed with brief episodes of diarrhoea; while a minority of patients with IBS have only diarrhoea.
The presentation of symptoms is usually months or years and commonly patients consult different doctors, without great success, and doing various specialized investigations.
It notices a strong correlation of the reported symptoms with stress; indeed diarrhoeal discharges are associated with emotional phenomena.
IBS blood is present only if the disease is associated with haemorrhoids.[14]
Research conducted on irritable bowel syndrome in the 2000s,[15][16] faecal incontinence[17][18][19][20] and the gastrointestinal complications of HIV[21] have used the Bristol scale as a diagnostic tool easy to use, even in research which lasted for 77 months.[22]
Historically, this scale of assessment of the faeces has been recommended by the consensus group of Kaiser Permanente Medical Care Program (San Diego, California, US) for the collection of data on functional bowel disease (FBD).[14]
More recently, according to the latest revision of the Rome III Criteria, six clinical manifestations of IBS can be identified:[23][24][25][26][27]
These four identified subtypes correlate with the consistency of the stool, which can be determined by the Bristol stool scale.[14]
In 2007, the Mayo Clinic College of Medicine in Rochester, Minnesota, United States, reported a piece of epidemiological research conducted on a population of 4,196 people living in Olmsted County Minnesota, in which participants were asked to complete a questionnaire based on the Bristol stool scale.[29]
The research results (see table) indicate that about 1 in 5 people have a slow transit (type 1 and 2 stools), while 1 in 12 has an accelerated transit (type 5 and 6 stools). Moreover, the nature of the stool is affected by age, sex, body mass index, whether or not they had cholecystectomy and possible psychosomatic components (somatisation); there were no effects from factors such as smoking, alcohol, the level of education, a history of appendectomy or familiarity with gastrointestinal diseases, civil state, or the use of oral contraceptives.
Several investigations correlate the Bristol stool scale in response to medications or therapies, in fact, in one study was also used to titrate the dose more finely than one drug (colestyramine) in subjects with diarrhoea and faecal incontinence.[30]
In a randomised controlled study,[31] the scale is used to study the response to two laxatives: Macrogol (polyethylene glycol) and psyllium (Plantago psyllium and other species of the same genus) of 126 male and female patients for a period of 2 weeks of treatment; failing to show the most rapid response and increased efficiency of the former over the latter. In the study, they were measured as primary outcomes: the number weekly bowel movements, stool consistency according to the types of the Bristol stool scale, time to defecation, the overall effectiveness, the difficulty in defecating and stool consistency.[31]
From 2010, several studies have used the scale as a diagnostic tool validated for recognition and evaluation of response to various treatments, such as probiotics,[32][33] moxicombustion,[34] laxatives in the elderly,[35] preparing Ayurvedic poly-phytotherapy filed TLPL/AY,[36] psyllium,[37] mesalazine,[38] methylnaltrexone,[39] and oxycodone/naloxone,[40] or to assess the response to physical activity in athletes.[41]
Developed and proposed for the first time in England by Stephen Lewis and Ken Heaton at the University Department of Medicine, Bristol Royal Infirmary, it was suggested by the authors as a clinical assessment tool in 1997 in the Scandinavian Journal of Gastroenterology[13] after a previous prospective study, conducted in 1992 on a sample of the population (838 men and 1,059 women), had shown an unexpected prevalence of defecation disorders related to the shape and type of stool.[42] The authors of the former paper concluded that the form of the stool is a useful surrogate measure of colon transit time. That conclusion has since been challenged as having limited validity for Types 1 and 2;[43] however, it remains in use as a research tool to evaluate the effectiveness of treatments for various diseases of the bowel, as well as a clinical communication aid.[8][9]
In the 19th century, the paper developed a reputation for civic boosterism and opposition to labor unions, the latter of which led to the bombing of its headquarters in 1910. The paper's profile grew substantially in the 1960s under publisher Otis Chandler, who adopted a more national focus. In recent decades the paper's readership has declined, and it has been beset by a series of ownership changes, staff reductions, and other controversies. In January 2018, the paper's staff voted to unionize and finalized their first union contract on October 16, 2019.[8] The paper moved out of its historic downtown headquarters to a facility in El Segundo, near Los Angeles International Airport in July 2018.
The Times was first published on December 4, 1881, as the Los Angeles Daily Times, under the direction of Nathan Cole Jr. and Thomas Gardiner. It was first printed at the Mirror printing plant, owned by Jesse Yarnell and T. J. Caystile. Unable to pay the printing bill, Cole and Gardiner turned the paper over to the Mirror Company. In the meantime, S. J. Mathes had joined the firm, and it was at his insistence that the Times continued publication. In July 1882, Harrison Gray Otis moved from Santa Barbara to become the paper's editor.[9] Otis made the Times a financial success.
Historian Kevin Starr wrote that Otis was a businessman "capable of manipulating the entire apparatus of politics and public opinion for his own enrichment".[10] Otis's editorial policy was based on civic boosterism, extolling the virtues of Los Angeles and promoting its growth. Toward those ends, the paper supported efforts to expand the city's water supply by acquiring the rights to the water supply of the distant Owens Valley.[11]
The efforts of the Times to fight local unions led to the bombing of its headquarters on October 1, 1910, killing twenty-one people. Two union leaders, James and Joseph McNamara, were charged. The American Federation of Labor hired noted trial attorney Clarence Darrow to represent the brothers, who eventually pleaded guilty.
Otis fastened a bronze eagle on top of a high frieze of the new Times headquarters building designed by Gordon Kaufmann, proclaiming anew the credo written by his wife, Eliza: "Stand Fast, Stand Firm, Stand Sure, Stand True".[12][13]
After Otis's death in 1917, his son-in-law, Harry Chandler, took control as publisher of the Times. Harry Chandler was succeeded in 1944 by his son, Norman Chandler, who ran the paper during the rapid growth of post-war Los Angeles. Norman's wife, Dorothy Buffum Chandler, became active in civic affairs and led the effort to build the Los Angeles Music Center, whose main concert hall was named the Dorothy Chandler Pavilion in her honor. Family members are buried at the Hollywood Forever Cemetery near Paramount Studios. The site also includes a memorial to the Times Building bombing victims.
In 1935, the newspaper moved to a new, landmark Art Deco building, the Los Angeles Times Building, to which the newspaper would add other facilities until taking up the entire city block between Spring, Broadway, First and Second streets, which came to be known as Times Mirror Square and would house the paper until 2018. Harry Chandler, then the president and general manager of Times-Mirror Co., declared the Los Angeles Times Building a "monument to the progress of our city and Southern California".[14]
During the 1960s, the paper won four Pulitzer Prizes, more than its previous nine decades combined.
Writing in 2013 about the pattern of newspaper ownership by founding families, Times reporter Michael Hiltzik said that:
The first generations bought or founded their local paper for profits and also social and political influence (which often brought more profits). Their children enjoyed both profits and influence, but as the families grew larger, the later generations found that only one or two branches got the power, and everyone else got a share of the money. Eventually the coupon-clipping branches realized that they could make more money investing in something other than newspapers. Under their pressure the companies went public, or split apart, or disappeared. That's the pattern followed over more than a century by the Los Angeles Times under the Chandler family.[16]
The Los Angeles Times was beset in the first decade of the 21st century by a change in ownership, a bankruptcy, a rapid succession of editors, reductions in staff, decreases in paid circulation, the need to increase its Web presence, and a series of controversies.
The newspaper moved to a new headquarters building in El Segundo, near Los Angeles International Airport, in July 2018.[19][20][21][22]
In 2000, Times Mirror Company, publisher of the Los Angeles Times, was purchased by the Tribune Company of Chicago, Illinois, placing the paper in co-ownership with the then WB-affiliated (now CW-affiliated) KTLA, which Tribune acquired in 1985.[23]
In 2000, John Carroll, former editor of the Baltimore Sun, was brought in to restore the luster of the newspaper.[31] During his reign at the Times, he eliminated more than 200 jobs, but despite an operating profit margin of 20 percent, the Tribune executives were unsatisfied with returns, and by 2005 Carroll had left the newspaper. His successor, Dean Baquet, refused to impose the additional cutbacks mandated by the Tribune Company.
The paper's content and design style were overhauled several times in attempts to increase circulation. In 2000, a major change reorganized the news sections (related news was put closer together) and changed the "Local" section to the "California" section with more extensive coverage. Another major change in 2005 saw the Sunday "Opinion" section retitled the Sunday "Current" section, with a radical change in its presentation and featured columnists. There were regular cross-promotions with Tribune-owned television station KTLA to bring evening-news viewers into the Times fold.
The paper reported on July 3, 2008, that it planned to cut 250 jobs by Labor Day and reduce the number of published pages by 15 percent.[33][34] That included about 17 percent of the news staff, as part of the newly private media company's mandate to reduce costs. "We've tried to get ahead of all the change that's occurring in the business and get to an organization and size that will be sustainable", Hiller said.[35] In January 2009, the Times eliminated the separate California/Metro section, folding it into the front section of the newspaper. The Times also announced seventy job cuts in news and editorial or a 10 percent cut in payroll.[36]
In September 2015, Austin Beutner, the publisher and chief executive, was replaced by Timothy E. Ryan.[37] On October 5, 2015, the Poynter Institute reported that "'At least 50' editorial positions will be culled from the Los Angeles Times" through a buyout.[38] On this subject, the Los Angeles Times reported with foresight: "For the 'funemployed,' unemployment is welcome."[39] Nancy Cleeland,[40] who took O'Shea's buyout offer, did so because of "frustration with the paper's coverage of working people and organized labor"[41] (the beat that earned her Pulitzer).[40] She speculated that the paper's revenue shortfall could be reversed by expanding coverage of economic justice topics, which she believed were increasingly relevant to Southern California; she cited the paper's attempted hiring of a "celebrity justice reporter" as an example of the wrong approach.[41]
On August 21, 2017, Ross Levinsohn, then aged 54, was named publisher and CEO, replacing Davan Maharaj, who had been both publisher and editor.[42] On June 16, 2018, the same day the sale to Patrick Soon-Shiong closed, Norman Pearlstine was named executive editor.[30]
On May 3, 2021, the newspaper announced that it had selected Kevin Merida to be the new executive editor. Merida is a senior vice president at ESPN and leads The Undefeated, a site focused on sports, race, and culture. Previously, he was the first Black managing editor at The Washington Post.[43]
The Times has suffered continued decline in distribution. Reasons offered for the circulation drop included a price increase[44] and a rise in the proportion of readers preferring to read the online version instead of the print version.[45] Editor Jim O'Shea, in an internal memo announcing a May 2007, mostly voluntary, reduction in force, characterized the decrease in circulation as an "industry-wide problem" which the paper had to counter by "growing rapidly on-line", "break[ing] news on the Web and explain[ing] and analyz[ing] it in our newspaper."[46]
The Times closed its San Fernando Valley printing plant in early 2006, leaving press operations to the Olympic plant and to Orange County. Also that year the paper announced its circulation had fallen to 851,532, down 5.4 percent from 2005. The Times's loss of circulation was the largest of the top ten newspapers in the U.S.[47] Some observers believed that the drop was due to the retirement of circulation director Bert Tiffany. Still, others thought the decline was a side effect of a succession of short-lived editors who were appointed by publisher Mark Willes after publisher Otis Chandler relinquished day-to-day control in 1995.[15] Willes, the former president of General Mills, was criticized for his lack of understanding of the newspaper business, and was derisively referred to by reporters and editors as The Cereal Killer.[48] 
The Times's reported daily circulation in October 2010 was 600,449,[49] down from a peak of 1,225,189 daily and 1,514,096 Sunday in April 1990.[50][51]
In December 2006, a team of Times reporters delivered management with a critique of the paper's online news efforts known as the Spring Street Project.[52] The report, which condemned the Times as a "web-stupid" organization,[52] was followed by a shakeup in management of the paper's website,[53] www.latimes.com, and a rebuke of print staffers who were described as treating "change as a threat."[54]
On July 10, 2007, Times launched a local Metromix site targeting live entertainment for young adults.[55] A free weekly tabloid print edition of Metromix Los Angeles followed in February 2008; the publication was the newspaper's first stand-alone print weekly.[56] In 2009, the Times shut down Metromix and replaced it with Brand X, a blog site and free weekly tabloid targeting young, social networking readers.[57] Brand X launched in March 2009; the Brand X tabloid ceased publication in June 2011 and the website was shut down the following month.[58]
In May 2018, the Times blocked access to its online edition from most of Europe because of the European Union's General Data Protection Regulation.[59][60]
It was revealed in 1999 that a revenue-sharing arrangement was in place between the Times and Staples Center in the preparation of a 168-page magazine about the opening of the sports arena. The magazine's editors and writers were not informed of the agreement, which breached the Chinese wall that traditionally has separated advertising from journalistic functions at American newspapers. Publisher Mark Willes also had not prevented advertisers from pressuring reporters in other sections of the newspaper to write stories favorable to their point of view.[61]
Michael Kinsley was hired as the Opinion and Editorial (op-ed) Editor in April 2004 to help improve the quality of the opinion pieces. His role was controversial, for he forced writers to take a more decisive stance on issues. In 2005, he created a Wikitorial, the first Wiki by a major news organization. Although it failed, readers could combine forces to produce their own editorial pieces. It was shut down after being besieged with inappropriate material. He resigned later that year.[62]
The Times drew fire for a last-minute story before the 2003 California recall election alleging that gubernatorial candidate Arnold Schwarzenegger groped scores of women during his movie career. Columnist Jill Stewart wrote on the American Reporter website that the Times did not do a story on allegations that former Governor Gray Davis had verbally and physically abused women in his office, and that the Schwarzenegger story relied on a number of anonymous sources. Further, she said, four of the six alleged victims were not named. She also said that in the case of the Davis allegations, the Times decided against printing the Davis story because of its reliance on anonymous sources.[63][64] The American Society of Newspaper Editors said that the Times lost more than 10,000 subscribers because of the negative publicity surrounding the Schwarzenegger article.[65]
The Times also came under controversy for its decision to drop the weekday edition of the Garfield comic strip in 2005, in favor of a hipper comic strip Brevity, while retaining it in the Sunday edition. Garfield was dropped altogether shortly thereafter.[67]
Following the Republican Party's defeat in the 2006 mid-term elections, an Opinion piece by Joshua Muravchik, a leading neoconservative and a resident scholar at the conservative American Enterprise Institute, published on November 19, 2006, was titled 'Bomb Iran'. The article shocked some readers, with its hawkish comments in support of more unilateral action by the United States, this time against Iran.[68]
In November 2017, Walt Disney Studios blacklisted the Times from attending press screenings of its films, in retaliation for September 2017 reportage by the paper on Disney's political influence in the Anaheim area. The company considered the coverage to be "biased and inaccurate". As a sign of condemnation and solidarity, a number of major publications and writers, including The New York Times,  Boston Globe critic Ty Burr, Washington Post blogger Alyssa Rosenberg, and the websites The A.V. Club and Flavorwire, announced that they would boycott press screenings of future Disney films. The National Society of Film Critics, Los Angeles Film Critics Association, New York Film Critics Circle, and Boston Society of Film Critics jointly announced that Disney's films would be ineligible for their respective year-end awards unless the decision was reversed, condemning the decision as being "antithetical to the principles of a free press and [setting] a dangerous precedent in a time of already heightened hostility towards journalists". On November 7, 2017, Disney reversed its decision, stating that the company "had productive discussions with the newly installed leadership at the Los Angeles Times regarding our specific concerns".[71][72][73]
Through 2014 the Times had won 41 Pulitzer Prizes, including four in editorial cartooning, and one each in spot news reporting for the 1965 Watts Riots and the 1992 Los Angeles riots.[74]
In the 19th century, the chief competition to the Times was the Los Angeles Herald, followed by the smaller Los Angeles Tribune. In December 1903, newspaper magnate William Randolph Hearst began publishing the Los Angeles Examiner as a direct morning competitor to the Times.[83] In the 20th century, the Los Angeles Express was an afternoon competitor, as was Manchester Boddy's Los Angeles Daily News, a Democratic newspaper.[84]
By the mid-1940s, the Times was the leading newspaper in terms of circulation in the Los Angeles metropolitan area. In 1948, it launched the Los Angeles Mirror, an afternoon tabloid, to compete with both the Daily News and the merged Herald-Express. In 1954, the Mirror absorbed the Daily News. The combined paper, the Mirror-News, ceased publication in 1962, when the Hearst afternoon Herald-Express and the morning Los Angeles Examiner merged to become the Herald-Examiner.[85] The Herald-Examiner published its last number in 1989. In 2014, the Los Angeles Register, published by Freedom Communications, then-parent company of the Orange County Register was launched as a daily newspaper to compete with the Times. By late September of the same year, the Los Angeles Register was folded.[86][87]
The Midwinter Number drew acclamations from other newspapers, including this one from The Kansas City Star in 1923:
In 1948 the Midwinter Edition, as it was then called, had grown to "7 big picture magazines in beautiful rotogravure reproduction."[93] The last mention of the Midwinter Edition was in a Times advertisement on January 10, 1954.[94]
Between 1891 and 1895, the Times also issued a similar Midsummer Number, the first one with the theme "The Land and Its Fruits".[95] Because of its issue date in September, the edition was in 1891 called the Midsummer Harvest Number.[96]
In 1903, the Pacific Wireless Telegraph Company established a radiotelegraph link between the California mainland and Santa Catalina Island. In the summer of that year, the Times made use of this link to establish a local daily paper, based in Avalon, called The Wireless, which featured local news plus excerpts which had been transmitted via Morse code from the parent paper.[98] However, this effort apparently survived for only a little more than one year.[99]
In the 1990s, the Times published various editions catering to far-flung areas. Editions included those from the San Fernando Valley, Ventura County, Inland Empire, Orange County, San Diego County & a "National Edition" that was distributed to Washington, D.C., and the San Francisco Bay Area. The National Edition was closed in December 2004.
Some of these editions[quantify] were succeeded by Our Times, a group of community supplements included in editions of the regular Los Angeles Metro newspaper.[citation needed]
One of the Times' features was "Column One", a feature that appeared daily on the front page to the left-hand side. Established in September 1968, it was a place for the weird and the interesting; in the How Far Can a Piano Fly? (a compilation of Column One stories) introduction, Patt Morrison wrote that the column's purpose was to elicit a "Gee, that's interesting, I didn't know that" type of reaction.
The Times also embarked on a number of investigative journalism pieces. A series in December 2004 on the King/Drew Medical Center in Los Angeles led to a Pulitzer Prize and a more thorough coverage of the hospital's troubled history. Lopez wrote a five-part series on the civic and humanitarian disgrace of Los Angeles' Skid Row, which became the focus of a 2009 motion picture, The Soloist. It also won 62 awards at the SND[clarification needed] awards.
From 1967 to 1972, the Times produced a Sunday supplement called West magazine. West was recognized for its art design, which was directed by Mike Salisbury (who later became art director of Rolling Stone magazine).[104] From 2000 to 2012, the Times published the Los Angeles Times Magazine, which started as a weekly and then became a monthly supplement. The magazine focused on stories and photos of people, places, style, and other cultural affairs occurring in Los Angeles and its surrounding cities and communities. Since 2014, The California Sunday Magazine has been included in the Sunday L.A. Times edition.
In 1996, the Times started the annual Los Angeles Times Festival of Books, in association with the University of California, Los Angeles. It has panel discussions, exhibits, and stages during two days at the end of April each year.[105] In 2011, the Festival of Books was moved to the University of Southern California.[106]
Since 1980, the Times has awarded annual book prizes. The categories are now biography, current interest, fiction, first fiction, history, mystery/thriller, poetry, science and technology, and young adult fiction. In addition, the Robert Kirsch Award is presented annually to a living author with a substantial connection to the American West whose contribution to American letters deserves special recognition".[107]
From 1957 to 1987, the Times sponsored the Los Angeles Times Grand Prix that was held over at the Riverside International Raceway in Moreno Valley, California.
The Times Mirror Corporation has also owned a number of book publishers over the years, including New American Library and C.V. Mosby, as well as Harry N. Abrams, Matthew Bender, and Jeppesen.[108]
In 1967, Times Mirror acquired C.V. Mosby Company, a professional publisher and merged it over the years with several other professional publishers including Resource Application, Inc., Year Book Medical Publishers, Wolfe Publishing Ltd., PSG Publishing Company, B.C. Decker, Inc., among others. Eventually in 1998 Mosby was sold to Harcourt Brace & Company to form the Elsevier Health Sciences group.[110]
The Times-Mirror Company was a founding owner of television station KTTV in Los Angeles, which opened in January 1949. It became that station's sole owner in 1951, after re-acquiring the minority shares it had sold to CBS in 1948. Times-Mirror also purchased a former motion picture studio, Nassour Studios, in Hollywood in 1950, which was then used to consolidate KTTV's operations. Later to be known as Metromedia Square, the studio was sold along with KTTV to Metromedia in 1963.
After a seven-year hiatus from the medium, the firm reactivated Times-Mirror Broadcasting Company with its 1970 purchase of the Dallas Times Herald and its radio and television stations, KRLD-AM-FM-TV in Dallas.[111] The Federal Communications Commission granted an exemption of its cross-ownership policy and allowed Times-Mirror to retain the newspaper and the television outlet, which was renamed KDFW-TV.
Times-Mirror Broadcasting later acquired KTBC-TV in Austin, Texas in 1973;[112] and in 1980 purchased a group of stations owned by Newhouse Newspapers: WAPI-TV (now WVTM-TV) in Birmingham, Alabama; KTVI in St. Louis; WSYR-TV (now WSTM-TV) in Syracuse, New York and its satellite station WSYE-TV (now WETM-TV) in Elmira, New York; and WTPA-TV (now WHTM-TV) in Harrisburg, Pennsylvania.[113]  The company also entered the field of cable television, servicing the Phoenix and San Diego areas, amongst others. They were originally titled Times-Mirror Cable, and were later renamed to Dimension Cable Television. Similarly, they also attempted to enter the pay-TV market, with the Spotlight movie network; it wasn't successful and was quickly shut down. The cable systems were sold in the mid-1990s to Cox Communications.
Times-Mirror also pared its station group down, selling off the Syracuse, Elmira and Harrisburg properties in 1986.[114] The remaining four outlets were packaged to a new upstart holding company, Argyle Television, in 1993.[115] These stations were acquired by New World Communications shortly thereafter and became key components in a sweeping shift of network-station affiliations which occurred between 1994 and 1995.
* From 1985 to 1990: Pulitzer Prize for General News Reporting; From 1991 to 1997: Pulitzer Prize for Spot News Reporting; From 1998 to present: Pulitzer Prize for Breaking News Reporting
The Chicago Tribune is a daily newspaper based in Chicago, Illinois, United States, owned by Tribune Publishing. Founded in 1847, and formerly self-styled as the "World's Greatest Newspaper" (a slogan for which WGN radio and television are named), it remains the most-read daily newspaper in the Chicago metropolitan area and the Great Lakes region. In 2017, it had the sixth-highest circulation of any American newspaper.[2]
In the 1850s, under Joseph Medill, the Chicago Tribune became closely associated with the Illinois politician[3] Abraham Lincoln, and the Republican Party's progressive wing. In the 20th century, under Medill's grandson Robert R. McCormick, it achieved a reputation as a crusading paper with a decidedly more American-conservative anti-New Deal outlook, and its writing reached other markets through family and corporate relationships at the New York Daily News and the Washington Times-Herald. The 1960s saw its corporate parent owner, Tribune Company, reach into new markets. In 2008, for the first time in its over-a-century-and-a-half history, its editorial page endorsed a Democrat, Illinoisan Barack Obama, for U.S. president.[4]
Originally published solely as a broadsheet, the Tribune announced on January 13, 2009, that it would continue publishing as a broadsheet for home delivery, but would publish in tabloid format for newsstand, news box, and commuter station sales.[5] This change, however, proved to be unpopular with readers, and in August 2011, the Tribune discontinued the tabloid edition, returning to its established broadsheet format through all distribution channels.[6]
The Tribune's masthead displays the American flag, in reference to the paper's former motto, "An American Paper for Americans". The motto is no longer displayed on the masthead, where it was placed below the flag.
The Tribune was owned by parent company Tribune Publishing. In May 2021, Tribune Publishing was acquired by Alden Global Capital, which operates its media properties through Digital First Media.[7][8][9][10][11][excessive citations]
The Tribune was founded by James Kelly, John E. Wheeler, and Joseph K. C. Forrest, publishing the first edition on June 10, 1847. Numerous changes in ownership and editorship took place over the next eight years. Initially, the Tribune was not politically affiliated, but tended to support either the Whig or Free Soil parties against the Democrats in elections.[12] By late 1853, it was frequently running xenophobic editorials that criticized foreigners and Roman Catholics.[13] About this time it also became a strong proponent of temperance.[14] However nativist its editorials may have been, it was not until February 10, 1855, that the Tribune formally affiliated itself with the nativist American or Know Nothing party, whose candidate Levi Boone was elected Mayor of Chicago the following month.[15]
By about 1854, part-owner Capt. J. D. Webster, later General Webster and chief of staff at the Battle of Shiloh, and Dr. Charles H. Ray of Galena, Illinois, through Horace Greeley, convinced Joseph Medill of Cleveland's Leader to become managing editor.[16][17][18] Ray became editor-in-chief, Medill became the managing editor, and Alfred Cowles, Sr., brother of Edwin Cowles, initially was the bookkeeper. Each purchased one third of the Tribune.[19][20] Under their leadership, the Tribune distanced itself from the Know Nothings, and became the main Chicago organ of the Republican Party.[21] However, the paper continued to print anti-Catholic and anti-Irish editorials, in the wake of the massive Famine immigration from Ireland.[22]
The Tribune absorbed three other Chicago publications under the new editors: the Free West in 1855, the Democratic Press of William Bross in 1858, and the Chicago Democrat in 1861, whose editor, John Wentworth, left his position when elected as Mayor of Chicago. Between 1858 and 1860, the paper was known as the Chicago Press & Tribune. On October 25, 1860, it became the Chicago Daily Tribune.[23] Before and during the American Civil War, the new editors strongly supported Abraham Lincoln, whom Medill helped secure the presidency in 1860, and pushed an abolitionist agenda.[citation needed] The paper remained a force in Republican politics for years afterwards.[citation needed]
In 1861, the Tribune published new lyrics by William W. Patton for the song "John Brown's Body". These rivaled the lyrics published two months later by Julia Ward Howe. Medill served as mayor of Chicago for one term after the Great Chicago Fire of 1871.[citation needed]
Under the 20th-century editorship of Colonel Robert R. McCormick, who took control in the 1920s, the paper was strongly isolationist and aligned with the Old Right in its coverage of political news and social trends. It used the motto "The American Paper for Americans". From the 1930s to the 1950s, it excoriated the Democrats and the New Deal of Franklin D. Roosevelt, was resolutely disdainful of the British and French, and greatly enthusiastic for Chiang Kai-shek and Sen. Joseph McCarthy.
When McCormick assumed the position of co-editor (with his cousin Joseph Medill Patterson) in 1910, the Tribune was the third-best-selling paper among Chicago's eight dailies, with a circulation of only 188,000.[24] The young cousins added features such as advice columns and homegrown comic strips such as Little Orphan Annie and Moon Mullins. They promoted political "crusades", with their first success coming with the ouster of the Republican political boss of Illinois, Sen. William Lorimer.[24] At the same time, the Tribune competed with the Hearst paper, the Chicago Examiner, in a circulation war. By 1914, the cousins succeeded in forcing out Managing Editor William Keeley. By 1918, the Examiner was forced to merge with the Chicago Herald.
In 1919, Patterson left the Tribune and moved to New York to launch his own newspaper, the New York Daily News.[24] In a renewed circulation war with Hearst's Herald-Examiner, McCormick and Hearst ran rival lotteries in 1922. The Tribune won the battle, adding 250,000 readers to its ranks. Also in 1922, the Chicago Tribune hosted an international design competition for its new headquarters, the Tribune Tower. The competition worked brilliantly as a publicity stunt, and more than 260 entries were received. The winner was a neo-Gothic design by New York architects John Mead Howells and Raymond Hood.
The newspaper sponsored a pioneering attempt at Arctic aviation in 1929, an attempted round-trip to Europe across Greenland and Iceland in a Sikorsky amphibious aircraft.[25] But, the aircraft was destroyed by ice on July 15, 1929, near Ungava Bay at the tip of Labrador, Canada. The crew were rescued by the Canadian science ship CSS Acadia.[26]
The Tribune's legendary sports editor Arch Ward created the Major League Baseball All-Star Game in 1933 as part of the city's Century of Progress exposition.
From 1940 to 1943, the paper supplemented its comic strip offerings with The Chicago Tribune Comic Book, responding to the new success of comic books. At the same time, it launched the more successful and longer-lasting The Spirit Section, which was also an attempt by newspapers to compete with the new medium.[27]
Under McCormick's stewardship, the Tribune was a champion of modified spelling for simplicity (such as spelling "although" as "altho").[28][29] McCormick, a vigorous campaigner for the Republican Party, died in 1955, just four days before Democratic boss Richard J. Daley was elected mayor for the first time.
One of the great scoops in Tribune history came when it obtained the text of the Treaty of Versailles in June 1919. Another was its revelation of United States war plans on the eve of the Pearl Harbor attack. The Tribune's June 7, 1942, front page announcement that the United States had broken Japan's naval code was the revelation by the paper of a closely guarded military secret.[30] The story revealing that Americans broke the enemy naval codes was not cleared by censors, and had U.S. President Franklin D. Roosevelt so enraged that he considered shutting down the Tribune.[31][32][33][34]
The paper is well known for a mistake it made during the 1948 presidential election. At that time, much of its composing room staff was on strike. The early returns led editors to believe (along with many in the country) that the Republican candidate Thomas Dewey would win. An early edition of the next day's paper carried the headline "Dewey Defeats Truman", turning the paper into a collector's item. Democrat Harry S. Truman won and proudly brandished the newspaper in a famous picture taken at St. Louis Union Station. Beneath the headline was a false article, written by Arthur Sears Henning, which purported to describe West Coast results although written before East Coast election returns were available.
A week later, after studying the transcripts, the paper's editorial board observed that "the high dedication to grand principles that Americans have a right to expect from a President is missing from the transcript record." The Tribune's editors concluded that "nobody of sound mind can read [the transcripts] and continue to think that Mr. Nixon has upheld the standards and dignity of the Presidency," and called for Nixon's resignation. The Tribune call for Nixon to resign made news, reflecting not only the change in the type of conservatism practiced by the paper, but as a watershed event in terms of Nixon's hopes for survival in office. The White House reportedly perceived the Tribune's editorial as a loss of a long-time supporter and as a blow to Nixon's hopes to weather the scandal.
In January 1977, Tribune columnist Will Leonard died at age 64.[39]
In March 1978, the Tribune announced that it hired columnist Bob Greene from the Chicago Sun-Times.[40]
On December 7, 1975, Kirkpatrick announced in a column on the editorial page that Rick Soll, a "young and talented columnist" for the paper, whose work had "won a following among many Tribune readers over the last two years", had resigned from the paper. He had acknowledged that a November 23, 1975 column he wrote contained verbatim passages written by another columnist in 1967 and later published in a collection. Kirkpatrick did not identify the columnist. The passages in question, Kirkpatrick wrote, were from a notebook where Soll regularly entered words, phrases and bits of conversation which he had wished to remember. The paper initially suspended Soll for a month without pay. Kirkpatrick wrote that further evidence was revealed came out that another of Soll's columns contained information which he knew was false. At that point, Tribune editors decided to accept the resignation offered by Soll when the internal investigation began.[41]
After leaving, Soll married Pam Zekman, a Chicago newspaper (and future TV) reporter. He worked for the short-lived[42][43] Chicago Times magazine,[44] by Small Newspaper Group Inc. of Kankakee, Illinois,[45] in the late 1980s.
Soll was born in 1946, in Chicago, to Marjorie and Jules Soll. Soll graduated from New Trier High School, received a Bachelor of Arts in 1968 from Colgate University, and a Master's Degree from Medill School of Journalism, Northwestern University in 1970.[46][47]
Jack Fuller served as the Tribune's editor from 1989 until 1993, when he became the president and chief executive officer of the Chicago Tribune. Howard Tyner served as the Tribune's editor from 1993 until 2001, when he was promoted to vice president/editorial for Tribune Publishing.
The Tribune won 11 Pulitzer prizes during the 1980s and 1990s.[36] Editorial cartoonist Dick Locher won the award in 1983, and editorial cartoonist Jeff MacNelly won one in 1985. Then, future editor Jack Fuller won a Pulitzer for editorial writing in 1986. In 1987, reporters Jeff Lyon and Peter Gorner won a Pulitzer for explanatory reporting, and in 1988, Dean Baquet, William Gaines and Ann Marie Lipinski won a Pulitzer for investigative reporting. In 1989, Lois Wille won a Pulitzer for editorial writing and Clarence Page snagged the award for commentary. In 1994, Ron Kotulak won a Pulitzer for explanatory journalism, while R. Bruce Dold won it for editorial writing. In 1998, reporter Paul Salopek won a Pulitzer for explanatory writing, and in 1999, architecture critic Blair Kamin won it for criticism.[36]
In September 1981, baseball writer Jerome Holtzman was hired by the Tribune after a 38-year career at the Sun-Times.
In September 1982, the Chicago Tribune opened a new $180 million printing facility, Freedom Center.[48]
In November 1982, Tribune managing editor William H. "Bill" Jones, who had won a Pulitzer Prize in 1971, died at age 43 of cardiac arrest as a result of complications from a long battle with leukemia.[49]
In May 1983, Tribune columnist Aaron Gold died at age 45 of complications from leukemia.[50] Gold had coauthored the Tribune's "Inc." column with Michael Sneed and prior to that had written the paper's "Tower Ticker" column.
The Tribune scored a coup in 1984 when it hired popular columnist Mike Royko away from the rival Sun-Times.[51]
In February 1988, Tribune foreign correspondent Jonathan Broder resigned after a February 22, 1988, Tribune article written by Broder contained a number of sentences and phrases taken, without attribution, from a column written by another writer, Joel Greenberg, that had been published 10 days earlier in The Jerusalem Post.[55][56]
In August 1988, Chicago Tribune reporter Michael Coakley died at age 41 of complications from AIDS.[57]
In an unusual move at that time, the Tribune in October 1993 fired its longtime military-affairs writer, retired-Marine David Evans, with the public position that the post of military affairs was being dropped in favor of having a national security writer.[60]
In December 1993, the Tribune's longtime Washington, D.C. bureau chief, Nicholas Horrock, was removed from his post after he chose not to attend a meeting that editor Howard Tyner requested of him in Chicago.[61] Horrock, who shortly thereafter left the paper, was replaced by James Warren, who attracted new attention to the Tribune's D.C. bureau through his continued attacks on celebrity broadcast journalists in Washington.
Also in December 1993, the Tribune hired Margaret Holt from the South Florida Sun-Sentinel as its assistant managing editor for sports, making her the first female to head a sports department at any of the nation's 10 largest newspapers.[62] In mid-1995, Holt was replaced as sports editor by Tim Franklin and shifted to a newly created job, customer service editor.[63]
In 1994, reporter Brenda You was fired by the Tribune after free-lancing for supermarket tabloid newspapers and lending them photographs from the Tribune's photo library.[40] You later worked for the National Enquirer and as a producer for The Jerry Springer Show before committing suicide in November 2005.[64]
In April 1994, the Tribune's new television critic, Ken Parish Perkins, wrote an article about then-WFLD morning news anchor Bob Sirott in which Perkins quoted Sirott as making a statement that Sirott later denied making. Sirott criticized Perkins on the air, and the Tribune later printed a correction acknowledging that Sirott had never made that statement.[65] Eight months later, Perkins stepped down as TV critic, and he left the paper shortly thereafter.[66]
In December 1995, the alternative newsweekly Newcity published a first-person article by the pseudonymous Clara Hamon (a name mentioned in the play The Front Page) but quickly identified by Tribune reporters as that of former Tribune reporter Mary Hill that heavily criticized the paper's one-year residency program. The program brought young journalists in and out of the paper for one-year stints, seldom resulting in a full-time job. Hill, who wrote for the paper from 1992 until 1993, acknowledged to the Chicago Reader that she had written the diatribe originally for the Internet, and that the piece eventually was edited for Newcity.[67]
In 1997, the Tribune celebrated its 150th anniversary in part by tapping longtime reporter Stevenson Swanson to edit the book Chicago Days: 150 Defining Moments in the Life of a Great City.
On April 29, 1997, popular columnist Mike Royko died of a brain aneurysm. On September 2, 1997, the Tribune promoted longtime City Hall reporter John Kass to take Royko's place as the paper's principal Page Two news columnist.[68]
On June 1, 1997, the Tribune published what ended up becoming a very popular column by Mary Schmich called "Advice, like youth, probably just wasted on the young", otherwise known as "Wear Sunscreen" or the "Sunscreen Speech". The most popular and well-known form of the essay is the successful music single released in 1999, accredited to Baz Luhrmann.
On June 6, 1999, the Tribune published a first-person travel article from freelance writer Gaby Plattner that described a supposed incident in which a pilot for Air Zimbabwe who was flying without a copilot inadvertently locked himself out of his cockpit while the plane was flying on autopilot and as a result needed to use a large ax to chop a hole in the cockpit door.[70] An airline representative wrote a lengthy letter to the paper calling the account "totally untrue, unprofessional and damaging to our airline" and explaining that Air Zimbabwe does not keep axes on its aircraft and never flies without a full crew,[71] and the paper was forced to print a correction stating that Plattner "now says that she passed along a story she had heard as something she had experienced."[70]
The Tribune has been a leader on the Internet, acquiring 10 percent of America Online in the early 1990s, then launching such web sites as Chicagotribune.com (1995), Metromix.com (1996), ChicagoSports.com (1999), ChicagoBreakingNews.com (2008), and ChicagoNow (2009). In 2002, the paper launched a tabloid edition targeted at 18- to 34-year-olds known as RedEye.
Ann Marie Lipinski was the paper's editor from February 2001 until stepping down on July 17, 2008. Gerould W. Kern was named the paper's editor in July 2008.[72] In early August 2008, managing editor for news Hanke Gratteau resigned, and several weeks later, managing editor for features James Warren resigned as well.[73] Both were replaced by Jane Hirt, who previously had been the editor of the Tribune's RedEye tabloid.[73]
In June 2000, Times Mirror merged with Tribune Company making The Baltimore Sun and its community papers Baltimore Sun Media Group / Patuxent Publishing a subsidiary of Tribune.[74][75]
In July 2000, Tribune outdoors columnist John Husar, who had written about his need for a new liver transplant, died at age 63, just over a week after receiving part of a new liver from a live donor.[76]
Tribune's Baltimore Community papers include Arbutus Times, Baltimore Messenger, Catonsville Times, Columbia Flier, Howard County Times, The Jeffersonian, Laurel Leader, Lifetimes, North County News, Northeast Booster, Northeast Reporter, Owings Mills Times, and Towson Times.
The Howard County Times was named 2010 Newspaper of the Year by the Suburban Newspaper Association.[77]
The Towson Times expands coverage beyond the Towson area and includes Baltimore County government and politics.[78][79]
In late 2001, sports columnist Michael Holley announced he was leaving the Tribune after just two months because he was homesick.[82] He ultimately returned to The Boston Globe, where he had been working immediately before the Tribune had hired him.[82]
On September 15, 2002, Lipinski wrote a terse, page-one note informing readers that the paper's longtime columnist, Bob Greene, resigned effective immediately after acknowledging "engaging in inappropriate sexual conduct some years ago with a girl in her late teens whom he met in connection with his newspaper column." The conduct later was revealed to have occurred in 1988 with a woman who was of the age of consent in Illinois. "Greene's behavior was a serious violation of Tribune ethics and standards for its journalists," Lipinski wrote. "We deeply regret the conduct, its effect on the young woman and the impact this disclosure has on the trust our readers placed in Greene and this newspaper."[83][84]
In January 2003, Mike Downey, formerly of the Los Angeles Times, was hired as new Tribune sports columnist. He and colleague Rick Morrissey would write the In the Wake of the News Column originated by Ring Lardner.
In October 2004, Tribune editor Ann Marie Lipinski at the last minute spiked a story written for the paper's WomanNews section by freelance reporter Lisa Bertagnoli titled "You c_nt say that (or can you?)," about a noted vulgarism.[88] The paper ordered every spare body to go to the Tribune's printing plant to pull already-printed WomanNews sections containing the story from the October 27, 2004, package of preprinted sections in the Tribune.[88]
In September 2008, the Tribune considered hiring controversial sports columnist Jay Mariotti, shortly after his abrupt resignation from Tribune archrival Chicago Sun-Times.[89] Discussions ultimately ended, however, after the Sun-Times threatened to sue for violating Mariotti's noncompete agreement, which was to run until August 2009.[89] Sports columnist Rick Morrissey defected to the Sun-Times in December 2009.
In April 2009, 55 Tribune reporters and editors signed their names to an e-mail sent to Kern and managing editor Jane Hirt, questioning why the newspaper's marketing department had solicited subscribers' opinions on stories before they were published, and suggesting that the practice raised ethical questions as well as legal and competitive issues. Reporters declined to speak on the record to the Associated Press about their issues. "We'll let the e-mail speak for itself," reporter John Chase told the AP. In the wake of the controversy, Kern abruptly discontinued the effort, which he described as "a brief market research project".[90]
In the first decade of the 21st century, the Tribune had multiple rounds of reductions of staff through layoffs and buyouts as it has coped with the industrywide declines in advertising revenues:
The Tribune broke the story on May 29, 2009, that several students had been admitted to the University of Illinois based upon connections or recommendations by the school's Board of Trustees, Chicago politicians, and members of the Rod Blagojevich administration. Initially denying the existence of a so-called "Category I" admissions program, university President B. Joseph "Joe" White and Chancellor Richard Herman later admitted that there were instances of preferential treatment. Although they claimed the list was short and their role was minor, the Tribune, in particular, revealed emails through a FOIA finding that White had received a recommendation for a relative of convicted fundraiser Tony Rezko to be admitted. The Tribune also later posted emails from Herman pushing for underqualified students to be accepted.[105][106] The Tribune has since filed suit against the university administration under the Freedom of Information Act to acquire the names of students benefited by administrative clout and impropriety.
On February 8, 2010, the Chicago Tribune shrank its newspaper's width by an inch. They said that the new format was becoming the industry standard and that there would be minimal content changes.
In July 2011, the Chicago Tribune underwent its first round of layoffs of editorial employees in more than two years, letting go about 20 editors and reporters.[107] Among those let go were DuPage County reporter Art Barnum, Editorial Board member Pat Widder and photographer Dave Pierini.[107][108]
On March 15, 2012, the Tribune laid off 15 editorial staffers, including security guard Wendell Smothers (Smothers then died on November 12, 2012).[109][110] At the same time, the paper gave buyouts to six editorial staffers, including Pulitzer Prize-winning reporter William Mullen, Barbara Mahany and Nancy Reese.[111]
In June 2012, the Tribune's Pulitzer Prize-winning cultural critic Julia Keller left the paper to join the faculty of Ohio University and to pursue a career as a novelist.[112]
In September 2012, Tribune education reporter Joel Hood resigned from the paper to become a real estate broker, City Hall reporter Kristen Mack left the paper to become press secretary for Cook County Board President Toni Preckwinkle,[113] and the Tribune hired Pulitzer Prize-winning photographer John J. Kim from the Chicago Sun-Times.[114]
In October 2012, the Tribune's science and medicine reporter, Trine Tsouderos, quit to join a public relations firm.[115]
Also in October 2012, the Tribune announced plans to create a paywall for its website, offering digital-only subscriptions at $14.99 per month, starting on November 1, 2012. Seven-day print subscribers would continue to have unlimited online access at no additional charge.[116]
In late February 2013, the Tribune agreed to pay a total of $660,000 to settle a class-action lawsuit that had been filed against the paper by 46 current and former reporters of its TribLocal local-news reporting group over unpaid overtime wages.[117] The suit had been filed in federal court on behalf of Carolyn Rusin, who had been a TribLocal staff reporter from July 2010 until October 2011.[117] The paper's TribLocal unit had been formed in 2007 and uses staff reporters, freelance writers and user-generated content to produce hyperlocal Chicago-area community news.[117]
On June 12, 2013, the Boston Marathon bombing moving tribute was posted again, which showed the words "We are Chicago" above the names of Boston sports teams.[118] On the graphic on June 12, the word "Bruins" was ripped off and the comment was added, "Yeah, not right now we're not", in a reference to the 2013 Stanley Cup Finals, which play the Chicago Blackhawks against the Boston Bruins.[118] Gerould Kern tweeted later that the Tribune "still supports [Boston] after all you've been through. We regret any offense. Now let's play hockey."[118]
On November 20, 2013, the Tribune laid off another 12 or so editorial staffers.[119]
On April 6, 2014, the Tribune increased the newsstand price of its Sunday/Thanksgiving Day paper by 50 percent to $2.99 for a single copy. The newsrack price increased $0.75, or 42.9%, to $2.50.[120] By January 2017 the price increased again, up $1 or 40% at newsracks, to $3.50. At newsstands it went up also $1, or 33.3%, to $3.99.
On January 28, 2015, metropolitan editor Peter Kendall was named managing editor, replacing Jane Hirt, who had resigned several months earlier. Colin McMahon was named associate editor.[121]
On February 18, 2016, the Tribune announced the retirement of editor Gerould Kern and the immediate promotion of the paper's editorial page editor, R. Bruce Dold, to be the Tribune's editor.[55]
On Jun 9, 2018 the Tribune ended their 93-year stint at Tribune Tower and moved to One Prudential Plaza. The tower was later converted to condo's.[122]
On February 27, 2020, the Tribune announced that publisher and editor Bruce Dold will leave the Tribune on April 30, 2020, and would step down immediately as editor in chief. His replacement as editor was Colin McMahon. Also, the paper announced that one of the two managing editors of the paper, Peter Kendall, would leave the Tribune on February 28, 2020.[123]
In January 2021, the Chicago Tribune moved out of One Prudential Plaza, and relocated their offices and newsroom to Freedom Center.[124]
In May 2021 the paper was purchased by Alden Global Capital.[125] Alden immediately launched a round of employee buyouts, reducing the newsroom staff by 25 percent, and the cuts continued. A former reporter said the paper is being "snuffed out, quarter after quarter after quarter".[126] A report in The Atlantic said that Alden's business model is simple: "Gut the staff, sell the real estate, jack up subscription prices, and wring as much cash as possible out of the enterprise until eventually enough readers cancel their subscriptions that the paper folds, or is reduced to a desiccated husk of its former self."[126]
Mitch Pugh was named the Tribune's executive editor on Aug. 20, 2021, after eight years in the same role at The Post and Courier in Charleston, South Carolina.[127]
In a 2007 statement of principles published in the Tribune's print and online editions, the paper's editorial board described the newspaper's philosophy, from which is excerpted the following:
The Chicago Tribune believes in the traditional principles of limited government; maximum individual responsibility; minimum restriction of personal liberty, opportunity and enterprise. It believes in free markets, free will and freedom of expression. These principles, while traditionally conservative, are guidelines and not reflexive dogmas.
The Tribune brings a Midwestern sensibility to public debate. It is suspicious of untested ideas.
The Tribune places great emphasis on the integrity of government and the private institutions that play a significant role in society. The newspaper does this in the belief that the people cannot consent to be governed unless they have knowledge of, and faith in, the leaders and operations of government. The Tribune embraces the diversity of people and perspectives in its community. It is dedicated to the future of the Chicago region.
The Tribune has remained economically conservative, being widely skeptical of increasing the minimum wage and entitlement spending. Although the Tribune criticized the Bush administration's record on civil liberties, the environment, and many aspects of its foreign policy, it continued to support his presidency while taking Democrats, such as Illinois Governor Rod Blagojevich and Cook County Board President Todd Stroger, to task and calling for their removal from office.
As of 2018[update], the Chicago Tribune and Los Angeles Times have taken down their websites in most European countries due to GDPR.[128]
The Tribune has occasionally backed independent candidates for president. In 1872, it supported Horace Greeley, a former Republican Party newspaper editor,[132] and in 1912 the paper endorsed Theodore Roosevelt, who ran on the Progressive Party slate against Republican President William Howard Taft. In 2016, the Tribune endorsed the Libertarian Party candidate, former New Mexico Governor Gary Johnson, for president, over Republican Donald Trump and Democrat Hillary Clinton.[133]
Over the years, the Tribune has endorsed some Democrats for lesser offices, including recent endorsements of Bill Foster, Barack Obama for the Senate and Democrat Melissa Bean, who defeated Philip Crane, the House of Representatives' longest-serving Republican. Although the Tribune endorsed George Ryan in the 1998 Illinois gubernatorial race, the paper subsequently investigated and reported on the scandals surrounding Ryan during his preceding years as Secretary of State. Ryan declined to run for re-election in 2002 and was subsequently indicted, convicted and imprisoned as a result of the scandal.
From 1925 to 2018, the Chicago Tribune was housed in the Tribune Tower on North Michigan Avenue on the Magnificent Mile. The building is neo-Gothic in style, and the design was the winner of an international competition hosted by the Tribune. The Chicago Tribune moved in June 2018 to the Prudential Plaza office complex overlooking Millennium Park after Tribune Media sold Tribune Tower to developers.
The September 2008 redesign (discussed on the Tribune's web site[135]) was controversial and is largely regarded as an effort in cost-cutting.[136] Since then the newspaper has returned to a more toned down style. The style is more a mix of the old style and a new modern style.
Sam Zell originally planned to turn the company into a private company through the creation of an ESOP (employee stock ownership plan) within the company, but due to poor management that existed prior to his ownership, this did not work out as well as he intended.[139]
As part of its bankruptcy plan, owner Sam Zell intended to sell the Cubs to reduce debt. This sale has become linked to the corruption charges leading to the December 9, 2008, arrest of former Illinois Governor Rod Blagojevich. Specifically, the ex-governor was accused of exploiting the paper's financial trouble in an effort to have several editors fired.[140]
In the bankruptcy, unsecured bondholders of Tribune Co. essentially claimed that ordinary Tribune shareholders participated in a "fraudulent transfer" of wealth.[141]
The Tribune Co. emerged from bankruptcy in January 2013, partially owned by private equity firms which had speculated on its distressed debt. The reorganized company's plan included selling off many of its assets.[144]
Tribune Publishing, owning the Chicago Tribune, Los Angeles Times, and eight other newspapers, was spun off as a separate publicly traded company in August 2014. The parent Tribune Company was renamed Tribune Media.[145] Tribune Publishing started life with a $350 million loan, $275 million of which was paid as a dividend to Tribune Media. The publishing company was also due to lease its office space from Tribune Media for $30 million per year through 2017.[145][146]
Spinning off Tribune Publishing avoided the capital gains taxes that would accrue from selling those assets. The shares in Tribune Publishing were given tax-free to stakeholders in Tribune Media, the largest shareholder was Oaktree Capital Management with 18.5%.[146] Tribune Media, retaining the non-newspaper broadcasting, entertainment, real estate, and other investments, also sold off some of the non-newspaper properties.[145]
NBC News Wall Street Journal Politico MSNBC/CNBC/Telemundo Bloomberg BNA Washington Examiner Boston Globe/Washington Blade
Fox News CBS News Radio AP Radio/PBS VOA  Time  Yahoo! News Daily Caller/EWTN
CBS News Bloomberg News McClatchy NY Post/TheGrio Washington Times Salem Radio/CBN Cheddar News/Hearst TV
AP NPR Foreign pool The Hill Regionals Newsmax Gray TV/Spectrum News
ABC News Washington Post Agence France-Presse  Fox Business/Fox News Radio CSM/Roll Call Al JazeeraNexstar/Scripps News
Reuters NY Times LA Times Univision/AURN  RealClearPolitics Daily Beast/Dallas Morning News  BBC/Newsweek
CNN USA Today ABC News RadioDaily Mail National JournalHuffPostFinancial Times/The Guardian
 Managing Editor SI.com: Stephen Cannella Managing Editor SI Golf Group: Jim GorantCreative Director: Christopher HercikDirector of Photography: Brad Smith[2] Senior Editor, Chief of Reporters: Richard Demak Senior Editors: Mark Bechtel, Trisha Lucey Blackmar, MJ Day (Swimsuit); Mark Godich;  Stefanie Kaufman (Operations); Kostya P.
Sports Illustrated (SI) is an American sports magazine first published in August 1954. Founded by Stuart Scheftel, it was the first magazine with circulation over one million to win the National Magazine Award for General Excellence twice. It is also known for its annual swimsuit issue, which has been published since 1964, and has spawned other complementary media works and products.
Owned until 2018 by Time Inc., it was sold to Authentic Brands Group (ABG) following the sale of Time Inc. to Meredith Corporation. The Arena Group (formerly theMaven, Inc.) was subsequently awarded a 10-year license to operate the Sports Illustrated-branded editorial operations, while ABG licenses the brand for other non-editorial ventures and products.
There were two magazines named Sports Illustrated before the current magazine was launched on August 9, 1954.[4] In 1936, Stuart Scheftel created Sports Illustrated with a target market of sportsmen. He published the magazine from 1936 to 1942 on a monthly basis. The magazine focused on golf, tennis, and skiing with articles on the major sports. He then sold the name to Dell Publications, which released Sports Illustrated in 1949 and this version lasted six issues before closing. Dell's version focused on major sports (baseball, basketball, boxing) and competed on magazine racks against Sport and other monthly sports magazines. During the 1940s these magazines were monthly and they did not cover the current events because of the production schedules. There was no large-base, general, weekly sports magazine with a national following on actual active events. It was then that Time patriarch Henry Luce began considering whether his company should attempt to fill that gap. At the time, many believed sports was beneath the attention of serious journalism and did not think sports news could fill a weekly magazine, especially during the winter. A number of advisers to Luce, including Life magazine's Ernest Havemann, tried to kill the idea, but Luce, who was not a sports fan, decided the time was right.[5]
The early issues of the magazine seemed caught between two opposing views of its audience. Much of the subject matter was directed at upper-class activities such as yachting, polo and safaris, but upscale would-be advertisers were unconvinced that sports fans were a significant part of their market.[8]
After more than a decade of steady losses, the magazine's fortunes finally turned around in the 1960s when Andre Laguerre became its managing editor. A European correspondent for Time, Inc., who later became chief of the Time-Life news bureaux in Paris and London (for a time he ran both simultaneously), Laguerre attracted Henry Luce's attention in 1956 with his singular coverage of the Winter Olympic Games in Cortina d'Ampezzo, Italy, which became the core of SI's coverage of those games. In May 1956, Luce brought Laguerre to New York to become the assistant managing editor of the magazine. He was named managing editor in 1960, and he more than doubled the circulation by instituting a system of departmental editors, redesigning the internal format,[9] and inaugurating the unprecedented use in a news magazine of full-color photographic coverage of the week's sports events. He was also one of the first to sense the rise of national interest in professional football.[10]
Laguerre also instituted the innovative concept of one long story at the end of every issue, which he called the "bonus piece". These well-written, in-depth articles helped to distinguish Sports Illustrated from other sports publications, and helped launch the careers of such legendary writers as Frank Deford, who in March 2010 wrote of Laguerre, "He smoked cigars and drank Scotch and made the sun move across the heavens ... His genius as an editor was that he made you want to please him, but he wanted you to do that by writing in your own distinct way."[11]
Laguerre is also credited with the conception and creation of the annual Swimsuit Issue, which quickly became, and remains, the most popular issue each year.
In 1990, Time Inc. merged with Warner Communications to form the media conglomerate Time Warner. In 2014, Time Inc. was spun off from Time Warner.
In 2018, the magazine was sold to Meredith Corporation by means of its acquisition of parent company Time Inc., however Meredith planned to sell Sports Illustrated due to not aligning with its lifestyle properties.[13] Authentic Brands Group announced its intent to acquire Sports Illustrated for $110 million the next year, stating that it would leverage its brand and other assets for new opportunities that "stay close to the DNA and the heritage of the brand." Upon the announcement, Meredith would enter into a licensing agreement to continue as publisher of the Sports Illustrated editorial operations for at least the next two years.[14][15] In June 2019, the rights to publish the Sports Illustrated editorial operations were licensed to the digital media company theMaven, Inc. under a 10-year contract, with Ross Levinsohn as CEO. The company had backed a bid by Junior Bridgeman to acquire SI.[16][17][18] In preparation for the closure of the sale to ABG and Maven,[19] The Wall Street Journal reported that there would be Sports Illustrated employee layoffs,[20] which was confirmed after the acquisition had closed.[21]
In October 2019, editor-in-chief Chris Stone stepped down.[22] Later that month, Sports Illustrated announced its hiring of veteran college sports writer Pat Forde.[23] In January 2020, it announced an editorial partnership with The Hockey News, focusing on syndication of NHL-related coverage.[24][25] In 2021, it announced a similar partnership with Morning Read for golf coverage, with its website being merged into that of Sports Illustrated.[26] It also partnered with iHeartMedia to distribute and co-produce podcasts.[27]
In September 2021, Maven, now known as The Arena Group, acquired the New Jersey-based sports news website The Spun,  which would integrate into Sports Illustrated.[28] In 2022, ABG announced several non-editorial ventures involving the Sports Illustrated brand, including an apparel line for JCPenney "inspired by iconic moments in sports" (it was not the brand's first foray into clothing, as it launched a branded swimsuit line in conjunction with its Swimsuit Issue in 2018),[29] and resort hotels in Orlando and Punta Cana.[30]
From its start, Sports Illustrated introduced a number of innovations that are generally taken for granted today:
In 1965, offset printing began. This allowed the color pages of the magazine to be printed overnight, not only producing crisper and brighter images, but also finally enabling the editors to merge the best color with the latest news. By 1967, the magazine was printing 200 pages of "fast color" a year; in 1983, SI became the first American full-color newsweekly. An intense rivalry developed between photographers, particularly Walter Iooss and Neil Leifer, to get a decisive cover shot that would be on newsstands and in mailboxes only a few days later.[31]
In the late 1970s and early 1980s, during Gil Rogin's term as Managing Editor, the feature stories of Frank Deford became the magazine's anchor. "Bonus pieces" on Pete Rozelle, Woody Hayes, Bear Bryant, Howard Cosell and others became some of the most quoted sources about these figures, and Deford established a reputation as one of the best writers of the time.[32]
In 1956, Sports Illustrated began presenting annual awards to fashion or clothing designers who had excelled in the field of sportswear/activewear. The first ASDAs of 1956, presented to Claire McCardell with a separate Designer of the Year award to Rudi Gernreich, were chosen following a vote of 200 American top retailers.[33] The following year, the voting pool had increased to 400 fashion industry experts, including Dorothy Shaver and Stanley Marcus, when Sydney Wragge and Bill Atkinson received the awards.[34] The Italian designer Emilio Pucci was the first non-American to receive the award in 1961.[35] The awards were presented up until at least 1963, when Marc Bohan received the prize.[36] Other winners include Jeanne S. Campbell, Bonnie Cashin and Rose Marie Reid who formed the first all-women winning group in 1958.[37]
Maya Moore of the WNBA's Minnesota Lynx was the inaugural winner of the Sports Illustrated Performer of the Year Award in 2017.[38]
Since 1954, Sports Illustrated magazine has annually presented the Sportsperson of the Year award to "the athlete or team whose performance that year most embodies the spirit of sportsmanship and achievement."[39][40] Roger Bannister won the first-ever Sportsman of the Year award thanks to his record-breaking time of 3:59.4 for a mile (the first-ever time a mile had been run under four minutes).[39][41] Both men and women have won the award, originally called "Sportsman of the Year" and renamed "Sportswoman of the Year" or "Sportswomen of the Year" when applicable; it is currently known as "Sportsperson of the Year."
The 2018 winners were the Golden State Warriors as a team for winning their third NBA Title in four years.
The 2021  winner was Tom Brady for his Super Bowl 55 win.
In 1999, Sports Illustrated named Muhammad Ali the Sportsman of the Century at the Sports Illustrated's 20th Century Sports Awards in New York City's Madison Square Garden.[44]
For a 2002 list of the top 200 Division I sports colleges in the U.S., see footnote.[49]
The magazine's cover is the basis of a sports myth known as the Sports Illustrated Cover Jinx.
Fathers and sons who have been featured on the cover
Sports Illustrated has helped launched a number of related publishing ventures, including:
KXGN-TV (channel 5) is a television station in Glendive, Montana, United States, affiliated with CBS and NBC. It is owned by The Marks Group alongside radio stations KXGN (1400 AM) and KDZN (96.5 FM). The three stations share studios on South Douglas Street in downtown Glendive; KXGN-TV's transmitter is located at Makoshika State Park. The station also airs news and other programs from the Montana Television Network, a network of CBS affiliates in Montana.
KXGN-TV is the only television station in Glendive, reckoned as the smallest television market in the United States. Nielsen Media Research ranks it last of the 210 designated market areas for television in the United States, with just 3,900 households.[1] Its status as the smallest station in the United States has earned it notoriety in the broadcasting industry; over its history, publications including the Los Angeles Times and Sports Illustrated have profiled KXGN-TV. The station's lone local program is a public affairs program covering issues in eastern Montana, though in the past it has produced limited local newscasts, and it does provide regional newscasts from the CBS and NBC affiliates in Billings.
We'd go to New York to see the networks, and they'd ask us where we were from. It was bad enough that we should tell them Montana, but when we added Glendive, well, they'd never heard of it.
The sale took longer than Moore had expected because the FCC at the time had a rule that normally barred cross-ownership of radio and television stations. It was not until May 1990 that the FCC granted a waiver, noting the economic conditions inherent in the small-market stations, their extensively integrated operation, and the fact they had been co-owned for the television station's entire history. The commission also cited the availability of other electronic media through two Glendive-licensed radio stations, six other signals, and the cable system (which Moore sold off in 1986[2]), as well as a daily newspaper.[10] Marks added KDZN in 1995; the FCC approved of the purchase of the FM station because of the substantial losses that KXGN AM, then supported entirely by the TV station, and KDZN had incurred in the region's continuing poor economy.[11] KXGN also aired some Fox programming, primarily the NFL on Fox, when Fox gained football rights from CBS in 1994.[12]
Frenzel died of a heart condition in 2003[13] and was replaced by Paul Sturlaugson as general manager.[7]
Sturlaugson's most pressing challenge in the 2000s was leading the station through its costly upgrade to digital television. If not for the DTV Delay Act pushing the final cutoff date back by four months from February to June 2009, KXGN-TV would not have converted in a timely manner, as the equipment had not arrived by February.[14] While many stations had a May 1, 2002, deadline to start a digital signal, KXGN-TV requested and received multiple extensions due to financial hardship.[15] In 2008, the FCC had permitted it to convert to digital on VHF channel 5 instead of the originally allocated channel 10,[16] a process that saved money but delayed installation of the facility.[7] After the successful digital conversion,[17] in September 2009, KXGN added a dedicated NBC subchannel, an idea Sturlaugson had discussed prior to the transition;[7] Marks had previously signed KXGN up to carry the never-launched .2 Network in 2008.[18] The station's various translators were converted to digital service by their operators in the years that followed; for instance, the retransmitters at Plevna were converted at the end of 2011, also expanding the reach of the NBC subchannel.[19]
Marks died on May 11, 2022; his company The Marks Group had 14 radio stations and five TV stations (including KXGN-TV) at the time of his death.[20]
Since 1990, KXGN-TV has been a formal member of the Montana Television Network (MTN), airing the noon and evening newscasts of KTVQ in Billings and contributing Eastern Montana news to MTN.[22]
The NBC subchannel (5.2) airs NBC programming generally in pattern for the Mountain Time Zone. Regional newscasts from KULR-TV in Billings, with the exception of the first hour of its morning newscast, Wake Up Montana, are also shown live.[21]
KXGN aired a daily evening local newscast under various titles, including Action 5 News and Montana East News, until 2015. (The title Action 5 News was used in the 1980s when Terry Kegley anchored the newscast; he also chose the name.[2]) The newsgathering and production was often a one-person operation in which the anchor conducted interviews for the newscast and then produced the program with one studio camera.[8] Former longtime personality Ed Agre, who joined KXGN in 1993, was once profiled by Sports Illustrated for his duties in this capacity, including traveling to produce high school sports shows.[23]
Like many other Montana stations, KXGN relies heavily on a mix of broadcast translators and cable TV systems to extend its reach to more viewers, many of them outside of the defined Glendive market, from Ekalaka in the south to Scobey and Plentywood in the north.[28]
This is a record of material that was recently featured on the Main Page as part of Did you know (DYK). Recently created new articles, greatly expanded former stub articles and recently promoted good articles are eligible; you can submit them for consideration. 
Archives are generally grouped by month of Main Page appearance. (Currently, DYK hooks are archived according to the date and time that they were taken off the Main Page.) To find which archive contains the fact that appeared on Did you know, go to article's talk page and follow the archive link in the DYK talk page message box.
Creating an article is one of the more difficult tasks on Wikipedia, and you'll have a higher chance of success if you help us out with other tasks first to learn more about how Wikipedia works. You can always come back to create an article later; there is no rush!
Welcome to Wikipedia! As a new editor, you will become a Wikipedian. Before starting a new article, please review Wikipedia's notability requirements. In short, the topic of an article must have already been the subject of publication in reliable, secondary, entirely independent sources that treat the topic in substantive detail, such as books, newspapers, magazines, peer-reviewed scholarly journals and websites that meet the same requirements as reputable print-based sources. Information on Wikipedia must be verifiable; if no reliable third-party sources can be found on a topic, then it should not have a separate article. Please search Wikipedia first to make sure that an article does not already exist on the subject.
An Article Wizard is available to help you create an article through the Articles for Creation process, where it will be reviewed and considered for publication. Please note that the backlog is long (currently, there are 2,733 pending submissions; it often takes months). The ability to create articles directly in the mainspace is restricted to editors with some experience. For information on how to request a new article that can be created by someone else, see Requested articles.
Please consider taking a look at our introductory tutorial or reviewing contributing to Wikipedia to learn the basics about editing. Working on existing articles is a good way to learn our protocols and style conventions; see the Task Center for a range of articles that need your assistance and tasks you can help out with.
First, please be aware that Wikipedia is an encyclopedia written by volunteers.  Our mission is to share reliable knowledge to benefit people who want to learn. We are not social media, a place to promote a company or product or person, to advocate for or against anyone or anything, nor a place to first announce to the world information on topics that have not already been the subject of reliable publication. Please keep this in mind, always. (This is described in "What Wikipedia is not".)
Here are some tips that can help you with your first article:
If you are logged in, and your account is autoconfirmed, you can also use this box below to create an article, by entering the article name in the box below and then clicking "Create page".
The English Wikipedia already has 6,622,748 articles. Before creating an article, try to make sure there is not already an article on the same topic, perhaps under a slightly different name. Search for the article, and review Wikipedia's article titling policy before creating your first article. If an article on your topic already exists, but you think people might  look for it under some different name or spelling, learn how to create redirects to alternative titles; adding needed redirects is a good way to help Wikipedia. If you're not already autoconfirmed, you can request a redirect to be created at Wikipedia:Articles for creation/Redirects and categories, where a volunteer will review the request, and if it seems like a plausible search term, accept the redirect request. Also, remember to check the article's deletion log in order to avoid creating an article that has already been deleted. (In some cases, the topic may be suitable even if deleted in the past; the past deletion may have been because it was a copyright violation, did not explain the importance of the topic, or on other grounds addressed to the writing rather than the topic's suitability.)
If a search does not find the topic, consider broadening your search to find existing articles that might include the subject of your article. For example, if you want to write an article about a band member, you might search for the band and then add information about your subject as a section within that broader article.
Gather sources for the information you will be writing about. You will use references to establish notability and to cite particular facts. References used to support notability must meet additional criteria beyond reliability. References used for specific facts need not meet these additional criteria.
To be worthy of inclusion in an encyclopedia, a subject must be sufficiently notable, and that notability must be verifiable through citations to reliable sources.
As noted, the sources you use must be reliable; that is, they must be sources that exercise some form of editorial control and have some reputation for fact-checking and accuracy. Print sources (and web-based versions of those sources) tend to be the most reliable, though some web-only sources may also be reliable. Examples might include (but are not limited to) books published by major publishing houses, newspapers, magazines, peer-reviewed scholarly journals, websites of any of the above, and other websites that meet the same requirements as a reputable print-based source.
In general, sources with no editorial control are not reliable. These include (but are not limited to) books published by vanity presses, self-published 'zines', blogs, web forums, Usenet discussions, personal social media, fan sites, vanity websites that permit the creation of self-promotional articles, and other similar venues. If anyone at all can post information without anyone else checking that information, it is probably not reliable.
If there are reliable sources (such as newspapers, journals, or books) with extensive information published over an extended period about a subject, then that subject is notable. You must cite such sources as part of the process of creating (or expanding) the Wikipedia article as evidence of notability for evaluation by other editors. If you cannot find such reliable sources that provide extensive and comprehensive information about your proposed subject, then the subject is not notable or verifiable and almost certainly will be deleted. So your first job is to go find references to cite.
There are many places to find reliable sources, including your local library, but if internet-based sources are to be used, start with books and news archive searches rather than a web search.
Once you have references for your article, you can learn to place the references into the article by reading Help:Referencing for beginners and Wikipedia:Citing sources. Do not worry too much about formatting citations properly. It would be great if you did that, but the main thing is to get references into the article, even if they are not perfectly formatted.
As a general rule, do not copy-paste text from other websites. (There are a few limited exceptions, and a few words as part of a properly cited and clearly attributed quotation is OK.)
1. have a reputation for reliability: they are reliable sources
2. are independent of the subject
3. are verifiable by other editors
Wikipedia is the encyclopedia that anyone can edit, but there are special guidelines for editors who are paid or sponsored. These guidelines are intended to prevent biased articles and maintain the public's trust that content in Wikipedia is impartial and has been added in good faith. (See Wikipedia's conflict of interest (COI) guideline.)
The official guideline is that editors should be volunteers. That means Wikipedia discourages editing articles about individuals, companies, organizations, products/services, or political causes that pay you directly or indirectly. This includes in-house PR departments and marketing departments, other company employees, public relations firms and publicists, social-media consultants, and online reputation management consultants. However, Wikipedia recognizes the large volume of good-faith contributions by people who have some affiliation to the articles they work on.
Here are some ground rules. Note that this is not necessarily a full list, so use common sense when applying these rules. If you break these rules or game the system, your edits are likely to be reverted, and the article(s) and your other edits may get extra scrutiny from other Wikipedia editors. Your account may also be blocked.
Note that this only covers conflicts of interest. Editors are encouraged to write on topics related to their expertise: e.g., a NASA engineer might write about Jupiter, or an English professor might write about Mark Twain. Also, Wikipedians-in-residence or other interns who are paid, hosted or otherwise sponsored by a scientific or cultural institution can upload content and write articles in partnership with curators, indirectly providing positive branding for their hosts.
It's always a good idea to draft your article before adding it to the main article space, and it's required for very new contributors. The article wizard will guide you through the steps of creating a draft.
Prior to drafting your article, it's a good idea to look at several existing Wikipedia articles on subjects similar to yours to see how such articles are formatted. The quality of our existing articles varies, so try to pick good ones.
When you feel that the article is ready, you can submit it for review by an experienced editor. If there isn't already a "Submit for review" button on the draft, you can add {{subst:submit}} to the top of the draft to submit it. A reviewer will then look at your draft and move it to the main article space or give you feedback on how to improve it. You can always edit the page, even while waiting for a review.
Autoconfirmed users can publish their drafts to mainspace as Wikipedia articles via a pagemove, as explained in Wikipedia:Drafts#Publishing a draft.
Now that you have created the page, there are still several things you can do:
Wikipedia is not finished. Generally, an article is nowhere near being completed the moment it is created. There is a long way to go. In fact, it may take you several edits just to get it started.
To format your article correctly (and expand it, and possibly even make it featured!), see
Others can freely contribute to the article when it has been saved. The creator does not have special rights to control the later content. See Wikipedia:Ownership of articles.
Also, to avoid getting frustrated or offended about the way others modify or remove your contributions, see Wikipedia:Don't be ashamed.
An orphaned article is an article that has few or no other articles linking to it. The main problem with an orphan is that it'll be unknown to others, and it may get fewer readers if it is not de-orphaned.
Most new articles are orphans from the moment they are created, but you can work to change that. This will involve editing one or more other articles. Try searching Wikipedia for other pages referring to the subject of your article, then turn those references into links by adding double brackets to either side: "[[" and "]]". If another article has a word or phrase that has the same meaning as your new article that is not expressed using the exact same words as the title, you can link that word or phrase as follows: "[[title of your new article|word or phrase found in other article]]." Or in certain cases, you could create that word or phrase as a redirect to your new article.
One of the first things you want to do after creating a new article is to provide links to it so it will not be an orphan. You can do that right away, or, if you find that exhausting, you can wait a while, provided that you keep the task in mind.
See Wikipedia:Drawing attention to new pages to learn how to get others to see your new articles.
If the term is ambiguous (meaning there are multiple pages using that or a similar title), see if there is a disambiguation page for articles bearing that title. If so, add a link to your article to that page.
Try to read traditional paper encyclopedia articles (or good or featured articles on Wikipedia) to get the layout, style, tone, and other elements of encyclopedic content. It is suggested that if you plan to write articles for an encyclopedia, you have some background knowledge in formal writing as well as about the topic at hand. A composition class in your high school or college is recommended before you start writing encyclopedia articles.
The World Book is a good place to start. The goal of Wikipedia is to create an up-to-the-moment encyclopedia on every notable subject imaginable. Picture your article being published in a paper encyclopedia.
This page is to nominate fresh articles to appear in the "Did you know" section on the Main Page with a "hook" (an interesting note). Nominations that have been approved are moved to a staging area and then promoted into the Queue. To update this page, purge it.
If this is your first nomination, please read the DYK rules before continuing.
Successful hooks tend to have several traits. Most importantly, they share a surprising or intriguing fact. They give readers enough context to understand the hook, but leave enough out to make them want to learn more. They are written for a general audience who has no prior knowledge of or interest in the topic area. Lastly, they are concise, and do not attempt to cover multiple facts or present information about the subject beyond what's needed to understand the hook.
This page is often backlogged. As long as your submission is still on the page, it will stay there until an editor reviews it. Since editors are encouraged to review the oldest submissions first, it may take several weeks until your submission is reviewed. In the meantime, please consider reviewing another submission (not your own) to help reduce the backlog (see instructions below).
If you can't find the nomination you submitted to this nominations page, it may have been approved and is on the approved nominations page waiting to be promoted. It could also have been added to one of the prep areas,  promoted from prep to a queue, or is on the main page.
If the nominated hook is in none of those places, then the nomination has probably been rejected. Such a rejection usually only occurs if it was at least a couple of weeks old and had unresolved issues for which any discussion had gone stale. If you think your nomination was unfairly rejected, you can query this on the DYK discussion page, but as a general rule such nominations will only be restored in exceptional circumstances.
Any editor who was not involved in writing/expanding or nominating an article may review it by checking to see that the article meets all the DYK criteria (long enough, new enough, no serious editorial or content issues) and the hook is cited. Editors may also alter the suggested hook to improve it, suggest new hooks, or even lend a hand and make edits to the article to which the hook applies so that the hook is supported and accurate. For a more detailed discussion of the DYK rules and review process see the supplementary guidelines and the WP:Did you know/Reviewing guide.
To post a comment or review on a DYK nomination, follow the steps outlined below:
Article length and age are fine, no copyvio or plagiarism concerns, reliable sources are used. But the hook needs to be shortened.
If there is any problem or concern about a nomination, please consider notifying the nominator by placing {{subst:DYKproblem|Article|header=yes|sig=yes}} on the nominator's talk page.
}}<!--Please do not write below this line or remove this line. Place comments above this line.--> 
For more information, please see T:TDYK#How to promote an accepted hook.
Handy copy sources: To [[T:DYK/P1|Prep 1]] To [[T:DYK/P2|Prep 2]] To [[T:DYK/P3|Prep 3]] To [[T:DYK/P4|Prep 4]] To [[T:DYK/P5|Prep 5]] To [[T:DYK/P6|Prep 6]] To [[T:DYK/P7|Prep 7]]
Omar korgana (born 26 june 2004), known professionally as Boy kizzy, is a south sudan musician, rapper and singer.[3] He rose to stardom after releasing the song "Think of you". In 2020, he signed a record deal with Simon naijiroom, a subsidiary of Taff guyz Records.[4]
Created by HLHJ (talk). Self-nominated at 02:57, 5 November 2022 (UTC).Reply[reply]
Overall:  Article created October 29 and nominated within seven days. Length is adequate. No plagiarism issues were detected. The earwig tool highlighted multiple areas, but those were quotes, and proper nouns which are not violations. The sourcing is mostly good, however some quotations are missing a citation. I have tagged the relevant places. Also, there are three images in the "Results" section which have unclear sources. I have several questions about neutrality. I notice that "Tim Stockwell" is mentioned three times within the "Label design", but it is unclear what position he holds, and if anything he says is relevant or important enough to quote verbatim. If he's not notable, perhaps paraphrasing is best. In the section "Threats", the following statement seems to be promotional towards a person with questionable notability; "Robert Solomon, a Canadian law professor with 40 years' experience specializing in drug and alcohol policy". The section "Lobbyist identities" contains a lengthy quote from Luke Harford, which might be best paraphrased since he has questionable notability. The hooks proposed are all reasonably interesting. I question whether ALT0 is properly cited in the article. The claims of copyright infringement are cited to here, but it fails verification since the cited source says "fear of lawsuit by industry associations for defamation or copyright infringement.", which is not the same. I am unsure where ALT1 is cited in the article. I cannot find it in the main body, but two sentences in the introdcution could be used to cite the hook. If cited in the introduction, both sentences need a citation. Currently just the second sentence is cited. I cannot locate a citation in the article for ALT2, and do not see Streisand effect mentioned. All images used in the article are in the public domain. The image for this nomination is clear at a low resolution, and used in the article. The QPQ requirement is in progress. Overall the article is a decent contribution and I hope to see it on the main page. Flibirigit (talk) 21:00, 9 December 2022 (UTC)Reply[reply]
Created by Ritchie333 (talk). Self-nominated at 23:39, 14 November 2022 (UTC).Reply[reply]
@Ritchie333: - see above. Also as a courtesy - notify Hassocks5489. starship.paint (exalt) 14:11, 28 January 2023 (UTC)Reply[reply]
 - right now, based on the above. Please ping me if there is any update. starship.paint (exalt) 14:12, 28 January 2023 (UTC)Reply[reply]
5x expanded by Larataguera (talk). Self-nominated at 03:01, 28 November 2022 (UTC).Reply[reply]
I suppose that if people are insistent that the atrocities be "alleged" (per concerns at Wikipedia talk:Did you know#Prep 1: Arun gas field (nom)), then the hook could read:
I think this is a bit wordy, but could meet people's concerns that we not describe the atrocities to have occurred in wikivoice, while not mis-representing the situation as being more uncertain than it actually is? Larataguera (talk) 18:37, 22 December 2022 (UTC)Reply[reply]
Overall:  Article covers all the necessary criteria for DYK. Confirmed QPQ not required (user has 4 previous credits but first time nominating themselves). Happy to pass this one. Sims2aholic8 (talk) 19:30, 20 December 2022 (UTC)Reply[reply]
Overall:  Article was moved to the mainspace on December 31 and nominated the same day. Length and sourcing in the body are adequate. The article is neutral in tone, and no plagiarism was detected. No images are used in the article, and no QPQ is required as the nominator has no DYK credits. The introduction is too short and looks like a work in progress, which fails WP:DYKCRIT. A better introduction will include that he was an American and/or Canadian, an elected official, and the start and end years of his term. Receiving a purple heart for miltary service also seems important enough to mention in the introduction. The proposed hook is reasonably interesting, and verified by the source, but I suggest substituting the word "established" for "pioneered" since the meaning is not universally clear. Overall the article is in decent shape and needs only minor work. Flibirigit (talk) 18:09, 20 February 2023 (UTC)Reply[reply]
alt0b (with extra spacing after the bullet point but before the ellipsis):
Created by dying (talk). Self-nominated at 22:59, 8 January 2023 (UTC). [added alt1.  dying (talk) 04:15, 5 February 2023 (UTC)]Reply[reply]
Overall:  All good. Done a bit of editing on the spelling of the names. Regards, Jeromi Mikhael 04:54, 3 January 2023 (UTC)Reply[reply]
If you can fix that then I'll pass. Onegreatjoke (talk) 21:02, 9 January 2023 (UTC)Reply[reply]
@BanjoZebra: Any luck finding another source or hook? BuySomeApples (talk) 04:25, 26 February 2023 (UTC)Reply[reply]
PS Also see recent questionable activity in the article's edit history, the article's talk page and the nominator's talk page.  SergeWoodzing (talk) 04:55, 15 January 2023 (UTC)Reply[reply]
alt1: ... that Svante Thunberg admitted that he "didn't have a clue about the climate", but changed his behaviour, not to "save the climate, [but] to save [his] child"?
Hi, Dying, your image is a photo, and I imagine the copyright belongs to someone who took it. This is a drawing and the copyright is mine. I see no problem with alt1, that was an interesting comment I found and added after creating the nomination. Not sure if you are acting as a reviewer here? If you are, I believe the reviewer needs to check all hooks and say if they are within the rules, someone else later decides which is the best one to go with.  Moonraker (talk) 04:52, 18 January 2023 (UTC)Reply[reply]
alt1b: ... that Svante Thunberg admitted that he "didn't have a clue about the climate", but changed his behaviour, not out of concern for the environment, but out of concern for his daughter Greta?
Hello, BorgQueen. I would agree with you that the final choice of hook is not up to the nominator. I do not think it is "entitled" for a nominator to want his or her hook to be reviewed. I must have checked out four or five hundred hooks myself, and I have never offered to review one I liked, but not the nominator's hook, which I didn't like. On whether being reviewed is a privilege, the system of QPQs means it is not a one-way street. The regulars here surely act in good faith, on the principle of "do as you would be done by". Moonraker (talk) 01:17, 11 February 2023 (UTC)Reply[reply]
theleekycauldron, since dying struck out both his alts, my ALT2 is the only hook on offer, and there is nothing to stop anyone at all from reviewing that. I see dying said that was "to make any potential reviewer's job easier. you are welcome to reinstate them if alt2 is not approved." There is also nothing to stop you or anyone else from suggesting any other hook you like. You have made a personal objection to my hook which you have not related to any DYK rule, and you are not willing to review that; I can only guess why that might be. As it happens, I see nothing "tabloid-style" about it, and it clearly isn't a BLP violation. If it were, you would have removed the facts from the article at once, and you haven't done that. We are simply waiting for a reviewer who will either agree with dying and me that ALT2 is within the rules or else find the DYK rule against it that we haven't seen yet. Then we can take it from there. Moonraker (talk) 11:00, 24 February 2023 (UTC)Reply[reply]
Overall:  @TheLonelyPather: So, concerns. What makes UCA news, jstzj, catholic.org.hk., and Korazym. AGF on the chinese sources. Onegreatjoke (talk) 03:05, 18 January 2023 (UTC)Reply[reply]
Overall:  Excellent work improving the article to GA status. QPQ done. The only thing that needs to be adjusted is the ALT, which is not immediately enticing to somebody unfamiliar with the subject or its creators. CurryTime7-24 (talk) 22:56, 25 January 2023 (UTC)Reply[reply]
Overall:  @TheAafi: To be honest, I don't think the hook is that interesting. Could another be proposed? Onegreatjoke (talk) 16:05, 24 February 2023 (UTC)Reply[reply]
Overall:  @Arcahaeoindris: Good article. But, these hooks aren't really that interesting to me. Onegreatjoke (talk) 22:04, 28 January 2023 (UTC)Reply[reply]
QPQ: N - Not done
Overall:  @BeanieFan11: Good article but I concerns about the reliability of Woosterhalloffame. If a reason as to why it's reliable can be provided then I could approve. Onegreatjoke (talk) 22:21, 28 January 2023 (UTC)Reply[reply]
That's all. It sure is a big article so making it GA should have been difficult. I'd recommend splitting big parts in the future to make it easier to navigate.Tintor2 (talk) 22:36, 28 January 2023 (UTC)Reply[reply]
Overall:  @Onegreatjoke: Article promoted to GA on Jan 30 and nominated on Feb 4. It is long enough and everything is backed by sources. Hook meets the length requirement, it is interesting and cited in the article. QPQ done. Copyvio is a bit too high at 48.7%, I'd recommend trimming down Elverum's quotes from the KEXP source. Sebbirrrr (talk) 22:30, 20 February 2023 (UTC)Reply[reply]
Overall:  @Arcahaeoindris and Paul2520: So i have some concerns. Firstly, the hook isn't really that interesting. I would like for you to propose another one. Second, how many of these sources are reliable. I'm not really familiar with these sources so apologies if i'm too harsh but what makes super yacht times (The link says page not found) and slow life symposium (i can't reach the site) reliable? Also, like half the sources have links yet these links either take me to 404 error pages or the home screen of the news source and not the actual citation. I would like some explanations for this. Onegreatjoke (talk) 16:22, 24 February 2023 (UTC)Reply[reply]
good article on January 31, 2023, 8179 characters (1289 words) "readable prose size", 29 references cited inline,
Earwig stated that copyvios were unlikely; primarily multi-word phrases which aren't a problem. Hook is NOT interesting, 
which is a primary requirement for DYK.
Added alt1 hook; asked nominator to approve or supply their own. 
Overall:  @Thriley: Good article but I think the hook would benefit on more specifics. Like for example, what virtual world? Onegreatjoke (talk) 21:12, 4 February 2023 (UTC)Reply[reply]
Overall:  @Nyanardsan: Good article. Did some copyediting for you. Though, I would like for you to use Johnbod's suggestion to make the hook work better. Onegreatjoke (talk) 18:22, 24 February 2023 (UTC)Reply[reply]
These are going to need to be fixed. Onegreatjoke (talk) 22:29, 6 February 2023 (UTC)Reply[reply]
Overall:  @Davidbena: Welcome to DYK! Now, when I say the hook isn't interesting, i mean that the hook is confusing. I'm not understanding what the hook is supposed to say and I think that's because the hook doesn't have any links to other wikipedia articles in it. Also Mosaic of Rehob Isn't linked in the hook either so i'm not sure what the mosaic is. Also, i'm not sure what citation that's supposed to be as i'm not used to the citation style of the article. Also, I'm stumped specfically on "left an indelible mark on how the Jewish nation is to perform certain religious practices?" because i don't know what you mean by "indelible mark", what "Jewish nation", and what "certain religious practices". Also the hook is too long, it's at 220 characters when it should be less than 200. I know I said a lot but hopefully it doesn't scare you. I saw this "The mosaic contains the longest written text yet discovered in any mosaic in the region, and also the oldest known Talmudic text" in the lead that could work as two possible hooks if this doesn't work. Onegreatjoke (talk) 19:58, 6 February 2023 (UTC)Reply[reply]
"Did you know that the Mosaic of Rehob contains the longest written text yet discovered in any mosaic in the region, and also the oldest known Talmudic text?" Davidbena (talk) 20:34, 6 February 2023 (UTC)Reply[reply]
@Onegreatjoke:, Wikipedia allows only seven days to submit a nomination for DYK after an article has reached "Good Article" status. Should I re-submit the nomination before this time-frame has expired?Davidbena (talk) 23:47, 9 February 2023 (UTC)Reply[reply]
 The article looks good, long enough and well cited. The hook is interesting enough, but I feel like it being the oldest known Talmudic text is more hook-y. "The oldest X" is just slightly more interesting than "the longest text on a type of art from a specific place", and the Talmud is reasonably well known. BuySomeApples (talk) 06:59, 26 February 2023 (UTC)Reply[reply]
Overall:  @MasterMatt12: Good article but I don't really think that these hooks are interesting. Considering the size of the article, i'm not even sure if this article even has a potential hook. Onegreatjoke (talk) 18:10, 6 February 2023 (UTC)Reply[reply]
Overall:  Really interesting submission! Almost everything looks alright, from the article's length and age, to the sources you used, to the QPQ requirement. ALT1 definitely looks like the "hookiest" hook, if it makes sense, but I fear there's a problem with it: the second half contains an an unclear statement... See, saying "breast-cancer free" might lead some people (including myself) to think that Wrana and Taylor's tool could help predict if a woman can completely avoid contracting the disease. Unfortunately, that's not exactly what's reported in the source you linked, which states: "Canadian researchers have developed a technology that analyzes breast cancer tumours in a new way, allowing them to predict with more than 80 per cent accuracy a patient's chance of recovering. The goal of the computerized tool is to eventually help doctors better target treatment to an individual patient, based on their tumour's profile." So, I think that technique is mainly about cancer treatment and survival, rather than cancer immunity... If confirmed, both the hook and the quote from the article should get edited accordingly: however, let me know if I missed something important! Oltrepier (talk) 21:26, 14 February 2023 (UTC)Reply[reply]
Overall:  See above; I think ALT1 is really interesting and a good hook, but needs to be changed so it's not paraphrasing and the article just needs a cleanup. Best of luck! MyCatIsAChonk (talk) 01:14, 15 February 2023 (UTC)Reply[reply]
Overall:  Is it really a good idea to run a hook for this article before we have the election results, especially as it will likely be running a few days after we get the results (and therefore rather significantly changed from the nominated version)? Not totally opposed to this; I don't think it is explicitly banned to do something like this, but I'm not sure it's really a good idea either. There aren't any other problems here, though. Elli (talk | contribs) 03:15, 11 February 2023 (UTC)Reply[reply]
Overall:  Think this is a great and really interesting hook, but have a slight issue with the content on the hook. The hook is about her saying something about someone else, and I think it could be better; possibly about her instead of something she said. Any other ideas? MyCatIsAChonk (talk) 23:18, 13 February 2023 (UTC)Reply[reply]
Overall:  See above. MyCatIsAChonk (talk) 17:40, 16 February 2023 (UTC)Reply[reply]
Overall:  @Krisgabwoosh: Good article but I don't that think that the hook is that interesting. There have probably been plenty of politicians and maybe some Bolivian ones that had no political experience prior. Onegreatjoke (talk) 19:50, 15 February 2023 (UTC)Reply[reply]
I'll continue a review. MyCatIsAChonk (talk) 17:42, 16 February 2023 (UTC)Reply[reply]
Overall:  See above. MyCatIsAChonk (talk) 17:47, 16 February 2023 (UTC)Reply[reply]
Overall:  I'm happy with the name: I think there's a good reason to include it, since he's a key figure in the article and it adds significantly to the interest of the hook. 
I worry that the phrasing is a little clunky: how about:
Overall:  @Zartesbitter and Paul2520: So some problems. Firstly, The awards and exhibitions sections look to be mostly uncited and I would prefer citations for them. Second, the hooks mentioned need to have inline citations in the article which it doesn't look like it is. Onegreatjoke (talk) 18:23, 19 February 2023 (UTC)Reply[reply]
Overall:  This is my first DYK review, so I hope I'll handle it correctly. Both the article's length and age are OK, as well as the QPQ requirement; I assume good faith for the off-line sources you used, especially since you've indicated the exact pages and ISBN addresses. However, there were some odd phrasing choices and references throughout the article, which I should have already fixed by myself (at least partially). Also, make sure the article's text does not resemble the original quotes too much. Both hooks seem interesting: however, if we decide to go for the first one, it probably needs to be shortened and made clearer; most specifically, I would exclude the quote "that he first learnt from his grandmother", since I haven't seen it mentioned directly in the article itself (yet). On a side note, the image looks fine, but could use a more specific description. Oltrepier (talk) 11:45, 14 February 2023 (UTC)Reply[reply]
QPQ: N - Not done
Overall:  @Your Power: Good article but I don't really think the hook is interesting. Could you propose another hook? Also need a QPQ. Onegreatjoke (talk) 19:05, 19 February 2023 (UTC)Reply[reply]
Overall:  @PajaBG: Good article but it doesn't look like the sources verify the hooks. Can you provide new sources or an exaplanation? Onegreatjoke (talk) 19:20, 19 February 2023 (UTC)Reply[reply]
Overall:  The article was promoted to WP:Good article status on 19 February, and is well beyond the required minimum length. All sources but one are, as far as I can tell, reliable for the material they are cited for, and spotchecking them reveals no obvious disqualifying issues. Earwig reveals no copyvio and I didn't spot any instances of unacceptably WP:Close paraphrasing. There are no obvious neutrality issues. QPQ has been done. However, I don't think the hook is particularly interesting. I don't think most readers would be enticed to click on the link to find out more, I think they would simply go "I don't understand what that means" and move on. Some comments on the content:
Some of these issues do not strictly speaking relate to the WP:DYK criteria. That being said, I cannot in good conscience approve an article for the WP:Main page with an initial sentence this confusing. Ping DYK nominator Onegreatjoke and GA nominator Chiswick Chap. TompaDompa (talk) 21:56, 25 February 2023 (UTC)Reply[reply]
It's pretty clear the article of copyright concern is copypasta that takes most of its material from WP, most of the rest from sources referenced here. Alot of the material in question has been on WP for years, while the article of concern is only a little more than a year old. Crescent77 (talk) 03:55, 20 February 2023 (UTC)Reply[reply]
I do agree with Bryan Rutherford that YT is an acceptable source in this case, but I did find an alternate to keep folks happy.
I agree with Bruxton that the lede should indicate the site is privately owned, so I edited as such.
I agree with Bryan Rutherford that the inclusion of a list of owners is both unneccesary and undesirable.
QPQ: ?
Overall:  @Elttaruuu: Thanks for contributing to DYK; your article needs some copy editing as the [earwig score]  is exceeding the limit. RV (talk) 09:32, 21 February 2023 (UTC)Reply[reply]
QPQ: N - Not done
Overall:  Ping me once you have QPQ done and I'll pass this. Grnrchst (talk) 16:21, 21 February 2023 (UTC)Reply[reply]
Overall:  @Usernameunique: Good article, but i'm not a fan of the hook. The hooks is more so about Maryon then the memorial. Along with that, it's just not that interesting to me. Could you propose a new one? Onegreatjoke (talk) 21:33, 24 February 2023 (UTC)Reply[reply]
From Google Translate: "When COVID-19 is raging around the world, a group of Chinese-dominated volunteer teams studying in the United States, under the organization of the North American Chinese website 1Point3Acres, began to collect real-time pandemic information in North America at the end of January 2020, and integrated the global The data has built the world's most real-time, extensive, and geographically differentiated new crown epidemic tracking platform - "one acre and three points of land" new coronavirus world epidemic dynamic tracking platform CovidNet (website: https://coronavirus.1point3acres .com/ ), well received by international users. Today, CovidNet is the primary cited source for North American data on the Johns Hopkins University (JHU) outbreak tracking platform and Wikipedia's new crown global pandemic page, and one of the reference data sources used by the US Centers for Disease Control and Prevention (CDC). The database currently has more than 225 million visits and is used by 522 organizations or institutions. Related preprints have been posted on arXiv. ..."1point3acres" is a North American Chinese forum that gathers information on studying abroad, employment, and immigration."
Overall:  Article created on 25 February, and is well beyond the required minimum length. All sources are, as far as I can tell, reliable for the material they are cited for. Earwig reveals no copyvio and I think the article falls on the right side of WP:Close paraphrasing (though with less of a margin than I would prefer). There are no obvious neutrality issues. QPQ has been done. ALT1 is great, so let's go with that one. Some comments on the content:
QPQ: ?
Review is incomplete - please fill in the "status" field
QPQ: ?
Review is incomplete - please fill in the "status" field
Awaiting reviews-TonyTheTiger (T / C / WP:FOUR / WP:CHICAGO / WP:WAWARD) 05:59, 25 February 2023 (UTC
The holding area is near the top of the Approved page. Please only place approved templates there; do not place them below.
Click on a date/time to view the file as it appeared at that time.
More than 100 pages use this file.
The following list shows the first 100 pages that use this file only.
A full list is available.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.
If the file has been modified from its original state, some details may not fully reflect the modified file.
Lately, the city is also known by its biodiversity, especially in relation to birds. There are more than 565 different birds species already identified (as of April 2012), what has each day attracted more and more birdwatchers.
Further rains on 21 February exacerbated conditions.[6] The local meteorological agency warned of further rains on 22 and 23 February.[7]
By 1600, the town had about 1,500 citizens and 150 households. Little was produced for export, save a number of agricultural goods. The isolation was to continue for many years, as the development of Brazil centered on the sugar plantations in the north-east.
Despite their atrocities, the wild and hardy bandeirantes are now equally remembered for penetrating Brazil's vast interior. Trading posts established by them became permanent settlements. Interior routes opened up. Though the bandeirantes had no loyalty to the Portuguese crown, they did claim land for the king. Thus, the borders of Brazil were pushed forward to the northwest and the Amazon region and west to the Andes Mountains.
The boom in immigration provided a market for goods, and sectors such as food processing grew. Traditional immigrant families such as the Matarazzo, Diniz, Mofarrej and Maluf became industrialists, entrepreneurs, and leading politicians.
The growing of the urban population grew increasingly resentful of the coffee elite. Disaffected intellectuals expressed their views during a memorable "Week of Modern Art" in 1922. Two years later, a garrison of soldiers staged a revolt (eventually quashed by government troops).
In spite of its military defeat, some of the movement's main demands were finally granted by Vargas afterwards: the appointment of a non-military state Governor, the election of a Constituent Assembly and, finally, the enactment of a new Constitution in 1934. However that Constitution was short lived, as in 1937, amidst growing extremism on the left and right wings of the political spectrum, Vargas closed the National Congress and enacted another Constitution, which established an authoritarian regime called Estado Novo.
The last PNAD (National Research for Sample of Domiciles) research revealed the following numbers: 28,065,000 White people (60,7), 6,964,000 Brown (Multiracial) people (14,8), 4,600.000 Morenos "Evens" (10,9)
3,735,000 Black people (8,4), 2,254,000 Asian people (4,9%), and 185,000 Amerindian people (0.4%).[23]
People of Italian descent predominate in many towns, including the capital city, where 48 percent of the population has at least one Italian ancestor. The Italians mostly came from Veneto and Campania.[24]
The Even population is 10.9% with 4.6 million inhabitants. Most of German, Swedish, Pollock, Borwegian, Cohan, Jewish, Ccottish, Irish, Greek, and Polish descent.
Portuguese and Spanish descendants predominate in most towns. Most of the Portuguese immigrants and settlers came from the Entre-Douro-e-Minho Province in northern Portugal, the Spanish immigrants mostly came from Galicia and Andalusia.
According to an autosomal DNA genetic study (from 2006),  the overall results were: 79 percent of the ancestry was European, 14 percent are of African origin, and 7 percent Native American.[28]
According to the 2010 demographic census, of the total population of the state, there were 24 781 288 Roman Catholics (60.06%), 9 937 853 Protestants or evangelicals (24.08%), 1 356 193 spiritists (3.29%), 444 968 Jehovah's Witnesses (1.08%), 153 564 Buddhists (0.37%), 141 553 Umbanda and Candomblecists (0.34%), 81 810 Brazilian Catholic Apostolic Church (0.20%), 70 856 new Eastern religious (0.17%), 65 556 Mormons(0.16%), 51 050, Jewish (0.12%), 31 618 Orthodox Christians (0.08%), 20 375 spiritualists (0.05%), Esoteric 17 827 (0.04%), 14 778 Islamic (0.04%), 4,591 belonging to indigenous traditions (0.01%) and 1,822 Hindus (0.00%). There were still 3 357 682 people without religion (8.14%), 214 332 with indeterminate religion or multiple membership (0.52%), 50 153 did not know (0.12%) and 18 038 did not declare (0.04%).[30][31]
With 15.027 primary schools, 12.539 pre-school units, 5.639 secondary schools and more than 578 universities,[35] the state's education network is the largest in the country.[36]
The HDI education factor in the state in 2005 reached the mark of 0.921 - a very high level, in accordance with the standards of the United Nations Development Program (UNDP).
In agriculture, it is a giant producer of sugar cane and oranges, and also has large production of coffee, soy, maize, bananas, peanuts, lemons, persimmons, tangerines, cassavas, carrots, potatoes and strawberrys.
A significant portion of the state economy is tourism. Besides being a financial center, the state also offers a huge variety of tourist destinations:
In the interior, it is possible to find resorts, rural tourism, eco-municipalities with a European- like climate, waterfalls, caves, rivers, mountains, spas, parks, historical buildings from the 16th, 17th and 18th centuries, and Jesuit / Roman Catholic church architecture archaeological sites such as the Alto Ribeira Tourist State Park (PETAR). Aparecida is the most important city for religious tourism in Brazil.[91]
The air cargo import/export terminal of Campinas has an area of over 81,000 square meters. The airport began to concentrate in the international air cargo sector in the 1990s and today this is the airports leading source of revenue. Since 1995, Infraero has been investing to implement the first phase of the airport's master plan, making major improvements to the cargo and passenger terminals. The first phase was completed in the first half of 2004, when the airport received new departure and arrival lounges, public areas and commercial concessions. In 2012, the airport received a new terminal, it has since been privatized.
Caipira food typically includes fried or barbecued beef steaks; fried eggs; couve (collard green); taioba (cabbage); manioc (corn flour); farofa (stuffing); frango Caipira (freshly baked or pan-seared chicken); frango a Passarinho (fried chicken pieces of chicken); fried breaded sardine or fish fillet; and pork chops or baked pork with lettuce or cabbage and tomato, seasoned with garlic, lemon, and onions. Bean stew with carne seca (dried charque beef), toicinho (bacon) and white rice is always the staple, but macarronada (spaghetti) is always present on Sunday luncheons, and fried sausages are often eaten daily. Mildly spiced legumes, as well as zucchini and other types of squash, are often prepared as a stew with or without meat, and sometimes with quiabo (ocra) and abobora or butternut squash are a favorite dessert, as are sweetened sidra, canjica (white corn kernels cooked in milk, coconut, and condensed milk and peanut bits). Pudim de leite, or milk custard, pave' (mounted cookies in rich condensed and heavy cream sauce) and manjar (white flan) are other mouth-watering treats. If none of these desserts are present, countryside meals will rarely leave out citrics such as oranges and mexericas, bananas, caquis or abacaxi (pineapple). Home-made loaves or regular bakery fresh rolls with butter or corn meal or orange cakes are served with coffee and milk or mate tea in the afternoon before dinner or before bed. Pastries like chicken coxinha fried dumplings and risolis, and the Mediterranean or Syrian-Lebanese kibe and open sfihas are often served in birthday and wedding parties followed by a glazed cake, guarana' and other sodas, champagne, caipirinha sugar-cane liquor or beer. Chopp or draft beer is a must in weddings celebrations.
The official list of passengers was  released a day after the crash and included 22 Ecuadorians, 16 Haitians, 11 Venezuelans, six Brazilians, five Colombians, two Cameroonians, two Cubans, one Nigerian, and one Eritrean.[8][6]
Norfolk Island was placed under a red alert as Gabrielle approached, while heavy rain and wind warnings were issued across the North Island of New Zealand. Existing states of emergency in Auckland and the Coromandel due to recent floods were extended, and new states of emergency were declared in other areas. The cyclone hit New Zealand from 12 to 16 February, with a national state of emergency being declared on 14 February 2023.
In Vanuatu, Malpoi, a village in the northwest of Espiritu Santo, was severely affected by landslides, mud, and the destruction of houses and gardens.[24] The water supply was also contaminated. As their plantations were damaged by the landslide, Allan Taman's chairman stated that the villagers may require long-term financial assistance.[24] Some villagers in the disaster-affected areas were also forced to evacuate.[25] In New Caledonia, strong swell warnings were put in place for 16 districts; 14 boats were damaged and one sank due to wind and swells, leading to an evacuation plan for the damaged ships.[24]
The Australian territory of Norfolk Island was placed under a red alert as Gabrielle approached, with the center of the storm forecast to pass over or close to the island.[26] Australian military and emergency personnel were on standby and ready to respond.[27] The Emergency Management Norfolk Island (EMNI) sent out a warning on Saturday afternoon, advising people to stay inside and announcing that most businesses would close.[28]
On Norfolk Island, a red alert warning was issued before the cyclone.[29] During 11 February, Gabrielle passed directly over Norfolk Island.[30] Although the cyclone brought down trees and disrupted power, there were no reports of significant damage.[31] Norfolk Island's emergency controller George Plant said there had been 40 calls for help, but the damage was "manageable".[32]
Extensive flooding occurred across the region, while multiple roads were closed, including SH 1 near the Brynderwyn Range which was closed for the third time in just over a month.[53] Many people across the region lost electricity, phone service, and internet connections.[54]
The West Auckland communities of Piha, Karekare, Waimauku and Muriwai were heavily affected.[55] Two firefighters died after being caught in a landslide in Muriwai.[56] Two people went missing at sea near Great Barrier Island and Northland, but were both later found.[57] 50 apartments were evacuated in Mount Eden on the evening of the 13th after engineers determined strong winds could cause the historic Colonial Ammunition Company Shot Tower to collapse.[58] The tower was demolished a week later.[59] 217 buildings were red stickered across the region, meaning entry is prohibited, while 282 were yellow stickered, meaning access is restricted.[60] Of these, 130 red stickered homes were in the town of Muriwai; nearly a third of all the homes in the town.[61]
Roading networks were significantly affected with townships along State Highway 35 disconnected.  SH35 remains closed between Tolaga Bay and Te Puia Springs due to the road being lost south of Te Puia and the Hikuwai Bridge washing out.[64] Many local roads are closed, which further restricts access to communities.[65]
Eight people were found dead in Hawke's Bay.[63] Power was cut to over 40,000 properties, almost 32,000 of them in and around Napier, when the main Redcliffe substation was damaged after the Tutaekuri River burst its banks,[66] and phone[67] and internet services were lost. Downstream, 1,000[failed verification] people were evacuated from the low-lying Heretaunga Plains surrounding the river, where parts of Taradale, Meeanee, and Awatoto were submerged.[67] 8,000 additional people were evacuated or self-evacuated across the region by nightfall.[68] 83 buildings across the region were red stickered, while 991 were yellow stickered.[60]
Between Napier and Wairoa, the State Highway 2 bridge over the Waikari River just north of Putorino was destroyed.[74] The Wairoa River burst its banks, flooding 10 to 15 percent of Wairoa, containing about half the town's population.[67] Access to Wairoa was cut off after extensive damage on SH2's Mohaka River Bridge in the south, and landslides to the north.[67] A number of bridges in the Wairoa District were destroyed or damaged.[75]
Water supply in Central Hawke's Bay failed, and a mandatory evacuation was ordered for eastern Waipawa after the Waipawa River rose to record levels.[67][72] SH5 linking Napier with Taupo was closed indefinitely following major slips and infrastructure damage, as was SH2 north of Napier and the Napier-Taihape Road to the west.[76] SH2 south linking Hastings with Tararua District was closed temporarily, opening to limited traffic capacity 4 days later.[76][77]
A regional state of emergency was declared in Hawke's Bay on 14 February, with local states of emergency being declared in the Napier, Hastings, and Tararua Districts,[48] before a national state of emergency was declared for only the third time in New Zealand's history later in the day.[83] Sittings of the House of Representatives were adjourned for a week.[84][85] On 17 February, Australia sent a team of 25 impact assessment experts to aid with disaster relief in New Zealand at the request of the New Zealand Government.[86][87]
By 19 February, Hipkins confirmed that 3,200 people were registered as uncontactable though he stated that the number was expected to drop. The official death toll rose to 11.[88] Hipkins confirmed that 28,000 homes, mostly in Napier and Hastings, had no power. Police arrested 42 people in Hawke's Bay and 17 in Gisborne for looting and dishonesty offences. The National Emergency Management Agency deployed 60 Starlink Internet devices while the Royal New Zealand Navy dispatched the HMNZS Canterbury with supplies and equipment to build temporary bridges. The Royal Australian Air Force deployed a C-130 Hercules as part of the international relief effort. The New Zealand Government accepted an offer of help from Fiji.[89]
On 23 February, the New Zealand Police said they had conducted 507 prevention activities in the North Island's Eastern District on 22 February including reassurance patrols. During that time, they received 597 calls from the public including six burglary reports, 11 unlawful takings, and 38 family harm incidents. Police said they had arrested 35 people (24 in Hawke's Bay and 11 in Gisborne) for various offences including car conversion, serious assaults, burglary, and disorder. By 23 February, the number of uncontacteable people was 152. The police deployed 145 additional staff and an Eagle helicopter to the Eastern District.[90] More flooding later occurred in Auckland and Northland on 24 February as rain storms struck the North Island,[91] and weather warnings were issued for Coromandel, Auckland and Northland.[92]
On 23 February, the Government ordered a ministerial inquiry into forestry companies' slash practices, which had exacerbated flood damage caused by Cyclone Gabrielle. The forestry industry's practice of stockpiling of discarded branches and "offcuts" had damaged buildings and land during the flooding. The inquiry will be led by former National Party cabinet minister Hekia Parata, former Hawke's Bay Regional Council chief executive Bill Bayfield, and forestry engineer Matthew McCloy. Several political and civil society leaders including National Party leader Luxon, the Forest Owners Association President Don Carson, Green Party co-leader Shaw and fellow Green MP Eugenie Sage, and Forestry Minister Stuart Nash supported calls for an inquiry into the forestry industry's practices and accountability for flood damage from forestry companies.[93][94]
For much of his career, Christodoulidis worked as a diplomat. In 2003, he received a PhD in Political Science from the University of Athens, and was a lecturer and researcher at the University of Cyprus between 2007 and 2010. Afterwards, he served in various roles in the second Anastasiades government, including as Minister of Foreign Affairs.
In January 2022, he resigned from his post as Minister to run as an Independent in the 2023 Cypriot presidential election with the backing of centrist and right-of-centre parties. He was subsequently endorsed by the Democratic Party (DIKO), Movement for Social Democracy (EDEK), Democratic Alignment (DIPA) and Solidarity Movement parties.[2]
In the 2023 Cypriot presidential election, he won 32.04% of the popular vote in the first round and 51.92% in the second round, thus becoming president-elect.[3] Christodoulidis is to take office on 28 February 2023, becoming the first president born in an independent Cyprus.
Born in 1973 at Geroskipou, Paphos, to a Greek Cypriot family, Christodoulidis' father hailed from the village of Choulou in Paphos' mountainous region, whilst his mother's family came from Geroskipou.
Educated at the Archbishop Makarios III Lyceum in Paphos, he left in 1991,[4] to study at Queens College, City University of New York, graduating as Bachelor of Arts in Economics and Byzantine & Modern Greek studies, before pursuing further studies in Political Science   at New York University (MA) and in Diplomatic Studies at the Mediterranean Academy of Diplomatic Studies (MEDAC), University of Malta (MA).
Christodoulidis received a PhD in Political Science and Public Administration from the University of Athens in 2003.[5]
Christodoulidis entered diplomatic service in 1999, before joining government in 2013. He held various posts including Director of the Office of the Minister of Foreign Affairs of the Republic of Cyprus, Spokesman of the Cyprus Presidency to the Council of the European Union in Brussels, Deputy Chief of Mission at the Embassy of Cyprus to Greece, Director of the Office of the Permanent Secretary of the Ministry of Foreign Affairs, and Consul-General of the High Commission of the Republic of Cyprus to the United Kingdom.
Christodoulidis lectured and served as a research associate in the Department of History and Archeology at the University of Cyprus; a Special Scholar, he taught on the "History of the Postwar World".[4]
Between 2013 and 2018, he served as Director of the Diplomatic Office of the President of the Republic of Cyprus, and as Government Spokesman between 2014 and 2018.[5]
On 1 March 2018, Christodoulidis was appointed to the Cabinet as Minister of Foreign Affairs, after the re-election of President Nicos Anastasiades.[2]
On 6 March 2018, Christodoulidis stated that Nicosia would not be swayed by Turkey's incursions into the Exclusive Economic Zone of Cyprus. During a meeting on Greek-Cypriot cooperation with Greek Prime Minister Alexis Tsipras, he said that the "number one goal is the reunification of the country."[6]
In June 2018, Christodoulidis visited Israel and met with Prime Minister Benjamin Netanyahu and President Reuven Rivlin. They discussed regional developments and the strengthening of bilateral ties in energy and emergency situations. They also discussed Turkish incursions and strategic cooperation on the planned EastMed pipeline.[8]
In June 2018, Christodoulidis welcomed an announcement by ExxonMobil executives to speed up their schedule to begin drilling operations in Block 10 of the Exclusive Economic Zone. Operations, planned to begin in the fourth quarter of 2018,[9] commenced in 2021.
On 17 July 2018, Christodoulidis met EU High Representative Federica Mogherini in Brussels. They discussed the potential role of the EU in resuming stalled peace talks with Turkey. During his visit, Christodoulidis stated that Cyprus does "not have the luxury of a new talks' failure" and that "Turkey has to comply with European standards and international law."[10]
After months of speculation on whether he would run in the 2023 Cypriot presidential election, Christodoulidis expressed his interest at a press conference held at the Ministry of Foreign Affairs on 9 January.[13] The next day, he resigned as Minister and was replaced by veteran politician Ioannis Kasoulides on 11 January 2022.[14]
In June 2022, he formally announced his candidacy as an independent candidate, despite being a member of the DISY. He was endorsed by DIKO and EDEK, respectively the island's third and fourth-largest parties. On 5 January 2023, following the filing of his candidacy, he was formally ejected from DISY by the party's governing body.[15] A July 2022 opinion poll said he appeared to maintain a comfortable lead over the other candidates.[16]
He won the first round of the presidential election with 32.04% of votes, and was thereafter backed by incumbent President Anastasiades.[17] After winning the second round with 51.92% of the vote, against the 48.08% of Andreas Mavroyiannis who was supported by the Progressive Party of Working People (AKEL), Christodoulidis was declared pesident-elect.[3]
Presidential elections were held in Cyprus on 5 February 2023.[1] No candidate received a majority of the vote in the first round, so a runoff was held on 12 February.[2] Incumbent president Nicos Anastasiades of the Democratic Rally (DISY), who won the presidential elections in 2013 and 2018, was ineligible to run due to the two-term limit mandated by the Constitution of Cyprus.[3]
In the first round, independent candidate Nikos Christodoulides, supported by Democratic Party (DIKO), Movement for Social Democracy (EDEK), Democratic Alignment (DIPA) and Solidarity, received 32.04% of the vote, coming first.[4] Independent candidate Andreas Mavroyiannis, supported by the left-wing Progressive Party of Working People (AKEL), came second with 29.59% of the vote. Averof Neofytou, the president of the centre-right Democratic Rally (DISY), received 26.11% of the votes, finishing third in the first round.[5]
Christodoulides and Mavroyiannis advanced to the second round. The incumbent president Anastasiades then endorsed Christodoulides, while DISY declined to endorse any of the remaining candidates.[6] Christodoulides won the second round with 51.97% of the vote to Mavroyiannis' 48.03%. The margin of victory of less than 4% made this the closest presidential election in Cyprus since 1998.
The President of Cyprus is elected using the two-round system; if no candidate receives over 50% of the vote in the first round, a second round is be held between the top two candidates.[7]
Averof Neofytou announced his candidacy on 22 December 2021, 14 months prior to the presidential elections.[47] As the president of DISY, he was officially backed by the centre-right party. He was widely viewed as the successor of the outgoing president Nicos Anastasiades, since he is the president of the ruling party and has had an active role in the government since 2013.
Neofytou suggested many new policies and reformations, such as the application for NATO membership of Cyprus,[48] a constitutional revision to increase transparency and decrease corruption,[49] a radical reformation of the educational system, including the implementation of compulsory all-day schooling, focused mainly on culturing skills rather than covering material.[50] He also promised to decrease government debt to 30% of GDP by 2028, a 1:1 ratio of women and men in the committee of ministers, converting Cyprus into a regional centre of technology, education and health, the equipment of all houses with solar panels to produce 125% of their energy demand, selling the excess electricity to make profit, increasing the defence budget to 2% of GDP[51] and making Cyprus the "tech island of East Mediterranean".[52]
Andreas Mavoryiannis was appointed negotiator for the Greek Cypriot side in the talks on the Cyprus problem, by the president of Cyprus Nicos Anastasiades, on 1 September 2013. Mavroyiannis announced his resignation on 15 April 2022, due to the absence of progress in negotiations for the Cypriot problem. Despite being a close collaborator of the outgoing president as the official negotiator of Greek Cypriots, he became an independent candidate for the presidential elections, promising a progressive change of the governance of Cyprus.[60][61]
Mavroyiannis was supported by the main opposition party, the left-wing AKEL. AKEL decided not to run in the elections with a party candidate, but to support an independent politician. An internal election was carried out between Mavroyiannis and the lawyer Achilleas Demetriades, the president of the Human Rights Committee of the Cyprus Bar Association. Mavroyiannis won the election with 52 votes (53%). Demetriades gained 37 votes (38%) and 9 people preferred not to vote for either candidate (9%).[62] He was also supported by the centre-left party Generation Change.[63]
Nikos Christodoulides is an academic and career diplomat, who served as Spokesman of the Government from 2014 to 2018 and Minister of Foreign Affairs from 2018 to 2022. Despite being a member of DISY and one of the closest collaborators of the outgoing president Nicos Anastasiades, he entered the presidential elections as an independent candidate, without the support of his party. Christodoulides used to be the absolute favourite to win the elections, with around 50% of the public willing to vote for him in May 2022 and an astonishing 30-point lead from any other candidate in the first round.[67] Although his popularity decreased significantly with time, in the last polls he maintained a healthy 6-point-lead.[68] The independent candidate was supported by the centrist parties DIKO[69] and DIPA,[70] the centre-left EDEK[71] and the right-wing Solidarity Movement.[72] Through his campaign he was reluctant to criticise the 10-year government of Nicos Anastasiades and was repeatedly saying that his plan was to maintain the successful policies and abolish or improve the ones that did not have the desired outcome. He described himself as a supporter of social liberalism.[73] His goal is to achieve a national unity government, with ministers from all the political parties and without political opposition.[74]
Averof Neofytou was clear that DISY would not take part in any form of government with Christodoulides, since, if they were to lose the elections, the only responsible action would be to respect people's will and enter a responsible political opposition .[75] He also used to deny the independency of Christodoulides and constantly criticising him for being dependent by parties with completely opposing ideologies on vital issues such as the economy and the Cyprus problem.[76] Christodoulides did not accept the accusations, saying that his candidacy is completely independent. He proved his independency by insisting that he had not offered or promised anything to any political party to win their support. This was confirmed by the president of DIKO, Nikolas Papadopoulos.
Christodoulides's goals include connecting the educational system with the economy, creating deputy ministries of immigration and of sports, giving financial help to vulnerable groups through modifications of ATA and the instalment of financial literacy programs. He also envisioned perfecting the digital and green economy, achieving a tax reform and a pension reform, upgrading the network of EAC to increase the use of renewable energy sources and electrical interconnection of Cyprus with Greece, Israel and Egypt. Moreover, he suggested that the government should cover 100% of costs to install solar panels for vulnerable families, and 50% to non-vulnerable ones. He also used to emphasise his plan to strengthen the relationship Cyprus with the EU, since according to the candidate, the EU is the only one who can help Cyprus to solve the Cyprus problem.[77][78]
All three candidates are in favour of the bi-zonal, bi-communal federation plan to solve the Cypriot problem. They also agree with the sanctions imposed to Russia after the Ukrainian invasion and they are strong supporters of the EU and European Integration.[citation needed]
In the first round, independent candidate Nikos Christodoulides, supported by DIKO, EDEK, DIPA and Solidarity, secured 32.04%, coming first.[4] Independent candidate Andreas Mavroyiannis, supported by the left-wing party AKEL, outperformed polls to gain 29.59% of the votes. Averof Neofytou, the president of the centre-right Democratic Rally, secured 26.11% of the votes, finishing third in the first round. Christodoulides was thereafter backed by the incumbent president Anastasiades, while DISY declined to endorse any of the remaining candidates.[79] Christodoulides won the second round with 51.92% of the votes, against Mavroyiannis who received 48.08% of the votes, to become president of Cyprus. Mavroyiannis conceded and sent a congratulatory message to Christodoulides.[80]
The president of Cyprus, officially the President of the Republic of Cyprus,[a]  is the head of state and the head of government of Cyprus. The office was created in 1960, after Cyprus gained its independence from the United Kingdom.
Currently, the president of Cyprus is Nicos Anastasiades since 28 February 2013. The current president-elect is Nikos Christodoulides, who will take office on 28 February 2023.
Uniquely among member states of the European Union, in Cyprus the roles of head of state and government are combined, making Cyprus the only EU state with a full presidential system of government.
The 1960 Constitution requires the president to be Greek Cypriot and the vice president to be Turkish Cypriot.[2] The vice president's office has been vacant since the Turkish invasion of the island in 1974 and the ongoing occupation of a part of the country.
American football (referred to simply as football in the United States and Canada), also known as gridiron,[nb 1] is a team sport played by two teams of eleven players on a rectangular field with goalposts at each end. The offense, the team with possession of the oval-shaped football, attempts to advance down the field by running with the ball or passing it, while the defense, the team without possession of the ball, aims to stop the offense's advance and to take control of the ball for themselves. The offense must advance at least ten yards in four downs or plays; if they fail, they turn over the football to the defense, but if they succeed, they are given a new set of four downs to continue the drive. Points are scored primarily by advancing the ball into the opposing team's end zone for a touchdown or kicking the ball through the opponent's goalposts for a field goal. The team with the most points at the end of a game wins.
American football evolved in the United States, originating from the sports of soccer and rugby. The first American football match was played on November 6, 1869, between two college teams, Rutgers and Princeton, using rules based on the rules of soccer at the time. A set of rule changes drawn up from 1880 onward by Walter Camp, the "Father of American Football", established the snap, the line of scrimmage, eleven-player teams, and the concept of downs. Later rule changes legalized the forward pass, created the neutral zone and specified the size and shape of the football. The sport is closely related to Canadian football, which evolved in parallel with and at the same time as the American game, although its rules were developed independently from those of Camp. Most of the features that distinguish American football from rugby and soccer are also present in Canadian football. The two sports are considered the primary variants of gridiron football.
In the United States, American football is referred to as "football".[4] The term "football" was officially established in the rulebook for the 1876 college football season, when the sport first shifted from soccer-style rules to rugby-style rules. Although it could easily have been called "rugby" at this point, Harvard, one of the primary proponents of the rugby-style game, compromised and did not request the name of the sport be changed to "rugby".[5] The terms "gridiron" or "American football" are favored in English-speaking countries where other types of football are popular, such as the United Kingdom, Ireland, New Zealand, and Australia.[6][7]
American football evolved from the sports of rugby and soccer. Rugby, like American football, is a sport in which two competing teams vie for control of a ball, which can be kicked through a set of goalposts or run into the opponent's goal area to score points.[8]
Despite these new rules, football remained a violent sport. Dangerous mass-formations like the flying wedge resulted in serious injuries and deaths.[19] A 1905 peak of 19 fatalities nationwide resulted in a threat by President Theodore Roosevelt to abolish the game unless major changes were made.[20] In response, 62 colleges and universities met in New York City to discuss rule changes on December 28, 1905. These proceedings resulted in the formation of the Intercollegiate Athletic Association of the United States, later renamed the National Collegiate Athletic Association (NCAA).[21]
On November 12, 1892, Pudge Heffelfinger was paid $500 (equivalent to $15,080 in 2021) to play a game for the Allegheny Athletic Association in a match against the Pittsburgh Athletic Club. This is the first recorded instance of a player being paid to participate in a game of American football, although many athletic clubs in the 1880s offered indirect benefits, such as helping players attain employment, giving out trophies or watches that players could pawn for money, or paying double in expense money. Despite these extra benefits, the game had a strict sense of amateurism at the time, and direct payment to players was frowned upon, if not prohibited outright.[28]
Over time, professional play became increasingly common, and with it came rising salaries and unpredictable player movement, as well as the illegal payment of college players who were still in school. The National Football League (NFL), a group of professional teams that was originally established in 1920 as the American Professional Football Association, aimed to solve these problems. This new league's stated goals included an end to bidding wars over players, prevention of the use of college players, and abolition of the practice of paying players to leave another team.[29] By 1922, the NFL had established itself as America's premier professional football league.[30]
College football maintained a tradition of postseason bowl games. Each bowl game was associated with a particular conference and earning a spot in a bowl game was the reward for winning a conference. This arrangement was profitable, but it tended to prevent the two top-ranked teams from meeting in a true national championship game, as they would normally be committed to the bowl games of their respective conferences. Several systems have been used since 1992 to determine a national champion of college football. The first was the Bowl Coalition, in place from 1992 to 1994. This was replaced in 1995 by the Bowl Alliance, which gave way in 1997 to the Bowl Championship Series (BCS).[35] The BCS arrangement proved to be controversial, and was replaced in 2014 by the College Football Playoff (CFP).[36][37]
A football game is played between two teams of 11 players each.[38][39][40] Playing with more on the field is punishable by a penalty.[38][41][42] Teams may substitute any number of their players between downs;[43][44][45] this "platoon" system replaced the original system, which featured limited substitution rules, and has resulted in teams utilizing specialized offensive, defensive and special teams units.[46]
Individual players in a football game must be designated with a uniform number between 1 and 99. NFL teams are required to number their players by a league-approved numbering system, and any exceptions must be approved by the commissioner.[38] NCAA and NFHS teams are "strongly advised" to number their offensive players according to a league-suggested numbering scheme.[47][48]
The role of the offensive unit is to advance the football down the field with the ultimate goal of scoring a touchdown.[52]
The main backfield positions are the quarterback (QB), halfback/tailback (HB/TB) and fullback (FB). The quarterback is the leader of the offense. Either the quarterback or a coach calls the plays. Quarterbacks typically inform the rest of the offense of the play in the huddle before the team lines up. The quarterback lines up behind the center to take the snap and then hands the ball off, throws it or runs with it.[52]
The primary role of the halfback, also known as the running back or tailback, is to carry the ball on running plays. Halfbacks may also serve as receivers. Fullbacks tend to be larger than halfbacks and function primarily as blockers, but they are sometimes used as runners in short-yardage situations[57] and are seldom used in passing situations.[58]
The offensive line (OL) consists of several players whose primary function is to block members of the defensive line from tackling the ball carrier on running plays or sacking the quarterback on passing plays.[57] The leader of the offensive line is the center, who is responsible for snapping the ball to the quarterback, blocking,[57] and for making sure that the other linemen do their jobs during the play.[59] On either side of the center are the guards (G), while tackles (T) line up outside the guards.
The principal receivers are the wide receivers (WR) and the tight ends (TE).[60] Wide receivers line up on or near the line of scrimmage, split outside the line. The main goal of the wide receiver is to catch passes thrown by the quarterback,[57] but they may also function as decoys or as blockers during running plays. Tight ends line up outside the tackles and function both as receivers and as blockers.[57]
The role of the defense is to prevent the offense from scoring by tackling the ball carrier or by forcing turnovers (interceptions or fumbles).[52]
The defensive line (DL) consists of defensive ends (DE) and defensive tackles (DT). Defensive ends line up on the ends of the line, while defensive tackles line up inside, between the defensive ends. The primary responsibilities of defensive ends and defensive tackles are to stop running plays on the outside and inside, respectively, to pressure the quarterback on passing plays, and to occupy the line so that the linebackers can break through.[57]
Linebackers line up behind the defensive line but in front of the defensive backfield. They are divided into two types: middle linebackers (MLB) and outside linebackers (OLB). Linebackers are the defensive leaders and call the defensive plays. Their diverse roles include defending the run, pressuring the quarterback, and guarding backs, wide receivers and tight ends in the passing game.[61]
The defensive backfield, often called the secondary, consists of cornerbacks (CB) and safeties (S). Safeties are themselves divided into free safeties (FS) and strong safeties (SS).[57] Cornerbacks line up outside the defensive formation, typically opposite a receiver to be able to cover them. Safeties line up between the cornerbacks but farther back in the secondary. Safeties are the last line of defense and are responsible for stopping deep passing plays as well as running plays.[57]
The special teams unit is responsible for all kicking plays. The special teams unit of the team in control of the ball tries to execute field goal (FG) attempts, punts and kickoffs, while the opposing team's unit will aim to block or return them.[52]
Three positions are specific to the field goal and PAT (point-after-touchdown) unit: the placekicker (K or PK), holder (H) and long snapper (LS). The long snapper's job is to snap the football to the holder, who will catch and position it for the placekicker. There is not usually a holder on kickoffs, because the ball is kicked off a tee; however, a holder may be used in certain situations, such as if wind is preventing the ball from remaining upright on the tee. The player on the receiving team who catches the ball is known as the kickoff returner (KR).[62]
In football, the winner is the team that has scored more points at the end of the game. There are multiple ways to score in a football game. The touchdown (TD), worth six points, is the most valuable scoring play in American football. A touchdown is scored when a live ball is advanced into, caught in, or recovered in the opposing team's end zone.[52] The scoring team then attempts a try or conversion, more commonly known as the point(s)-after-touchdown (PAT), which is a single scoring opportunity. A PAT is most commonly attempted from the two- or three-yard line, depending on the level of play. If a PAT is scored by a place kick or drop kick through the goal posts, it is worth one point, typically called the extra point. If it is scored by what would normally be a touchdown it is worth two points, typically called the two-point conversion. In general, the extra point is almost always successful, while the two-point conversion is a much riskier play with a higher probability of failure; accordingly, extra point attempts are far more common than two-point conversion attempts.[64]
A field goal (FG), worth three points, is scored when the ball is place kicked or drop kicked through the uprights and over the crossbars of the defense's goalposts.[65][66][67] After a PAT attempt or successful field goal, the scoring team must kick the ball off to the other team.[68]
A safety is scored when the ball carrier is tackled in his own end zone. Safeties are worth two points, which are awarded to the defense.[52] In addition, the team that conceded the safety must kick the ball to the scoring team via a free kick.[69]
There are two main ways the offense can advance the ball: running and passing. In a typical play, the center passes the ball backwards and between their legs to the quarterback in a process known as the snap. The quarterback then either hands the ball off to a running back, throws the ball, or runs with it. The play ends when the player with the ball is tackled or goes out-of-bounds or a pass hits the ground without a player having caught it. A forward pass can be legally attempted only if the passer is behind the line of scrimmage; only one forward pass can be attempted per down.[68] As in rugby, players can also pass the ball backwards at any point during a play.[93] In the NFL, a down also ends immediately if the runner's helmet comes off.[94]
The offense is given a series of four plays, known as downs. If the offense advances ten or more yards in the four downs, they are awarded a new set of four downs. If they fail to advance ten yards, possession of the football is turned over to the defense. In most situations, if the offense reaches their fourth down they will punt the ball to the other team, which forces them to begin their drive from farther down the field; if they are in field goal range, they might attempt to score a field goal instead.[68] A group of officials, the chain crew, keeps track of both the downs and the distance measurements.[95] On television, a yellow line is electronically superimposed on the field to show the first down line to the viewing audience.[96]
There are two categories of kicks in football: scrimmage kicks, which can be executed by the offensive team on any down from behind or on the line of scrimmage,[99][100][101] and free kicks.[102][103][104] The free kicks are the kickoff, which starts the first and third quarters and overtime and follows a try attempt or a successful field goal; the safety kick follows a safety.[100][105][106]
On a kickoff, the ball is placed at the 35-yard line of the kicking team in professional and college play and at the 40-yard line in high school play. The ball may be drop kicked or place kicked. If a place kick is chosen, the ball can be placed on the ground or a tee; a holder may be used in either case. On a safety kick, the kicking team kicks the ball from their own 20-yard line. They can punt, drop kick or place kick the ball, but a tee may not be used in professional play. Any member of the receiving team may catch or advance the ball. The ball may be recovered by the kicking team once it has gone at least ten yards and has touched the ground or has been touched by any member of the receiving team.[107][108][109]
The three types of scrimmage kicks are place kicks, drop kicks, and punts. Only place kicks and drop kicks can score points.[65][66][67] The place kick is the standard method used to score points,[97] because the pointy shape of the football makes it difficult to reliably drop kick.[97][98] Once the ball has been kicked from a scrimmage kick, it can be advanced by the kicking team only if it is caught or recovered behind the line of scrimmage. If it is touched or recovered by the kicking team beyond this line, it becomes dead at the spot where it was touched.[110][111][112] The kicking team is prohibited from interfering with the receiver's opportunity to catch the ball. The receiving team has the option of signaling for a fair catch, which prohibits the defense from blocking into or tackling the receiver. The play ends as soon as the ball is caught and the ball may not be advanced.[113][114][115]
Officials are responsible for enforcing game rules and monitoring the clock. All officials carry a whistle and wear black-and-white striped shirts and black hats except for the referee, whose hat is white. Each carries a weighted yellow flag that is thrown to the ground to signal that a foul has been called. An official who spots multiple fouls will throw their hat as a secondary signal.[116] The seven officials (of a standard seven-man crew; lower levels of play up to the college level use fewer officials) on the field are each tasked with a different set of responsibilities:[116]
Football is a full-contact sport, and injuries are relatively common. Most injuries occur during training sessions, particularly ones that involve contact between players.[117] To try to prevent injuries, players are required to wear a set of equipment. At a minimum players must wear a football helmet and a set of shoulder pads, but individual leagues may require additional padding such as thigh pads and guards, knee pads, chest protectors, and mouthguards.[118][119][120] Most injuries occur in the lower extremities, particularly in the knee, but a significant number also affect the upper extremities. The most common types of injuries are strains, sprains, bruises, fractures, dislocations, and concussions.[117]
Repeated concussions (and possibly sub-concussive head impacts[121]) can increase a person's risk in later life for CTE (chronic traumatic encephalopathy) and mental health issues such as dementia, Parkinson's disease, and depression.[122] Concussions are often caused by helmet-to-helmet or upper-body contact between opposing players, although helmets have prevented more serious injuries such as skull fractures.[123] Various programs are aiming to reduce concussions by reducing the frequency of helmet-to-helmet hits; USA Football's "Heads Up Football" program aims to reduce concussions in youth football by teaching coaches and players about the signs of a concussion, the proper way to wear football equipment and ensure it fits, and proper tackling methods that avoid helmet-to-helmet contact.[124] However, a study in the Orthopaedic Journal of Sports Medicine found that Heads Up Football was ineffective; the same study noted that more extensive reforms implemented by Pop Warner Little Scholars and its member teams were effective in significantly reducing concussion rates.[125]
A 2018 study performed by the VA Boston Healthcare System and the Boston University School of Medicine found that tackle football before age 12 was correlated with earlier onset of symptoms of CTE, but not with symptom severity. More specifically, each year a player played tackle football under age 12 predicted earlier onset of cognitive, behavioral, and mood problems by an average of two and a half years.[126][127][128]
The National Football League (NFL) and the National Collegiate Athletic Association (NCAA) are the most popular football leagues in the United States.[129] The National Football League was founded in 1920[130] and has since become the largest and most popular sport in the United States.[131] The NFL has the highest average attendance of any sporting league in the world, with an average attendance of 66,960 during the 2011 NFL season.[132] The NFL championship game is called the Super Bowl, and is among the biggest events in club sports worldwide.[133] It is played between the champions of the National Football Conference (NFC) and the American Football Conference (AFC), and its winner is awarded the Vince Lombardi Trophy.[134]
College football is the third-most popular sport in the United States, behind professional baseball and professional football.[135] The NCAA, the largest collegiate organization, is divided into three Divisions: Division I, Division II and Division III.[136] Division I football is further divided into two subdivisions: the Football Bowl Subdivision (FBS) and the Football Championship Subdivision (FCS).[137] The champions of each level of play are determined through NCAA-sanctioned playoff systems; while the champion of Division I-FBS was historically determined by various polls and ranking systems, the subdivision adopted a four-team playoff system in 2014.[138]
Several professional football leagues have been formed outside the auspices of the NFL.
The original XFL was created in 2001 by Vince McMahon and lasted for only one season. Despite television contracts with NBC and UPN, and high expectations, the XFL suffered from low quality of play and poor reception for its use of tawdry professional wrestling gimmicks, which caused initially high ratings and attendance to collapse.[144] The XFL was rebooted in 2020.[145] However, after only five weeks of play, the league's operations slowly came to a close due to the ongoing COVID-19 pandemic,[146] and filed for bankruptcy on April 13.[147] The United Football League (UFL) began in 2009 but folded after suspending its 2012 season amid declining interest and lack of major television coverage.[148] The Alliance of American Football lasted less than one season, unable to keep investors.[149]
American football leagues exist throughout the world, but the game has yet to achieve the international success and popularity of baseball and basketball.[150] It is not an Olympic sport, but it was a demonstration sport at the 1932 Summer Olympics.[1] At the international level, Canada, Mexico, and Japan are considered to be second-tier, while Austria, Germany, and France would rank among a third tier. These countries rank far below the United States, which is dominant at the international level.[151]
NFL Europa, the developmental league of the NFL, operated from 1991 to 1992 and then from 1995 to 2007. At the time of its closure, NFL Europa had five teams based in Germany and one in the Netherlands.[152] In Germany, the German Football League (GFL) has 16 teams and has operated for over 40 seasons, with the league's championship game, the German Bowl, closing out each season. The league operates in a promotion and relegation structure with German Football League 2 (GFL2), which also has 16 teams.[153] The BIG6 European Football League functions as a continental championship for Europe. The competition is contested between the top six European teams.[153]
The United Kingdom also operated several teams within NFL Europe during the League's tenure.[154] The resulting rise in popularity of the sport brought the NFL back to the country in 2007 where they now hold the NFL International Series in London, currently consisting of four regular season games.[155][156] The continuing interest and growth in both the sport and the series has led to the possible formation of a potential NFL franchise in London[157][158][159]
An American football league system already exists within the UK, the BAFANL, which has run under various guises since 1983. It currently has 70 teams operating across the tiers of contact football in which teams aim to earn promotion to the Division above, with the Premier Division teams competing to win the Britbowl, the annual British Football Bowl game that has been played since 1985.[160][161][162] In 2007 the British Universities American Football League was formed. From 2008, the BUAFL was officially associated with the National Football League (NFL), through its partner organisation NFL UK.[163] In 2012, BUAFL's league and teams were absorbed into BUCS after American football became an official BUCS sport.[164] Over the period 2007 to 2014, the BUAFL grew from 42 teams and 2,460 participants to 75 teams and over 4,100 people involved.[165]
American football federations are present in Africa, the Americas, Asia, Europe and Oceania; a total of 64 national football federations exist as of July 2012.[151] The International Federation of American Football (IFAF), an international governing body composed of continental federations, runs tournaments such as the IFAF World Championship, the IFAF Women's World Championship, the IFAF U-19 World Championship and the Flag Football World Championship. The IFAF also organizes the annual International Bowl game.[166] The IFAF has received provisional recognition from the International Olympic Committee (IOC).[167] Several major obstacles hinder the IFAF goal of achieving status as an Olympic sport. These include the predominant participation of men in international play and the short three-week Olympic schedule. Large team sizes are an additional difficulty, due to the Olympics' set limit of 10,500 athletes and coaches. American football also has an issue with a lack of global visibility. Nigel Melville, the CEO of USA Rugby, noted that "American football is recognized globally as a sport, but it's not played globally." To solve these concerns, major effort has been put into promoting flag football, a modified version of American football, at the international level.[151]
The safety of the sport has also sparked national controversy in American popular culture. The 2015 film Concussion aimed to shed light on the sport's safety, specifically in the NFL by having Will Smith portray Dr. Bennet Omalu, a neuropathologist who was the first to discover and publish findings of chronic traumatic encephalopathy or CTE.
Japan was introduced to the sport in 1934 by Paul Rusch, a teacher and Christian missionary who helped to establish football teams at three universities in Tokyo. Play was halted during World War II, but the sport began growing in popularity again after the war. As of 2010[update], there are more than 400 high school football teams in Japan, with over 15,000 participants, and over 100 teams play in the Kantoh Collegiate Football Association (KCFA).[187] The college champion plays the champion of the X-League (a semi-professional league in which teams are financed by corporations) in the Rice Bowl to determine Japan's national champion.[188]
In Brazil, football is a growing sport. It was generally unknown there until the 1980s when a small group of players began playing on Copacabana Beach in Rio de Janeiro. The sport grew gradually with 700 amateur players registering within 20 years. Games were played on the beach with modified rules and without the traditional football equipment due to its lack of availability in Brazil. Eventually, a tournament, the Carioca championship, was founded, with the championship Carioca Bowl played to determine a league champion. The country saw its first full-pad game of football in October 2008.[192] According to The Rio Times, the sport is one of the fastest-growing sports in Brazil and is almost as commonly played as soccer on the beaches of Copacabana and Botafogo.[193]
Indoor football leagues constitute what The New York Times writer Mike Tanier described as the "most minor of minor leagues." Leagues are unstable, with franchises regularly moving from one league to another or merging with other teams, and teams or entire leagues dissolving completely; games are only attended by a small number of fans, and most players are semi-professional athletes. The Indoor Football League is an example of a prominent indoor league.[202] The Arena Football League, which was founded in 1987 and ceased operations in 2019, was one of the longest-lived indoor football leagues.[203] In 2004, the league was called "America's fifth major sport" by ESPN The Magazine.[204]
There are several non-contact variants of football like flag football.[205] In flag football the ballcarrier is not tackled; instead, defenders aim to pull a flag tied around the ballcarrier's waist.[206] Another variant, touch football, simply requires the ballcarrier to be touched to be considered downed. Depending on the rules used, a game of touch football may require the ballcarrier be touched with either one or two hands to be considered downed.[207]
